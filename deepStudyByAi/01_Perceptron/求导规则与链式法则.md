# Derivative Rules and Chain Rule: The Mathematical Foundation of Neural Networks

求导规则与链式法则：神经网络的数学基础

## 1. Why Do We Need Derivatives in Neural Networks?

为什么神经网络需要求导？

**Analogy: Derivatives are like a GPS for optimization**

类比：导数就像优化的GPS

Imagine you're driving in a mountainous area and want to find the lowest point (valley). Without GPS, you'd be lost. The derivative is like a GPS that tells you:
- Which direction to go (gradient direction)
- How steep the current slope is (gradient magnitude)
- Whether you're going uphill or downhill (positive or negative gradient)

想象你在山区开车，想找到最低点（山谷）。没有GPS你会迷路。导数就像GPS，告诉你：
- 该往哪个方向走（梯度方向）
- 当前坡度有多陡（梯度大小）
- 你是在上坡还是下坡（梯度正负）

In neural networks, we use derivatives to:
1. **Find the minimum of loss functions** (gradient descent)
2. **Update model parameters** (backpropagation)
3. **Measure sensitivity** (how much output changes with input changes)

在神经网络中，我们用导数来：
1. **寻找损失函数的最小值**（梯度下降）
2. **更新模型参数**（反向传播）
3. **测量敏感性**（输出对输入变化的响应程度）

## 2. Basic Derivative Rules

基本求导规则

### 2.1 Power Rule

幂函数求导法则

**Rule:** $\frac{d}{dx}(x^n) = nx^{n-1}$

规则：$\frac{d}{dx}(x^n) = nx^{n-1}$

**Examples:**

例子：

1. $\frac{d}{dx}(x^2) = 2x$
2. $\frac{d}{dx}(x^3) = 3x^2$
3. $\frac{d}{dx}(x^{-1}) = -x^{-2} = -\frac{1}{x^2}$
4. $\frac{d}{dx}(\sqrt{x}) = \frac{d}{dx}(x^{1/2}) = \frac{1}{2}x^{-1/2} = \frac{1}{2\sqrt{x}}$

**Numerical Verification:**

数值验证：

For $f(x) = x^2$ at $x = 3$:
- Derivative: $f'(3) = 2 \times 3 = 6$
- Numerical approximation: $\frac{f(3.001) - f(3)}{0.001} = \frac{9.006001 - 9}{0.001} = 6.001 ≈ 6$ ✓

对于 $f(x) = x^2$ 在 $x = 3$ 处：
- 导数：$f'(3) = 2 \times 3 = 6$
- 数值近似：$\frac{f(3.001) - f(3)}{0.001} = \frac{9.006001 - 9}{0.001} = 6.001 ≈ 6$ ✓

### 2.2 Exponential and Logarithmic Functions

指数函数和对数函数

**Exponential Functions:**

指数函数：

1. $\frac{d}{dx}(e^x) = e^x$
2. $\frac{d}{dx}(a^x) = a^x \ln(a)$

**Logarithmic Functions:**

对数函数：

1. $\frac{d}{dx}(\ln(x)) = \frac{1}{x}$
2. $\frac{d}{dx}(\log_a(x)) = \frac{1}{x \ln(a)}$

**Why $e$ is special:**

为什么 $e$ 很特殊：

The natural exponential function $e^x$ is the only function that is its own derivative! This makes it extremely useful in calculus and neural networks.

自然指数函数 $e^x$ 是唯一一个导数等于自身的函数！这使得它在微积分和神经网络中极其有用。

**Practical Example:**

实际例子：

In neural networks, we often use $e^x$ in the Softmax function:
$$\text{Softmax}(x_i) = \frac{e^{x_i}}{\sum_{j=1}^n e^{x_j}}$$

在神经网络中，我们经常在Softmax函数中使用 $e^x$：

### 2.3 Trigonometric Functions

三角函数

**Basic Trigonometric Derivatives:**

基本三角函数导数：

1. $\frac{d}{dx}(\sin(x)) = \cos(x)$
2. $\frac{d}{dx}(\cos(x)) = -\sin(x)$
3. $\frac{d}{dx}(\tan(x)) = \sec^2(x) = \frac{1}{\cos^2(x)}$

**Memory Trick:**

记忆技巧：

Think of sine and cosine as a "circular dance":
- Sine → Cosine (positive)
- Cosine → -Sine (negative)

把正弦和余弦想象成"圆形舞蹈"：
- 正弦 → 余弦（正号）
- 余弦 → -正弦（负号）

## 3. Combination Rules

组合规则

### 3.1 Sum and Difference Rule

和差法则

**Rule:** $\frac{d}{dx}[f(x) + g(x)] = f'(x) + g'(x)$

规则：$\frac{d}{dx}[f(x) + g(x)] = f'(x) + g'(x)$

**Example:**

例子：

$$\frac{d}{dx}(x^2 + 3x + 5) = \frac{d}{dx}(x^2) + \frac{d}{dx}(3x) + \frac{d}{dx}(5) = 2x + 3 + 0 = 2x + 3$$

### 3.2 Product Rule

乘积法则

**Rule:** $\frac{d}{dx}[f(x) \cdot g(x)] = f'(x) \cdot g(x) + f(x) \cdot g'(x)$

规则：$\frac{d}{dx}[f(x) \cdot g(x)] = f'(x) \cdot g(x) + f(x) \cdot g'(x)$

**Memory Trick:** "First times derivative of second, plus second times derivative of first"

记忆技巧："第一个乘以第二个的导数，加上第二个乘以第一个的导数"

**Example:**

例子：

Let $f(x) = x^2$ and $g(x) = e^x$, find $\frac{d}{dx}(x^2 e^x)$:

设 $f(x) = x^2$ 和 $g(x) = e^x$，求 $\frac{d}{dx}(x^2 e^x)$：

$$\frac{d}{dx}(x^2 e^x) = \frac{d}{dx}(x^2) \cdot e^x + x^2 \cdot \frac{d}{dx}(e^x) = 2x \cdot e^x + x^2 \cdot e^x = e^x(2x + x^2)$$

### 3.3 Quotient Rule

商法则

**Rule:** $\frac{d}{dx}\left[\frac{f(x)}{g(x)}\right] = \frac{f'(x) \cdot g(x) - f(x) \cdot g'(x)}{[g(x)]^2}$

规则：$\frac{d}{dx}\left[\frac{f(x)}{g(x)}\right] = \frac{f'(x) \cdot g(x) - f(x) \cdot g'(x)}{[g(x)]^2}$

**Memory Trick:** "Low D-High minus High D-Low, over Low squared"

记忆技巧："分母乘分子的导数 减去 分子乘分母的导数，除以分母的平方"

**Example:**

例子：

Find $\frac{d}{dx}\left(\frac{x^2}{x+1}\right)$:

求 $\frac{d}{dx}\left(\frac{x^2}{x+1}\right)$：

$$\frac{d}{dx}\left(\frac{x^2}{x+1}\right) = \frac{2x(x+1) - x^2(1)}{(x+1)^2} = \frac{2x^2 + 2x - x^2}{(x+1)^2} = \frac{x^2 + 2x}{(x+1)^2}$$

## 4. Chain Rule: The Heart of Backpropagation

链式法则：反向传播的核心

### 4.1 Basic Chain Rule

基本链式法则

**Rule:** If $y = f(u)$ and $u = g(x)$, then $\frac{dy}{dx} = \frac{dy}{du} \cdot \frac{du}{dx}$

规则：如果 $y = f(u)$ 且 $u = g(x)$，那么 $\frac{dy}{dx} = \frac{dy}{du} \cdot \frac{du}{dx}$

**Intuitive Understanding:**

直观理解：

Think of the chain rule as a "chain of responsibility":
- How does $y$ change with respect to $u$? → $\frac{dy}{du}$
- How does $u$ change with respect to $x$? → $\frac{du}{dx}$
- How does $y$ change with respect to $x$? → Multiply them together!

把链式法则想象成"责任链"：
- $y$ 相对于 $u$ 如何变化？→ $\frac{dy}{du}$
- $u$ 相对于 $x$ 如何变化？→ $\frac{du}{dx}$
- $y$ 相对于 $x$ 如何变化？→ 把它们乘起来！

### 4.2 Simple Chain Rule Examples

简单链式法则例子

**Example 1:** $y = (x^2 + 1)^3$

例子1：$y = (x^2 + 1)^3$

Let $u = x^2 + 1$, then $y = u^3$

设 $u = x^2 + 1$，那么 $y = u^3$

$$\frac{dy}{du} = 3u^2 = 3(x^2 + 1)^2$$
$$\frac{du}{dx} = 2x$$
$$\frac{dy}{dx} = \frac{dy}{du} \cdot \frac{du}{dx} = 3(x^2 + 1)^2 \cdot 2x = 6x(x^2 + 1)^2$$

**Example 2:** $y = e^{x^2}$

例子2：$y = e^{x^2}$

Let $u = x^2$, then $y = e^u$

设 $u = x^2$，那么 $y = e^u$

$$\frac{dy}{du} = e^u = e^{x^2}$$
$$\frac{du}{dx} = 2x$$
$$\frac{dy}{dx} = e^{x^2} \cdot 2x = 2xe^{x^2}$$

### 4.3 Multivariable Chain Rule

多元链式法则

**Rule:** If $z = f(x, y)$ where $x = g(t)$ and $y = h(t)$, then:

规则：如果 $z = f(x, y)$ 其中 $x = g(t)$ 和 $y = h(t)$，那么：

$$\frac{dz}{dt} = \frac{\partial z}{\partial x} \frac{dx}{dt} + \frac{\partial z}{\partial y} \frac{dy}{dt}$$

**Example:** Neural Network Forward Pass

例子：神经网络前向传播

Consider a simple neural network:
$$z = w_1 x_1 + w_2 x_2 + b$$
$$a = \sigma(z)$$
$$L = (a - t)^2$$

考虑一个简单的神经网络：

To find $\frac{\partial L}{\partial w_1}$:

要求 $\frac{\partial L}{\partial w_1}$：

$$\frac{\partial L}{\partial w_1} = \frac{\partial L}{\partial a} \frac{\partial a}{\partial z} \frac{\partial z}{\partial w_1}$$

Where:
- $\frac{\partial L}{\partial a} = 2(a - t)$
- $\frac{\partial a}{\partial z} = \sigma'(z)$
- $\frac{\partial z}{\partial w_1} = x_1$

其中：
- $\frac{\partial L}{\partial a} = 2(a - t)$
- $\frac{\partial a}{\partial z} = \sigma'(z)$
- $\frac{\partial z}{\partial w_1} = x_1$

Therefore:

因此：

$$\frac{\partial L}{\partial w_1} = 2(a - t) \cdot \sigma'(z) \cdot x_1$$

## 5. Common Activation Functions and Their Derivatives

常见激活函数及其导数

### 5.1 Sigmoid Function

Sigmoid函数

**Function:** $\sigma(x) = \frac{1}{1 + e^{-x}}$

函数：$\sigma(x) = \frac{1}{1 + e^{-x}}$

**Derivative:** $\sigma'(x) = \sigma(x)(1 - \sigma(x))$

导数：$\sigma'(x) = \sigma(x)(1 - \sigma(x))$

**Detailed Derivation:**

详细推导：

Using the quotient rule:

使用商法则：

$$\sigma(x) = \frac{1}{1 + e^{-x}} = (1 + e^{-x})^{-1}$$

Using chain rule:

使用链式法则：

$$\sigma'(x) = -1 \cdot (1 + e^{-x})^{-2} \cdot (-e^{-x}) = \frac{e^{-x}}{(1 + e^{-x})^2}$$

Simplifying:

化简：

$$\sigma'(x) = \frac{1}{1 + e^{-x}} \cdot \frac{e^{-x}}{1 + e^{-x}} = \sigma(x) \cdot \frac{e^{-x}}{1 + e^{-x}}$$

Since $\frac{e^{-x}}{1 + e^{-x}} = 1 - \frac{1}{1 + e^{-x}} = 1 - \sigma(x)$:

由于 $\frac{e^{-x}}{1 + e^{-x}} = 1 - \frac{1}{1 + e^{-x}} = 1 - \sigma(x)$：

$$\sigma'(x) = \sigma(x)(1 - \sigma(x))$$

### 5.2 Hyperbolic Tangent (tanh)

双曲正切函数

**Function:** $\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$

函数：$\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$

**Derivative:** $\tanh'(x) = 1 - \tanh^2(x)$

导数：$\tanh'(x) = 1 - \tanh^2(x)$

**Relationship to Sigmoid:**

与Sigmoid的关系：

$$\tanh(x) = 2\sigma(2x) - 1$$

### 5.3 ReLU (Rectified Linear Unit)

ReLU（修正线性单元）

**Function:** $\text{ReLU}(x) = \max(0, x) = \begin{cases} x & \text{if } x > 0 \\ 0 & \text{if } x \leq 0 \end{cases}$

函数：$\text{ReLU}(x) = \max(0, x) = \begin{cases} x & \text{if } x > 0 \\ 0 & \text{if } x \leq 0 \end{cases}$

**Derivative:** $\text{ReLU}'(x) = \begin{cases} 1 & \text{if } x > 0 \\ 0 & \text{if } x < 0 \\ \text{undefined} & \text{if } x = 0 \end{cases}$

导数：$\text{ReLU}'(x) = \begin{cases} 1 & \text{if } x > 0 \\ 0 & \text{if } x < 0 \\ \text{未定义} & \text{if } x = 0 \end{cases}$

**Practical Note:**

实际注意事项：

In practice, we often define $\text{ReLU}'(0) = 0$ for computational convenience.

在实践中，为了计算方便，我们通常定义 $\text{ReLU}'(0) = 0$。

### 5.4 Leaky ReLU

Leaky ReLU

**Function:** $\text{LeakyReLU}(x) = \begin{cases} x & \text{if } x > 0 \\ \alpha x & \text{if } x \leq 0 \end{cases}$

函数：$\text{LeakyReLU}(x) = \begin{cases} x & \text{if } x > 0 \\ \alpha x & \text{if } x \leq 0 \end{cases}$

where $\alpha$ is a small positive constant (typically 0.01).

其中 $\alpha$ 是一个小的正常数（通常为0.01）。

**Derivative:** $\text{LeakyReLU}'(x) = \begin{cases} 1 & \text{if } x > 0 \\ \alpha & \text{if } x \leq 0 \end{cases}$

导数：$\text{LeakyReLU}'(x) = \begin{cases} 1 & \text{if } x > 0 \\ \alpha & \text{if } x \leq 0 \end{cases}$

## 6. Practical Examples in Neural Networks

神经网络中的实际例子

### 6.1 Computing Gradients in a Simple Network

简单网络中的梯度计算

Consider a 2-layer neural network:

考虑一个2层神经网络：

$$z_1 = w_1 x + b_1$$
$$a_1 = \sigma(z_1)$$
$$z_2 = w_2 a_1 + b_2$$
$$a_2 = \sigma(z_2)$$
$$L = \frac{1}{2}(a_2 - t)^2$$

**Forward Pass Example:**

前向传播例子：

Given: $x = 1$, $w_1 = 0.5$, $b_1 = 0.2$, $w_2 = 0.8$, $b_2 = 0.1$, $t = 0.9$

给定：$x = 1$, $w_1 = 0.5$, $b_1 = 0.2$, $w_2 = 0.8$, $b_2 = 0.1$, $t = 0.9$

$$z_1 = 0.5 \times 1 + 0.2 = 0.7$$
$$a_1 = \sigma(0.7) = \frac{1}{1 + e^{-0.7}} = 0.668$$
$$z_2 = 0.8 \times 0.668 + 0.1 = 0.634$$
$$a_2 = \sigma(0.634) = 0.653$$
$$L = \frac{1}{2}(0.653 - 0.9)^2 = 0.030$$

**Backward Pass (Gradient Calculation):**

反向传播（梯度计算）：

$$\frac{\partial L}{\partial a_2} = a_2 - t = 0.653 - 0.9 = -0.247$$

$$\frac{\partial L}{\partial z_2} = \frac{\partial L}{\partial a_2} \frac{\partial a_2}{\partial z_2} = -0.247 \times a_2(1-a_2) = -0.247 \times 0.653 \times 0.347 = -0.056$$

$$\frac{\partial L}{\partial w_2} = \frac{\partial L}{\partial z_2} \frac{\partial z_2}{\partial w_2} = -0.056 \times a_1 = -0.056 \times 0.668 = -0.037$$

$$\frac{\partial L}{\partial b_2} = \frac{\partial L}{\partial z_2} = -0.056$$

$$\frac{\partial L}{\partial a_1} = \frac{\partial L}{\partial z_2} \frac{\partial z_2}{\partial a_1} = -0.056 \times w_2 = -0.056 \times 0.8 = -0.045$$

$$\frac{\partial L}{\partial z_1} = \frac{\partial L}{\partial a_1} \frac{\partial a_1}{\partial z_1} = -0.045 \times a_1(1-a_1) = -0.045 \times 0.668 \times 0.332 = -0.010$$

$$\frac{\partial L}{\partial w_1} = \frac{\partial L}{\partial z_1} \frac{\partial z_1}{\partial w_1} = -0.010 \times x = -0.010 \times 1 = -0.010$$

$$\frac{\partial L}{\partial b_1} = \frac{\partial L}{\partial z_1} = -0.010$$

### 6.2 Matrix Form of Chain Rule

链式法则的矩阵形式

For a layer with multiple neurons:

对于有多个神经元的层：

$$\mathbf{z} = \mathbf{W}\mathbf{x} + \mathbf{b}$$
$$\mathbf{a} = \sigma(\mathbf{z})$$

The gradients are:

梯度为：

$$\frac{\partial L}{\partial \mathbf{W}} = \frac{\partial L}{\partial \mathbf{z}} \mathbf{x}^T$$
$$\frac{\partial L}{\partial \mathbf{b}} = \frac{\partial L}{\partial \mathbf{z}}$$
$$\frac{\partial L}{\partial \mathbf{x}} = \mathbf{W}^T \frac{\partial L}{\partial \mathbf{z}}$$

where $\frac{\partial L}{\partial \mathbf{z}} = \frac{\partial L}{\partial \mathbf{a}} \odot \sigma'(\mathbf{z})$ and $\odot$ denotes element-wise multiplication.

其中 $\frac{\partial L}{\partial \mathbf{z}} = \frac{\partial L}{\partial \mathbf{a}} \odot \sigma'(\mathbf{z})$，$\odot$ 表示逐元素乘法。

## 7. Common Mistakes and Tips

常见错误和技巧

### 7.1 Common Mistakes

常见错误

1. **Forgetting the chain rule**: Always trace the path from output to the variable you're differentiating with respect to.

1. **忘记链式法则**：总是追踪从输出到你要求导的变量的路径。

2. **Sign errors**: Be careful with negative signs, especially in quotient rule and chain rule.

2. **符号错误**：小心负号，特别是在商法则和链式法则中。

3. **Dimension mismatches**: In matrix calculus, always check that dimensions match.

3. **维度不匹配**：在矩阵微积分中，总是检查维度是否匹配。

### 7.2 Helpful Tips

有用技巧

1. **Practice with simple examples**: Start with single-variable functions before moving to multivariable.

1. **用简单例子练习**：从单变量函数开始，然后再转向多变量。

2. **Use computational graphs**: Draw the computation graph to visualize the chain rule.

2. **使用计算图**：画出计算图来可视化链式法则。

3. **Verify numerically**: Always check your analytical derivatives with numerical approximations.

3. **数值验证**：总是用数值近似来检查你的解析导数。

**Numerical Verification Template:**

数值验证模板：

```python
def numerical_gradient(f, x, h=1e-7):
    return (f(x + h) - f(x - h)) / (2 * h)

def analytical_gradient(x):
    # Your analytical derivative here
    pass

# Compare
x = 2.0
num_grad = numerical_gradient(lambda x: x**2, x)
ana_grad = analytical_gradient(x)
print(f"Numerical: {num_grad}, Analytical: {ana_grad}")
```

## 8. Summary

总结

**Key Takeaways:**

关键要点：

1. **Derivatives measure rates of change** - essential for optimization in neural networks
2. **Chain rule is fundamental** - it enables backpropagation in deep networks
3. **Practice makes perfect** - the more you work with derivatives, the more intuitive they become
4. **Verify your work** - always double-check with numerical methods

1. **导数测量变化率** - 对神经网络优化至关重要
2. **链式法则是基础** - 它使深度网络中的反向传播成为可能
3. **熟能生巧** - 你越多地使用导数，它们就越直观
4. **验证你的工作** - 总是用数值方法双重检查

The mastery of derivatives and the chain rule is crucial for understanding how neural networks learn. These mathematical tools allow us to compute gradients efficiently, which is the foundation of all modern deep learning algorithms.

掌握导数和链式法则对于理解神经网络如何学习至关重要。这些数学工具使我们能够高效地计算梯度，这是所有现代深度学习算法的基础。 