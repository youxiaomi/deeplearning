# 第七章：注意力机制与Transformer - 测试题

## 选择题

### 1. 注意力机制的核心思想是什么？
A. 增加模型参数  
B. 动态关注输入的不同部分  
C. 减少计算复杂度  
D. 提高训练速度  

### 2. 在自注意力机制中，Query、Key、Value分别代表什么？
A. 三个不同的输入序列  
B. 三种不同的权重矩阵  
C. 同一输入的三种不同变换  
D. 三个不同的损失函数  

### 3. Transformer相比RNN的主要优势是什么？
A. 参数更少  
B. 可以并行计算  
C. 结构更简单  
D. 内存需求更小  

### 4. 多头注意力机制的作用是什么？
A. 增加计算复杂度  
B. 允许模型关注不同类型的信息  
C. 减少过拟合  
D. 提高模型深度  

### 5. 位置编码(Positional Encoding)在Transformer中的作用是什么？
A. 增加模型容量  
B. 提供序列位置信息  
C. 减少计算量  
D. 防止梯度消失  

## 填空题

### 1. 注意力权重的计算公式是：Attention(Q,K,V) = ______((QK^T)/√d_k)V

### 2. Transformer的编码器由______个相同层组成，每层包含______和______两个子层。

### 3. 在多头注意力中，如果有h个头，每个头的维度是d_k，那么总的模型维度是______。

### 4. Transformer使用______编码来为序列中的每个位置提供位置信息。

### 5. 自注意力机制的时间复杂度是______，其中n是序列长度。

## 简答题

### 1. 详细解释注意力机制中Query、Key、Value的概念和计算过程。

### 2. 描述Transformer编码器的结构，包括各个组件的作用。

### 3. 解释为什么Transformer需要位置编码，以及常用的位置编码方法。

### 4. 比较自注意力和传统注意力机制的区别和优势。

## 编程题

### 1. 实现基础的注意力机制

### 2. 实现多头注意力

### 3. 实现Transformer编码器层

### 4. 实现位置编码

---

## 答案解析

### 选择题答案
1. **B** - 动态关注输入的不同部分
2. **C** - 同一输入的三种不同变换
3. **B** - 可以并行计算
4. **B** - 允许模型关注不同类型的信息
5. **B** - 提供序列位置信息

### 填空题答案
1. **Softmax**
2. **6**，**多头注意力**，**前馈网络**
3. **h × d_k**
4. **位置**
5. **O(n²)**

### 简答题答案要点

#### 1. Query、Key、Value概念：

**Query (查询):**
- 代表当前要处理的元素
- 用于计算与其他元素的相关性
- 形象比喻：图书馆中的检索请求

**Key (键):**
- 代表序列中的每个元素
- 用于与Query计算相似度
- 形象比喻：图书的索引标签

**Value (值):**
- 代表实际要获取的信息
- 根据注意力权重进行加权求和
- 形象比喻：图书的实际内容

**计算过程：**
1. 计算Query和Key的相似度
2. 应用Softmax得到注意力权重
3. 用权重对Value进行加权求和

#### 2. Transformer编码器结构：

**多头注意力子层：**
- 允许模型并行关注不同位置
- 捕获不同类型的依赖关系
- 包含残差连接和层归一化

**前馈网络子层：**
- 两个线性变换和ReLU激活
- 为每个位置独立应用
- 增加模型的非线性表达能力

**残差连接：**
- 缓解深度网络的训练困难
- 公式：LayerNorm(x + Sublayer(x))

#### 3. 位置编码的必要性：

**问题：**
- 注意力机制本身没有位置概念
- 序列的顺序信息会丢失
- 需要显式提供位置信息

**解决方案：**
- 正弦和余弦位置编码
- 学习的位置嵌入
- 相对位置编码

**正弦位置编码：**
- PE(pos,2i) = sin(pos/10000^(2i/d_model))
- PE(pos,2i+1) = cos(pos/10000^(2i/d_model))

#### 4. 自注意力 vs 传统注意力：

**传统注意力：**
- Query来自解码器，Key和Value来自编码器
- 用于序列到序列任务
- 单向信息流动

**自注意力：**
- Query、Key、Value都来自同一序列
- 每个位置都可以关注序列中的所有位置
- 能够捕获长距离依赖关系
- 支持并行计算

**优势：**
- 计算效率高
- 能够建模复杂的依赖关系
- 可解释性强（注意力权重可视化） 