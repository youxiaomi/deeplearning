# Mathematics for Deep Learning
## 深度学习数学基础

Mathematics is the foundation of deep learning. Just like you need to understand basic arithmetic before learning advanced math, you need solid mathematical foundations to truly understand how neural networks work.
数学是深度学习的基础。就像你需要先理解基本算术才能学习高等数学一样，你需要扎实的数学基础才能真正理解神经网络的工作原理。

Think of mathematics as the "language" that computers use to learn. When a neural network learns to recognize cats in photos, it's actually doing millions of mathematical calculations behind the scenes.
把数学想象成计算机用来学习的"语言"。当神经网络学习识别照片中的猫时，它实际上在幕后进行着数百万次数学计算。

## 19.1 Geometry and Linear Algebraic Operations
## 几何与线性代数运算

### Vector Geometry 向量几何

A vector is like an arrow that has both direction and length. Imagine you're giving directions to a friend: "Walk 3 blocks north and 2 blocks east." That's essentially a vector - it tells you how far to go and in which direction.
向量就像一个既有方向又有长度的箭头。想象你在给朋友指路："向北走3个街区，然后向东走2个街区。"这本质上就是一个向量——它告诉你要走多远以及朝哪个方向走。

In mathematics, we write this as: v = [3, 2]
在数学中，我们写成：v = [3, 2]

```python
import numpy as np
import matplotlib.pyplot as plt

# 创建一个向量
vector = np.array([3, 2])
print(f"向量: {vector}")
print(f"向量长度: {np.linalg.norm(vector):.2f}")

# 可视化向量
plt.figure(figsize=(8, 6))
plt.arrow(0, 0, vector[0], vector[1], head_width=0.1, head_length=0.1, fc='blue', ec='blue')
plt.xlim(-1, 4)
plt.ylim(-1, 3)
plt.grid(True)
plt.xlabel('X')
plt.ylabel('Y')
plt.title('Vector Visualization / 向量可视化')
plt.show()
```

### Dot Product 点积

The dot product is like measuring how much two vectors "agree" with each other. If two people are walking in the same direction, their dot product is positive. If they're walking in opposite directions, it's negative.
点积就像测量两个向量彼此"一致"的程度。如果两个人朝同一个方向走，他们的点积是正数。如果他们朝相反方向走，点积是负数。

```python
# 两个向量的点积
v1 = np.array([3, 2])
v2 = np.array([1, 4])

dot_product = np.dot(v1, v2)
print(f"向量 {v1} 和 {v2} 的点积: {dot_product}")

# 点积公式: v1 · v2 = |v1| * |v2| * cos(θ)
# 其中 θ 是两个向量之间的夹角
angle = np.arccos(dot_product / (np.linalg.norm(v1) * np.linalg.norm(v2)))
print(f"两向量夹角: {np.degrees(angle):.2f} 度")
```

### Matrix Operations 矩阵运算

A matrix is like a table of numbers. Think of it as a spreadsheet where each row and column has meaning. In deep learning, matrices are used to represent data and transformations.
矩阵就像一个数字表格。把它想象成一个电子表格，其中每一行和每一列都有意义。在深度学习中，矩阵用于表示数据和变换。

```python
# 创建矩阵
A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])

print("矩阵 A:")
print(A)
print("\n矩阵 B:")
print(B)

# 矩阵乘法
C = np.dot(A, B)
print("\n矩阵乘法 A × B:")
print(C)

# 矩阵转置
A_T = A.T
print("\n矩阵 A 的转置:")
print(A_T)
```

## 19.2 Eigendecompositions 特征分解

Eigenvalues and eigenvectors help us understand the "essence" of a matrix. Think of a matrix as a transformation that stretches and rotates space. Eigenvectors are the special directions that don't change direction when the transformation is applied - they only get stretched or shrunk.
特征值和特征向量帮助我们理解矩阵的"本质"。把矩阵想象成一个拉伸和旋转空间的变换。特征向量是在应用变换时不改变方向的特殊方向——它们只会被拉伸或收缩。

```python
# 特征分解示例
A = np.array([[4, 1], [2, 3]])

# 计算特征值和特征向量
eigenvalues, eigenvectors = np.linalg.eig(A)

print("矩阵 A:")
print(A)
print(f"\n特征值: {eigenvalues}")
print(f"特征向量:\n{eigenvectors}")

# 验证特征值和特征向量的关系: A * v = λ * v
for i in range(len(eigenvalues)):
    v = eigenvectors[:, i]
    λ = eigenvalues[i]
    Av = np.dot(A, v)
    λv = λ * v
    print(f"\n特征向量 {i+1}: {v}")
    print(f"A * v = {Av}")
    print(f"λ * v = {λv}")
    print(f"差值: {np.abs(Av - λv).max():.10f}")
```

## 19.3 Single Variable Calculus 单变量微积分

Calculus is about understanding change. Imagine you're driving a car - your speedometer shows your speed (the derivative of your position), and your odometer shows the total distance (the integral of your speed).
微积分是关于理解变化的。想象你在开车——你的速度表显示你的速度（位置的导数），而你的里程表显示总距离（速度的积分）。

### Derivatives 导数

The derivative tells us how fast something is changing. In deep learning, we use derivatives to find the best way to adjust our model's parameters.
导数告诉我们某物变化的速度。在深度学习中，我们使用导数来找到调整模型参数的最佳方法。

```python
import sympy as sp
import numpy as np

# 符号计算
x = sp.Symbol('x')
f = x**2 + 3*x + 2

print(f"函数: f(x) = {f}")

# 计算导数
f_prime = sp.diff(f, x)
print(f"导数: f'(x) = {f_prime}")

# 数值计算和可视化
x_vals = np.linspace(-5, 2, 100)
y_vals = x_vals**2 + 3*x_vals + 2
dy_vals = 2*x_vals + 3

plt.figure(figsize=(10, 6))
plt.subplot(1, 2, 1)
plt.plot(x_vals, y_vals, 'b-', label='f(x) = x² + 3x + 2')
plt.grid(True)
plt.xlabel('x')
plt.ylabel('f(x)')
plt.title('原函数')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(x_vals, dy_vals, 'r-', label="f'(x) = 2x + 3")
plt.grid(True)
plt.xlabel('x')
plt.ylabel("f'(x)")
plt.title('导数')
plt.legend()

plt.tight_layout()
plt.show()
```

### Common Derivative Rules 常见导数规则

```python
# 常见函数的导数
x = sp.Symbol('x')

functions = [
    x**2,           # 幂函数
    sp.sin(x),      # 正弦函数
    sp.exp(x),      # 指数函数
    sp.log(x),      # 对数函数
    1/(1 + sp.exp(-x))  # Sigmoid函数
]

print("常见函数及其导数:")
for func in functions:
    derivative = sp.diff(func, x)
    print(f"f(x) = {func}")
    print(f"f'(x) = {derivative}")
    print()
```

## 19.4 Multivariable Calculus 多变量微积分

When we have functions with multiple variables (like height and weight predicting health), we need multivariable calculus. This is crucial for deep learning because neural networks have millions of parameters.
当我们有多个变量的函数时（比如身高和体重预测健康状况），我们需要多变量微积分。这对深度学习至关重要，因为神经网络有数百万个参数。

### Partial Derivatives 偏导数

A partial derivative tells us how a function changes when we change just one variable while keeping others constant.
偏导数告诉我们当我们只改变一个变量而保持其他变量不变时，函数如何变化。

```python
# 多变量函数示例
x, y = sp.symbols('x y')
f = x**2 + 2*x*y + y**2

print(f"函数: f(x,y) = {f}")

# 偏导数
df_dx = sp.diff(f, x)
df_dy = sp.diff(f, y)

print(f"对x的偏导数: ∂f/∂x = {df_dx}")
print(f"对y的偏导数: ∂f/∂y = {df_dy}")

# 梯度向量
gradient = [df_dx, df_dy]
print(f"梯度: ∇f = {gradient}")
```

### Chain Rule 链式法则

The chain rule is the backbone of backpropagation in neural networks. It tells us how to find derivatives of composite functions.
链式法则是神经网络中反向传播的支柱。它告诉我们如何找到复合函数的导数。

```python
# 链式法则示例
# 假设 z = f(y), y = g(x)，那么 dz/dx = (dz/dy) * (dy/dx)

x = sp.Symbol('x')
y = x**2 + 1        # y = g(x)
z = sp.sin(y)       # z = f(y) = sin(y)

# 直接求导
dz_dx_direct = sp.diff(z, x)

# 使用链式法则
dy_dx = sp.diff(y, x)
dz_dy = sp.diff(z, y)
dz_dx_chain = dz_dy * dy_dx

print(f"y = {y}")
print(f"z = {z}")
print(f"直接求导: dz/dx = {dz_dx_direct}")
print(f"链式法则: dz/dx = (dz/dy) * (dy/dx) = ({dz_dy}) * ({dy_dx}) = {dz_dx_chain}")
print(f"结果相同: {sp.simplify(dz_dx_direct - dz_dx_chain) == 0}")
```

### Gradient Descent 梯度下降

Gradient descent is like finding the bottom of a valley while blindfolded. You feel the slope around you and take steps in the steepest downward direction.
梯度下降就像蒙着眼睛寻找山谷的底部。你感受周围的坡度，然后朝最陡的下坡方向迈步。

```python
def gradient_descent_demo():
    # 定义一个简单的二次函数 f(x) = (x-3)² + 1
    def f(x):
        return (x - 3)**2 + 1
    
    def df_dx(x):
        return 2 * (x - 3)
    
    # 梯度下降参数
    x = 0.0  # 起始点
    learning_rate = 0.1
    iterations = 20
    
    # 记录优化过程
    x_history = [x]
    f_history = [f(x)]
    
    print("梯度下降优化过程:")
    print(f"步骤 0: x = {x:.3f}, f(x) = {f(x):.3f}")
    
    for i in range(iterations):
        gradient = df_dx(x)
        x = x - learning_rate * gradient
        
        x_history.append(x)
        f_history.append(f(x))
        
        if i < 5 or i % 5 == 4:  # 显示前5步和每5步
            print(f"步骤 {i+1}: x = {x:.3f}, f(x) = {f(x):.3f}, 梯度 = {gradient:.3f}")
    
    # 可视化
    x_plot = np.linspace(-1, 6, 100)
    y_plot = (x_plot - 3)**2 + 1
    
    plt.figure(figsize=(10, 6))
    plt.plot(x_plot, y_plot, 'b-', label='f(x) = (x-3)² + 1')
    plt.plot(x_history, f_history, 'ro-', label='梯度下降路径')
    plt.xlabel('x')
    plt.ylabel('f(x)')
    plt.title('梯度下降优化可视化')
    plt.legend()
    plt.grid(True)
    plt.show()
    
    return x_history, f_history

# 运行演示
x_hist, f_hist = gradient_descent_demo()
```

## 19.5 Integral Calculus 积分学

Integration is the reverse of differentiation. If derivatives tell us about rates of change, integrals tell us about accumulation. Think of it like this: if you know your speed at every moment during a trip, integration tells you the total distance traveled.
积分是微分的逆过程。如果导数告诉我们变化率，那么积分告诉我们累积量。这样想：如果你知道旅行中每一刻的速度，积分就能告诉你总的行程距离。

```python
# 积分示例
x = sp.Symbol('x')

# 定义函数
f = x**2

print(f"函数: f(x) = {f}")

# 不定积分
indefinite_integral = sp.integrate(f, x)
print(f"不定积分: ∫f(x)dx = {indefinite_integral}")

# 定积分 (从0到2)
definite_integral = sp.integrate(f, (x, 0, 2))
print(f"定积分: ∫₀²f(x)dx = {definite_integral}")

# 数值验证
x_vals = np.linspace(0, 2, 1000)
y_vals = x_vals**2
area_numerical = np.trapz(y_vals, x_vals)
print(f"数值积分结果: {area_numerical:.6f}")
```

### Applications in Probability 概率中的应用

Integrals are crucial for working with continuous probability distributions.
积分对于处理连续概率分布至关重要。

```python
# 概率密度函数示例：正态分布
from scipy import stats

# 标准正态分布 N(0,1)
mu, sigma = 0, 1
x = np.linspace(-4, 4, 100)
pdf = stats.norm.pdf(x, mu, sigma)

# 计算概率 P(-1 ≤ X ≤ 1)
prob = stats.norm.cdf(1) - stats.norm.cdf(-1)

plt.figure(figsize=(10, 6))
plt.plot(x, pdf, 'b-', label='标准正态分布 PDF')
plt.fill_between(x[(x >= -1) & (x <= 1)], 
                 pdf[(x >= -1) & (x <= 1)], 
                 alpha=0.3, label=f'P(-1 ≤ X ≤ 1) = {prob:.3f}')
plt.xlabel('x')
plt.ylabel('概率密度')
plt.title('正态分布的积分（概率计算）')
plt.legend()
plt.grid(True)
plt.show()

print(f"P(-1 ≤ X ≤ 1) = {prob:.4f}")
```

## 19.6 Random Variables 随机变量

A random variable is like a function that assigns numbers to the outcomes of a random experiment. For example, when you roll a die, the random variable X could represent the number that comes up.
随机变量就像一个函数，它为随机实验的结果分配数字。例如，当你掷骰子时，随机变量X可以表示出现的数字。

### Discrete Random Variables 离散随机变量

```python
import numpy as np
from collections import Counter

# 掷骰子示例
def roll_die():
    return np.random.randint(1, 7)

# 模拟1000次掷骰子
rolls = [roll_die() for _ in range(1000)]
counts = Counter(rolls)

# 计算概率
probabilities = {k: v/1000 for k, v in counts.items()}

print("掷骰子结果统计:")
for outcome in sorted(probabilities.keys()):
    print(f"点数 {outcome}: 概率 = {probabilities[outcome]:.3f}")

# 理论期望值
theoretical_mean = sum(range(1, 7)) / 6
empirical_mean = np.mean(rolls)

print(f"\n理论期望值: {theoretical_mean:.3f}")
print(f"实验期望值: {empirical_mean:.3f}")
```

### Continuous Random Variables 连续随机变量

```python
# 连续随机变量：正态分布
mu, sigma = 100, 15  # IQ分数：均值100，标准差15

# 生成随机样本
samples = np.random.normal(mu, sigma, 1000)

plt.figure(figsize=(12, 4))

# 直方图
plt.subplot(1, 2, 1)
plt.hist(samples, bins=30, density=True, alpha=0.7, color='skyblue')
x = np.linspace(50, 150, 100)
pdf = stats.norm.pdf(x, mu, sigma)
plt.plot(x, pdf, 'r-', label='理论PDF')
plt.xlabel('IQ分数')
plt.ylabel('概率密度')
plt.title('IQ分数分布')
plt.legend()

# 累积分布函数
plt.subplot(1, 2, 2)
sorted_samples = np.sort(samples)
cdf_empirical = np.arange(1, len(sorted_samples) + 1) / len(sorted_samples)
plt.plot(sorted_samples, cdf_empirical, 'b-', label='经验CDF')

x = np.linspace(50, 150, 100)
cdf_theoretical = stats.norm.cdf(x, mu, sigma)
plt.plot(x, cdf_theoretical, 'r-', label='理论CDF')
plt.xlabel('IQ分数')
plt.ylabel('累积概率')
plt.title('累积分布函数')
plt.legend()

plt.tight_layout()
plt.show()

# 一些有趣的概率计算
prob_above_130 = 1 - stats.norm.cdf(130, mu, sigma)
prob_between_85_115 = stats.norm.cdf(115, mu, sigma) - stats.norm.cdf(85, mu, sigma)

print(f"IQ > 130 的概率: {prob_above_130:.4f} ({prob_above_130*100:.2f}%)")
print(f"85 < IQ < 115 的概率: {prob_between_85_115:.4f} ({prob_between_85_115*100:.2f}%)")
```

## 19.7 Maximum Likelihood 最大似然

Maximum likelihood is like being a detective. You observe some evidence (data) and try to figure out what parameters of your model would make this evidence most likely to occur.
最大似然就像当侦探。你观察一些证据（数据），然后试图找出模型的哪些参数会使这些证据最有可能发生。

### Simple Example: Coin Flip 简单例子：抛硬币

```python
# 抛硬币的最大似然估计
def coin_flip_mle():
    # 假设我们抛硬币10次，得到7次正面
    heads = 7
    total = 10
    
    # 测试不同的p值（硬币正面朝上的概率）
    p_values = np.linspace(0.01, 0.99, 100)
    likelihoods = []
    
    for p in p_values:
        # 二项分布的似然函数
        likelihood = (p ** heads) * ((1 - p) ** (total - heads))
        likelihoods.append(likelihood)
    
    # 找到最大似然估计
    max_idx = np.argmax(likelihoods)
    mle_p = p_values[max_idx]
    
    # 理论最大似然估计
    theoretical_mle = heads / total
    
    plt.figure(figsize=(10, 6))
    plt.plot(p_values, likelihoods, 'b-', linewidth=2)
    plt.axvline(mle_p, color='r', linestyle='--', label=f'数值MLE: p = {mle_p:.3f}')
    plt.axvline(theoretical_mle, color='g', linestyle='--', label=f'理论MLE: p = {theoretical_mle:.3f}')
    plt.xlabel('p (硬币正面朝上的概率)')
    plt.ylabel('似然函数值')
    plt.title('抛硬币的最大似然估计')
    plt.legend()
    plt.grid(True)
    plt.show()
    
    print(f"观察结果: {heads}次正面，{total-heads}次反面")
    print(f"最大似然估计: p = {theoretical_mle:.3f}")
    
    return theoretical_mle

mle_result = coin_flip_mle()
```

### Normal Distribution Parameter Estimation 正态分布参数估计

```python
# 正态分布的最大似然估计
def normal_mle_demo():
    # 生成一些样本数据
    true_mu, true_sigma = 5, 2
    data = np.random.normal(true_mu, true_sigma, 100)
    
    # 最大似然估计
    mle_mu = np.mean(data)
    mle_sigma = np.std(data, ddof=0)  # 使用MLE公式（分母为n）
    
    print(f"真实参数: μ = {true_mu}, σ = {true_sigma}")
    print(f"MLE估计: μ̂ = {mle_mu:.3f}, σ̂ = {mle_sigma:.3f}")
    
    # 可视化
    plt.figure(figsize=(12, 4))
    
    # 数据直方图和拟合的分布
    plt.subplot(1, 2, 1)
    plt.hist(data, bins=20, density=True, alpha=0.7, color='skyblue', label='数据')
    
    x = np.linspace(data.min(), data.max(), 100)
    true_pdf = stats.norm.pdf(x, true_mu, true_sigma)
    mle_pdf = stats.norm.pdf(x, mle_mu, mle_sigma)
    
    plt.plot(x, true_pdf, 'r-', label=f'真实分布 N({true_mu}, {true_sigma}²)')
    plt.plot(x, mle_pdf, 'g--', label=f'MLE拟合 N({mle_mu:.2f}, {mle_sigma:.2f}²)')
    plt.xlabel('值')
    plt.ylabel('概率密度')
    plt.title('正态分布MLE拟合')
    plt.legend()
    
    # 似然函数对μ的依赖
    plt.subplot(1, 2, 2)
    mu_range = np.linspace(3, 7, 100)
    log_likelihoods = []
    
    for mu in mu_range:
        # 对数似然函数
        log_likelihood = -0.5 * len(data) * np.log(2 * np.pi * mle_sigma**2) - \
                        0.5 * np.sum((data - mu)**2) / mle_sigma**2
        log_likelihoods.append(log_likelihood)
    
    plt.plot(mu_range, log_likelihoods, 'b-')
    plt.axvline(mle_mu, color='r', linestyle='--', label=f'MLE: μ = {mle_mu:.3f}')
    plt.xlabel('μ')
    plt.ylabel('对数似然')
    plt.title('对数似然函数')
    plt.legend()
    plt.grid(True)
    
    plt.tight_layout()
    plt.show()

normal_mle_demo()
```

## 19.8 Distributions 分布

Probability distributions are like templates that describe how likely different outcomes are. They're the building blocks of machine learning models.
概率分布就像描述不同结果可能性的模板。它们是机器学习模型的构建块。

### Common Distributions 常见分布

```python
# 展示几种常见的概率分布
fig, axes = plt.subplots(2, 3, figsize=(15, 10))

# 1. 正态分布 (Normal/Gaussian)
x = np.linspace(-4, 4, 100)
for mu, sigma in [(0, 1), (0, 0.5), (1, 1)]:
    y = stats.norm.pdf(x, mu, sigma)
    axes[0, 0].plot(x, y, label=f'N({mu}, {sigma}²)')
axes[0, 0].set_title('正态分布 (Normal)')
axes[0, 0].legend()
axes[0, 0].grid(True)

# 2. 二项分布 (Binomial)
n_vals = [10, 20, 10]
p_vals = [0.3, 0.3, 0.7]
for n, p in zip(n_vals, p_vals):
    x = np.arange(0, n+1)
    y = stats.binom.pmf(x, n, p)
    axes[0, 1].plot(x, y, 'o-', label=f'B({n}, {p})')
axes[0, 1].set_title('二项分布 (Binomial)')
axes[0, 1].legend()
axes[0, 1].grid(True)

# 3. 泊松分布 (Poisson)
x = np.arange(0, 15)
for lam in [1, 4, 7]:
    y = stats.poisson.pmf(x, lam)
    axes[0, 2].plot(x, y, 'o-', label=f'Poisson({lam})')
axes[0, 2].set_title('泊松分布 (Poisson)')
axes[0, 2].legend()
axes[0, 2].grid(True)

# 4. 指数分布 (Exponential)
x = np.linspace(0, 5, 100)
for lam in [0.5, 1, 2]:
    y = stats.expon.pdf(x, scale=1/lam)
    axes[1, 0].plot(x, y, label=f'Exp({lam})')
axes[1, 0].set_title('指数分布 (Exponential)')
axes[1, 0].legend()
axes[1, 0].grid(True)

# 5. Beta分布
x = np.linspace(0, 1, 100)
for a, b in [(2, 2), (1, 3), (3, 1)]:
    y = stats.beta.pdf(x, a, b)
    axes[1, 1].plot(x, y, label=f'Beta({a}, {b})')
axes[1, 1].set_title('Beta分布')
axes[1, 1].legend()
axes[1, 1].grid(True)

# 6. 伽马分布 (Gamma)
x = np.linspace(0, 10, 100)
for a, scale in [(2, 1), (5, 1), (2, 2)]:
    y = stats.gamma.pdf(x, a, scale=scale)
    axes[1, 2].plot(x, y, label=f'Gamma({a}, {scale})')
axes[1, 2].set_title('伽马分布 (Gamma)')
axes[1, 2].legend()
axes[1, 2].grid(True)

plt.tight_layout()
plt.show()
```

### Distribution Properties 分布性质

```python
# 计算各种分布的统计性质
distributions = {
    '正态分布 N(0,1)': stats.norm(0, 1),
    '二项分布 B(10,0.3)': stats.binom(10, 0.3),
    '泊松分布 Poisson(3)': stats.poisson(3),
    '指数分布 Exp(1)': stats.expon(scale=1),
    'Beta分布 Beta(2,2)': stats.beta(2, 2)
}

print("分布性质比较:")
print("=" * 60)
print(f"{'分布名称':<20} {'均值':<8} {'方差':<8} {'偏度':<8} {'峰度':<8}")
print("-" * 60)

for name, dist in distributions.items():
    mean = dist.mean()
    var = dist.var()
    skew = dist.stats(moments='s')
    kurt = dist.stats(moments='k')
    
    print(f"{name:<20} {mean:<8.3f} {var:<8.3f} {skew:<8.3f} {kurt:<8.3f}")
```

## 19.9 Naive Bayes 朴素贝叶斯

Naive Bayes is like making decisions based on past experience. If you see dark clouds, you guess it might rain because you've observed this pattern before. The "naive" part assumes that different pieces of evidence (like clouds, humidity, wind) are independent.
朴素贝叶斯就像根据过去的经验做决定。如果你看到乌云，你会猜测可能要下雨，因为你以前观察过这种模式。"朴素"的部分假设不同的证据（如云、湿度、风）是独立的。

### Bayes' Theorem 贝叶斯定理

```python
# 贝叶斯定理示例：医疗诊断
def medical_diagnosis_example():
    # 假设有一种疾病，患病率为0.1%
    P_disease = 0.001
    P_no_disease = 1 - P_disease
    
    # 测试的准确性：
    # 如果有病，测试阳性的概率是99%
    P_positive_given_disease = 0.99
    # 如果没病，测试阳性的概率是2%（假阳性）
    P_positive_given_no_disease = 0.02
    
    # 计算测试阳性的总概率
    P_positive = (P_positive_given_disease * P_disease + 
                  P_positive_given_no_disease * P_no_disease)
    
    # 使用贝叶斯定理：如果测试阳性，真的有病的概率是多少？
    P_disease_given_positive = (P_positive_given_disease * P_disease) / P_positive
    
    print("医疗诊断示例:")
    print(f"疾病患病率: {P_disease:.1%}")
    print(f"测试敏感性: {P_positive_given_disease:.1%}")
    print(f"测试特异性: {1-P_positive_given_no_disease:.1%}")
    print(f"测试阳性概率: {P_positive:.3%}")
    print(f"测试阳性时真的有病的概率: {P_disease_given_positive:.2%}")
    
    return P_disease_given_positive

prob = medical_diagnosis_example()
```

### Text Classification Example 文本分类示例

```python
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split

# 简单的垃圾邮件分类示例
def spam_classification_demo():
    # 模拟邮件数据
    emails = [
        "Buy now! Limited time offer! Click here!",
        "Meeting tomorrow at 3pm",
        "Free money! Act now!",
        "Can you send me the report?",
        "Congratulations! You won $1000!",
        "Please review the attached document",
        "Urgent! Your account will be closed!",
        "How was your weekend?",
        "Make money fast! No experience needed!",
        "Let's have lunch next week"
    ]
    
    labels = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]  # 1: 垃圾邮件, 0: 正常邮件
    
    # 特征提取
    vectorizer = CountVectorizer()
    X = vectorizer.fit_transform(emails)
    
    # 训练朴素贝叶斯分类器
    nb_classifier = MultinomialNB()
    nb_classifier.fit(X, labels)
    
    # 测试新邮件
    test_emails = [
        "Free gift for you!",
        "Can we schedule a meeting?",
        "Win big prizes now!"
    ]
    
    test_X = vectorizer.transform(test_emails)
    predictions = nb_classifier.predict(test_X)
    probabilities = nb_classifier.predict_proba(test_X)
    
    print("垃圾邮件分类结果:")
    print("-" * 50)
    for i, email in enumerate(test_emails):
        pred = "垃圾邮件" if predictions[i] == 1 else "正常邮件"
        prob_spam = probabilities[i][1]
        print(f"邮件: '{email}'")
        print(f"分类: {pred} (垃圾邮件概率: {prob_spam:.3f})")
        print()

spam_classification_demo()
```

## 19.10 Statistics 统计学

Statistics helps us make sense of data and quantify uncertainty. It's like being a scientist who wants to draw conclusions from experiments while acknowledging that there's always some uncertainty.
统计学帮助我们理解数据和量化不确定性。这就像是一个科学家，想要从实验中得出结论，同时承认总是存在一些不确定性。

### Descriptive Statistics 描述性统计

```python
# 生成一些示例数据：学生考试成绩
np.random.seed(42)
scores = np.random.normal(75, 12, 100)  # 均值75，标准差12的正态分布

# 基本统计量
mean_score = np.mean(scores)
median_score = np.median(scores)
std_score = np.std(scores)
min_score = np.min(scores)
max_score = np.max(scores)

# 分位数
q25 = np.percentile(scores, 25)
q75 = np.percentile(scores, 75)
iqr = q75 - q25

print("学生考试成绩统计:")
print(f"样本大小: {len(scores)}")
print(f"平均分: {mean_score:.2f}")
print(f"中位数: {median_score:.2f}")
print(f"标准差: {std_score:.2f}")
print(f"最低分: {min_score:.2f}")
print(f"最高分: {max_score:.2f}")
print(f"第25百分位数: {q25:.2f}")
print(f"第75百分位数: {q75:.2f}")
print(f"四分位距: {iqr:.2f}")

# 可视化
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

# 直方图
axes[0].hist(scores, bins=20, density=True, alpha=0.7, color='skyblue')
axes[0].axvline(mean_score, color='red', linestyle='--', label=f'均值: {mean_score:.1f}')
axes[0].axvline(median_score, color='green', linestyle='--', label=f'中位数: {median_score:.1f}')
axes[0].set_xlabel('分数')
axes[0].set_ylabel('概率密度')
axes[0].set_title('分数分布')
axes[0].legend()

# 箱线图
axes[1].boxplot(scores, labels=['分数'])
axes[1].set_ylabel('分数')
axes[1].set_title('箱线图')

# Q-Q图 (检查正态性)
from scipy import stats
stats.probplot(scores, dist="norm", plot=axes[2])
axes[2].set_title('Q-Q图 (正态性检验)')

plt.tight_layout()
plt.show()
```

### Hypothesis Testing 假设检验

```python
# 假设检验示例：t检验
def t_test_example():
    # 假设我们想检验一个新的教学方法是否能提高学生成绩
    # 原假设 H0: 新方法没有效果 (μ = 75)
    # 备择假设 H1: 新方法有效果 (μ ≠ 75)
    
    # 使用新方法的学生成绩
    new_method_scores = np.random.normal(78, 12, 30)  # 实际均值稍高
    
    # 执行单样本t检验
    t_stat, p_value = stats.ttest_1samp(new_method_scores, 75)
    
    alpha = 0.05  # 显著性水平
    
    print("单样本t检验:")
    print(f"样本均值: {np.mean(new_method_scores):.2f}")
    print(f"原假设: μ = 75")
    print(f"备择假设: μ ≠ 75")
    print(f"t统计量: {t_stat:.3f}")
    print(f"p值: {p_value:.4f}")
    print(f"显著性水平: {alpha}")
    
    if p_value < alpha:
        print("拒绝原假设：新教学方法显著提高了成绩")
    else:
        print("接受原假设：没有足够证据表明新方法有效")
    
    # 可视化
    plt.figure(figsize=(10, 6))
    
    # 画出t分布
    df = len(new_method_scores) - 1  # 自由度
    x = np.linspace(-4, 4, 100)
    y = stats.t.pdf(x, df)
    plt.plot(x, y, 'b-', label=f't分布 (df={df})')
    
    # 标记临界值
    t_critical = stats.t.ppf(1 - alpha/2, df)
    plt.axvline(t_critical, color='r', linestyle='--', label=f'临界值: ±{t_critical:.2f}')
    plt.axvline(-t_critical, color='r', linestyle='--')
    
    # 标记观察到的t统计量
    plt.axvline(t_stat, color='g', linestyle='-', linewidth=3, label=f'观察值: {t_stat:.2f}')
    
    # 填充拒绝域
    x_reject_right = x[x >= t_critical]
    x_reject_left = x[x <= -t_critical]
    plt.fill_between(x_reject_right, 0, stats.t.pdf(x_reject_right, df), alpha=0.3, color='red')
    plt.fill_between(x_reject_left, 0, stats.t.pdf(x_reject_left, df), alpha=0.3, color='red')
    
    plt.xlabel('t值')
    plt.ylabel('概率密度')
    plt.title('t检验可视化')
    plt.legend()
    plt.grid(True)
    plt.show()

t_test_example()
```

### Confidence Intervals 置信区间

```python
# 置信区间计算
def confidence_interval_demo():
    # 样本数据
    sample = np.random.normal(100, 15, 50)
    
    # 计算95%置信区间
    confidence_level = 0.95
    alpha = 1 - confidence_level
    
    sample_mean = np.mean(sample)
    sample_std = np.std(sample, ddof=1)  # 样本标准差
    n = len(sample)
    
    # t分布临界值
    t_critical = stats.t.ppf(1 - alpha/2, n-1)
    
    # 置信区间
    margin_error = t_critical * sample_std / np.sqrt(n)
    ci_lower = sample_mean - margin_error
    ci_upper = sample_mean + margin_error
    
    print("95%置信区间:")
    print(f"样本均值: {sample_mean:.2f}")
    print(f"样本标准差: {sample_std:.2f}")
    print(f"样本大小: {n}")
    print(f"误差范围: ±{margin_error:.2f}")
    print(f"置信区间: [{ci_lower:.2f}, {ci_upper:.2f}]")
    print(f"解释：我们有95%的信心相信总体均值在此区间内")
    
    # 模拟置信区间的含义
    def simulate_confidence_intervals():
        num_simulations = 100
        true_mean = 100  # 已知的总体均值
        intervals = []
        contains_true_mean = []
        
        for _ in range(num_simulations):
            sim_sample = np.random.normal(true_mean, 15, 50)
            sim_mean = np.mean(sim_sample)
            sim_std = np.std(sim_sample, ddof=1)
            
            sim_margin = t_critical * sim_std / np.sqrt(50)
            sim_ci_lower = sim_mean - sim_margin
            sim_ci_upper = sim_mean + sim_margin
            
            intervals.append((sim_ci_lower, sim_ci_upper))
            contains_true_mean.append(sim_ci_lower <= true_mean <= sim_ci_upper)
        
        coverage_rate = np.mean(contains_true_mean)
        
        print(f"\n置信区间模拟结果:")
        print(f"模拟次数: {num_simulations}")
        print(f"包含真实均值的区间比例: {coverage_rate:.2%}")
        print(f"理论覆盖率: {confidence_level:.0%}")
        
        return intervals, contains_true_mean
    
    intervals, coverage = simulate_confidence_intervals()
    
    # 可视化前20个置信区间
    plt.figure(figsize=(12, 8))
    true_mean = 100
    
    for i in range(min(20, len(intervals))):
        color = 'green' if coverage[i] else 'red'
        plt.plot([intervals[i][0], intervals[i][1]], [i, i], color=color, linewidth=2)
        plt.plot(np.mean(intervals[i]), i, 'o', color=color, markersize=4)
    
    plt.axvline(true_mean, color='blue', linestyle='--', linewidth=2, label=f'真实均值: {true_mean}')
    plt.xlabel('值')
    plt.ylabel('模拟次数')
    plt.title('95%置信区间模拟\n绿色：包含真实均值，红色：不包含真实均值')
    plt.legend()
    plt.grid(True)
    plt.show()

confidence_interval_demo()
```

## 19.11 Information Theory 信息论

Information theory is about measuring and quantifying information. Think of it like this: if someone tells you "the sun will rise tomorrow," that's not much information because you already knew that. But if they tell you the exact lottery numbers, that's a lot of information!
信息论是关于测量和量化信息的。这样想：如果有人告诉你"明天太阳会升起"，这不是很多信息，因为你已经知道了。但如果他们告诉你确切的彩票号码，那就是大量信息！

### Entropy 熵

Entropy measures the average amount of information contained in a message. Higher entropy means more uncertainty or randomness.
熵测量消息中包含的平均信息量。熵越高意味着更多的不确定性或随机性。

```python
def calculate_entropy(probabilities):
    """计算给定概率分布的熵"""
    probabilities = np.array(probabilities)
    # 避免log(0)
    probabilities = probabilities[probabilities > 0]
    return -np.sum(probabilities * np.log2(probabilities))

# 示例1：公平硬币
fair_coin = [0.5, 0.5]
entropy_fair = calculate_entropy(fair_coin)

# 示例2：偏向硬币
biased_coin = [0.9, 0.1]
entropy_biased = calculate_entropy(biased_coin)

# 示例3：确定性事件
certain_event = [1.0, 0.0]
entropy_certain = calculate_entropy(certain_event)

# 示例4：均匀分布（4个等可能结果）
uniform_4 = [0.25, 0.25, 0.25, 0.25]
entropy_uniform_4 = calculate_entropy(uniform_4)

print("不同概率分布的熵:")
print(f"公平硬币 [0.5, 0.5]: {entropy_fair:.3f} bits")
print(f"偏向硬币 [0.9, 0.1]: {entropy_biased:.3f} bits")
print(f"确定事件 [1.0, 0.0]: {entropy_certain:.3f} bits")
print(f"均匀分布4 [0.25×4]: {entropy_uniform_4:.3f} bits")

# 可视化熵随概率变化
def plot_binary_entropy():
    p_values = np.linspace(0.001, 0.999, 1000)
    entropies = [-p * np.log2(p) - (1-p) * np.log2(1-p) for p in p_values]
    
    plt.figure(figsize=(10, 6))
    plt.plot(p_values, entropies, 'b-', linewidth=2)
    plt.axvline(0.5, color='r', linestyle='--', label='最大熵点 (p=0.5)')
    plt.xlabel('概率 p')
    plt.ylabel('熵 (bits)')
    plt.title('二元分布的熵函数')
    plt.grid(True)
    plt.legend()
    plt.show()
    
    max_entropy_idx = np.argmax(entropies)
    print(f"最大熵: {max(entropies):.3f} bits，在 p = {p_values[max_entropy_idx]:.3f}")

plot_binary_entropy()
```

### Cross-Entropy Loss 交叉熵损失

Cross-entropy is widely used in machine learning as a loss function for classification problems.
交叉熵在机器学习中被广泛用作分类问题的损失函数。

```python
def cross_entropy_demo():
    # 模拟分类问题：预测3个类别
    true_labels = np.array([0, 1, 2, 0, 1])  # 真实类别
    
    # 模型的预测概率
    predictions = np.array([
        [0.7, 0.2, 0.1],  # 预测类别0，正确
        [0.1, 0.8, 0.1],  # 预测类别1，正确
        [0.2, 0.2, 0.6],  # 预测类别2，正确
        [0.6, 0.3, 0.1],  # 预测类别0，正确
        [0.3, 0.4, 0.3]   # 预测类别1，不太确定
    ])
    
    # 计算交叉熵损失
    def cross_entropy_loss(y_true, y_pred):
        # 转换为one-hot编码
        num_classes = y_pred.shape[1]
        y_true_onehot = np.eye(num_classes)[y_true]
        
        # 避免log(0)
        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)
        
        # 计算交叉熵
        ce = -np.sum(y_true_onehot * np.log(y_pred), axis=1)
        return ce
    
    losses = cross_entropy_loss(true_labels, predictions)
    mean_loss = np.mean(losses)
    
    print("交叉熵损失示例:")
    print("样本  真实类别  预测概率                损失")
    print("-" * 50)
    for i in range(len(true_labels)):
        pred_str = f"[{predictions[i][0]:.2f}, {predictions[i][1]:.2f}, {predictions[i][2]:.2f}]"
        print(f"{i+1:2d}    {true_labels[i]:4d}      {pred_str}    {losses[i]:.3f}")
    print(f"\n平均交叉熵损失: {mean_loss:.3f}")
    
    # 可视化：展示预测置信度与损失的关系
    confidences = np.max(predictions, axis=1)
    correct_predictions = np.argmax(predictions, axis=1) == true_labels
    
    plt.figure(figsize=(10, 6))
    colors = ['red' if not correct else 'green' for correct in correct_predictions]
    plt.scatter(confidences, losses, c=colors, s=100, alpha=0.7)
    
    for i, (conf, loss) in enumerate(zip(confidences, losses)):
        plt.annotate(f'样本{i+1}', (conf, loss), xytext=(5, 5), 
                    textcoords='offset points', fontsize=9)
    
    plt.xlabel('预测置信度 (最大概率)')
    plt.ylabel('交叉熵损失')
    plt.title('预测置信度 vs 交叉熵损失\n绿色：正确预测，红色：错误预测')
    plt.grid(True)
    plt.show()

cross_entropy_demo()
```

### Mutual Information 互信息

Mutual information measures how much knowing one variable tells us about another variable.
互信息测量知道一个变量对另一个变量的信息量。

```python
def mutual_information_demo():
    # 创建两个相关的随机变量
    np.random.seed(42)
    n_samples = 1000
    
    # X: 温度 (正态分布)
    X = np.random.normal(20, 5, n_samples)
    
    # Y: 冰淇淋销量 (与温度正相关 + 噪声)
    Y = 2 * X + np.random.normal(0, 3, n_samples)
    
    # Z: 随机噪声 (与X无关)
    Z = np.random.normal(0, 1, n_samples)
    
    # 离散化变量以计算互信息
    def discretize(data, bins=10):
        return np.digitize(data, np.linspace(data.min(), data.max(), bins))
    
    X_disc = discretize(X)
    Y_disc = discretize(Y)
    Z_disc = discretize(Z)
    
    # 计算互信息
    from sklearn.metrics import mutual_info_score
    
    mi_XY = mutual_info_score(X_disc, Y_disc)
    mi_XZ = mutual_info_score(X_disc, Z_disc)
    mi_YZ = mutual_info_score(Y_disc, Z_disc)
    
    print("互信息分析:")
    print(f"I(X; Y) = {mi_XY:.3f} - 温度与冰淇淋销量")
    print(f"I(X; Z) = {mi_XZ:.3f} - 温度与随机噪声")
    print(f"I(Y; Z) = {mi_YZ:.3f} - 冰淇淋销量与随机噪声")
    
    # 可视化
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    axes[0].scatter(X, Y, alpha=0.5, color='blue')
    axes[0].set_xlabel('温度 (X)')
    axes[0].set_ylabel('冰淇淋销量 (Y)')
    axes[0].set_title(f'X vs Y\nMI = {mi_XY:.3f}')
    
    axes[1].scatter(X, Z, alpha=0.5, color='red')
    axes[1].set_xlabel('温度 (X)')
    axes[1].set_ylabel('随机噪声 (Z)')
    axes[1].set_title(f'X vs Z\nMI = {mi_XZ:.3f}')
    
    axes[2].scatter(Y, Z, alpha=0.5, color='green')
    axes[2].set_xlabel('冰淇淋销量 (Y)')
    axes[2].set_ylabel('随机噪声 (Z)')
    axes[2].set_title(f'Y vs Z\nMI = {mi_YZ:.3f}')
    
    for ax in axes:
        ax.grid(True)
    
    plt.tight_layout()
    plt.show()

mutual_information_demo()
```

### KL Divergence KL散度

KL divergence measures how different two probability distributions are. It's like comparing two recipes - the more different they are, the higher the KL divergence.
KL散度测量两个概率分布的差异程度。就像比较两个食谱——它们越不同，KL散度就越高。

```python
def kl_divergence_demo():
    # 定义两个概率分布
    x = np.linspace(-5, 5, 1000)
    
    # 分布P: 标准正态分布
    P = stats.norm.pdf(x, 0, 1)
    P = P / np.sum(P)  # 归一化
    
    # 分布Q: 不同均值和方差的正态分布
    Q1 = stats.norm.pdf(x, 0, 1)    # 相同分布
    Q2 = stats.norm.pdf(x, 1, 1)    # 不同均值
    Q3 = stats.norm.pdf(x, 0, 2)    # 不同方差
    Q4 = stats.norm.pdf(x, 2, 0.5)  # 均值和方差都不同
    
    # 归一化
    Q1 = Q1 / np.sum(Q1)
    Q2 = Q2 / np.sum(Q2)
    Q3 = Q3 / np.sum(Q3)
    Q4 = Q4 / np.sum(Q4)
    
    # 计算KL散度
    def kl_divergence(p, q):
        # 避免除零和log(0)
        epsilon = 1e-10
        p = p + epsilon
        q = q + epsilon
        return np.sum(p * np.log(p / q))
    
    kl_PQ1 = kl_divergence(P, Q1)
    kl_PQ2 = kl_divergence(P, Q2)
    kl_PQ3 = kl_divergence(P, Q3)
    kl_PQ4 = kl_divergence(P, Q4)
    
    print("KL散度比较:")
    print(f"KL(P || Q1) = {kl_PQ1:.6f} - 相同分布")
    print(f"KL(P || Q2) = {kl_PQ2:.3f} - 不同均值")
    print(f"KL(P || Q3) = {kl_PQ3:.3f} - 不同方差")
    print(f"KL(P || Q4) = {kl_PQ4:.3f} - 均值方差都不同")
    
    # 可视化
    plt.figure(figsize=(12, 8))
    
    distributions = [
        (Q1, f'Q1: N(0,1), KL={kl_PQ1:.6f}'),
        (Q2, f'Q2: N(1,1), KL={kl_PQ2:.3f}'),
        (Q3, f'Q3: N(0,2), KL={kl_PQ3:.3f}'),
        (Q4, f'Q4: N(2,0.5), KL={kl_PQ4:.3f}')
    ]
    
    for i, (Q, label) in enumerate(distributions):
        plt.subplot(2, 2, i+1)
        plt.plot(x, P, 'b-', linewidth=2, label='P: N(0,1)')
        plt.plot(x, Q, 'r--', linewidth=2, label=label)
        plt.xlabel('x')
        plt.ylabel('概率密度')
        plt.title(f'分布比较 {i+1}')
        plt.legend()
        plt.grid(True)
    
    plt.tight_layout()
    plt.show()
    
    # 展示KL散度的非对称性
    kl_Q2P = kl_divergence(Q2, P)
    print(f"\nKL散度的非对称性:")
    print(f"KL(P || Q2) = {kl_PQ2:.3f}")
    print(f"KL(Q2 || P) = {kl_Q2P:.3f}")
    print("注意：KL(P||Q) ≠ KL(Q||P)")

kl_divergence_demo()
```

## Summary 总结

Mathematics is the foundation that makes deep learning possible. Each concept we've covered plays a crucial role:
数学是使深度学习成为可能的基础。我们涵盖的每个概念都起着至关重要的作用：

- **Linear Algebra**: Represents data and transformations 线性代数：表示数据和变换
- **Calculus**: Enables optimization through gradients 微积分：通过梯度实现优化
- **Probability**: Models uncertainty and randomness 概率论：建模不确定性和随机性
- **Statistics**: Helps us understand and validate models 统计学：帮助我们理解和验证模型
- **Information Theory**: Quantifies information and guides loss functions 信息论：量化信息并指导损失函数

Understanding these mathematical foundations will make you a more effective deep learning practitioner. You'll be able to debug problems, design better architectures, and understand why certain techniques work.
理解这些数学基础将使你成为更有效的深度学习实践者。你将能够调试问题、设计更好的架构，并理解某些技术为什么有效。

Remember: mathematics in deep learning is not just abstract theory - it's a practical tool that helps us build intelligent systems that can learn from data!
记住：深度学习中的数学不仅仅是抽象理论——它是帮助我们构建能够从数据中学习的智能系统的实用工具！ 