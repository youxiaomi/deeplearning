# Chapter 11: Computer Vision | ç¬¬åä¸€ç« ï¼šè®¡ç®—æœºè§†è§‰

Computer Vision: Teaching Machines to "See" and Understand the Visual World | è®¡ç®—æœºè§†è§‰ï¼šæ•™ä¼šæœºå™¨"çœ‹è§"å¹¶ç†è§£è§†è§‰ä¸–ç•Œ

Computer vision is a crucial branch of artificial intelligence that aims to enable computers to understand and analyze image and video content just like humans do. When you look at a photograph, you can instantly recognize people, objects, scenes, and even understand the emotions and meanings conveyed in the image. The goal of computer vision is to give machines this kind of "visual understanding" capability.

è®¡ç®—æœºè§†è§‰æ˜¯äººå·¥æ™ºèƒ½é¢†åŸŸä¸­çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œå®ƒè‡´åŠ›äºè®©è®¡ç®—æœºèƒ½å¤Ÿåƒäººç±»ä¸€æ ·ç†è§£å’Œåˆ†æå›¾åƒåŠè§†é¢‘å†…å®¹ã€‚å½“ä½ çœ‹åˆ°ä¸€å¼ ç…§ç‰‡æ—¶ï¼Œä½ èƒ½ç¬é—´è¯†åˆ«å‡ºé‡Œé¢çš„äººç‰©ã€ç‰©ä½“ã€åœºæ™¯ï¼Œç”šè‡³ç†è§£å›¾ç‰‡æ‰€è¡¨è¾¾çš„æƒ…æ„Ÿå’Œå«ä¹‰ã€‚è®¡ç®—æœºè§†è§‰çš„ç›®æ ‡å°±æ˜¯èµ‹äºˆæœºå™¨è¿™æ ·çš„"è§†è§‰ç†è§£"èƒ½åŠ›ã€‚

---

## 11.1 Image Augmentation | å›¾åƒå¢å¼º

Image Augmentation: Making Your Training Data More Diverse | å›¾åƒå¢å¼ºï¼šè®©ä½ çš„è®­ç»ƒæ•°æ®æ›´åŠ å¤šæ ·åŒ–

### 11.1.1 Why Do We Need Image Augmentation? | ä¸ºä»€ä¹ˆéœ€è¦å›¾åƒå¢å¼ºï¼Ÿ

In deep learning, data is like "nutrition" - models need large amounts of diverse data to learn better. However, collecting massive real datasets is often expensive and time-consuming. This is where image augmentation comes in handy.

åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œæ•°æ®å°±åƒæ˜¯"è¥å…»"ï¼Œæ¨¡å‹éœ€è¦å¤§é‡å¤šæ ·åŒ–çš„æ•°æ®æ‰èƒ½å­¦å¾—æ›´å¥½ã€‚ä½†æ”¶é›†å¤§é‡çœŸå®æ•°æ®å¾€å¾€æˆæœ¬é«˜æ˜‚ä¸”è€—æ—¶ã€‚è¿™æ—¶å›¾åƒå¢å¼ºå°±æ´¾ä¸Šç”¨åœºäº†ã€‚

#### The Core Problem | æ ¸å¿ƒé—®é¢˜

Deep learning models, especially Convolutional Neural Networks (CNNs), require enormous amounts of training data to generalize well. Without sufficient data diversity, models tend to:

æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ï¼Œéœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®æ‰èƒ½å¾ˆå¥½åœ°æ³›åŒ–ã€‚æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®å¤šæ ·æ€§ï¼Œæ¨¡å‹å®¹æ˜“ï¼š

- **Overfit to the training data** | **å¯¹è®­ç»ƒæ•°æ®è¿‡æ‹Ÿåˆ**
- **Fail to handle variations in real-world scenarios** | **æ— æ³•å¤„ç†ç°å®åœºæ™¯ä¸­çš„å˜åŒ–**
- **Perform poorly on unseen data** | **åœ¨æœªè§è¿‡çš„æ•°æ®ä¸Šè¡¨ç°ä¸ä½³**

#### Analogy | ç±»æ¯”ä¸¾ä¾‹

Think of learning to drive: if you only practice on straight roads on sunny days, you'll struggle when encountering rainy weather or curved roads. Image augmentation is like letting the model practice driving in various "weather conditions" and "road types," improving its adaptability.

å°±åƒå­¦ä¹ å¼€è½¦ä¸€æ ·ï¼Œå¦‚æœä½ åªåœ¨æ™´å¤©çš„ç›´è·¯ä¸Šç»ƒä¹ ï¼Œé‡åˆ°é›¨å¤©æˆ–å¼¯é“æ—¶å°±ä¼šæ‰‹å¿™è„šä¹±ã€‚å›¾åƒå¢å¼ºå°±æ˜¯è®©æ¨¡å‹åœ¨å„ç§"å¤©æ°”"å’Œ"è·¯å†µ"ä¸‹ç»ƒä¹ ï¼Œæé«˜å®ƒçš„é€‚åº”èƒ½åŠ›ã€‚

### 11.1.2 Common Image Augmentation Methods | å¸¸è§çš„å›¾åƒå¢å¼ºæ–¹æ³•

#### 1. Geometric Transformations | å‡ ä½•å˜æ¢

Geometric transformations modify the spatial properties of images while preserving the essential content.

å‡ ä½•å˜æ¢ä¿®æ”¹å›¾åƒçš„ç©ºé—´å±æ€§ï¼ŒåŒæ—¶ä¿ç•™åŸºæœ¬å†…å®¹ã€‚

```python
import torch
import torchvision.transforms as transforms
from torchvision import datasets
import matplotlib.pyplot as plt
import numpy as np

def demonstrate_geometric_transforms():
    """
    Demonstrate various geometric transformations
    æ¼”ç¤ºå„ç§å‡ ä½•å˜æ¢
    """
    print("=== Geometric Transformations | å‡ ä½•å˜æ¢ ===")
    
    # Load a sample image | åŠ è½½æ ·æœ¬å›¾åƒ
    from PIL import Image
    import requests
    from io import BytesIO
    
    # Create a simple test image or use a solid color image | åˆ›å»ºç®€å•æµ‹è¯•å›¾åƒæˆ–ä½¿ç”¨çº¯è‰²å›¾åƒ
    # For demonstration, we'll create a simple pattern | ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç®€å•çš„å›¾æ¡ˆ
    def create_test_image():
        img = Image.new('RGB', (224, 224), color='white')
        # Add some patterns for visibility | æ·»åŠ ä¸€äº›å›¾æ¡ˆä»¥ä¾¿è§‚å¯Ÿ
        from PIL import ImageDraw
        draw = ImageDraw.Draw(img)
        draw.rectangle([50, 50, 150, 150], fill='red')
        draw.ellipse([100, 100, 200, 200], fill='blue')
        return img
    
    original_img = create_test_image()
    
    # Define geometric transformations | å®šä¹‰å‡ ä½•å˜æ¢
    transforms_dict = {
        'Original | åŸå§‹': transforms.Compose([transforms.ToTensor()]),
        
        'Rotation | æ—‹è½¬': transforms.Compose([
            transforms.RandomRotation(30),  # Rotate by Â±30 degrees | æ—‹è½¬Â±30åº¦
            transforms.ToTensor()
        ]),
        
        'Horizontal Flip | æ°´å¹³ç¿»è½¬': transforms.Compose([
            transforms.RandomHorizontalFlip(p=1.0),  # Always flip | æ€»æ˜¯ç¿»è½¬
            transforms.ToTensor()
        ]),
        
        'Vertical Flip | å‚ç›´ç¿»è½¬': transforms.Compose([
            transforms.RandomVerticalFlip(p=1.0),  # Always flip | æ€»æ˜¯ç¿»è½¬
            transforms.ToTensor()
        ]),
        
        'Random Crop | éšæœºè£å‰ª': transforms.Compose([
            transforms.RandomCrop(180),  # Crop to 180x180 | è£å‰ªåˆ°180x180
            transforms.Resize(224),      # Resize back to original | è°ƒæ•´å›åŸå§‹å¤§å°
            transforms.ToTensor()
        ]),
        
        'Scaling | ç¼©æ”¾': transforms.Compose([
            transforms.RandomResizedCrop(224, scale=(0.7, 1.0)),  # Scale 70-100% | ç¼©æ”¾70-100%
            transforms.ToTensor()
        ])
    }
    
    # Apply transformations and display results | åº”ç”¨å˜æ¢å¹¶æ˜¾ç¤ºç»“æœ
    print("Applying geometric transformations...")
    for name, transform in transforms_dict.items():
        transformed = transform(original_img)
        print(f"  Applied: {name}")
    
    print("\nGeometric transformations help models learn:")
    print("  âœ… Rotation invariance | æ—‹è½¬ä¸å˜æ€§")
    print("  âœ… Position independence | ä½ç½®æ— å…³æ€§") 
    print("  âœ… Scale robustness | å°ºåº¦é²æ£’æ€§")

# Run geometric transformations demo | è¿è¡Œå‡ ä½•å˜æ¢æ¼”ç¤º
demonstrate_geometric_transforms()
```

#### Key Geometric Transformations | ä¸»è¦å‡ ä½•å˜æ¢

1. **Rotation | æ—‹è½¬**
   - Rotates images clockwise or counterclockwise by a certain angle
   - Helps models recognize objects regardless of orientation
   - å°†å›¾åƒé¡ºæ—¶é’ˆæˆ–é€†æ—¶é’ˆæ—‹è½¬ä¸€å®šè§’åº¦
   - å¸®åŠ©æ¨¡å‹è¯†åˆ«ä¸åŒæ–¹å‘çš„ç‰©ä½“

2. **Flipping | ç¿»è½¬**
   - Horizontal or vertical flipping of images
   - Doubles the effective dataset size
   - æ°´å¹³æˆ–å‚ç›´ç¿»è½¬å›¾åƒ
   - æœ‰æ•ˆåœ°å°†æ•°æ®é›†å¤§å°ç¿»å€

3. **Scaling | ç¼©æ”¾**
   - Enlarges or shrinks images
   - Makes models robust to objects of different sizes
   - æ”¾å¤§æˆ–ç¼©å°å›¾åƒ
   - ä½¿æ¨¡å‹å¯¹ä¸åŒå¤§å°çš„ç‰©ä½“ä¿æŒé²æ£’æ€§

4. **Cropping | è£å‰ª**
   - Extracts a portion of the original image
   - Simulates objects at different positions
   - ä»åŸå›¾ä¸­æˆªå–éƒ¨åˆ†åŒºåŸŸ
   - æ¨¡æ‹Ÿç‰©ä½“åœ¨ä¸åŒä½ç½®çš„æƒ…å†µ

#### 2. Color Transformations | é¢œè‰²å˜æ¢

Color transformations modify the photometric properties of images while keeping the geometric structure intact.

é¢œè‰²å˜æ¢ä¿®æ”¹å›¾åƒçš„å…‰åº¦å±æ€§ï¼ŒåŒæ—¶ä¿æŒå‡ ä½•ç»“æ„ä¸å˜ã€‚

```python
def demonstrate_color_transforms():
    """
    Demonstrate various color transformations
    æ¼”ç¤ºå„ç§é¢œè‰²å˜æ¢
    """
    print("\n=== Color Transformations | é¢œè‰²å˜æ¢ ===")
    
    # Create test image | åˆ›å»ºæµ‹è¯•å›¾åƒ
    def create_colorful_test_image():
        img = Image.new('RGB', (224, 224), color='lightblue')
        from PIL import ImageDraw
        draw = ImageDraw.Draw(img)
        # Add colorful patterns | æ·»åŠ å½©è‰²å›¾æ¡ˆ
        draw.rectangle([30, 30, 100, 100], fill='red')
        draw.rectangle([120, 30, 190, 100], fill='green')
        draw.rectangle([30, 120, 100, 190], fill='yellow')
        draw.rectangle([120, 120, 190, 190], fill='purple')
        return img
    
    original_img = create_colorful_test_image()
    
    # Define color transformations | å®šä¹‰é¢œè‰²å˜æ¢
    color_transforms = {
        'Original | åŸå§‹': transforms.Compose([transforms.ToTensor()]),
        
        'Brightness Adjustment | äº®åº¦è°ƒæ•´': transforms.Compose([
            transforms.ColorJitter(brightness=0.5),  # Â±50% brightness | Â±50%äº®åº¦
            transforms.ToTensor()
        ]),
        
        'Contrast Adjustment | å¯¹æ¯”åº¦è°ƒæ•´': transforms.Compose([
            transforms.ColorJitter(contrast=0.5),    # Â±50% contrast | Â±50%å¯¹æ¯”åº¦
            transforms.ToTensor()
        ]),
        
        'Saturation Adjustment | é¥±å’Œåº¦è°ƒæ•´': transforms.Compose([
            transforms.ColorJitter(saturation=0.5),  # Â±50% saturation | Â±50%é¥±å’Œåº¦
            transforms.ToTensor()
        ]),
        
        'Hue Shifting | è‰²è°ƒå˜æ¢': transforms.Compose([
            transforms.ColorJitter(hue=0.2),         # Â±20% hue shift | Â±20%è‰²è°ƒåç§»
            transforms.ToTensor()
        ]),
        
        'Combined Color Jitter | ç»„åˆé¢œè‰²æŠ–åŠ¨': transforms.Compose([
            transforms.ColorJitter(
                brightness=0.2,  # Â±20% brightness | Â±20%äº®åº¦
                contrast=0.2,    # Â±20% contrast | Â±20%å¯¹æ¯”åº¦
                saturation=0.2,  # Â±20% saturation | Â±20%é¥±å’Œåº¦
                hue=0.1          # Â±10% hue | Â±10%è‰²è°ƒ
            ),
            transforms.ToTensor()
        ])
    }
    
    # Apply color transformations | åº”ç”¨é¢œè‰²å˜æ¢
    print("Applying color transformations...")
    for name, transform in color_transforms.items():
        transformed = transform(original_img)
        print(f"  Applied: {name}")
    
    print("\nColor transformations help models learn:")
    print("  âœ… Lighting condition robustness | å…‰ç…§æ¡ä»¶é²æ£’æ€§")
    print("  âœ… Color variation tolerance | é¢œè‰²å˜åŒ–å®¹é”™æ€§")
    print("  âœ… Camera setting independence | ç›¸æœºè®¾ç½®æ— å…³æ€§")

# Run color transformations demo | è¿è¡Œé¢œè‰²å˜æ¢æ¼”ç¤º
demonstrate_color_transforms()
```

#### 3. Noise Addition | å™ªå£°æ·»åŠ 

Adding noise to images simulates real-world conditions where images might be corrupted or of poor quality.

å‘å›¾åƒæ·»åŠ å™ªå£°æ¨¡æ‹Ÿç°å®ä¸–ç•Œä¸­å›¾åƒå¯èƒ½æŸåæˆ–è´¨é‡è¾ƒå·®çš„æƒ…å†µã€‚

```python
def demonstrate_noise_addition():
    """
    Demonstrate various noise addition techniques
    æ¼”ç¤ºå„ç§å™ªå£°æ·»åŠ æŠ€æœ¯
    """
    print("\n=== Noise Addition | å™ªå£°æ·»åŠ  ===")
    
    # Custom noise transforms | è‡ªå®šä¹‰å™ªå£°å˜æ¢
    class AddGaussianNoise:
        """Add Gaussian noise to tensor images | å‘å¼ é‡å›¾åƒæ·»åŠ é«˜æ–¯å™ªå£°"""
        
        def __init__(self, mean=0., std=0.1):
            self.mean = mean
            self.std = std
        
        def __call__(self, tensor):
            noise = torch.randn(tensor.size()) * self.std + self.mean
            return tensor + noise
        
        def __repr__(self):
            return f'{self.__class__.__name__}(mean={self.mean}, std={self.std})'
    
    class AddSaltPepperNoise:
        """Add salt and pepper noise | æ·»åŠ æ¤’ç›å™ªå£°"""
        
        def __init__(self, noise_ratio=0.05):
            self.noise_ratio = noise_ratio
        
        def __call__(self, tensor):
            noise_mask = torch.rand(tensor.size())
            salt_mask = noise_mask < self.noise_ratio / 2
            pepper_mask = noise_mask > 1 - self.noise_ratio / 2
            
            tensor = tensor.clone()
            tensor[salt_mask] = 1.0   # Salt (white) | ç›ï¼ˆç™½è‰²ï¼‰
            tensor[pepper_mask] = 0.0 # Pepper (black) | èƒ¡æ¤’ï¼ˆé»‘è‰²ï¼‰
            return tensor
    
    # Create test image | åˆ›å»ºæµ‹è¯•å›¾åƒ
    original_img = create_test_image()
    
    # Define noise transformations | å®šä¹‰å™ªå£°å˜æ¢
    noise_transforms = {
        'Original | åŸå§‹': transforms.Compose([transforms.ToTensor()]),
        
        'Gaussian Noise (Light) | é«˜æ–¯å™ªå£°ï¼ˆè½»å¾®ï¼‰': transforms.Compose([
            transforms.ToTensor(),
            AddGaussianNoise(mean=0, std=0.05)
        ]),
        
        'Gaussian Noise (Heavy) | é«˜æ–¯å™ªå£°ï¼ˆä¸¥é‡ï¼‰': transforms.Compose([
            transforms.ToTensor(),
            AddGaussianNoise(mean=0, std=0.15)
        ]),
        
        'Salt & Pepper Noise | æ¤’ç›å™ªå£°': transforms.Compose([
            transforms.ToTensor(),
            AddSaltPepperNoise(noise_ratio=0.1)
        ])
    }
    
    # Apply noise transformations | åº”ç”¨å™ªå£°å˜æ¢
    print("Applying noise transformations...")
    for name, transform in noise_transforms.items():
        transformed = transform(original_img)
        print(f"  Applied: {name}")
    
    print("\nNoise addition helps models learn:")
    print("  âœ… Robustness to image corruption | å›¾åƒæŸåé²æ£’æ€§")
    print("  âœ… Real-world condition adaptation | çœŸå®ä¸–ç•Œæ¡ä»¶é€‚åº”")
    print("  âœ… Generalization to poor quality images | ä½è´¨é‡å›¾åƒæ³›åŒ–èƒ½åŠ›")

# Run noise addition demo | è¿è¡Œå™ªå£°æ·»åŠ æ¼”ç¤º
demonstrate_noise_addition()
```

### 11.1.3 Training with Image Augmentation | ä½¿ç”¨å›¾åƒå¢å¼ºè¿›è¡Œè®­ç»ƒ

Now let's see how to implement a complete training pipeline with image augmentation:

ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•å®ç°å®Œæ•´çš„å›¾åƒå¢å¼ºè®­ç»ƒæµæ°´çº¿ï¼š

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import torchvision.transforms as transforms
from torchvision import datasets
import time

def create_augmentation_pipeline():
    """
    Create a comprehensive image augmentation pipeline
    åˆ›å»ºç»¼åˆçš„å›¾åƒå¢å¼ºæµæ°´çº¿
    """
    print("=== Creating Augmentation Pipeline | åˆ›å»ºå¢å¼ºæµæ°´çº¿ ===")
    
    # Training augmentations | è®­ç»ƒæ—¶å¢å¼º
    train_transform = transforms.Compose([
        # Geometric transformations | å‡ ä½•å˜æ¢
        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  # Random crop and resize | éšæœºè£å‰ªå’Œè°ƒæ•´å¤§å°
        transforms.RandomHorizontalFlip(p=0.5),               # 50% chance horizontal flip | 50%æ¦‚ç‡æ°´å¹³ç¿»è½¬
        transforms.RandomRotation(degrees=15),                # Random rotation Â±15Â° | éšæœºæ—‹è½¬Â±15Â°
        
        # Color transformations | é¢œè‰²å˜æ¢
        transforms.ColorJitter(
            brightness=0.2,   # Â±20% brightness variation | Â±20%äº®åº¦å˜åŒ–
            contrast=0.2,     # Â±20% contrast variation | Â±20%å¯¹æ¯”åº¦å˜åŒ–
            saturation=0.2,   # Â±20% saturation variation | Â±20%é¥±å’Œåº¦å˜åŒ–
            hue=0.1           # Â±10% hue variation | Â±10%è‰²è°ƒå˜åŒ–
        ),
        
        # Conversion and normalization | è½¬æ¢å’Œæ ‡å‡†åŒ–
        transforms.ToTensor(),
        transforms.Normalize(
            mean=[0.485, 0.456, 0.406],  # ImageNet mean | ImageNetå‡å€¼
            std=[0.229, 0.224, 0.225]    # ImageNet std | ImageNetæ ‡å‡†å·®
        )
    ])
    
    # Validation augmentations (minimal) | éªŒè¯æ—¶å¢å¼ºï¼ˆæœ€å°åŒ–ï¼‰
    val_transform = transforms.Compose([
        transforms.Resize(256),                    # Resize to 256 | è°ƒæ•´åˆ°256
        transforms.CenterCrop(224),               # Center crop to 224 | ä¸­å¿ƒè£å‰ªåˆ°224
        transforms.ToTensor(),
        transforms.Normalize(
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225]
        )
    ])
    
    print("Training augmentations include:")
    print("  ğŸ”„ Random resized crop (80-100% scale)")
    print("  â†”ï¸ Random horizontal flip (50% probability)")
    print("  ğŸ”ƒ Random rotation (Â±15 degrees)")
    print("  ğŸ¨ Color jitter (brightness, contrast, saturation, hue)")
    print("  ğŸ“Š ImageNet normalization")
    
    print("\nValidation augmentations include:")
    print("  ğŸ“ Resize to 256x256")
    print("  âœ‚ï¸ Center crop to 224x224")
    print("  ğŸ“Š ImageNet normalization")
    
    return train_transform, val_transform

def demonstrate_augmentation_effect():
    """
    Demonstrate the effect of augmentation on model training
    æ¼”ç¤ºå¢å¼ºå¯¹æ¨¡å‹è®­ç»ƒçš„å½±å“
    """
    print("\n=== Augmentation Effect Demonstration | å¢å¼ºæ•ˆæœæ¼”ç¤º ===")
    
    # Create transforms | åˆ›å»ºå˜æ¢
    train_transform, val_transform = create_augmentation_pipeline()
    
    # Create a simple model for demonstration | åˆ›å»ºç®€å•æ¨¡å‹ç”¨äºæ¼”ç¤º
    class SimpleClassifier(nn.Module):
        def __init__(self, num_classes=10):
            super().__init__()
            self.features = nn.Sequential(
                nn.Conv2d(3, 32, 3, padding=1),
                nn.ReLU(),
                nn.MaxPool2d(2),
                nn.Conv2d(32, 64, 3, padding=1),
                nn.ReLU(),
                nn.MaxPool2d(2),
                nn.AdaptiveAvgPool2d((7, 7))
            )
            self.classifier = nn.Sequential(
                nn.Linear(64 * 7 * 7, 128),
                nn.ReLU(),
                nn.Dropout(0.5),
                nn.Linear(128, num_classes)
            )
        
        def forward(self, x):
            x = self.features(x)
            x = x.view(x.size(0), -1)
            x = self.classifier(x)
            return x
    
    # Create datasets with different augmentation strategies | åˆ›å»ºä¸åŒå¢å¼ºç­–ç•¥çš„æ•°æ®é›†
    print("Comparing training with and without augmentation...")
    
    # Simulate training comparison | æ¨¡æ‹Ÿè®­ç»ƒæ¯”è¾ƒ
    def simulate_training(use_augmentation=True):
        """Simulate training process | æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹"""
        model = SimpleClassifier(num_classes=10)
        
        # Training parameters | è®­ç»ƒå‚æ•°
        num_epochs = 3
        batch_size = 32
        learning_rate = 0.001
        
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
        criterion = nn.CrossEntropyLoss()
        
        # Simulate training data | æ¨¡æ‹Ÿè®­ç»ƒæ•°æ®
        if use_augmentation:
            transform = train_transform
            aug_type = "with augmentation"
        else:
            transform = val_transform  # No augmentation | æ— å¢å¼º
            aug_type = "without augmentation"
        
        print(f"\nSimulating training {aug_type}:")
        
        # Simulate training loop | æ¨¡æ‹Ÿè®­ç»ƒå¾ªç¯
        for epoch in range(num_epochs):
            epoch_loss = 0.0
            num_batches = 10  # Simulate 10 batches per epoch | æ¯ä¸ªepochæ¨¡æ‹Ÿ10ä¸ªæ‰¹æ¬¡
            
            for batch in range(num_batches):
                # Simulate batch data | æ¨¡æ‹Ÿæ‰¹æ¬¡æ•°æ®
                batch_data = torch.randn(batch_size, 3, 224, 224)
                batch_labels = torch.randint(0, 10, (batch_size,))
                
                # Forward pass | å‰å‘ä¼ æ’­
                optimizer.zero_grad()
                outputs = model(batch_data)
                loss = criterion(outputs, batch_labels)
                
                # Backward pass | åå‘ä¼ æ’­
                loss.backward()
                optimizer.step()
                
                epoch_loss += loss.item()
            
            avg_loss = epoch_loss / num_batches
            print(f"  Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}")
        
        return avg_loss
    
    # Compare training with and without augmentation | æ¯”è¾ƒæœ‰æ— å¢å¼ºçš„è®­ç»ƒ
    loss_without_aug = simulate_training(use_augmentation=False)
    loss_with_aug = simulate_training(use_augmentation=True)
    
    print(f"\n=== Results Comparison | ç»“æœæ¯”è¾ƒ ===")
    print(f"Final loss without augmentation: {loss_without_aug:.4f}")
    print(f"Final loss with augmentation: {loss_with_aug:.4f}")
    
    print("\nBenefits of image augmentation:")
    print("  ğŸ“ˆ Increased effective dataset size | å¢åŠ æœ‰æ•ˆæ•°æ®é›†å¤§å°")
    print("  ğŸ›¡ï¸ Improved model robustness | æé«˜æ¨¡å‹é²æ£’æ€§")
    print("  ğŸ¯ Better generalization to unseen data | æ›´å¥½åœ°æ³›åŒ–åˆ°æœªè§æ•°æ®")
    print("  ğŸš« Reduced overfitting | å‡å°‘è¿‡æ‹Ÿåˆ")

# Run augmentation demonstrations | è¿è¡Œå¢å¼ºæ¼”ç¤º
create_augmentation_pipeline()
demonstrate_augmentation_effect()
```

### 11.1.4 Advanced Augmentation Techniques | é«˜çº§å¢å¼ºæŠ€æœ¯

Modern computer vision also employs more sophisticated augmentation techniques:

ç°ä»£è®¡ç®—æœºè§†è§‰è¿˜é‡‡ç”¨æ›´å¤æ‚çš„å¢å¼ºæŠ€æœ¯ï¼š

```python
def demonstrate_advanced_augmentations():
    """
    Demonstrate advanced augmentation techniques
    æ¼”ç¤ºé«˜çº§å¢å¼ºæŠ€æœ¯
    """
    print("\n=== Advanced Augmentation Techniques | é«˜çº§å¢å¼ºæŠ€æœ¯ ===")
    
    # Custom advanced transforms | è‡ªå®šä¹‰é«˜çº§å˜æ¢
    class RandomErasing:
        """Randomly erase rectangular regions | éšæœºæ“¦é™¤çŸ©å½¢åŒºåŸŸ"""
        
        def __init__(self, probability=0.5, sl=0.02, sh=0.4, r1=0.3):
            self.probability = probability
            self.sl = sl  # Minimum erased area ratio | æœ€å°æ“¦é™¤é¢ç§¯æ¯”
            self.sh = sh  # Maximum erased area ratio | æœ€å¤§æ“¦é™¤é¢ç§¯æ¯”
            self.r1 = r1  # Minimum aspect ratio | æœ€å°å®½é«˜æ¯”
        
        def __call__(self, img):
            if torch.rand(1) < self.probability:
                return img
            
            _, h, w = img.size()
            area = h * w
            
            for _ in range(100):  # Try 100 times | å°è¯•100æ¬¡
                target_area = torch.rand(1) * (self.sh - self.sl) + self.sl
                target_area = target_area * area
                
                aspect_ratio = torch.rand(1) * (1/self.r1 - self.r1) + self.r1
                
                h_e = int(torch.sqrt(target_area * aspect_ratio))
                w_e = int(torch.sqrt(target_area / aspect_ratio))
                
                if h_e < h and w_e < w:
                    x1 = torch.randint(0, h - h_e, (1,))
                    y1 = torch.randint(0, w - w_e, (1,))
                    
                    img[:, x1:x1+h_e, y1:y1+w_e] = torch.rand(3, h_e, w_e)
                    break
            
            return img
    
    class Mixup:
        """Mixup augmentation | Mixupå¢å¼º"""
        
        def __init__(self, alpha=1.0):
            self.alpha = alpha
        
        def __call__(self, batch_x, batch_y):
            """Apply mixup to a batch | å¯¹æ‰¹æ¬¡åº”ç”¨mixup"""
            if self.alpha > 0:
                lam = torch.distributions.Beta(self.alpha, self.alpha).sample()
            else:
                lam = 1
            
            batch_size = batch_x.size(0)
            index = torch.randperm(batch_size)
            
            mixed_x = lam * batch_x + (1 - lam) * batch_x[index, :]
            y_a, y_b = batch_y, batch_y[index]
            
            return mixed_x, y_a, y_b, lam
    
    # Demonstrate advanced techniques | æ¼”ç¤ºé«˜çº§æŠ€æœ¯
    print("Advanced augmentation techniques:")
    
    print("\n1. Random Erasing | éšæœºæ“¦é™¤:")
    print("   - Randomly masks rectangular regions | éšæœºé®ç›–çŸ©å½¢åŒºåŸŸ")
    print("   - Improves robustness to occlusion | æé«˜å¯¹é®æŒ¡çš„é²æ£’æ€§")
    print("   - Prevents overfitting to specific features | é˜²æ­¢å¯¹ç‰¹å®šç‰¹å¾è¿‡æ‹Ÿåˆ")
    
    print("\n2. Mixup | æ··åˆå¢å¼º:")
    print("   - Blends images and labels from two samples | æ··åˆä¸¤ä¸ªæ ·æœ¬çš„å›¾åƒå’Œæ ‡ç­¾")
    print("   - Creates smooth decision boundaries | åˆ›å»ºå¹³æ»‘çš„å†³ç­–è¾¹ç•Œ")
    print("   - Improves generalization performance | æé«˜æ³›åŒ–æ€§èƒ½")
    
    print("\n3. CutMix | å‰ªåˆ‡æ··åˆ:")
    print("   - Combines patches from different images | ç»„åˆä¸åŒå›¾åƒçš„è¡¥ä¸")
    print("   - Maintains spatial structure better than Mixup | æ¯”Mixupæ›´å¥½åœ°ä¿æŒç©ºé—´ç»“æ„")
    print("   - Effective for object detection tasks | å¯¹ç›®æ ‡æ£€æµ‹ä»»åŠ¡å¾ˆæœ‰æ•ˆ")
    
    print("\n4. AutoAugment | è‡ªåŠ¨å¢å¼º:")
    print("   - Automatically searches for optimal policies | è‡ªåŠ¨æœç´¢æœ€ä¼˜ç­–ç•¥")
    print("   - Uses reinforcement learning approach | ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ–¹æ³•")
    print("   - Dataset-specific optimization | æ•°æ®é›†ç‰¹å®šä¼˜åŒ–")

demonstrate_advanced_augmentations()
```

#### Best Practices for Image Augmentation | å›¾åƒå¢å¼ºæœ€ä½³å®è·µ

```python
def augmentation_best_practices():
    """
    Demonstrate best practices for image augmentation
    æ¼”ç¤ºå›¾åƒå¢å¼ºçš„æœ€ä½³å®è·µ
    """
    print("\n=== Image Augmentation Best Practices | å›¾åƒå¢å¼ºæœ€ä½³å®è·µ ===")
    
    print("1. **Task-Specific Augmentation | ä»»åŠ¡ç‰¹å®šå¢å¼º**")
    print("   - Object Classification: Heavy geometric + color augmentation")
    print("   - Medical Imaging: Careful with color changes, focus on geometric")
    print("   - Satellite Imagery: Rotation important, horizontal flip usually ok")
    print("   - ç‰©ä½“åˆ†ç±»ï¼šå¤§é‡å‡ ä½•+é¢œè‰²å¢å¼º")
    print("   - åŒ»å­¦å½±åƒï¼šè°¨æ…é¢œè‰²å˜åŒ–ï¼Œä¸“æ³¨å‡ ä½•å˜æ¢")
    print("   - å«æ˜Ÿå›¾åƒï¼šæ—‹è½¬é‡è¦ï¼Œæ°´å¹³ç¿»è½¬é€šå¸¸å¯è¡Œ")
    
    print("\n2. **Validation Strategy | éªŒè¯ç­–ç•¥**")
    print("   - Use minimal or no augmentation for validation/test sets")
    print("   - Maintain consistency in evaluation metrics")
    print("   - éªŒè¯/æµ‹è¯•é›†ä½¿ç”¨æœ€å°æˆ–æ— å¢å¼º")
    print("   - ä¿æŒè¯„ä¼°æŒ‡æ ‡çš„ä¸€è‡´æ€§")
    
    print("\n3. **Intensity Guidelines | å¼ºåº¦æŒ‡å¯¼åŸåˆ™**")
    print("   - Start with mild augmentation, gradually increase")
    print("   - Monitor training/validation loss gap")
    print("   - Too much augmentation can hurt performance")
    print("   - ä»è½»å¾®å¢å¼ºå¼€å§‹ï¼Œé€æ¸å¢åŠ ")
    print("   - ç›‘æ§è®­ç»ƒ/éªŒè¯æŸå¤±å·®è·")
    print("   - è¿‡åº¦å¢å¼ºå¯èƒ½æŸå®³æ€§èƒ½")
    
    print("\n4. **Performance Monitoring | æ€§èƒ½ç›‘æ§**")
    print("   - Track both training and validation metrics")
    print("   - Use early stopping to prevent overfitting")
    print("   - Visualize augmented samples regularly")
    print("   - è·Ÿè¸ªè®­ç»ƒå’ŒéªŒè¯æŒ‡æ ‡")
    print("   - ä½¿ç”¨æ—©åœé˜²æ­¢è¿‡æ‹Ÿåˆ")
    print("   - å®šæœŸå¯è§†åŒ–å¢å¼ºæ ·æœ¬")
    
    # Example of optimal augmentation pipeline | æœ€ä¼˜å¢å¼ºæµæ°´çº¿ç¤ºä¾‹
    optimal_transform = transforms.Compose([
        # Conservative geometric augmentations | ä¿å®ˆçš„å‡ ä½•å¢å¼º
        transforms.RandomResizedCrop(224, scale=(0.85, 1.0)),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomRotation(degrees=10),
        
        # Moderate color augmentations | é€‚åº¦çš„é¢œè‰²å¢å¼º
        transforms.ColorJitter(
            brightness=0.15,
            contrast=0.15,
            saturation=0.15,
            hue=0.05
        ),
        
        # Normalization | æ ‡å‡†åŒ–
        transforms.ToTensor(),
        transforms.Normalize(
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225]
        )
    ])
    
    print("\n5. **Recommended Starting Pipeline | æ¨èçš„èµ·å§‹æµæ°´çº¿**")
    print("   âœ… Random resized crop (85-100% scale)")
    print("   âœ… Random horizontal flip (50% probability)")
    print("   âœ… Small rotation (Â±10 degrees)")
    print("   âœ… Mild color jitter (15% brightness/contrast/saturation)")
    print("   âœ… Standard ImageNet normalization")

augmentation_best_practices()
```

---

## 11.2 Fine-Tuning | å¾®è°ƒ

Fine-Tuning: Standing on the Shoulders of Giants | å¾®è°ƒï¼šç«™åœ¨å·¨äººçš„è‚©è†€ä¸Š

### 11.2.1 What is Fine-Tuning? | ä»€ä¹ˆæ˜¯å¾®è°ƒï¼Ÿ

Fine-tuning is a transfer learning technique that leverages pre-trained models (usually trained on large datasets like ImageNet) and adapts them for specific tasks. Instead of training a model from scratch, we start with a model that already knows how to recognize basic visual features and teach it to recognize specific objects or patterns.

å¾®è°ƒæ˜¯ä¸€ç§è¿ç§»å­¦ä¹ æŠ€æœ¯ï¼Œå®ƒåˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼ˆé€šå¸¸åœ¨ImageNetç­‰å¤§å‹æ•°æ®é›†ä¸Šè®­ç»ƒï¼‰å¹¶å°†å…¶é€‚é…åˆ°ç‰¹å®šä»»åŠ¡ã€‚æˆ‘ä»¬ä¸æ˜¯ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹ï¼Œè€Œæ˜¯ä»ä¸€ä¸ªå·²ç»çŸ¥é“å¦‚ä½•è¯†åˆ«åŸºæœ¬è§†è§‰ç‰¹å¾çš„æ¨¡å‹å¼€å§‹ï¼Œç„¶åæ•™å®ƒè¯†åˆ«ç‰¹å®šçš„ç‰©ä½“æˆ–æ¨¡å¼ã€‚

#### The Power of Pre-trained Models | é¢„è®­ç»ƒæ¨¡å‹çš„å¨åŠ›

Pre-trained models have already learned hierarchical feature representations:

é¢„è®­ç»ƒæ¨¡å‹å·²ç»å­¦ä¼šäº†åˆ†å±‚ç‰¹å¾è¡¨ç¤ºï¼š

- **Lower layers**: Detect edges, corners, and basic shapes | **åº•å±‚**ï¼šæ£€æµ‹è¾¹ç¼˜ã€è§’è½å’ŒåŸºæœ¬å½¢çŠ¶
- **Middle layers**: Recognize patterns, textures, and simple objects | **ä¸­å±‚**ï¼šè¯†åˆ«å›¾æ¡ˆã€çº¹ç†å’Œç®€å•ç‰©ä½“
- **Higher layers**: Understand complex objects and scenes | **é«˜å±‚**ï¼šç†è§£å¤æ‚ç‰©ä½“å’Œåœºæ™¯

#### Analogy | ç±»æ¯”ä¸¾ä¾‹

Think of fine-tuning like hiring an experienced artist to paint a portrait. The artist already knows how to hold a brush, mix colors, and create realistic shapes (pre-trained knowledge). You just need to teach them the specific features of the person you want painted (task-specific adaptation).

å°†å¾®è°ƒæƒ³è±¡æˆé›‡ä½£ä¸€ä½ç»éªŒä¸°å¯Œçš„è‰ºæœ¯å®¶æ¥ç”»è‚–åƒã€‚è‰ºæœ¯å®¶å·²ç»çŸ¥é“å¦‚ä½•æ¡ç¬”ã€è°ƒè‰²å’Œåˆ›é€ é€¼çœŸçš„å½¢çŠ¶ï¼ˆé¢„è®­ç»ƒçŸ¥è¯†ï¼‰ã€‚ä½ åªéœ€è¦æ•™ä»–ä»¬ä½ æƒ³è¦ç”»çš„äººçš„å…·ä½“ç‰¹å¾ï¼ˆä»»åŠ¡ç‰¹å®šé€‚åº”ï¼‰ã€‚

### 11.2.2 Steps of Fine-Tuning | å¾®è°ƒçš„æ­¥éª¤

```python
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, TensorDataset
import time

def demonstrate_fine_tuning_process():
    """
    Demonstrate the complete fine-tuning process
    æ¼”ç¤ºå®Œæ•´çš„å¾®è°ƒè¿‡ç¨‹
    """
    print("=== Fine-Tuning Process | å¾®è°ƒè¿‡ç¨‹ ===")
    
    # Step 1: Choose a Pre-trained Model | æ­¥éª¤1ï¼šé€‰æ‹©é¢„è®­ç»ƒæ¨¡å‹
    print("\nStep 1: Choosing a Pre-trained Model | é€‰æ‹©é¢„è®­ç»ƒæ¨¡å‹")
    print("Available popular architectures:")
    print("  ğŸ—ï¸ ResNet (18, 34, 50, 101, 152)")
    print("  ğŸ” VGG (11, 13, 16, 19)")
    print("  ğŸ’¡ DenseNet (121, 161, 169, 201)")
    print("  ğŸš€ EfficientNet (B0-B7)")
    print("  ğŸ¯ Vision Transformer (ViT)")
    
    # Load pre-trained ResNet | åŠ è½½é¢„è®­ç»ƒResNet
    print("\nLoading pre-trained ResNet-18...")
    model = models.resnet18(pretrained=True)
    print(f"âœ… Model loaded with ImageNet weights")
    print(f"   Original classification head: {model.fc}")
    
    return model

def setup_fine_tuning_strategies():
    """
    Demonstrate different fine-tuning strategies
    æ¼”ç¤ºä¸åŒçš„å¾®è°ƒç­–ç•¥
    """
    print("\n=== Fine-Tuning Strategies | å¾®è°ƒç­–ç•¥ ===")
    
    # Strategy 1: Feature Extraction | ç­–ç•¥1ï¼šç‰¹å¾æå–
    def setup_feature_extraction(model, num_classes):
        """
        Freeze all layers except the final classifier
        å†»ç»“é™¤æœ€ç»ˆåˆ†ç±»å™¨å¤–çš„æ‰€æœ‰å±‚
        """
        print("\nğŸ”’ Strategy 1: Feature Extraction | ç‰¹å¾æå–")
        print("   - Freeze all convolutional layers | å†»ç»“æ‰€æœ‰å·ç§¯å±‚")
        print("   - Only train the final classifier | åªè®­ç»ƒæœ€ç»ˆåˆ†ç±»å™¨")
        
        # Freeze all parameters | å†»ç»“æ‰€æœ‰å‚æ•°
        for param in model.parameters():
            param.requires_grad = False
        
        # Replace final layer | æ›¿æ¢æœ€ç»ˆå±‚
        num_features = model.fc.in_features
        model.fc = nn.Linear(num_features, num_classes)
        
        # Only the final layer requires gradients | åªæœ‰æœ€ç»ˆå±‚éœ€è¦æ¢¯åº¦
        for param in model.fc.parameters():
            param.requires_grad = True
        
        print(f"   âœ… New classifier: Linear({num_features} -> {num_classes})")
        return model
    
    # Strategy 2: Fine-tuning | ç­–ç•¥2ï¼šå¾®è°ƒ
    def setup_full_fine_tuning(model, num_classes):
        """
        Fine-tune all layers with different learning rates
        ç”¨ä¸åŒå­¦ä¹ ç‡å¾®è°ƒæ‰€æœ‰å±‚
        """
        print("\nğŸ¯ Strategy 2: Full Fine-tuning | å…¨é¢å¾®è°ƒ")
        print("   - All layers are trainable | æ‰€æœ‰å±‚éƒ½å¯è®­ç»ƒ")
        print("   - Use different learning rates for different layers | ä¸åŒå±‚ä½¿ç”¨ä¸åŒå­¦ä¹ ç‡")
        
        # Replace final layer | æ›¿æ¢æœ€ç»ˆå±‚
        num_features = model.fc.in_features
        model.fc = nn.Linear(num_features, num_classes)
        
        # All parameters require gradients | æ‰€æœ‰å‚æ•°éƒ½éœ€è¦æ¢¯åº¦
        for param in model.parameters():
            param.requires_grad = True
        
        print(f"   âœ… New classifier: Linear({num_features} -> {num_classes})")
        return model
    
    # Strategy 3: Gradual Unfreezing | ç­–ç•¥3ï¼šæ¸è¿›è§£å†»
    def setup_gradual_unfreezing(model, num_classes):
        """
        Gradually unfreeze layers during training
        è®­ç»ƒæœŸé—´é€æ¸è§£å†»å±‚
        """
        print("\nğŸŒ¡ï¸ Strategy 3: Gradual Unfreezing | æ¸è¿›è§£å†»")
        print("   - Start with frozen backbone | ä»å†»ç»“çš„éª¨å¹²ç½‘ç»œå¼€å§‹")
        print("   - Gradually unfreeze layers | é€æ¸è§£å†»å±‚")
        print("   - Allows for more stable training | å…è®¸æ›´ç¨³å®šçš„è®­ç»ƒ")
        
        # Initially freeze all except classifier | åˆå§‹æ—¶é™¤åˆ†ç±»å™¨å¤–å…¨éƒ¨å†»ç»“
        for param in model.parameters():
            param.requires_grad = False
        
        num_features = model.fc.in_features
        model.fc = nn.Linear(num_features, num_classes)
        
        for param in model.fc.parameters():
            param.requires_grad = True
        
        print(f"   âœ… Starting with frozen backbone")
        return model
    
    # Demonstrate each strategy | æ¼”ç¤ºæ¯ç§ç­–ç•¥
    base_model = models.resnet18(pretrained=True)
    num_classes = 2  # Binary classification example | äºŒåˆ†ç±»ç¤ºä¾‹
    
    # Test each strategy | æµ‹è¯•æ¯ç§ç­–ç•¥
    strategies = [
        ("feature_extraction", setup_feature_extraction),
        ("full_fine_tuning", setup_full_fine_tuning),
        ("gradual_unfreezing", setup_gradual_unfreezing)
    ]
    
    strategy_models = {}
    for name, setup_func in strategies:
        model_copy = models.resnet18(pretrained=True)
        strategy_models[name] = setup_func(model_copy, num_classes)
    
    return strategy_models

def implement_learning_rate_scheduling():
    """
    Implement proper learning rate scheduling for fine-tuning
    å®ç°å¾®è°ƒçš„é€‚å½“å­¦ä¹ ç‡è°ƒåº¦
    """
    print("\n=== Learning Rate Scheduling | å­¦ä¹ ç‡è°ƒåº¦ ===")
    
    # Different learning rates for different parts | ä¸åŒéƒ¨åˆ†ä½¿ç”¨ä¸åŒå­¦ä¹ ç‡
    def create_parameter_groups(model):
        """
        Create parameter groups with different learning rates
        åˆ›å»ºå…·æœ‰ä¸åŒå­¦ä¹ ç‡çš„å‚æ•°ç»„
        """
        # Backbone parameters (lower learning rate) | éª¨å¹²ç½‘ç»œå‚æ•°ï¼ˆè¾ƒä½å­¦ä¹ ç‡ï¼‰
        backbone_params = []
        
        # Classifier parameters (higher learning rate) | åˆ†ç±»å™¨å‚æ•°ï¼ˆè¾ƒé«˜å­¦ä¹ ç‡ï¼‰
        classifier_params = []
        
        for name, param in model.named_parameters():
            if 'fc' in name:  # Final classifier layer | æœ€ç»ˆåˆ†ç±»å™¨å±‚
                classifier_params.append(param)
            else:  # Backbone layers | éª¨å¹²ç½‘ç»œå±‚
                backbone_params.append(param)
        
        param_groups = [
            {'params': backbone_params, 'lr': 1e-4},    # Lower LR for backbone | éª¨å¹²ç½‘ç»œè¾ƒä½å­¦ä¹ ç‡
            {'params': classifier_params, 'lr': 1e-3}   # Higher LR for classifier | åˆ†ç±»å™¨è¾ƒé«˜å­¦ä¹ ç‡
        ]
        
        print("Parameter groups created:")
        print(f"  ğŸ”§ Backbone: {len(backbone_params)} params, LR = 1e-4")
        print(f"  ğŸ¯ Classifier: {len(classifier_params)} params, LR = 1e-3")
        
        return param_groups
    
    # Demonstration model | æ¼”ç¤ºæ¨¡å‹
    model = models.resnet18(pretrained=True)
    num_features = model.fc.in_features
    model.fc = nn.Linear(num_features, 2)
    
    # Create optimizer with different learning rates | åˆ›å»ºå…·æœ‰ä¸åŒå­¦ä¹ ç‡çš„ä¼˜åŒ–å™¨
    param_groups = create_parameter_groups(model)
    optimizer = optim.Adam(param_groups)
    
    # Learning rate scheduler | å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)
    
    print("\nScheduler configuration:")
    print("  ğŸ“… StepLR: Reduce LR by 10x every 7 epochs")
    print("  ğŸ¯ Helps fine-tune gradually and avoid overfitting")
    
    return optimizer, scheduler

def demonstrate_fine_tuning_training():
    """
    Demonstrate a complete fine-tuning training loop
    æ¼”ç¤ºå®Œæ•´çš„å¾®è°ƒè®­ç»ƒå¾ªç¯
    """
    print("\n=== Fine-Tuning Training Loop | å¾®è°ƒè®­ç»ƒå¾ªç¯ ===")
    
    # Setup model | è®¾ç½®æ¨¡å‹
    model = models.resnet18(pretrained=True)
    num_classes = 2
    num_features = model.fc.in_features
    model.fc = nn.Linear(num_features, num_classes)
    
    # Create sample data | åˆ›å»ºæ ·æœ¬æ•°æ®
    def create_sample_dataset():
        """Create a sample dataset for demonstration | åˆ›å»ºæ¼”ç¤ºç”¨æ ·æœ¬æ•°æ®é›†"""
        # Generate synthetic data | ç”Ÿæˆåˆæˆæ•°æ®
        num_samples = 1000
        data = torch.randn(num_samples, 3, 224, 224)
        labels = torch.randint(0, num_classes, (num_samples,))
        
        # Split into train/val | åˆ†ä¸ºè®­ç»ƒ/éªŒè¯é›†
        train_size = int(0.8 * num_samples)
        
        train_data = data[:train_size]
        train_labels = labels[:train_size]
        val_data = data[train_size:]
        val_labels = labels[train_size:]
        
        train_dataset = TensorDataset(train_data, train_labels)
        val_dataset = TensorDataset(val_data, val_labels)
        
        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
        
        return train_loader, val_loader
    
    # Training setup | è®­ç»ƒè®¾ç½®
    train_loader, val_loader = create_sample_dataset()
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=1e-3)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)
    
    # Training loop | è®­ç»ƒå¾ªç¯
    print("\nStarting fine-tuning training...")
    num_epochs = 5
    
    for epoch in range(num_epochs):
        # Training phase | è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        correct_train = 0
        total_train = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            optimizer.zero_grad()
            outputs = model(data)
            loss = criterion(outputs, target)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            _, predicted = outputs.max(1)
            total_train += target.size(0)
            correct_train += predicted.eq(target).sum().item()
            
            if batch_idx % 10 == 0:
                print(f"  Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.4f}")
        
        # Validation phase | éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0.0
        correct_val = 0
        total_val = 0
        
        with torch.no_grad():
            for data, target in val_loader:
                outputs = model(data)
                loss = criterion(outputs, target)
                
                val_loss += loss.item()
                _, predicted = outputs.max(1)
                total_val += target.size(0)
                correct_val += predicted.eq(target).sum().item()
        
        # Calculate metrics | è®¡ç®—æŒ‡æ ‡
        train_acc = 100. * correct_train / total_train
        val_acc = 100. * correct_val / total_val
        avg_train_loss = train_loss / len(train_loader)
        avg_val_loss = val_loss / len(val_loader)
        
        print(f"Epoch {epoch+1}/{num_epochs}:")
        print(f"  Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}%")
        print(f"  Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%")
        print(f"  Learning Rate: {optimizer.param_groups[0]['lr']:.6f}")
        
        # Step scheduler | æ­¥è¿›è°ƒåº¦å™¨
        scheduler.step()
        print("-" * 50)
    
    print("âœ… Fine-tuning completed!")

# Run fine-tuning demonstrations | è¿è¡Œå¾®è°ƒæ¼”ç¤º
model = demonstrate_fine_tuning_process()
strategy_models = setup_fine_tuning_strategies()
optimizer, scheduler = implement_learning_rate_scheduling()
demonstrate_fine_tuning_training()
```

### 11.2.3 Hot Dog Recognition Example | çƒ­ç‹—è¯†åˆ«å®ä¾‹

Let's implement a practical fine-tuning example - a hot dog vs. not hot dog classifier:

è®©æˆ‘ä»¬å®ç°ä¸€ä¸ªå®é™…çš„å¾®è°ƒç¤ºä¾‹â€”â€”çƒ­ç‹—ä¸éçƒ­ç‹—åˆ†ç±»å™¨ï¼š

```python
def implement_hotdog_classifier():
    """
    Implement a practical hot dog vs. not hot dog classifier
    å®ç°å®ç”¨çš„çƒ­ç‹—ä¸éçƒ­ç‹—åˆ†ç±»å™¨
    """
    print("=== Hot Dog Classifier Implementation | çƒ­ç‹—åˆ†ç±»å™¨å®ç° ===")
    
    class HotDogClassifier(nn.Module):
        """
        Hot dog binary classifier using pre-trained ResNet
        ä½¿ç”¨é¢„è®­ç»ƒResNetçš„çƒ­ç‹—äºŒåˆ†ç±»å™¨
        """
        
        def __init__(self, pretrained=True):
            super().__init__()
            
            # Load pre-trained ResNet | åŠ è½½é¢„è®­ç»ƒResNet
            self.backbone = models.resnet18(pretrained=pretrained)
            
            # Remove the original classifier | ç§»é™¤åŸå§‹åˆ†ç±»å™¨
            num_features = self.backbone.fc.in_features
            self.backbone.fc = nn.Identity()  # Remove FC layer | ç§»é™¤FCå±‚
            
            # Add custom classifier for binary classification | æ·»åŠ äºŒåˆ†ç±»çš„è‡ªå®šä¹‰åˆ†ç±»å™¨
            self.classifier = nn.Sequential(
                nn.Dropout(0.5),
                nn.Linear(num_features, 128),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(128, 2)  # 2 classes: hot dog, not hot dog | 2ç±»ï¼šçƒ­ç‹—ï¼Œéçƒ­ç‹—
            )
            
            print("ğŸŒ­ Hot Dog Classifier Architecture:")
            print(f"   Backbone: ResNet-18 (pretrained={pretrained})")
            print(f"   Features: {num_features}")
            print(f"   Classifier: {num_features} -> 128 -> 2")
        
        def forward(self, x):
            # Extract features using backbone | ä½¿ç”¨éª¨å¹²ç½‘ç»œæå–ç‰¹å¾
            features = self.backbone(x)
            
            # Classify using custom head | ä½¿ç”¨è‡ªå®šä¹‰å¤´éƒ¨åˆ†ç±»
            output = self.classifier(features)
            
            return output
        
        def freeze_backbone(self):
            """Freeze backbone parameters for feature extraction | å†»ç»“éª¨å¹²ç½‘ç»œå‚æ•°è¿›è¡Œç‰¹å¾æå–"""
            for param in self.backbone.parameters():
                param.requires_grad = False
            print("ğŸ”’ Backbone frozen - Feature extraction mode")
        
        def unfreeze_backbone(self):
            """Unfreeze backbone for fine-tuning | è§£å†»éª¨å¹²ç½‘ç»œè¿›è¡Œå¾®è°ƒ"""
            for param in self.backbone.parameters():
                param.requires_grad = True
            print("ğŸ”“ Backbone unfrozen - Fine-tuning mode")
    
    # Create model instance | åˆ›å»ºæ¨¡å‹å®ä¾‹
    model = HotDogClassifier(pretrained=True)
    
    # Demonstrate freezing strategies | æ¼”ç¤ºå†»ç»“ç­–ç•¥
    print("\nDemonstrating training strategies:")
    
    # Phase 1: Feature extraction | é˜¶æ®µ1ï¼šç‰¹å¾æå–
    print("\nğŸ“š Phase 1: Feature Extraction (5 epochs)")
    model.freeze_backbone()
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    print(f"   Trainable parameters: {trainable_params:,} / {total_params:,}")
    
    # Phase 2: Fine-tuning | é˜¶æ®µ2ï¼šå¾®è°ƒ
    print("\nğŸ¯ Phase 2: Fine-tuning (3 epochs)")
    model.unfreeze_backbone()
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"   Trainable parameters: {trainable_params:,} / {total_params:,}")
    
    return model

def create_hotdog_dataset():
    """
    Create a simulated hot dog dataset
    åˆ›å»ºæ¨¡æ‹Ÿçš„çƒ­ç‹—æ•°æ®é›†
    """
    print("\n=== Creating Hot Dog Dataset | åˆ›å»ºçƒ­ç‹—æ•°æ®é›† ===")
    
    # In practice, you would load real images | å®é™…ä¸­ï¼Œæ‚¨ä¼šåŠ è½½çœŸå®å›¾åƒ
    # Here we simulate with random data | è¿™é‡Œæˆ‘ä»¬ç”¨éšæœºæ•°æ®æ¨¡æ‹Ÿ
    
    # Dataset statistics | æ•°æ®é›†ç»Ÿè®¡
    num_hotdog_samples = 500
    num_not_hotdog_samples = 1500
    
    print(f"Dataset composition:")
    print(f"  ğŸŒ­ Hot dog samples: {num_hotdog_samples}")
    print(f"  ğŸ¥— Not hot dog samples: {num_not_hotdog_samples}")
    print(f"  ğŸ“Š Class ratio: 1:{num_not_hotdog_samples//num_hotdog_samples} (imbalanced)")
    
    # Create synthetic data | åˆ›å»ºåˆæˆæ•°æ®
    hotdog_data = torch.randn(num_hotdog_samples, 3, 224, 224)
    hotdog_labels = torch.ones(num_hotdog_samples, dtype=torch.long)  # Label 1 for hot dog
    
    not_hotdog_data = torch.randn(num_not_hotdog_samples, 3, 224, 224)
    not_hotdog_labels = torch.zeros(num_not_hotdog_samples, dtype=torch.long)  # Label 0 for not hot dog
    
    # Combine datasets | ç»„åˆæ•°æ®é›†
    all_data = torch.cat([hotdog_data, not_hotdog_data], dim=0)
    all_labels = torch.cat([hotdog_labels, not_hotdog_labels], dim=0)
    
    # Shuffle data | æ‰“ä¹±æ•°æ®
    indices = torch.randperm(len(all_data))
    all_data = all_data[indices]
    all_labels = all_labels[indices]
    
    # Split into train/validation | åˆ†ä¸ºè®­ç»ƒ/éªŒè¯é›†
    train_size = int(0.8 * len(all_data))
    
    train_data = all_data[:train_size]
    train_labels = all_labels[:train_size]
    val_data = all_data[train_size:]
    val_labels = all_labels[train_size:]
    
    # Create data loaders | åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = TensorDataset(train_data, train_labels)
    val_dataset = TensorDataset(val_data, val_labels)
    
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
    
    print(f"  ğŸ“š Training samples: {len(train_dataset)}")
    print(f"  ğŸ” Validation samples: {len(val_dataset)}")
    
    return train_loader, val_loader

def train_hotdog_classifier():
    """
    Train the hot dog classifier with proper fine-tuning
    ä½¿ç”¨é€‚å½“çš„å¾®è°ƒè®­ç»ƒçƒ­ç‹—åˆ†ç±»å™¨
    """
    print("\n=== Training Hot Dog Classifier | è®­ç»ƒçƒ­ç‹—åˆ†ç±»å™¨ ===")
    
    # Setup | è®¾ç½®
    model = implement_hotdog_classifier()
    train_loader, val_loader = create_hotdog_dataset()
    
    # Handle class imbalance with weighted loss | ç”¨åŠ æƒæŸå¤±å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
    class_weights = torch.tensor([3.0, 1.0])  # Higher weight for hot dog class | çƒ­ç‹—ç±»åˆ«æ›´é«˜æƒé‡
    criterion = nn.CrossEntropyLoss(weight=class_weights)
    
    def train_phase(model, train_loader, val_loader, optimizer, scheduler, num_epochs, phase_name):
        """Train for specified number of epochs | è®­ç»ƒæŒ‡å®šè½®æ•°"""
        print(f"\n{phase_name} Training:")
        
        for epoch in range(num_epochs):
            # Training | è®­ç»ƒ
            model.train()
            train_loss = 0.0
            correct = 0
            total = 0
            
            for batch_idx, (data, target) in enumerate(train_loader):
                optimizer.zero_grad()
                outputs = model(data)
                loss = criterion(outputs, target)
                loss.backward()
                optimizer.step()
                
                train_loss += loss.item()
                _, predicted = outputs.max(1)
                total += target.size(0)
                correct += predicted.eq(target).sum().item()
            
            # Validation | éªŒè¯
            model.eval()
            val_loss = 0.0
            val_correct = 0
            val_total = 0
            
            with torch.no_grad():
                for data, target in val_loader:
                    outputs = model(data)
                    loss = criterion(outputs, target)
                    
                    val_loss += loss.item()
                    _, predicted = outputs.max(1)
                    val_total += target.size(0)
                    val_correct += predicted.eq(target).sum().item()
            
            # Print metrics | æ‰“å°æŒ‡æ ‡
            train_acc = 100. * correct / total
            val_acc = 100. * val_correct / val_total
            
            print(f"  Epoch {epoch+1}: Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%")
            
            scheduler.step()
    
    # Phase 1: Feature extraction | é˜¶æ®µ1ï¼šç‰¹å¾æå–
    model.freeze_backbone()
    optimizer_phase1 = optim.Adam(
        filter(lambda p: p.requires_grad, model.parameters()), 
        lr=1e-3
    )
    scheduler_phase1 = optim.lr_scheduler.StepLR(optimizer_phase1, step_size=2, gamma=0.1)
    
    train_phase(model, train_loader, val_loader, optimizer_phase1, scheduler_phase1, 
                3, "ğŸ”’ Feature Extraction")
    
    # Phase 2: Fine-tuning | é˜¶æ®µ2ï¼šå¾®è°ƒ
    model.unfreeze_backbone()
    optimizer_phase2 = optim.Adam([
        {'params': model.backbone.parameters(), 'lr': 1e-5},  # Lower LR for backbone | éª¨å¹²ç½‘ç»œè¾ƒä½å­¦ä¹ ç‡
        {'params': model.classifier.parameters(), 'lr': 1e-4}  # Higher LR for classifier | åˆ†ç±»å™¨è¾ƒé«˜å­¦ä¹ ç‡
    ])
    scheduler_phase2 = optim.lr_scheduler.StepLR(optimizer_phase2, step_size=2, gamma=0.5)
    
    train_phase(model, train_loader, val_loader, optimizer_phase2, scheduler_phase2, 
                3, "ğŸ¯ Fine-tuning")
    
    print("\nâœ… Hot dog classifier training completed!")
    
    # Model evaluation summary | æ¨¡å‹è¯„ä¼°æ€»ç»“
    print("\nğŸ“Š Model Performance Summary:")
    print("  ğŸ¯ Two-phase training approach used")
    print("  âš–ï¸ Class imbalance handled with weighted loss")
    print("  ğŸ“ˆ Different learning rates for backbone vs classifier")
    print("  ğŸ‰ Ready for hot dog detection!")
    
    return model

# Run hot dog classifier implementation | è¿è¡Œçƒ­ç‹—åˆ†ç±»å™¨å®ç°
hotdog_model = train_hotdog_classifier()
```

---

## 11.3 Object Detection and Bounding Boxes | ç›®æ ‡æ£€æµ‹ä¸è¾¹ç•Œæ¡†

Object Detection: Finding and Locating Objects in Images | ç›®æ ‡æ£€æµ‹ï¼šåœ¨å›¾åƒä¸­å¯»æ‰¾å¹¶å®šä½ç‰©ä½“

### 11.3.1 What is Object Detection? | ä»€ä¹ˆæ˜¯ç›®æ ‡æ£€æµ‹ï¼Ÿ

Object detection goes beyond simple image classification. While classification answers "what is in this image?", object detection answers both "what objects are in this image?" and "where are they located?" This involves two main tasks:

ç›®æ ‡æ£€æµ‹è¶…è¶Šäº†ç®€å•çš„å›¾åƒåˆ†ç±»ã€‚åˆ†ç±»å›ç­”"è¿™å¼ å›¾åƒä¸­æœ‰ä»€ä¹ˆï¼Ÿ"ï¼Œè€Œç›®æ ‡æ£€æµ‹å›ç­”"è¿™å¼ å›¾åƒä¸­æœ‰ä»€ä¹ˆç‰©ä½“ï¼Ÿ"å’Œ"å®ƒä»¬åœ¨å“ªé‡Œï¼Ÿ"è¿™æ¶‰åŠä¸¤ä¸ªä¸»è¦ä»»åŠ¡ï¼š

1. **Classification**: Identifying what objects are present | **åˆ†ç±»**ï¼šè¯†åˆ«å­˜åœ¨ä»€ä¹ˆç‰©ä½“
2. **Localization**: Determining where objects are located | **å®šä½**ï¼šç¡®å®šç‰©ä½“çš„ä½ç½®

#### Analogy | ç±»æ¯”ä¸¾ä¾‹

Think of object detection like being a security guard looking at surveillance footage. You don't just need to know if there are people in the scene (classification) - you need to know exactly where each person is located and be able to point them out with rectangular frames (localization).

å°†ç›®æ ‡æ£€æµ‹æƒ³è±¡æˆå®‰ä¿äººå‘˜æŸ¥çœ‹ç›‘æ§å½•åƒã€‚ä½ ä¸ä»…éœ€è¦çŸ¥é“åœºæ™¯ä¸­æ˜¯å¦æœ‰äººï¼ˆåˆ†ç±»ï¼‰â€”â€”ä½ éœ€è¦çŸ¥é“æ¯ä¸ªäººçš„ç¡®åˆ‡ä½ç½®ï¼Œå¹¶èƒ½ç”¨çŸ©å½¢æ¡†æŒ‡å‡ºä»–ä»¬ï¼ˆå®šä½ï¼‰ã€‚

### 11.3.2 Bounding Boxes | è¾¹ç•Œæ¡†

A bounding box is a rectangular frame that tightly encloses an object in an image. It's the fundamental way to represent object locations in computer vision.

è¾¹ç•Œæ¡†æ˜¯ç´§å¯†åŒ…å›´å›¾åƒä¸­ç‰©ä½“çš„çŸ©å½¢æ¡†ã€‚å®ƒæ˜¯è®¡ç®—æœºè§†è§‰ä¸­è¡¨ç¤ºç‰©ä½“ä½ç½®çš„åŸºæœ¬æ–¹å¼ã€‚

#### Bounding Box Representation | è¾¹ç•Œæ¡†è¡¨ç¤º

```python
import torch
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import numpy as np

def demonstrate_bounding_boxes():
    """
    Demonstrate bounding box concepts and representations
    æ¼”ç¤ºè¾¹ç•Œæ¡†æ¦‚å¿µå’Œè¡¨ç¤ºæ–¹æ³•
    """
    print("=== Bounding Box Fundamentals | è¾¹ç•Œæ¡†åŸºç¡€ ===")
    
    # Different bounding box formats | ä¸åŒçš„è¾¹ç•Œæ¡†æ ¼å¼
    print("\nğŸ“¦ Bounding Box Representation Formats:")
    print("1. (x, y, width, height) - Top-left corner + dimensions")
    print("2. (x1, y1, x2, y2) - Top-left + bottom-right corners")
    print("3. (cx, cy, width, height) - Center + dimensions")
    print("4. Normalized coordinates (0-1 range)")
    
    # Example bounding boxes | ç¤ºä¾‹è¾¹ç•Œæ¡†
    image_width, image_height = 640, 480
    
    # Different representations of the same bounding box | åŒä¸€è¾¹ç•Œæ¡†çš„ä¸åŒè¡¨ç¤º
    print(f"\nExample: Object in {image_width}x{image_height} image")
    
    # Format 1: (x, y, w, h) - COCO format | æ ¼å¼1ï¼š(x, y, w, h) - COCOæ ¼å¼
    bbox_xywh = [100, 150, 200, 120]  # x, y, width, height
    print(f"COCO format (x,y,w,h): {bbox_xywh}")
    
    # Format 2: (x1, y1, x2, y2) - Pascal VOC format | æ ¼å¼2ï¼š(x1, y1, x2, y2) - Pascal VOCæ ¼å¼
    x1, y1 = bbox_xywh[0], bbox_xywh[1]
    x2, y2 = x1 + bbox_xywh[2], y1 + bbox_xywh[3]
    bbox_xyxy = [x1, y1, x2, y2]
    print(f"Pascal VOC format (x1,y1,x2,y2): {bbox_xyxy}")
    
    # Format 3: Center coordinates | æ ¼å¼3ï¼šä¸­å¿ƒåæ ‡
    cx = x1 + bbox_xywh[2] / 2
    cy = y1 + bbox_xywh[3] / 2
    bbox_cxcywh = [cx, cy, bbox_xywh[2], bbox_xywh[3]]
    print(f"Center format (cx,cy,w,h): {bbox_cxcywh}")
    
    # Format 4: Normalized coordinates | æ ¼å¼4ï¼šå½’ä¸€åŒ–åæ ‡
    bbox_norm = [
        bbox_xywh[0] / image_width,   # x_norm
        bbox_xywh[1] / image_height,  # y_norm
        bbox_xywh[2] / image_width,   # w_norm
        bbox_xywh[3] / image_height   # h_norm
    ]
    print(f"Normalized (0-1): {bbox_norm}")
    
    return bbox_xywh, bbox_xyxy, bbox_cxcywh, bbox_norm

def convert_bbox_formats():
    """
    Demonstrate bounding box format conversions
    æ¼”ç¤ºè¾¹ç•Œæ¡†æ ¼å¼è½¬æ¢
    """
    print("\n=== Bounding Box Format Conversions | è¾¹ç•Œæ¡†æ ¼å¼è½¬æ¢ ===")
    
    def xywh_to_xyxy(bbox):
        """Convert (x, y, w, h) to (x1, y1, x2, y2) | è½¬æ¢(x, y, w, h)åˆ°(x1, y1, x2, y2)"""
        x, y, w, h = bbox
        return [x, y, x + w, y + h]
    
    def xyxy_to_xywh(bbox):
        """Convert (x1, y1, x2, y2) to (x, y, w, h) | è½¬æ¢(x1, y1, x2, y2)åˆ°(x, y, w, h)"""
        x1, y1, x2, y2 = bbox
        return [x1, y1, x2 - x1, y2 - y1]
    
    def xywh_to_cxcywh(bbox):
        """Convert (x, y, w, h) to (cx, cy, w, h) | è½¬æ¢(x, y, w, h)åˆ°(cx, cy, w, h)"""
        x, y, w, h = bbox
        return [x + w/2, y + h/2, w, h]
    
    def cxcywh_to_xywh(bbox):
        """Convert (cx, cy, w, h) to (x, y, w, h) | è½¬æ¢(cx, cy, w, h)åˆ°(x, y, w, h)"""
        cx, cy, w, h = bbox
        return [cx - w/2, cy - h/2, w, h]
    
    def normalize_bbox(bbox, img_width, img_height):
        """Normalize bounding box coordinates | å½’ä¸€åŒ–è¾¹ç•Œæ¡†åæ ‡"""
        x, y, w, h = bbox
        return [x/img_width, y/img_height, w/img_width, h/img_height]
    
    def denormalize_bbox(bbox_norm, img_width, img_height):
        """Denormalize bounding box coordinates | åå½’ä¸€åŒ–è¾¹ç•Œæ¡†åæ ‡"""
        x_norm, y_norm, w_norm, h_norm = bbox_norm
        return [x_norm*img_width, y_norm*img_height, w_norm*img_width, h_norm*img_height]
    
    # Test conversions | æµ‹è¯•è½¬æ¢
    original_bbox = [100, 150, 200, 120]  # (x, y, w, h)
    img_w, img_h = 640, 480
    
    print(f"Original (x,y,w,h): {original_bbox}")
    
    # Convert to different formats | è½¬æ¢ä¸ºä¸åŒæ ¼å¼
    xyxy = xywh_to_xyxy(original_bbox)
    print(f"Converted to (x1,y1,x2,y2): {xyxy}")
    
    cxcywh = xywh_to_cxcywh(original_bbox)
    print(f"Converted to (cx,cy,w,h): {cxcywh}")
    
    normalized = normalize_bbox(original_bbox, img_w, img_h)
    print(f"Normalized: {normalized}")
    
    # Convert back to verify | è½¬æ¢å›æ¥éªŒè¯
    back_to_xywh = xyxy_to_xywh(xyxy)
    print(f"Back to (x,y,w,h): {back_to_xywh}")
    print(f"Conversion accurate: {original_bbox == back_to_xywh}")

def demonstrate_bbox_operations():
    """
    Demonstrate important bounding box operations
    æ¼”ç¤ºé‡è¦çš„è¾¹ç•Œæ¡†æ“ä½œ
    """
    print("\n=== Bounding Box Operations | è¾¹ç•Œæ¡†æ“ä½œ ===")
    
    def calculate_area(bbox):
        """Calculate bounding box area | è®¡ç®—è¾¹ç•Œæ¡†é¢ç§¯"""
        x, y, w, h = bbox
        return w * h
    
    def calculate_iou(bbox1, bbox2):
        """
        Calculate Intersection over Union (IoU) between two bounding boxes
        è®¡ç®—ä¸¤ä¸ªè¾¹ç•Œæ¡†ä¹‹é—´çš„äº¤å¹¶æ¯”(IoU)
        """
        # Convert to (x1, y1, x2, y2) format | è½¬æ¢ä¸º(x1, y1, x2, y2)æ ¼å¼
        x1_1, y1_1, w1, h1 = bbox1
        x2_1, y2_1 = x1_1 + w1, y1_1 + h1
        
        x1_2, y1_2, w2, h2 = bbox2
        x2_2, y2_2 = x1_2 + w2, y1_2 + h2
        
        # Calculate intersection | è®¡ç®—äº¤é›†
        x1_inter = max(x1_1, x1_2)
        y1_inter = max(y1_1, y1_2)
        x2_inter = min(x2_1, x2_2)
        y2_inter = min(y2_1, y2_2)
        
        # Check if there's an intersection | æ£€æŸ¥æ˜¯å¦æœ‰äº¤é›†
        if x1_inter >= x2_inter or y1_inter >= y2_inter:
            intersection = 0
        else:
            intersection = (x2_inter - x1_inter) * (y2_inter - y1_inter)
        
        # Calculate union | è®¡ç®—å¹¶é›†
        area1 = w1 * h1
        area2 = w2 * h2
        union = area1 + area2 - intersection
        
        # Calculate IoU | è®¡ç®—IoU
        iou = intersection / union if union > 0 else 0
        
        return iou, intersection, union
    
    def non_max_suppression_simple(bboxes, scores, iou_threshold=0.5):
        """
        Simple implementation of Non-Maximum Suppression
        éæå¤§å€¼æŠ‘åˆ¶çš„ç®€å•å®ç°
        """
        # Sort by scores in descending order | æŒ‰åˆ†æ•°é™åºæ’åˆ—
        indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)
        
        keep = []
        while indices:
            # Keep the box with highest score | ä¿ç•™å¾—åˆ†æœ€é«˜çš„æ¡†
            current = indices.pop(0)
            keep.append(current)
            
            # Remove boxes with high IoU with current box | ç§»é™¤ä¸å½“å‰æ¡†IoUè¾ƒé«˜çš„æ¡†
            remaining = []
            for idx in indices:
                iou, _, _ = calculate_iou(bboxes[current], bboxes[idx])
                if iou < iou_threshold:
                    remaining.append(idx)
            indices = remaining
        
        return keep
    
    # Demonstrate operations | æ¼”ç¤ºæ“ä½œ
    print("\nğŸ“Š Bounding Box Operations Examples:")
    
    # Example bounding boxes | ç¤ºä¾‹è¾¹ç•Œæ¡†
    bbox1 = [100, 100, 150, 100]  # (x, y, w, h)
    bbox2 = [125, 125, 150, 100]  # Overlapping box | é‡å çš„æ¡†
    bbox3 = [300, 300, 100, 80]   # Non-overlapping box | ä¸é‡å çš„æ¡†
    
    print(f"Box 1: {bbox1}, Area: {calculate_area(bbox1)}")
    print(f"Box 2: {bbox2}, Area: {calculate_area(bbox2)}")
    print(f"Box 3: {bbox3}, Area: {calculate_area(bbox3)}")
    
    # Calculate IoU | è®¡ç®—IoU
    iou_12, inter_12, union_12 = calculate_iou(bbox1, bbox2)
    iou_13, inter_13, union_13 = calculate_iou(bbox1, bbox3)
    
    print(f"\nIoU between Box 1 and Box 2: {iou_12:.3f}")
    print(f"  Intersection: {inter_12}, Union: {union_12}")
    print(f"IoU between Box 1 and Box 3: {iou_13:.3f}")
    print(f"  Intersection: {inter_13}, Union: {union_13}")
    
    # Non-Maximum Suppression example | éæå¤§å€¼æŠ‘åˆ¶ç¤ºä¾‹
    bboxes = [bbox1, bbox2, bbox3]
    scores = [0.9, 0.8, 0.95]  # Confidence scores | ç½®ä¿¡åº¦åˆ†æ•°
    
    print(f"\nBefore NMS: {len(bboxes)} boxes")
    print(f"Scores: {scores}")
    
    keep_indices = non_max_suppression_simple(bboxes, scores, iou_threshold=0.3)
    print(f"After NMS: {len(keep_indices)} boxes kept")
    print(f"Kept indices: {keep_indices}")

# Run bounding box demonstrations | è¿è¡Œè¾¹ç•Œæ¡†æ¼”ç¤º
bbox_formats = demonstrate_bounding_boxes()
convert_bbox_formats()
demonstrate_bbox_operations()
```

### 11.3.3 Intersection over Union (IoU) | äº¤å¹¶æ¯”

IoU is a fundamental metric in object detection that measures how well a predicted bounding box matches the ground truth.

IoUæ˜¯ç›®æ ‡æ£€æµ‹ä¸­çš„åŸºæœ¬æŒ‡æ ‡ï¼Œè¡¡é‡é¢„æµ‹è¾¹ç•Œæ¡†ä¸çœŸå®æ ‡æ³¨çš„åŒ¹é…ç¨‹åº¦ã€‚

#### Understanding IoU | ç†è§£IoU

```python
def deep_dive_iou():
    """
    Deep dive into IoU calculation and its importance
    æ·±å…¥äº†è§£IoUè®¡ç®—åŠå…¶é‡è¦æ€§
    """
    print("=== Intersection over Union (IoU) Deep Dive | äº¤å¹¶æ¯”æ·±å…¥æ¢è®¨ ===")
    
    print("\nğŸ“ IoU Formula:")
    print("   IoU = Area of Intersection / Area of Union")
    print("   IoU = |A âˆ© B| / |A âˆª B|")
    print("   Range: [0, 1], where 1 means perfect overlap")
    
    def visualize_iou_scenarios():
        """Demonstrate different IoU scenarios | æ¼”ç¤ºä¸åŒçš„IoUåœºæ™¯"""
        
        scenarios = [
            {
                "name": "Perfect Match | å®Œç¾åŒ¹é…",
                "bbox1": [100, 100, 100, 100],
                "bbox2": [100, 100, 100, 100],
                "expected_iou": 1.0
            },
            {
                "name": "Good Overlap | è‰¯å¥½é‡å ", 
                "bbox1": [100, 100, 100, 100],
                "bbox2": [120, 120, 100, 100],
                "expected_iou": 0.64  # Approximate
            },
            {
                "name": "Poor Overlap | å·®é‡å ",
                "bbox1": [100, 100, 100, 100], 
                "bbox2": [150, 150, 100, 100],
                "expected_iou": 0.25  # Approximate
            },
            {
                "name": "No Overlap | æ— é‡å ",
                "bbox1": [100, 100, 100, 100],
                "bbox2": [250, 250, 100, 100],
                "expected_iou": 0.0
            }
        ]
        
        for scenario in scenarios:
            bbox1 = scenario["bbox1"]
            bbox2 = scenario["bbox2"]
            
            # Calculate actual IoU | è®¡ç®—å®é™…IoU
            def calc_iou_detailed(box1, box2):
                x1_1, y1_1, w1, h1 = box1
                x1_2, y1_2, w2, h2 = box2
                
                x2_1, y2_1 = x1_1 + w1, y1_1 + h1
                x2_2, y2_2 = x1_2 + w2, y1_2 + h2
                
                # Intersection | äº¤é›†
                x1_i = max(x1_1, x1_2)
                y1_i = max(y1_1, y1_2)
                x2_i = min(x2_1, x2_2)
                y2_i = min(y2_1, y2_2)
                
                if x1_i >= x2_i or y1_i >= y2_i:
                    intersection = 0
                else:
                    intersection = (x2_i - x1_i) * (y2_i - y1_i)
                
                # Union | å¹¶é›†
                area1 = w1 * h1
                area2 = w2 * h2
                union = area1 + area2 - intersection
                
                iou = intersection / union if union > 0 else 0
                
                return iou, intersection, area1, area2, union
            
            iou, intersection, area1, area2, union = calc_iou_detailed(bbox1, bbox2)
            
            print(f"\n{scenario['name']}:")
            print(f"  Box 1: {bbox1} (area: {area1})")
            print(f"  Box 2: {bbox2} (area: {area2})")
            print(f"  Intersection: {intersection}")
            print(f"  Union: {union}")
            print(f"  IoU: {iou:.3f}")
    
    # IoU thresholds in practice | å®è·µä¸­çš„IoUé˜ˆå€¼
    def iou_thresholds_guide():
        """Guide to IoU thresholds used in practice | å®è·µä¸­ä½¿ç”¨çš„IoUé˜ˆå€¼æŒ‡å—"""
        
        print("\nğŸ¯ IoU Thresholds in Practice | å®è·µä¸­çš„IoUé˜ˆå€¼:")
        
        thresholds = [
            (0.5, "Common detection threshold | å¸¸è§æ£€æµ‹é˜ˆå€¼", "COCO 'loose' standard"),
            (0.7, "Strict detection threshold | ä¸¥æ ¼æ£€æµ‹é˜ˆå€¼", "High precision requirement"),
            (0.3, "NMS threshold | NMSé˜ˆå€¼", "Non-maximum suppression"),
            (0.9, "Very strict threshold | éå¸¸ä¸¥æ ¼é˜ˆå€¼", "Fine-grained localization")
        ]
        
        for threshold, description, usage in thresholds:
            print(f"  IoU â‰¥ {threshold}: {description}")
            print(f"    Usage: {usage}")
    
    # IoU-based metrics | åŸºäºIoUçš„æŒ‡æ ‡
    def iou_based_metrics():
        """Explain IoU-based evaluation metrics | è§£é‡ŠåŸºäºIoUçš„è¯„ä¼°æŒ‡æ ‡"""
        
        print("\nğŸ“Š IoU-based Evaluation Metrics | åŸºäºIoUçš„è¯„ä¼°æŒ‡æ ‡:")
        
        print("\n1. Average Precision (AP) | å¹³å‡ç²¾åº¦:")
        print("   - AP@0.5: Average precision at IoU threshold 0.5")
        print("   - AP@0.75: Average precision at IoU threshold 0.75") 
        print("   - AP@[0.5:0.95]: Average over IoU thresholds 0.5 to 0.95")
        
        print("\n2. Mean Average Precision (mAP) | å¹³å‡ç²¾åº¦å‡å€¼:")
        print("   - Average of AP across all classes")
        print("   - Standard metric for object detection evaluation")
        print("   - æ‰€æœ‰ç±»åˆ«APçš„å¹³å‡å€¼")
        print("   - ç›®æ ‡æ£€æµ‹è¯„ä¼°çš„æ ‡å‡†æŒ‡æ ‡")
        
        print("\n3. True/False Positives | çœŸ/å‡æ­£ä¾‹:")
        print("   - True Positive: IoU â‰¥ threshold")
        print("   - False Positive: IoU < threshold") 
        print("   - False Negative: Missed ground truth object")
        print("   - çœŸæ­£ä¾‹ï¼šIoU â‰¥ é˜ˆå€¼")
        print("   - å‡æ­£ä¾‹ï¼šIoU < é˜ˆå€¼")
        print("   - å‡è´Ÿä¾‹ï¼šæ¼æ£€çš„çœŸå®ç›®æ ‡")
    
    # Run demonstrations | è¿è¡Œæ¼”ç¤º
    visualize_iou_scenarios()
    iou_thresholds_guide()
    iou_based_metrics()

deep_dive_iou()
```

### 11.3.4 Non-Maximum Suppression (NMS) | éæå¤§å€¼æŠ‘åˆ¶

When object detection models generate multiple overlapping bounding boxes for the same object, NMS helps remove redundant detections by keeping only the best ones.

å½“ç›®æ ‡æ£€æµ‹æ¨¡å‹ä¸ºåŒä¸€ç‰©ä½“ç”Ÿæˆå¤šä¸ªé‡å è¾¹ç•Œæ¡†æ—¶ï¼ŒNMSé€šè¿‡åªä¿ç•™æœ€ä½³çš„æ¡†æ¥å¸®åŠ©ç§»é™¤å†—ä½™æ£€æµ‹ã€‚

```python
def comprehensive_nms_demo():
    """
    Comprehensive demonstration of Non-Maximum Suppression
    éæå¤§å€¼æŠ‘åˆ¶çš„ç»¼åˆæ¼”ç¤º
    """
    print("=== Non-Maximum Suppression (NMS) | éæå¤§å€¼æŠ‘åˆ¶ ===")
    
    print("\nğŸ¯ Problem: Multiple detections for same object")
    print("   Object detection models often generate multiple boxes for one object")
    print("   These boxes have different confidence scores and slight position differences")
    print("   NMS helps select the best box and remove redundant ones")
    print("\n   é—®é¢˜ï¼šåŒä¸€ç‰©ä½“çš„å¤šä¸ªæ£€æµ‹ç»“æœ")
    print("   ç›®æ ‡æ£€æµ‹æ¨¡å‹ç»å¸¸ä¸ºä¸€ä¸ªç‰©ä½“ç”Ÿæˆå¤šä¸ªæ¡†")
    print("   è¿™äº›æ¡†æœ‰ä¸åŒçš„ç½®ä¿¡åº¦åˆ†æ•°å’Œè½»å¾®çš„ä½ç½®å·®å¼‚")
    print("   NMSå¸®åŠ©é€‰æ‹©æœ€ä½³æ¡†å¹¶ç§»é™¤å†—ä½™æ¡†")
    
    def classic_nms(boxes, scores, iou_threshold=0.5, score_threshold=0.1):
        """
        Classic Non-Maximum Suppression algorithm
        ç»å…¸éæå¤§å€¼æŠ‘åˆ¶ç®—æ³•
        """
        # Filter out low-confidence boxes | è¿‡æ»¤ä½ç½®ä¿¡åº¦æ¡†
        valid_indices = [i for i, score in enumerate(scores) if score >= score_threshold]
        boxes = [boxes[i] for i in valid_indices]
        scores = [scores[i] for i in valid_indices]
        
        if not boxes:
            return [], []
        
        # Sort by confidence scores in descending order | æŒ‰ç½®ä¿¡åº¦åˆ†æ•°é™åºæ’åˆ—
        sorted_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)
        
        keep_boxes = []
        keep_scores = []
        
        while sorted_indices:
            # Keep the box with highest confidence | ä¿ç•™ç½®ä¿¡åº¦æœ€é«˜çš„æ¡†
            current = sorted_indices.pop(0)
            current_box = boxes[current]
            current_score = scores[current]
            
            keep_boxes.append(current_box)
            keep_scores.append(current_score)
            
            # Remove boxes with high IoU with current box | ç§»é™¤ä¸å½“å‰æ¡†IoUé«˜çš„æ¡†
            remaining_indices = []
            for idx in sorted_indices:
                candidate_box = boxes[idx]
                
                # Calculate IoU | è®¡ç®—IoU
                iou = calculate_bbox_iou(current_box, candidate_box)
                
                # Keep if IoU is below threshold | å¦‚æœIoUä½äºé˜ˆå€¼åˆ™ä¿ç•™
                if iou < iou_threshold:
                    remaining_indices.append(idx)
            
            sorted_indices = remaining_indices
        
        return keep_boxes, keep_scores
    
    def calculate_bbox_iou(box1, box2):
        """Calculate IoU between two bounding boxes | è®¡ç®—ä¸¤ä¸ªè¾¹ç•Œæ¡†çš„IoU"""
        x1_1, y1_1, w1, h1 = box1
        x1_2, y1_2, w2, h2 = box2
        
        x2_1, y2_1 = x1_1 + w1, y1_1 + h1
        x2_2, y2_2 = x1_2 + w2, y1_2 + h2
        
        # Intersection coordinates | äº¤é›†åæ ‡
        x1_i = max(x1_1, x1_2)
        y1_i = max(y1_1, y1_2)
        x2_i = min(x2_1, x2_2)
        y2_i = min(y2_1, y2_2)
        
        if x1_i >= x2_i or y1_i >= y2_i:
            intersection = 0
        else:
            intersection = (x2_i - x1_i) * (y2_i - y1_i)
        
        # Areas | é¢ç§¯
        area1 = w1 * h1
        area2 = w2 * h2
        union = area1 + area2 - intersection
        
        return intersection / union if union > 0 else 0
    
    def demonstrate_nms_scenarios():
        """Demonstrate different NMS scenarios | æ¼”ç¤ºä¸åŒçš„NMSåœºæ™¯"""
        
        print("\nğŸ“‹ NMS Demonstration Scenarios | NMSæ¼”ç¤ºåœºæ™¯:")
        
        # Scenario 1: Multiple detections of same object | åœºæ™¯1ï¼šåŒä¸€ç‰©ä½“çš„å¤šä¸ªæ£€æµ‹
        print("\n1. Same Object - Multiple Detections | åŒä¸€ç‰©ä½“å¤šä¸ªæ£€æµ‹:")
        same_object_boxes = [
            [100, 100, 120, 80],   # Main detection | ä¸»è¦æ£€æµ‹
            [105, 105, 115, 75],   # Slightly shifted | è½»å¾®åç§»
            [95, 95, 125, 85],     # Slightly larger | ç¨å¤§ä¸€äº›
            [110, 110, 110, 70]    # Another detection | å¦ä¸€ä¸ªæ£€æµ‹
        ]
        same_object_scores = [0.95, 0.87, 0.92, 0.83]
        
        print(f"   Before NMS: {len(same_object_boxes)} boxes")
        for i, (box, score) in enumerate(zip(same_object_boxes, same_object_scores)):
            print(f"     Box {i+1}: {box}, Score: {score:.2f}")
        
        kept_boxes, kept_scores = classic_nms(same_object_boxes, same_object_scores, 
                                             iou_threshold=0.3)
        
        print(f"   After NMS: {len(kept_boxes)} boxes")
        for i, (box, score) in enumerate(zip(kept_boxes, kept_scores)):
            print(f"     Kept Box {i+1}: {box}, Score: {score:.2f}")
        
        # Scenario 2: Multiple different objects | åœºæ™¯2ï¼šå¤šä¸ªä¸åŒç‰©ä½“
        print("\n2. Different Objects - Should Keep All | ä¸åŒç‰©ä½“åº”å…¨éƒ¨ä¿ç•™:")
        different_objects_boxes = [
            [50, 50, 100, 80],     # Object 1 | ç‰©ä½“1
            [200, 200, 90, 70],    # Object 2 | ç‰©ä½“2
            [350, 100, 110, 90]    # Object 3 | ç‰©ä½“3
        ]
        different_objects_scores = [0.88, 0.91, 0.85]
        
        print(f"   Before NMS: {len(different_objects_boxes)} boxes")
        kept_boxes_2, kept_scores_2 = classic_nms(different_objects_boxes, 
                                                  different_objects_scores,
                                                  iou_threshold=0.3)
        print(f"   After NMS: {len(kept_boxes_2)} boxes (should be same)")
        
        # Scenario 3: Effect of different IoU thresholds | åœºæ™¯3ï¼šä¸åŒIoUé˜ˆå€¼çš„å½±å“
        print("\n3. IoU Threshold Effects | IoUé˜ˆå€¼å½±å“:")
        test_boxes = [
            [100, 100, 100, 100],  # Reference box | å‚è€ƒæ¡†
            [120, 120, 100, 100],  # Moderate overlap | ä¸­ç­‰é‡å 
            [140, 140, 100, 100]   # Less overlap | è¾ƒå°‘é‡å 
        ]
        test_scores = [0.9, 0.8, 0.7]
        
        for threshold in [0.1, 0.3, 0.5, 0.7]:
            kept, _ = classic_nms(test_boxes, test_scores, iou_threshold=threshold)
            print(f"   IoU threshold {threshold}: {len(kept)} boxes kept")
    
    def advanced_nms_variants():
        """Discuss advanced NMS variants | è®¨è®ºé«˜çº§NMSå˜ä½“"""
        
        print("\nğŸš€ Advanced NMS Variants | é«˜çº§NMSå˜ä½“:")
        
        print("\n1. Soft-NMS | è½¯NMS:")
        print("   - Instead of removing boxes, reduces their scores")
        print("   - Uses Gaussian decay based on IoU")
        print("   - Better for overlapping objects")
        print("   - ä¸æ˜¯ç§»é™¤æ¡†ï¼Œè€Œæ˜¯é™ä½å®ƒä»¬çš„åˆ†æ•°")
        print("   - ä½¿ç”¨åŸºäºIoUçš„é«˜æ–¯è¡°å‡")
        print("   - å¯¹é‡å ç‰©ä½“æ›´å¥½")
        
        print("\n2. Fast NMS | å¿«é€ŸNMS:")
        print("   - Parallel implementation for speed")
        print("   - Matrix operations instead of loops")
        print("   - Trade-off between speed and accuracy")
        print("   - å¹¶è¡Œå®ç°ä»¥æé«˜é€Ÿåº¦")
        print("   - ä½¿ç”¨çŸ©é˜µæ“ä½œè€Œéå¾ªç¯")
        print("   - é€Ÿåº¦å’Œç²¾åº¦ä¹‹é—´çš„æƒè¡¡")
        
        print("\n3. Class-Agnostic vs Class-Specific NMS:")
        print("   - Class-Agnostic: NMS across all classes")
        print("   - Class-Specific: NMS within each class separately")
        print("   - ç±»åˆ«æ— å…³ï¼šè·¨æ‰€æœ‰ç±»åˆ«çš„NMS")
        print("   - ç±»åˆ«ç‰¹å®šï¼šæ¯ä¸ªç±»åˆ«å†…åˆ†åˆ«è¿›è¡ŒNMS")
    
    def nms_best_practices():
        """NMS best practices | NMSæœ€ä½³å®è·µ"""
        
        print("\nğŸ’¡ NMS Best Practices | NMSæœ€ä½³å®è·µ:")
        
        print("\n1. Threshold Selection | é˜ˆå€¼é€‰æ‹©:")
        print("   - IoU threshold: 0.3-0.5 for general objects")
        print("   - Lower for overlapping objects (crowds)")
        print("   - Higher for well-separated objects")
        print("   - IoUé˜ˆå€¼ï¼šä¸€èˆ¬ç‰©ä½“0.3-0.5")
        print("   - é‡å ç‰©ä½“ï¼ˆäººç¾¤ï¼‰ä½¿ç”¨æ›´ä½é˜ˆå€¼")
        print("   - åˆ†ç¦»è‰¯å¥½çš„ç‰©ä½“ä½¿ç”¨æ›´é«˜é˜ˆå€¼")
        
        print("\n2. Score Threshold | åˆ†æ•°é˜ˆå€¼:")
        print("   - Pre-filter low-confidence detections")
        print("   - Reduces computation and false positives")
        print("   - Typical range: 0.05-0.3")
        print("   - é¢„è¿‡æ»¤ä½ç½®ä¿¡åº¦æ£€æµ‹")
        print("   - å‡å°‘è®¡ç®—é‡å’Œå‡æ­£ä¾‹")
        print("   - å…¸å‹èŒƒå›´ï¼š0.05-0.3")
        
        print("\n3. Implementation Tips | å®ç°æŠ€å·§:")
        print("   - Sort by confidence score first")
        print("   - Use vectorized operations when possible")
        print("   - Consider batch processing for multiple images")
        print("   - é¦–å…ˆæŒ‰ç½®ä¿¡åº¦åˆ†æ•°æ’åº")
        print("   - å°½å¯èƒ½ä½¿ç”¨å‘é‡åŒ–æ“ä½œ")
        print("   - è€ƒè™‘å¤šå›¾åƒçš„æ‰¹å¤„ç†")
    
    # Run all demonstrations | è¿è¡Œæ‰€æœ‰æ¼”ç¤º
    demonstrate_nms_scenarios()
    advanced_nms_variants()
    nms_best_practices()

comprehensive_nms_demo()
```

## Summary | æ€»ç»“

This chapter introduced the fundamental concepts of computer vision, focusing on practical techniques that form the backbone of modern computer vision systems.

æœ¬ç« ä»‹ç»äº†è®¡ç®—æœºè§†è§‰çš„åŸºæœ¬æ¦‚å¿µï¼Œé‡ç‚¹å…³æ³¨æ„æˆç°ä»£è®¡ç®—æœºè§†è§‰ç³»ç»Ÿéª¨å¹²çš„å®ç”¨æŠ€æœ¯ã€‚

### Key Takeaways | å…³é”®è¦ç‚¹

1. **Image Augmentation | å›¾åƒå¢å¼º**
   - Essential for creating diverse training data from limited datasets
   - Includes geometric, color, and noise-based transformations
   - Significantly improves model generalization and robustness
   - ä»æœ‰é™æ•°æ®é›†åˆ›å»ºå¤šæ ·åŒ–è®­ç»ƒæ•°æ®çš„å¿…è¦æŠ€æœ¯
   - åŒ…æ‹¬å‡ ä½•ã€é¢œè‰²å’ŒåŸºäºå™ªå£°çš„å˜æ¢
   - æ˜¾è‘—æé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§

2. **Fine-Tuning | å¾®è°ƒ**
   - Leverages pre-trained models to solve specific tasks efficiently
   - Uses transfer learning to reduce training time and data requirements
   - Different strategies: feature extraction, full fine-tuning, gradual unfreezing
   - åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹é«˜æ•ˆè§£å†³ç‰¹å®šä»»åŠ¡
   - ä½¿ç”¨è¿ç§»å­¦ä¹ å‡å°‘è®­ç»ƒæ—¶é—´å’Œæ•°æ®éœ€æ±‚
   - ä¸åŒç­–ç•¥ï¼šç‰¹å¾æå–ã€å…¨é¢å¾®è°ƒã€æ¸è¿›è§£å†»

3. **Object Detection Fundamentals | ç›®æ ‡æ£€æµ‹åŸºç¡€**
   - Combines classification and localization tasks
   - Bounding boxes represent object locations using various formats
   - IoU measures overlap quality between predicted and ground truth boxes
   - NMS removes redundant detections to produce clean results
   - ç»“åˆåˆ†ç±»å’Œå®šä½ä»»åŠ¡
   - è¾¹ç•Œæ¡†ä½¿ç”¨å„ç§æ ¼å¼è¡¨ç¤ºç‰©ä½“ä½ç½®
   - IoUè¡¡é‡é¢„æµ‹æ¡†ä¸çœŸå®æ¡†çš„é‡å è´¨é‡
   - NMSç§»é™¤å†—ä½™æ£€æµ‹ä»¥äº§ç”Ÿæ¸…æ´ç»“æœ

### Practical Applications | å®é™…åº”ç”¨

These techniques are widely used in:
è¿™äº›æŠ€æœ¯å¹¿æ³›åº”ç”¨äºï¼š

- **Autonomous vehicles** for object detection and recognition | **è‡ªåŠ¨é©¾é©¶è½¦è¾†**çš„ç‰©ä½“æ£€æµ‹å’Œè¯†åˆ«
- **Medical imaging** for disease diagnosis and analysis | **åŒ»å­¦å½±åƒ**çš„ç–¾ç—…è¯Šæ–­å’Œåˆ†æ
- **Surveillance systems** for security and monitoring | **ç›‘æ§ç³»ç»Ÿ**çš„å®‰å…¨å’Œç›‘æ§
- **Retail applications** for product recognition and inventory | **é›¶å”®åº”ç”¨**çš„äº§å“è¯†åˆ«å’Œåº“å­˜
- **Social media** for content moderation and tagging | **ç¤¾äº¤åª’ä½“**çš„å†…å®¹å®¡æ ¸å’Œæ ‡è®°

### Next Steps | ä¸‹ä¸€æ­¥

Building on these fundamentals, advanced computer vision topics include:
åœ¨è¿™äº›åŸºç¡€ä¸Šï¼Œé«˜çº§è®¡ç®—æœºè§†è§‰ä¸»é¢˜åŒ…æ‹¬ï¼š

- Modern object detection architectures (YOLO, R-CNN family, etc.)
- Instance and semantic segmentation
- Object tracking and video analysis  
- 3D computer vision and depth estimation
- Generative models for image synthesis

- ç°ä»£ç›®æ ‡æ£€æµ‹æ¶æ„ï¼ˆYOLOã€R-CNNç³»åˆ—ç­‰ï¼‰
- å®ä¾‹å’Œè¯­ä¹‰åˆ†å‰²
- ç‰©ä½“è·Ÿè¸ªå’Œè§†é¢‘åˆ†æ
- 3Dè®¡ç®—æœºè§†è§‰å’Œæ·±åº¦ä¼°è®¡
- å›¾åƒåˆæˆçš„ç”Ÿæˆæ¨¡å‹

Understanding these foundational concepts provides a solid base for exploring more advanced computer vision techniques and applications.

ç†è§£è¿™äº›åŸºç¡€æ¦‚å¿µä¸ºæ¢ç´¢æ›´é«˜çº§çš„è®¡ç®—æœºè§†è§‰æŠ€æœ¯å’Œåº”ç”¨æä¾›äº†åšå®çš„åŸºç¡€ã€‚ 