# Chapter 10: Computational Performance | ç¬¬åç« ï¼šè®¡ç®—æ€§èƒ½

## Chapter Overview | ç« èŠ‚æ¦‚è¿°

Computational performance is a crucial aspect of deep learning practice. As model complexity increases and data scales expand, efficiently training and deploying deep learning models becomes increasingly important.

è®¡ç®—æ€§èƒ½æ˜¯æ·±åº¦å­¦ä¹ å®è·µä¸­è‡³å…³é‡è¦çš„ä¸€ç¯ã€‚éšç€æ¨¡å‹å¤æ‚åº¦çš„å¢åŠ å’Œæ•°æ®è§„æ¨¡çš„æ‰©å¤§ï¼Œå¦‚ä½•é«˜æ•ˆåœ°è®­ç»ƒå’Œéƒ¨ç½²æ·±åº¦å­¦ä¹ æ¨¡å‹å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚

This chapter will delve into computational performance optimization in deep learning, covering key technologies including compilers and interpreters, asynchronous computation, automatic parallelism, hardware understanding, and multi-GPU training.

æœ¬ç« å°†æ·±å…¥æ¢è®¨æ·±åº¦å­¦ä¹ ä¸­çš„è®¡ç®—æ€§èƒ½ä¼˜åŒ–ï¼ŒåŒ…æ‹¬ç¼–è¯‘å™¨ä¸è§£é‡Šå™¨ã€å¼‚æ­¥è®¡ç®—ã€è‡ªåŠ¨å¹¶è¡ŒåŒ–ã€ç¡¬ä»¶ç†è§£ä»¥åŠå¤šGPUè®­ç»ƒç­‰å…³é”®æŠ€æœ¯ã€‚

---

## 10.1 Compilers and Interpreters | ç¼–è¯‘å™¨ä¸è§£é‡Šå™¨

### 10.1.1 Symbolic Programming | ç¬¦å·å¼ç¼–ç¨‹

In deep learning, we can execute computations in two fundamental ways: **imperative programming** and **symbolic programming**. Understanding the difference between these approaches is crucial for optimizing computational performance.

åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ä¸¤ç§åŸºæœ¬æ–¹å¼æ¥æ‰§è¡Œè®¡ç®—ï¼š**å‘½ä»¤å¼ç¼–ç¨‹**å’Œ**ç¬¦å·å¼ç¼–ç¨‹**ã€‚ç†è§£è¿™ä¸¤ç§æ–¹æ³•çš„åŒºåˆ«å¯¹äºä¼˜åŒ–è®¡ç®—æ€§èƒ½è‡³å…³é‡è¦ã€‚

#### Imperative Programming | å‘½ä»¤å¼ç¼–ç¨‹

Imperative programming executes code line by line, similar to how we write regular Python code. Each operation is executed immediately when encountered, and results are available right away.

å‘½ä»¤å¼ç¼–ç¨‹é€è¡Œæ‰§è¡Œä»£ç ï¼Œç±»ä¼¼äºæˆ‘ä»¬ç¼–å†™å¸¸è§„Pythonä»£ç çš„æ–¹å¼ã€‚æ¯ä¸ªæ“ä½œåœ¨é‡åˆ°æ—¶ç«‹å³æ‰§è¡Œï¼Œç»“æœç«‹å³å¯ç”¨ã€‚

**Characteristics | ç‰¹ç‚¹:**
- Immediate execution | ç«‹å³æ‰§è¡Œ
- Easy debugging | æ˜“äºè°ƒè¯•
- Natural programming flow | è‡ªç„¶çš„ç¼–ç¨‹æµç¨‹
- Interactive development | äº¤äº’å¼å¼€å‘

```python
# Imperative Programming Example | å‘½ä»¤å¼ç¼–ç¨‹ç¤ºä¾‹
import torch

# Each operation executes immediately | æ¯ä¸ªæ“ä½œç«‹å³æ‰§è¡Œ
a = torch.tensor([1, 2, 3])  # Creates tensor right away | ç«‹å³åˆ›å»ºå¼ é‡
b = torch.tensor([4, 5, 6])  # Creates another tensor | åˆ›å»ºå¦ä¸€ä¸ªå¼ é‡
c = a + b                    # Addition happens now | åŠ æ³•ç°åœ¨æ‰§è¡Œ
print(c)                     # Output: [5, 7, 9] | è¾“å‡º: [5, 7, 9]

# You can inspect intermediate results | å¯ä»¥æ£€æŸ¥ä¸­é—´ç»“æœ
print(f"a = {a}")
print(f"b = {b}")
print(f"c = a + b = {c}")
```

#### Symbolic Programming | ç¬¦å·å¼ç¼–ç¨‹

Symbolic programming first defines a computational graph that describes the operations to be performed, then executes the entire graph at once. This is like writing a recipe before cooking - you plan all steps first, then execute them.

ç¬¦å·å¼ç¼–ç¨‹é¦–å…ˆå®šä¹‰ä¸€ä¸ªæè¿°è¦æ‰§è¡Œæ“ä½œçš„è®¡ç®—å›¾ï¼Œç„¶åä¸€æ¬¡æ€§æ‰§è¡Œæ•´ä¸ªå›¾ã€‚è¿™å°±åƒåšèœå‰å…ˆå†™é£Ÿè°±â€”â€”å…ˆè®¡åˆ’æ‰€æœ‰æ­¥éª¤ï¼Œç„¶åæ‰§è¡Œå®ƒä»¬ã€‚

**Characteristics | ç‰¹ç‚¹:**
- Deferred execution | å»¶è¿Ÿæ‰§è¡Œ
- Global optimization opportunities | å…¨å±€ä¼˜åŒ–æœºä¼š
- Better performance potential | æ›´å¥½çš„æ€§èƒ½æ½œåŠ›
- More complex debugging | è°ƒè¯•æ›´å¤æ‚

```python
# Symbolic Programming Concept Example | ç¬¦å·å¼ç¼–ç¨‹æ¦‚å¿µç¤ºä¾‹
# Note: This is pseudocode to illustrate the concept | æ³¨æ„ï¼šè¿™æ˜¯ä¼ªä»£ç æ¥è¯´æ˜æ¦‚å¿µ

# Step 1: Define computational graph | æ­¥éª¤1ï¼šå®šä¹‰è®¡ç®—å›¾
# f(x, y) = x + y
def symbolic_add(x, y):
    # This doesn't execute immediately | è¿™ä¸ä¼šç«‹å³æ‰§è¡Œ
    return GraphNode("add", inputs=[x, y])

# Step 2: Build the graph | æ­¥éª¤2ï¼šæ„å»ºå›¾
x = Variable("x")
y = Variable("y")
result = symbolic_add(x, y)

# Step 3: Compile and optimize | æ­¥éª¤3ï¼šç¼–è¯‘å’Œä¼˜åŒ–
compiled_graph = compile(result)

# Step 4: Execute with actual values | æ­¥éª¤4ï¼šä½¿ç”¨å®é™…å€¼æ‰§è¡Œ
output = compiled_graph.execute(x=torch.tensor([1, 2, 3]), 
                               y=torch.tensor([4, 5, 6]))
```

#### Analogy | ç±»æ¯”

Think of the difference this way:

è¿™æ ·æƒ³è±¡ä¸¤è€…çš„åŒºåˆ«ï¼š

- **Imperative programming** is like live television broadcasting - everything happens in real-time as you speak
- **Symbolic programming** is like recording a TV show - you write the script first, edit and optimize it, then broadcast the final version

- **å‘½ä»¤å¼ç¼–ç¨‹**åƒç°åœºç”µè§†ç›´æ’­â€”â€”å½“ä½ è¯´è¯æ—¶ä¸€åˆ‡éƒ½åœ¨å®æ—¶å‘ç”Ÿ
- **ç¬¦å·å¼ç¼–ç¨‹**åƒå½•åˆ¶ç”µè§†èŠ‚ç›®â€”â€”å…ˆå†™è„šæœ¬ï¼Œç¼–è¾‘ä¼˜åŒ–ï¼Œç„¶åæ’­æ”¾æœ€ç»ˆç‰ˆæœ¬

### 10.1.2 Hybrid Programming | æ··åˆå¼ç¼–ç¨‹

Modern deep learning frameworks typically use **hybrid programming**, which combines the advantages of both imperative and symbolic approaches. This gives developers the flexibility of imperative programming during development while providing the performance benefits of symbolic programming during deployment.

ç°ä»£æ·±åº¦å­¦ä¹ æ¡†æ¶é€šå¸¸ä½¿ç”¨**æ··åˆå¼ç¼–ç¨‹**ï¼Œå®ƒç»“åˆäº†å‘½ä»¤å¼å’Œç¬¦å·å¼æ–¹æ³•çš„ä¼˜åŠ¿ã€‚è¿™ä¸ºå¼€å‘äººå‘˜åœ¨å¼€å‘è¿‡ç¨‹ä¸­æä¾›äº†å‘½ä»¤å¼ç¼–ç¨‹çš„çµæ´»æ€§ï¼ŒåŒæ—¶åœ¨éƒ¨ç½²æœŸé—´æä¾›äº†ç¬¦å·å¼ç¼–ç¨‹çš„æ€§èƒ½ä¼˜åŠ¿ã€‚

#### PyTorch's TorchScript | PyTorchçš„TorchScript

TorchScript is PyTorch's approach to hybrid programming. It allows you to seamlessly transition from imperative mode (for development and debugging) to symbolic mode (for production deployment).

TorchScriptæ˜¯PyTorchçš„æ··åˆç¼–ç¨‹æ–¹æ³•ã€‚å®ƒå…è®¸æ‚¨ä»å‘½ä»¤å¼æ¨¡å¼ï¼ˆç”¨äºå¼€å‘å’Œè°ƒè¯•ï¼‰æ— ç¼è¿‡æ¸¡åˆ°ç¬¦å·å¼æ¨¡å¼ï¼ˆç”¨äºç”Ÿäº§éƒ¨ç½²ï¼‰ã€‚

```python
import torch
import torch.nn as nn
import time

class SimpleNet(nn.Module):
    """A simple neural network for demonstration | ç”¨äºæ¼”ç¤ºçš„ç®€å•ç¥ç»ç½‘ç»œ"""
    
    def __init__(self):
        super().__init__()
        self.linear1 = nn.Linear(10, 5)
        self.relu = nn.ReLU()
        self.linear2 = nn.Linear(5, 1)
    
    def forward(self, x):
        """Forward pass through the network | é€šè¿‡ç½‘ç»œçš„å‰å‘ä¼ æ’­"""
        x = self.linear1(x)
        x = self.relu(x)
        x = self.linear2(x)
        return x

# Create model and sample input | åˆ›å»ºæ¨¡å‹å’Œæ ·æœ¬è¾“å…¥
model = SimpleNet()
sample_input = torch.randn(1, 10)

# Imperative mode (eager execution) | å‘½ä»¤å¼æ¨¡å¼ï¼ˆæ€¥åˆ‡æ‰§è¡Œï¼‰
print("=== Imperative Mode | å‘½ä»¤å¼æ¨¡å¼ ===")
output_eager = model(sample_input)  # Executes immediately | ç«‹å³æ‰§è¡Œ
print(f"Eager output: {output_eager}")

# Symbolic mode (scripted execution) | ç¬¦å·å¼æ¨¡å¼ï¼ˆè„šæœ¬æ‰§è¡Œï¼‰
print("\n=== Symbolic Mode | ç¬¦å·å¼æ¨¡å¼ ===")
scripted_model = torch.jit.script(model)  # Compile to computational graph | ç¼–è¯‘ä¸ºè®¡ç®—å›¾
output_scripted = scripted_model(sample_input)  # Optimized execution | ä¼˜åŒ–æ‰§è¡Œ
print(f"Scripted output: {output_scripted}")

# Verify outputs are identical | éªŒè¯è¾“å‡ºç›¸åŒ
print(f"Outputs match: {torch.allclose(output_eager, output_scripted)}")
```

#### Advantages of Hybrid Programming | æ··åˆå¼ç¼–ç¨‹çš„ä¼˜åŠ¿

1. **Development Convenience | å¼€å‘ä¾¿åˆ©æ€§**
   - Use imperative mode for debugging and experimentation | ä½¿ç”¨å‘½ä»¤å¼æ¨¡å¼è¿›è¡Œè°ƒè¯•å’Œå®éªŒ
   - Use symbolic mode for deployment and production | ä½¿ç”¨ç¬¦å·å¼æ¨¡å¼è¿›è¡Œéƒ¨ç½²å’Œç”Ÿäº§

2. **Performance Optimization | æ€§èƒ½ä¼˜åŒ–**
   - Compilers can perform graph-level optimizations | ç¼–è¯‘å™¨å¯ä»¥æ‰§è¡Œå›¾çº§ä¼˜åŒ–
   - Operator fusion and memory optimization | ç®—å­èåˆå’Œå†…å­˜ä¼˜åŒ–
   - Better utilization of hardware accelerators | æ›´å¥½åœ°åˆ©ç”¨ç¡¬ä»¶åŠ é€Ÿå™¨

3. **Cross-platform Deployment | è·¨å¹³å°éƒ¨ç½²**
   - Symbolic graphs are easier to port to different platforms | ç¬¦å·å›¾æ›´å®¹æ˜“ç§»æ¤åˆ°ä¸åŒå¹³å°
   - Mobile and embedded deployment | ç§»åŠ¨å’ŒåµŒå…¥å¼éƒ¨ç½²
   - Hardware-specific optimizations | ç¡¬ä»¶ç‰¹å®šä¼˜åŒ–

### 10.1.3 Hybridizing the Sequential Class | åºåˆ—ç±»çš„æ··åˆåŒ–

Let's explore how to hybridize a simple sequential model and observe the performance benefits:

è®©æˆ‘ä»¬æ¢ç´¢å¦‚ä½•æ··åˆåŒ–ä¸€ä¸ªç®€å•çš„åºåˆ—æ¨¡å‹å¹¶è§‚å¯Ÿæ€§èƒ½ä¼˜åŠ¿ï¼š

```python
import torch
import torch.nn as nn
import time
import numpy as np

class OriginalNet(nn.Module):
    """Original model without hybridization | æœªæ··åˆåŒ–çš„åŸå§‹æ¨¡å‹"""
    
    def __init__(self, input_size=784, hidden_size=256, num_classes=10):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Linear(hidden_size // 2, num_classes)
        )
    
    def forward(self, x):
        return self.layers(x)

def create_hybrid_model(input_size=784, hidden_size=256, num_classes=10):
    """Create a hybridized version of the model | åˆ›å»ºæ¨¡å‹çš„æ··åˆåŒ–ç‰ˆæœ¬"""
    model = OriginalNet(input_size, hidden_size, num_classes)
    
    # Compile with TorchScript | ä½¿ç”¨TorchScriptç¼–è¯‘
    model_scripted = torch.jit.script(model)
    
    return model_scripted

def benchmark_models(num_warmup=10, num_iterations=100, batch_size=1000):
    """
    Benchmark performance between eager and scripted models
    å¯¹æ¯”æ€¥åˆ‡æ¨¡å¼å’Œè„šæœ¬æ¨¡å¼æ¨¡å‹çš„æ€§èƒ½
    """
    print("=== Model Performance Benchmark | æ¨¡å‹æ€§èƒ½åŸºå‡†æµ‹è¯• ===")
    
    # Create models | åˆ›å»ºæ¨¡å‹
    model_eager = OriginalNet()
    model_scripted = create_hybrid_model()
    
    # Create sample data | åˆ›å»ºæ ·æœ¬æ•°æ®
    sample_input = torch.randn(batch_size, 784)
    
    # Set models to evaluation mode | å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    model_eager.eval()
    model_scripted.eval()
    
    # Warmup runs | é¢„çƒ­è¿è¡Œ
    print(f"Warming up with {num_warmup} iterations...")
    with torch.no_grad():
        for _ in range(num_warmup):
            _ = model_eager(sample_input)
            _ = model_scripted(sample_input)
    
    # Benchmark eager mode | åŸºå‡†æµ‹è¯•æ€¥åˆ‡æ¨¡å¼
    print(f"\nBenchmarking eager mode with {num_iterations} iterations...")
    start_time = time.time()
    with torch.no_grad():
        for _ in range(num_iterations):
            _ = model_eager(sample_input)
    eager_time = time.time() - start_time
    
    # Benchmark scripted mode | åŸºå‡†æµ‹è¯•è„šæœ¬æ¨¡å¼
    print(f"Benchmarking scripted mode with {num_iterations} iterations...")
    start_time = time.time()
    with torch.no_grad():
        for _ in range(num_iterations):
            _ = model_scripted(sample_input)
    scripted_time = time.time() - start_time
    
    # Calculate and display results | è®¡ç®—å¹¶æ˜¾ç¤ºç»“æœ
    speedup = eager_time / scripted_time
    print(f"\n=== Results | ç»“æœ ===")
    print(f"Eager mode time | æ€¥åˆ‡æ¨¡å¼æ—¶é—´: {eager_time:.4f}s")
    print(f"Scripted mode time | è„šæœ¬æ¨¡å¼æ—¶é—´: {scripted_time:.4f}s")
    print(f"Speedup | åŠ é€Ÿæ¯”: {speedup:.2f}x")
    
    if speedup > 1:
        print("âœ… Scripted model is faster! | è„šæœ¬æ¨¡å‹æ›´å¿«!")
    else:
        print("âš ï¸ Eager model is faster (unusual) | æ€¥åˆ‡æ¨¡å‹æ›´å¿«ï¼ˆä¸å¯»å¸¸ï¼‰")
    
    return eager_time, scripted_time, speedup

# Run the benchmark | è¿è¡ŒåŸºå‡†æµ‹è¯•
benchmark_results = benchmark_models()
```

#### Why Hybridization Improves Performance | ä¸ºä»€ä¹ˆæ··åˆåŒ–èƒ½æé«˜æ€§èƒ½

1. **Operator Fusion | ç®—å­èåˆ**
   - Multiple operations can be fused into single kernels | å¤šä¸ªæ“ä½œå¯ä»¥èåˆä¸ºå•ä¸ªå†…æ ¸
   - Reduces memory bandwidth requirements | å‡å°‘å†…å­˜å¸¦å®½éœ€æ±‚
   - Minimizes kernel launch overhead | æœ€å°åŒ–å†…æ ¸å¯åŠ¨å¼€é”€

2. **Memory Optimization | å†…å­˜ä¼˜åŒ–**
   - Better memory layout and access patterns | æ›´å¥½çš„å†…å­˜å¸ƒå±€å’Œè®¿é—®æ¨¡å¼
   - Reduced intermediate tensor allocations | å‡å°‘ä¸­é—´å¼ é‡åˆ†é…
   - In-place operations where possible | å°½å¯èƒ½è¿›è¡ŒåŸåœ°æ“ä½œ

3. **Graph-level Optimizations | å›¾çº§ä¼˜åŒ–**
   - Dead code elimination | æ­»ä»£ç æ¶ˆé™¤
   - Constant folding | å¸¸é‡æŠ˜å 
   - Loop optimization | å¾ªç¯ä¼˜åŒ–

---

## 10.2 Asynchronous Computation | å¼‚æ­¥è®¡ç®—

### 10.2.1 Asynchrony via Backend | é€šè¿‡åç«¯å®ç°å¼‚æ­¥æ€§

Asynchronous computation is a fundamental concept in modern deep learning frameworks. It allows operations to be executed without blocking the CPU, enabling better utilization of computational resources.

å¼‚æ­¥è®¡ç®—æ˜¯ç°ä»£æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸­çš„ä¸€ä¸ªåŸºæœ¬æ¦‚å¿µã€‚å®ƒå…è®¸æ“ä½œåœ¨ä¸é˜»å¡CPUçš„æƒ…å†µä¸‹æ‰§è¡Œï¼Œä»è€Œæ›´å¥½åœ°åˆ©ç”¨è®¡ç®—èµ„æºã€‚

#### Understanding Synchronous vs Asynchronous Execution | ç†è§£åŒæ­¥ä¸å¼‚æ­¥æ‰§è¡Œ

Think of synchronous execution like a single-threaded restaurant kitchen where one chef does everything step by step. Asynchronous execution is like a professional kitchen where multiple chefs work simultaneously on different parts of the meal.

å°†åŒæ­¥æ‰§è¡Œæƒ³è±¡æˆå•çº¿ç¨‹é¤å…å¨æˆ¿ï¼Œä¸€ä¸ªå¨å¸ˆé€æ­¥å®Œæˆæ‰€æœ‰å·¥ä½œã€‚å¼‚æ­¥æ‰§è¡Œå°±åƒä¸“ä¸šå¨æˆ¿ï¼Œå¤šä¸ªå¨å¸ˆåŒæ—¶å¤„ç†é¤ç‚¹çš„ä¸åŒéƒ¨åˆ†ã€‚

```python
import torch
import time

def demonstrate_synchronous_computation():
    """
    Demonstrate synchronous computation patterns
    æ¼”ç¤ºåŒæ­¥è®¡ç®—æ¨¡å¼
    """
    print("=== Synchronous Computation | åŒæ­¥è®¡ç®— ===")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    # Create large tensors | åˆ›å»ºå¤§å¼ é‡
    size = 2000
    a = torch.randn(size, size, device=device)
    b = torch.randn(size, size, device=device)
    
    start_time = time.time()
    
    # Synchronous operations - each waits for the previous to complete
    # åŒæ­¥æ“ä½œ - æ¯ä¸ªæ“ä½œéƒ½ç­‰å¾…å‰ä¸€ä¸ªå®Œæˆ
    c = torch.mm(a, b)  # Matrix multiplication | çŸ©é˜µä¹˜æ³•
    if device.type == 'cuda':
        torch.cuda.synchronize()  # Force synchronization | å¼ºåˆ¶åŒæ­¥
    
    d = torch.mm(c, a)  # Another matrix multiplication | å¦ä¸€ä¸ªçŸ©é˜µä¹˜æ³•
    if device.type == 'cuda':
        torch.cuda.synchronize()
    
    end_time = time.time()
    print(f"Synchronous computation time: {end_time - start_time:.4f}s")
    
    return end_time - start_time

def demonstrate_asynchronous_computation():
    """
    Demonstrate asynchronous computation patterns
    æ¼”ç¤ºå¼‚æ­¥è®¡ç®—æ¨¡å¼
    """
    print("\n=== Asynchronous Computation | å¼‚æ­¥è®¡ç®— ===")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # Create large tensors | åˆ›å»ºå¤§å¼ é‡
    size = 2000
    a = torch.randn(size, size, device=device)
    b = torch.randn(size, size, device=device)
    
    start_time = time.time()
    
    # Asynchronous operations - can execute in parallel
    # å¼‚æ­¥æ“ä½œ - å¯ä»¥å¹¶è¡Œæ‰§è¡Œ
    c = torch.mm(a, b)  # This returns immediately on GPU | GPUä¸Šç«‹å³è¿”å›
    d = torch.mm(a, b)  # This can start in parallel | å¯ä»¥å¹¶è¡Œå¼€å§‹
    
    # Only synchronize when we need the final result
    # åªæœ‰åœ¨éœ€è¦æœ€ç»ˆç»“æœæ—¶æ‰åŒæ­¥
    result = c + d
    if device.type == 'cuda':
        torch.cuda.synchronize()  # Wait for all operations to complete | ç­‰å¾…æ‰€æœ‰æ“ä½œå®Œæˆ
    
    end_time = time.time()
    print(f"Asynchronous computation time: {end_time - start_time:.4f}s")
    
    return end_time - start_time

def compare_computation_modes():
    """Compare synchronous and asynchronous computation | æ¯”è¾ƒåŒæ­¥å’Œå¼‚æ­¥è®¡ç®—"""
    sync_time = demonstrate_synchronous_computation()
    async_time = demonstrate_asynchronous_computation()
    
    if sync_time > 0 and async_time > 0:
        speedup = sync_time / async_time
        print(f"\n=== Comparison | æ¯”è¾ƒ ===")
        print(f"Potential speedup from asynchrony: {speedup:.2f}x")
        
        if speedup > 1.1:
            print("âœ… Asynchronous execution shows clear benefits!")
        else:
            print("â„¹ï¸ Benefits may vary based on hardware and operation complexity")

# Run the comparison | è¿è¡Œæ¯”è¾ƒ
compare_computation_modes()
```

#### Key Benefits of Asynchronous Computation | å¼‚æ­¥è®¡ç®—çš„ä¸»è¦ä¼˜åŠ¿

1. **Better Resource Utilization | æ›´å¥½çš„èµ„æºåˆ©ç”¨**
   - CPU can prepare next operations while GPU is busy | CPUå¯ä»¥åœ¨GPUå¿™ç¢Œæ—¶å‡†å¤‡ä¸‹ä¸€ä¸ªæ“ä½œ
   - Multiple GPU operations can overlap | å¤šä¸ªGPUæ“ä½œå¯ä»¥é‡å 
   - Improved overall throughput | æé«˜æ•´ä½“ååé‡

2. **Reduced Latency | å‡å°‘å»¶è¿Ÿ**
   - Operations don't wait unnecessarily | æ“ä½œä¸ä¼šä¸å¿…è¦åœ°ç­‰å¾…
   - Pipeline parallelism opportunities | æµæ°´çº¿å¹¶è¡Œæœºä¼š
   - Better interactive performance | æ›´å¥½çš„äº¤äº’æ€§èƒ½

### 10.2.2 Barriers and Blockers | å±éšœå’Œé˜»å¡å™¨

Not all operations can be asynchronous. Some operations force synchronization, acting like traffic lights in the computation flow. Understanding these synchronization points is crucial for performance optimization.

å¹¶éæ‰€æœ‰æ“ä½œéƒ½å¯ä»¥æ˜¯å¼‚æ­¥çš„ã€‚ä¸€äº›æ“ä½œä¼šå¼ºåˆ¶åŒæ­¥ï¼Œå°±åƒè®¡ç®—æµä¸­çš„äº¤é€šä¿¡å·ç¯ã€‚ç†è§£è¿™äº›åŒæ­¥ç‚¹å¯¹æ€§èƒ½ä¼˜åŒ–è‡³å…³é‡è¦ã€‚

#### Common Synchronization Points | å¸¸è§çš„åŒæ­¥ç‚¹

```python
import torch
import time

def demonstrate_synchronization_points():
    """
    Demonstrate operations that cause synchronization
    æ¼”ç¤ºå¯¼è‡´åŒæ­¥çš„æ“ä½œ
    """
    if not torch.cuda.is_available():
        print("CUDA not available, using CPU for demonstration")
        return
    
    device = torch.device('cuda')
    
    print("=== Synchronization Points | åŒæ­¥ç‚¹ ===")
    
    # Create a large tensor on GPU | åœ¨GPUä¸Šåˆ›å»ºå¤§å¼ é‡
    a = torch.randn(2000, 2000, device=device)
    
    print("\n1. GPU to CPU Transfer | GPUåˆ°CPUä¼ è¾“")
    start_time = time.time()
    cpu_data = a.cpu()  # This blocks until GPU computation is done | é˜»å¡ç›´åˆ°GPUè®¡ç®—å®Œæˆ
    transfer_time = time.time() - start_time
    print(f"   Transfer time: {transfer_time:.4f}s (blocking operation)")
    
    print("\n2. Printing GPU Tensor Values | æ‰“å°GPUå¼ é‡å€¼")
    start_time = time.time()
    print(f"   First element: {a[0, 0]}")  # This forces synchronization | å¼ºåˆ¶åŒæ­¥
    print_time = time.time() - start_time
    print(f"   Print operation time: {print_time:.4f}s (blocking)")
    
    print("\n3. Accessing Python Scalar Values | è®¿é—®Pythonæ ‡é‡å€¼")
    start_time = time.time()
    scalar_val = a[0, 0].item()  # Blocks to get scalar value | é˜»å¡è·å–æ ‡é‡å€¼
    scalar_time = time.time() - start_time
    print(f"   Scalar extraction time: {scalar_time:.4f}s (blocking)")
    print(f"   Extracted value: {scalar_val}")
    
    print("\n4. Non-blocking Operations | éé˜»å¡æ“ä½œ")
    start_time = time.time()
    shape = a.shape  # This doesn't block | ä¸ä¼šé˜»å¡
    size = a.numel()  # This doesn't block | ä¸ä¼šé˜»å¡
    nonblock_time = time.time() - start_time
    print(f"   Shape and size access time: {nonblock_time:.6f}s (non-blocking)")
    print(f"   Shape: {shape}, Size: {size}")

def demonstrate_avoiding_synchronization():
    """
    Show how to avoid unnecessary synchronization
    å±•ç¤ºå¦‚ä½•é¿å…ä¸å¿…è¦çš„åŒæ­¥
    """
    if not torch.cuda.is_available():
        return
        
    device = torch.device('cuda')
    
    print("\n=== Avoiding Synchronization | é¿å…åŒæ­¥ ===")
    
    # Create test data | åˆ›å»ºæµ‹è¯•æ•°æ®
    data = torch.randn(1000, 1000, device=device)
    
    print("\nâŒ Bad Practice: Frequent synchronization")
    start_time = time.time()
    for i in range(10):
        result = torch.mm(data, data)
        # Bad: accessing result forces sync | åä¹ æƒ¯ï¼šè®¿é—®ç»“æœå¼ºåˆ¶åŒæ­¥
        loss_value = result.sum().item()
        print(f"   Iteration {i}: loss = {loss_value:.4f}")
    bad_time = time.time() - start_time
    
    print(f"\nâœ… Good Practice: Batched synchronization")
    start_time = time.time()
    losses = []
    for i in range(10):
        result = torch.mm(data, data)
        # Good: store tensors, sync later | å¥½ä¹ æƒ¯ï¼šå­˜å‚¨å¼ é‡ï¼Œç¨ååŒæ­¥
        losses.append(result.sum())
    
    # Single synchronization point | å•ä¸ªåŒæ­¥ç‚¹
    loss_values = [loss.item() for loss in losses]
    for i, loss_value in enumerate(loss_values):
        print(f"   Iteration {i}: loss = {loss_value:.4f}")
    good_time = time.time() - start_time
    
    print(f"\nPerformance comparison:")
    print(f"Frequent sync time: {bad_time:.4f}s")
    print(f"Batched sync time: {good_time:.4f}s")
    print(f"Speedup: {bad_time/good_time:.2f}x")

# Run demonstrations | è¿è¡Œæ¼”ç¤º
demonstrate_synchronization_points()
demonstrate_avoiding_synchronization()
```

### 10.2.3 Improving Computation | æå‡è®¡ç®—æ•ˆç‡

There are several strategies to improve computational efficiency through better asynchronous programming patterns.

æœ‰å‡ ç§ç­–ç•¥å¯ä»¥é€šè¿‡æ›´å¥½çš„å¼‚æ­¥ç¼–ç¨‹æ¨¡å¼æ¥æé«˜è®¡ç®—æ•ˆç‡ã€‚

#### Pipeline Parallelism | æµæ°´çº¿å¹¶è¡Œ

Pipeline parallelism allows different stages of computation to run simultaneously, like an assembly line in a factory.

æµæ°´çº¿å¹¶è¡Œå…è®¸è®¡ç®—çš„ä¸åŒé˜¶æ®µåŒæ—¶è¿è¡Œï¼Œå°±åƒå·¥å‚çš„è£…é…çº¿ã€‚

```python
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
import threading
import queue
import time

class PipelineModel(nn.Module):
    """
    Model designed for pipeline parallelism across multiple GPUs
    ä¸ºè·¨å¤šGPUæµæ°´çº¿å¹¶è¡Œè®¾è®¡çš„æ¨¡å‹
    """
    
    def __init__(self, input_size=784, hidden_size=512, num_classes=10):
        super().__init__()
        
        # Stage 1: Input processing | é˜¶æ®µ1ï¼šè¾“å…¥å¤„ç†
        self.stage1 = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size // 2)
        )
        
        # Stage 2: Feature extraction | é˜¶æ®µ2ï¼šç‰¹å¾æå–
        self.stage2 = nn.Sequential(
            nn.ReLU(),
            nn.Linear(hidden_size // 2, hidden_size // 4),
            nn.ReLU(),
            nn.Linear(hidden_size // 4, num_classes)
        )
    
    def forward(self, x):
        # If multiple GPUs available, use pipeline | å¦‚æœæœ‰å¤šä¸ªGPUï¼Œä½¿ç”¨æµæ°´çº¿
        if torch.cuda.device_count() > 1:
            # Stage 1 on GPU 0 | é˜¶æ®µ1åœ¨GPU 0
            x = x.cuda(0)
            x = self.stage1(x)
            
            # Transfer to GPU 1 for stage 2 | ä¼ è¾“åˆ°GPU 1è¿›è¡Œé˜¶æ®µ2
            x = x.cuda(1)
            x = self.stage2(x)
        else:
            # Single GPU or CPU execution | å•GPUæˆ–CPUæ‰§è¡Œ
            x = self.stage1(x)
            x = self.stage2(x)
        
        return x

def demonstrate_pipeline_parallelism():
    """
    Demonstrate pipeline parallelism concept
    æ¼”ç¤ºæµæ°´çº¿å¹¶è¡Œæ¦‚å¿µ
    """
    print("=== Pipeline Parallelism Demo | æµæ°´çº¿å¹¶è¡Œæ¼”ç¤º ===")
    
    # Create model | åˆ›å»ºæ¨¡å‹
    model = PipelineModel()
    
    # Move different stages to different devices if available
    # å¦‚æœå¯ç”¨ï¼Œå°†ä¸åŒé˜¶æ®µç§»åŠ¨åˆ°ä¸åŒè®¾å¤‡
    if torch.cuda.device_count() > 1:
        print(f"Using {torch.cuda.device_count()} GPUs for pipeline")
        model.stage1 = model.stage1.cuda(0)
        model.stage2 = model.stage2.cuda(1)
    else:
        print("Single GPU/CPU mode")
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model = model.to(device)
    
    # Create sample data | åˆ›å»ºæ ·æœ¬æ•°æ®
    batch_size = 32
    sample_input = torch.randn(batch_size, 784)
    
    # Time the pipeline execution | è®¡æ—¶æµæ°´çº¿æ‰§è¡Œ
    start_time = time.time()
    with torch.no_grad():
        output = model(sample_input)
    execution_time = time.time() - start_time
    
    print(f"Pipeline execution time: {execution_time:.4f}s")
    print(f"Output shape: {output.shape}")

def create_efficient_dataloader(dataset_size=10000, batch_size=32):
    """
    Create an efficient data loader with optimizations
    åˆ›å»ºå…·æœ‰ä¼˜åŒ–çš„é«˜æ•ˆæ•°æ®åŠ è½½å™¨
    """
    print("\n=== Efficient Data Loading | é«˜æ•ˆæ•°æ®åŠ è½½ ===")
    
    # Create dummy dataset | åˆ›å»ºè™šæ‹Ÿæ•°æ®é›†
    data = torch.randn(dataset_size, 784)
    labels = torch.randint(0, 10, (dataset_size,))
    from torch.utils.data import TensorDataset
    dataset = TensorDataset(data, labels)
    
    # Create optimized data loader | åˆ›å»ºä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=4,        # Multi-process loading | å¤šè¿›ç¨‹åŠ è½½
        pin_memory=True,      # Faster GPU transfer | æ›´å¿«çš„GPUä¼ è¾“
        prefetch_factor=2,    # Prefetch batches | é¢„å–æ‰¹æ¬¡
        persistent_workers=True  # Keep workers alive | ä¿æŒå·¥ä½œè¿›ç¨‹æ´»è·ƒ
    )
    
    print(f"Created dataloader with {len(dataloader)} batches")
    print("Optimizations enabled:")
    print("  âœ… Multi-process loading (num_workers=4)")
    print("  âœ… Pinned memory for faster GPU transfer")
    print("  âœ… Prefetching for reduced I/O wait")
    print("  âœ… Persistent workers to avoid process overhead")
    
    return dataloader

def benchmark_data_loading():
    """
    Benchmark different data loading configurations
    åŸºå‡†æµ‹è¯•ä¸åŒçš„æ•°æ®åŠ è½½é…ç½®
    """
    print("\n=== Data Loading Benchmark | æ•°æ®åŠ è½½åŸºå‡†æµ‹è¯• ===")
    
    dataset_size = 1000
    batch_size = 32
    
    # Create dataset | åˆ›å»ºæ•°æ®é›†
    data = torch.randn(dataset_size, 784)
    labels = torch.randint(0, 10, (dataset_size,))
    dataset = TensorDataset(data, labels)
    
    # Configuration 1: Basic loader | é…ç½®1ï¼šåŸºæœ¬åŠ è½½å™¨
    basic_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    
    # Configuration 2: Optimized loader | é…ç½®2ï¼šä¼˜åŒ–åŠ è½½å™¨
    optimized_loader = DataLoader(
        dataset, batch_size=batch_size, shuffle=True,
        num_workers=2, pin_memory=True, prefetch_factor=2
    )
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # Benchmark basic loader | åŸºå‡†æµ‹è¯•åŸºæœ¬åŠ è½½å™¨
    start_time = time.time()
    for i, (batch_data, batch_labels) in enumerate(basic_loader):
        batch_data = batch_data.to(device)
        batch_labels = batch_labels.to(device)
        if i >= 10:  # Test first 10 batches | æµ‹è¯•å‰10ä¸ªæ‰¹æ¬¡
            break
    basic_time = time.time() - start_time
    
    # Benchmark optimized loader | åŸºå‡†æµ‹è¯•ä¼˜åŒ–åŠ è½½å™¨
    start_time = time.time()
    for i, (batch_data, batch_labels) in enumerate(optimized_loader):
        batch_data = batch_data.to(device)
        batch_labels = batch_labels.to(device)
        if i >= 10:  # Test first 10 batches | æµ‹è¯•å‰10ä¸ªæ‰¹æ¬¡
            break
    optimized_time = time.time() - start_time
    
    print(f"Basic loader time: {basic_time:.4f}s")
    print(f"Optimized loader time: {optimized_time:.4f}s")
    
    if basic_time > 0:
        speedup = basic_time / optimized_time
        print(f"Speedup: {speedup:.2f}x")

# Run demonstrations | è¿è¡Œæ¼”ç¤º
demonstrate_pipeline_parallelism()
create_efficient_dataloader()
benchmark_data_loading()
```

### 10.3 Automatic Parallelism | è‡ªåŠ¨å¹¶è¡ŒåŒ–

### 10.3.1 Parallel Computation on GPUs | GPUä¸Šçš„å¹¶è¡Œè®¡ç®—

GPUs are designed for massive parallelism. Think of a GPU as a factory with thousands of workers (CUDA cores) who can all work simultaneously on different parts of the same task.

GPUè®¾è®¡ç”¨äºå¤§è§„æ¨¡å¹¶è¡Œã€‚å°†GPUæƒ³è±¡æˆæ‹¥æœ‰æ•°åƒåå·¥äººï¼ˆCUDAæ ¸å¿ƒï¼‰çš„å·¥å‚ï¼Œä»–ä»¬éƒ½å¯ä»¥åŒæ—¶å¤„ç†åŒä¸€ä»»åŠ¡çš„ä¸åŒéƒ¨åˆ†ã€‚

#### Understanding GPU Architecture | ç†è§£GPUæ¶æ„

```python
import torch
import time
import numpy as np

def analyze_gpu_architecture():
    """
    Analyze and display GPU architecture information
    åˆ†æå¹¶æ˜¾ç¤ºGPUæ¶æ„ä¿¡æ¯
    """
    print("=== GPU Architecture Analysis | GPUæ¶æ„åˆ†æ ===")
    
    if not torch.cuda.is_available():
        print("CUDA not available. Using CPU for demonstration.")
        return analyze_cpu_parallelism()
    
    device_count = torch.cuda.device_count()
    print(f"Available GPUs: {device_count}")
    
    for i in range(device_count):
        props = torch.cuda.get_device_properties(i)
        print(f"\nGPU {i}: {props.name}")
        print(f"  Compute Capability: {props.major}.{props.minor}")
        print(f"  Total Memory: {props.total_memory / 1e9:.1f} GB")
        print(f"  Multiprocessors: {props.multi_processor_count}")
        print(f"  Max threads per multiprocessor: {props.max_threads_per_multi_processor}")
        print(f"  Max threads per block: {props.max_threads_per_block}")
        print(f"  Shared memory per block: {props.shared_memory_per_block / 1024:.1f} KB")
        
        # Calculate theoretical peak performance | è®¡ç®—ç†è®ºå³°å€¼æ€§èƒ½
        total_cores = props.multi_processor_count * 64  # Rough estimate | ç²—ç•¥ä¼°è®¡
        clock_rate_ghz = props.max_clock_rate / 1e6
        print(f"  Estimated CUDA cores: {total_cores}")
        print(f"  Base clock rate: {clock_rate_ghz:.2f} GHz")

def demonstrate_gpu_parallelism():
    """
    Demonstrate the power of GPU parallel computation
    æ¼”ç¤ºGPUå¹¶è¡Œè®¡ç®—çš„å¨åŠ›
    """
    print("\n=== GPU Parallelism Demonstration | GPUå¹¶è¡Œæ¼”ç¤º ===")
    
    if not torch.cuda.is_available():
        print("CUDA not available, using CPU")
        device = torch.device('cpu')
    else:
        device = torch.device('cuda')
        print(f"Using GPU: {torch.cuda.get_device_name()}")
    
    # Test different matrix sizes | æµ‹è¯•ä¸åŒçš„çŸ©é˜µå¤§å°
    sizes = [1024, 2048, 4096]
    
    for size in sizes:
        print(f"\nTesting {size}x{size} matrix multiplication:")
        
        # Create large matrices | åˆ›å»ºå¤§çŸ©é˜µ
        a = torch.randn(size, size, device=device)
        b = torch.randn(size, size, device=device)
        
        # Warm up GPU | é¢„çƒ­GPU
        for _ in range(3):
            _ = torch.mm(a, b)
        
        if device.type == 'cuda':
            torch.cuda.synchronize()
        
        # Time the computation | è®¡æ—¶è®¡ç®—
        start_time = time.time()
        c = torch.mm(a, b)  # Thousands of CUDA cores work simultaneously | æ•°åƒä¸ªCUDAæ ¸å¿ƒåŒæ—¶å·¥ä½œ
        
        if device.type == 'cuda':
            torch.cuda.synchronize()
        
        end_time = time.time()
        elapsed_time = (end_time - start_time) * 1000  # Convert to milliseconds | è½¬æ¢ä¸ºæ¯«ç§’
        
        # Calculate FLOPS | è®¡ç®—FLOPS
        flops = 2 * size**3  # Matrix multiplication FLOPS | çŸ©é˜µä¹˜æ³•FLOPS
        tflops = flops / ((end_time - start_time) * 1e12)
        
        print(f"  Execution time: {elapsed_time:.2f} ms")
        print(f"  Performance: {tflops:.2f} TFLOPS")
        print(f"  Memory used: {a.element_size() * a.numel() * 3 / 1e9:.2f} GB")

def analyze_cpu_parallelism():
    """
    Analyze CPU parallelism capabilities
    åˆ†æCPUå¹¶è¡Œèƒ½åŠ›
    """
    print("=== CPU Parallelism Analysis | CPUå¹¶è¡Œåˆ†æ ===")
    
    import multiprocessing
    cpu_count = multiprocessing.cpu_count()
    print(f"CPU cores available: {cpu_count}")
    
    # Test CPU matrix multiplication | æµ‹è¯•CPUçŸ©é˜µä¹˜æ³•
    size = 1024
    a = torch.randn(size, size)
    b = torch.randn(size, size)
    
    # Set number of threads | è®¾ç½®çº¿ç¨‹æ•°
    torch.set_num_threads(1)
    start_time = time.time()
    c1 = torch.mm(a, b)
    single_thread_time = time.time() - start_time
    
    torch.set_num_threads(cpu_count)
    start_time = time.time()
    c2 = torch.mm(a, b)
    multi_thread_time = time.time() - start_time
    
    speedup = single_thread_time / multi_thread_time
    print(f"Single thread time: {single_thread_time:.4f}s")
    print(f"Multi-thread time: {multi_thread_time:.4f}s")
    print(f"CPU parallelism speedup: {speedup:.2f}x")

# Run analysis | è¿è¡Œåˆ†æ
analyze_gpu_architecture()
demonstrate_gpu_parallelism()
```

#### Memory Access Patterns | å†…å­˜è®¿é—®æ¨¡å¼

Understanding memory access patterns is crucial for optimizing GPU performance. GPUs perform best when memory accesses are coalesced (consecutive threads access consecutive memory locations).

ç†è§£å†…å­˜è®¿é—®æ¨¡å¼å¯¹äºä¼˜åŒ–GPUæ€§èƒ½è‡³å…³é‡è¦ã€‚å½“å†…å­˜è®¿é—®æ˜¯åˆå¹¶çš„ï¼ˆè¿ç»­çº¿ç¨‹è®¿é—®è¿ç»­å†…å­˜ä½ç½®ï¼‰æ—¶ï¼ŒGPUè¡¨ç°æœ€ä½³ã€‚

```python
def demonstrate_memory_coalescing():
    """
    Demonstrate the importance of memory coalescing
    æ¼”ç¤ºå†…å­˜åˆå¹¶çš„é‡è¦æ€§
    """
    print("=== Memory Coalescing Demo | å†…å­˜åˆå¹¶æ¼”ç¤º ===")
    
    if not torch.cuda.is_available():
        print("CUDA not available for memory coalescing demo")
        return
    
    device = torch.device('cuda')
    size = 4096
    
    # Create test matrix | åˆ›å»ºæµ‹è¯•çŸ©é˜µ
    matrix = torch.randn(size, size, device=device)
    
    print("Testing memory access patterns...")
    
    # Coalesced access (row-major) | åˆå¹¶è®¿é—®ï¼ˆè¡Œä¸»åºï¼‰
    start_time = time.time()
    for i in range(min(100, size)):
        # Access consecutive elements in memory | è®¿é—®å†…å­˜ä¸­çš„è¿ç»­å…ƒç´ 
        row_sum = matrix[i, :].sum()
    torch.cuda.synchronize()
    coalesced_time = time.time() - start_time
    
    # Non-coalesced access (column-major) | éåˆå¹¶è®¿é—®ï¼ˆåˆ—ä¸»åºï¼‰
    start_time = time.time()
    for j in range(min(100, size)):
        # Access strided elements in memory | è®¿é—®å†…å­˜ä¸­çš„è·¨æ­¥å…ƒç´ 
        col_sum = matrix[:, j].sum()
    torch.cuda.synchronize()
    non_coalesced_time = time.time() - start_time
    
    print(f"Coalesced access time: {coalesced_time:.4f}s")
    print(f"Non-coalesced access time: {non_coalesced_time:.4f}s")
    print(f"Performance ratio: {non_coalesced_time/coalesced_time:.2f}x")
    
    if non_coalesced_time > coalesced_time:
        print("âœ… Coalesced access is faster as expected")
    else:
        print("âš ï¸ Results may vary based on GPU architecture and caching")

demonstrate_memory_coalescing()
```

### 10.3.2 Parallel Computation and Communication | å¹¶è¡Œè®¡ç®—ä¸é€šä¿¡

When using multiple GPUs, communication between devices becomes a critical factor in overall performance. This is like coordinating multiple teams working on the same project.

å½“ä½¿ç”¨å¤šä¸ªGPUæ—¶ï¼Œè®¾å¤‡é—´çš„é€šä¿¡æˆä¸ºæ•´ä½“æ€§èƒ½çš„å…³é”®å› ç´ ã€‚è¿™å°±åƒåè°ƒå¤šä¸ªå›¢é˜Ÿåœ¨åŒä¸€ä¸ªé¡¹ç›®ä¸Šå·¥ä½œã€‚

#### Data Parallelism Implementation | æ•°æ®å¹¶è¡Œå®ç°

```python
import torch
import torch.nn as nn
from torch.nn.parallel import DataParallel, DistributedDataParallel as DDP
import time

class BenchmarkModel(nn.Module):
    """
    Model for benchmarking parallel computation
    ç”¨äºåŸºå‡†æµ‹è¯•å¹¶è¡Œè®¡ç®—çš„æ¨¡å‹
    """
    
    def __init__(self, input_size=1000, hidden_size=512, num_classes=10):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, num_classes)
        )
    
    def forward(self, x):
        return self.layers(x)

def setup_data_parallel_model():
    """
    Set up model for data parallel training
    è®¾ç½®æ•°æ®å¹¶è¡Œè®­ç»ƒæ¨¡å‹
    """
    print("=== Data Parallel Setup | æ•°æ®å¹¶è¡Œè®¾ç½® ===")
    
    model = BenchmarkModel()
    
    if torch.cuda.device_count() > 1:
        print(f"Using {torch.cuda.device_count()} GPUs for data parallelism")
        
        # Wrap model with DataParallel | ç”¨DataParallelåŒ…è£…æ¨¡å‹
        model = DataParallel(model)
        model = model.cuda()
        
        print("Data parallelism enabled:")
        print("  âœ… Model replicated across all GPUs")
        print("  âœ… Input batches split across GPUs")
        print("  âœ… Gradients averaged across GPUs")
        
    elif torch.cuda.is_available():
        print("Single GPU mode")
        model = model.cuda()
    else:
        print("CPU mode")
    
    return model

def analyze_communication_overhead():
    """
    Analyze communication overhead in multi-GPU setups
    åˆ†æå¤šGPUè®¾ç½®ä¸­çš„é€šä¿¡å¼€é”€
    """
    print("\n=== Communication Overhead Analysis | é€šä¿¡å¼€é”€åˆ†æ ===")
    
    if torch.cuda.device_count() < 2:
        print("Need at least 2 GPUs for communication analysis")
        return
    
    # Test different batch sizes | æµ‹è¯•ä¸åŒçš„æ‰¹æ¬¡å¤§å°
    batch_sizes = [32, 64, 128, 256]
    model = setup_data_parallel_model()
    
    for batch_size in batch_sizes:
        print(f"\nTesting batch size: {batch_size}")
        
        # Create input data | åˆ›å»ºè¾“å…¥æ•°æ®
        input_data = torch.randn(batch_size, 1000).cuda()
        target = torch.randint(0, 10, (batch_size,)).cuda()
        
        # Warm up | é¢„çƒ­
        for _ in range(3):
            output = model(input_data)
        
        torch.cuda.synchronize()
        
        # Time forward pass | è®¡æ—¶å‰å‘ä¼ æ’­
        start_time = time.time()
        output = model(input_data)
        torch.cuda.synchronize()
        forward_time = time.time() - start_time
        
        print(f"  Forward pass time: {forward_time*1000:.2f} ms")
        print(f"  Throughput: {batch_size/forward_time:.0f} samples/sec")

def demonstrate_gradient_synchronization():
    """
    Demonstrate how gradients are synchronized in data parallelism
    æ¼”ç¤ºæ•°æ®å¹¶è¡Œä¸­æ¢¯åº¦å¦‚ä½•åŒæ­¥
    """
    print("\n=== Gradient Synchronization | æ¢¯åº¦åŒæ­¥ ===")
    
    # Simulate multi-GPU gradient synchronization | æ¨¡æ‹Ÿå¤šGPUæ¢¯åº¦åŒæ­¥
    def simulate_allreduce(gradients):
        """
        Simulate AllReduce operation for gradient synchronization
        æ¨¡æ‹Ÿæ¢¯åº¦åŒæ­¥çš„AllReduceæ“ä½œ
        """
        print("Simulating AllReduce operation:")
        
        for i, grad in enumerate(gradients):
            print(f"  GPU {i} gradient: {grad}")
        
        # AllReduce: sum then average | AllReduceï¼šæ±‚å’Œç„¶åå¹³å‡
        total_grad = sum(gradients)
        avg_grad = total_grad / len(gradients)
        
        print(f"  Total gradient: {total_grad}")
        print(f"  Averaged gradient: {avg_grad}")
        
        # All GPUs now have the same gradient | æ‰€æœ‰GPUç°åœ¨æœ‰ç›¸åŒçš„æ¢¯åº¦
        synchronized_grads = [avg_grad.clone() for _ in gradients]
        
        print("After synchronization:")
        for i, grad in enumerate(synchronized_grads):
            print(f"  GPU {i} gradient: {grad}")
        
        return synchronized_grads
    
    # Example gradients from different GPUs | æ¥è‡ªä¸åŒGPUçš„ç¤ºä¾‹æ¢¯åº¦
    gradients = [
        torch.tensor([1.0, 2.0, 3.0]),  # GPU 0
        torch.tensor([2.0, 3.0, 1.0]),  # GPU 1
        torch.tensor([3.0, 1.0, 2.0]),  # GPU 2
        torch.tensor([1.5, 2.5, 3.5])   # GPU 3
    ]
    
    synchronized = simulate_allreduce(gradients)

# Run demonstrations | è¿è¡Œæ¼”ç¤º
setup_data_parallel_model()
analyze_communication_overhead()
demonstrate_gradient_synchronization()
```

---

## 10.4 Hardware | ç¡¬ä»¶

### 10.4.1 Computer Architecture | è®¡ç®—æœºæ¶æ„

Understanding computer architecture is essential for optimizing deep learning performance. Think of a computer system as a city with different transportation networks - some are highways (high bandwidth, high latency), others are local roads (low bandwidth, low latency).

ç†è§£è®¡ç®—æœºæ¶æ„å¯¹äºä¼˜åŒ–æ·±åº¦å­¦ä¹ æ€§èƒ½è‡³å…³é‡è¦ã€‚å°†è®¡ç®—æœºç³»ç»Ÿæƒ³è±¡æˆæ‹¥æœ‰ä¸åŒäº¤é€šç½‘ç»œçš„åŸå¸‚â€”â€”ä¸€äº›æ˜¯é«˜é€Ÿå…¬è·¯ï¼ˆé«˜å¸¦å®½ï¼Œé«˜å»¶è¿Ÿï¼‰ï¼Œå…¶ä»–æ˜¯æœ¬åœ°é“è·¯ï¼ˆä½å¸¦å®½ï¼Œä½å»¶è¿Ÿï¼‰ã€‚

#### Memory Hierarchy | å†…å­˜å±‚æ¬¡ç»“æ„

```python
import torch
import time
import psutil
import numpy as np

def analyze_memory_hierarchy():
    """
    Analyze and demonstrate the computer memory hierarchy
    åˆ†æå’Œæ¼”ç¤ºè®¡ç®—æœºå†…å­˜å±‚æ¬¡ç»“æ„
    """
    print("=== Memory Hierarchy Analysis | å†…å­˜å±‚æ¬¡ç»“æ„åˆ†æ ===")
    
    # System memory information | ç³»ç»Ÿå†…å­˜ä¿¡æ¯
    memory = psutil.virtual_memory()
    print(f"System RAM: {memory.total / 1e9:.1f} GB")
    print(f"Available RAM: {memory.available / 1e9:.1f} GB")
    print(f"RAM usage: {memory.percent:.1f}%")
    
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.get_device_properties(0).total_memory
        print(f"GPU Memory: {gpu_memory / 1e9:.1f} GB")
    
    print("\nMemory Hierarchy (fastest to slowest):")
    print("  1. CPU Registers - ~1 cycle access")
    print("  2. L1 Cache - ~4 cycles access, ~32-64 KB")
    print("  3. L2 Cache - ~10 cycles access, ~256 KB - 1 MB")
    print("  4. L3 Cache - ~40 cycles access, ~8-32 MB")
    print("  5. Main Memory (RAM) - ~200 cycles access, GB scale")
    print("  6. Storage (SSD/HDD) - millions of cycles, TB scale")

def demonstrate_cache_effects():
    """
    Demonstrate the effects of CPU cache on performance
    æ¼”ç¤ºCPUç¼“å­˜å¯¹æ€§èƒ½çš„å½±å“
    """
    print("\n=== Cache Effects Demonstration | ç¼“å­˜æ•ˆæœæ¼”ç¤º ===")
    
    # Test different array sizes to show cache effects | æµ‹è¯•ä¸åŒæ•°ç»„å¤§å°ä»¥æ˜¾ç¤ºç¼“å­˜æ•ˆæœ
    sizes = [
        (1024, "Fits in L1 cache"),           # ~4KB
        (65536, "Fits in L2 cache"),         # ~256KB  
        (1048576, "Fits in L3 cache"),       # ~4MB
        (16777216, "Exceeds L3 cache")       # ~64MB
    ]
    
    print("Testing memory access patterns with different array sizes:")
    
    for size, description in sizes:
        # Create array | åˆ›å»ºæ•°ç»„
        arr = np.random.randn(size).astype(np.float32)
        
        # Sequential access (cache-friendly) | é¡ºåºè®¿é—®ï¼ˆç¼“å­˜å‹å¥½ï¼‰
        start_time = time.time()
        total = 0
        for i in range(min(10000, size)):
            total += arr[i]
        sequential_time = time.time() - start_time
        
        # Random access (cache-unfriendly) | éšæœºè®¿é—®ï¼ˆç¼“å­˜ä¸å‹å¥½ï¼‰
        indices = np.random.randint(0, size, min(10000, size))
        start_time = time.time()
        total = 0
        for i in indices:
            total += arr[i]
        random_time = time.time() - start_time
        
        ratio = random_time / sequential_time if sequential_time > 0 else 1
        
        print(f"\n{description} ({size} elements):")
        print(f"  Sequential access: {sequential_time*1000:.2f} ms")
        print(f"  Random access: {random_time*1000:.2f} ms")
        print(f"  Random/Sequential ratio: {ratio:.2f}x")

def analyze_bandwidth_vs_latency():
    """
    Analyze bandwidth vs latency trade-offs
    åˆ†æå¸¦å®½ä¸å»¶è¿Ÿçš„æƒè¡¡
    """
    print("\n=== Bandwidth vs Latency Analysis | å¸¦å®½ä¸å»¶è¿Ÿåˆ†æ ===")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # Test different transfer sizes | æµ‹è¯•ä¸åŒçš„ä¼ è¾“å¤§å°
    sizes_mb = [1, 10, 100, 1000]  # Megabytes | å…†å­—èŠ‚
    
    if device.type == 'cuda':
        print("GPU Memory Transfer Analysis:")
        
        for size_mb in sizes_mb:
            # Calculate number of elements | è®¡ç®—å…ƒç´ æ•°é‡
            num_elements = int(size_mb * 1024 * 1024 / 4)  # 4 bytes per float32
            
            # Create data on CPU | åœ¨CPUä¸Šåˆ›å»ºæ•°æ®
            cpu_data = torch.randn(num_elements)
            
            # Measure transfer time | æµ‹é‡ä¼ è¾“æ—¶é—´
            start_time = time.time()
            gpu_data = cpu_data.cuda()
            torch.cuda.synchronize()
            transfer_time = time.time() - start_time
            
            # Calculate bandwidth | è®¡ç®—å¸¦å®½
            bandwidth_gbps = (size_mb / 1024) / transfer_time if transfer_time > 0 else 0
            
            print(f"  {size_mb} MB transfer:")
            print(f"    Time: {transfer_time*1000:.2f} ms")
            print(f"    Bandwidth: {bandwidth_gbps:.2f} GB/s")
            
            # Clean up GPU memory | æ¸…ç†GPUå†…å­˜
            del gpu_data
            torch.cuda.empty_cache()
    
    print("\nKey Insights:")
    print("  ğŸ“Š Small transfers: Latency-dominated")
    print("  ğŸ“Š Large transfers: Bandwidth-dominated")
    print("  ğŸ“Š Optimal performance needs both low latency and high bandwidth")

# Run analysis | è¿è¡Œåˆ†æ
analyze_memory_hierarchy()
demonstrate_cache_effects()
analyze_bandwidth_vs_latency()
```

### 10.4.2 Storage Systems | å­˜å‚¨ç³»ç»Ÿ

Storage systems are crucial for deep learning workloads, especially when dealing with large datasets. Understanding I/O patterns and optimization strategies can significantly impact training performance.

å­˜å‚¨ç³»ç»Ÿå¯¹æ·±åº¦å­¦ä¹ å·¥ä½œè´Ÿè½½è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤§å‹æ•°æ®é›†æ—¶ã€‚ç†è§£I/Oæ¨¡å¼å’Œä¼˜åŒ–ç­–ç•¥å¯ä»¥æ˜¾è‘—å½±å“è®­ç»ƒæ€§èƒ½ã€‚

```python
import torch
from torch.utils.data import Dataset, DataLoader
import os
import time
import numpy as np
import tempfile
import shutil

class OptimizedDataset(Dataset):
    """
    Dataset with various I/O optimization strategies
    å…·æœ‰å„ç§I/Oä¼˜åŒ–ç­–ç•¥çš„æ•°æ®é›†
    """
    
    def __init__(self, data_path, num_samples=10000, use_memory_mapping=False):
        self.data_path = data_path
        self.num_samples = num_samples
        self.use_memory_mapping = use_memory_mapping
        
        if use_memory_mapping and os.path.exists(data_path):
            # Use memory mapping for large files | å¯¹å¤§æ–‡ä»¶ä½¿ç”¨å†…å­˜æ˜ å°„
            self.data = np.memmap(data_path, dtype='float32', mode='r', 
                                shape=(num_samples, 784))
            print(f"âœ… Using memory mapping for {data_path}")
        else:
            # Generate or load data normally | æ­£å¸¸ç”Ÿæˆæˆ–åŠ è½½æ•°æ®
            self.data = None
            print(f"ğŸ“ Using regular file I/O for {data_path}")
    
    def __len__(self):
        return self.num_samples
    
    def __getitem__(self, idx):
        if self.use_memory_mapping and self.data is not None:
            # Memory mapped access | å†…å­˜æ˜ å°„è®¿é—®
            return torch.from_numpy(self.data[idx].copy())
        else:
            # Simulate regular file loading | æ¨¡æ‹Ÿå¸¸è§„æ–‡ä»¶åŠ è½½
            # In practice, this would load from disk | å®é™…ä¸­ï¼Œè¿™ä¼šä»ç£ç›˜åŠ è½½
            return torch.randn(784)

def create_test_dataset(size_mb=100):
    """
    Create a test dataset file for I/O benchmarking
    ä¸ºI/OåŸºå‡†æµ‹è¯•åˆ›å»ºæµ‹è¯•æ•°æ®é›†æ–‡ä»¶
    """
    print(f"=== Creating {size_mb}MB test dataset | åˆ›å»º{size_mb}MBæµ‹è¯•æ•°æ®é›† ===")
    
    # Calculate number of samples | è®¡ç®—æ ·æœ¬æ•°é‡
    bytes_per_sample = 784 * 4  # 784 floats * 4 bytes per float
    num_samples = int((size_mb * 1024 * 1024) / bytes_per_sample)
    
    # Create temporary file | åˆ›å»ºä¸´æ—¶æ–‡ä»¶
    temp_dir = tempfile.mkdtemp()
    data_path = os.path.join(temp_dir, "test_data.npy")
    
    print(f"Creating {num_samples} samples in {data_path}")
    
    # Generate and save data | ç”Ÿæˆå¹¶ä¿å­˜æ•°æ®
    data = np.random.randn(num_samples, 784).astype(np.float32)
    np.save(data_path, data)
    
    file_size_mb = os.path.getsize(data_path) / (1024 * 1024)
    print(f"Created file: {file_size_mb:.1f} MB")
    
    return data_path, num_samples, temp_dir

def benchmark_io_strategies():
    """
    Benchmark different I/O strategies
    åŸºå‡†æµ‹è¯•ä¸åŒçš„I/Oç­–ç•¥
    """
    print("\n=== I/O Strategy Benchmark | I/Oç­–ç•¥åŸºå‡†æµ‹è¯• ===")
    
    # Create test dataset | åˆ›å»ºæµ‹è¯•æ•°æ®é›†
    data_path, num_samples, temp_dir = create_test_dataset(50)  # 50MB dataset
    
    try:
        # Strategy 1: Regular file I/O | ç­–ç•¥1ï¼šå¸¸è§„æ–‡ä»¶I/O
        print("\n1. Testing regular file I/O:")
        dataset_regular = OptimizedDataset(data_path, num_samples, use_memory_mapping=False)
        loader_regular = DataLoader(dataset_regular, batch_size=32, num_workers=0)
        
        start_time = time.time()
        for i, batch in enumerate(loader_regular):
            if i >= 50:  # Test first 50 batches | æµ‹è¯•å‰50ä¸ªæ‰¹æ¬¡
                break
        regular_time = time.time() - start_time
        
        # Strategy 2: Memory mapping | ç­–ç•¥2ï¼šå†…å­˜æ˜ å°„
        print("\n2. Testing memory mapping:")
        dataset_mmap = OptimizedDataset(data_path, num_samples, use_memory_mapping=True)
        loader_mmap = DataLoader(dataset_mmap, batch_size=32, num_workers=0)
        
        start_time = time.time()
        for i, batch in enumerate(loader_mmap):
            if i >= 50:  # Test first 50 batches | æµ‹è¯•å‰50ä¸ªæ‰¹æ¬¡
                break
        mmap_time = time.time() - start_time
        
        # Strategy 3: Multi-process loading | ç­–ç•¥3ï¼šå¤šè¿›ç¨‹åŠ è½½
        print("\n3. Testing multi-process loading:")
        loader_multiproc = DataLoader(dataset_regular, batch_size=32, num_workers=2)
        
        start_time = time.time()
        for i, batch in enumerate(loader_multiproc):
            if i >= 50:  # Test first 50 batches | æµ‹è¯•å‰50ä¸ªæ‰¹æ¬¡
                break
        multiproc_time = time.time() - start_time
        
        # Results | ç»“æœ
        print(f"\n=== I/O Performance Results | I/Oæ€§èƒ½ç»“æœ ===")
        print(f"Regular I/O time: {regular_time:.4f}s")
        print(f"Memory mapping time: {mmap_time:.4f}s")
        print(f"Multi-process time: {multiproc_time:.4f}s")
        
        if regular_time > 0:
            print(f"Memory mapping speedup: {regular_time/mmap_time:.2f}x")
            print(f"Multi-process speedup: {regular_time/multiproc_time:.2f}x")
    
    finally:
        # Clean up | æ¸…ç†
        shutil.rmtree(temp_dir)
        print(f"Cleaned up temporary directory: {temp_dir}")

def demonstrate_prefetching():
    """
    Demonstrate the benefits of data prefetching
    æ¼”ç¤ºæ•°æ®é¢„å–çš„å¥½å¤„
    """
    print("\n=== Data Prefetching Demo | æ•°æ®é¢„å–æ¼”ç¤º ===")
    
    # Create simple dataset | åˆ›å»ºç®€å•æ•°æ®é›†
    dataset_size = 1000
    data = torch.randn(dataset_size, 784)
    labels = torch.randint(0, 10, (dataset_size,))
    from torch.utils.data import TensorDataset
    dataset = TensorDataset(data, labels)
    
    # Without prefetching | æ— é¢„å–
    print("Testing without prefetching:")
    loader_no_prefetch = DataLoader(dataset, batch_size=32, num_workers=0)
    
    start_time = time.time()
    for i, (batch_data, batch_labels) in enumerate(loader_no_prefetch):
        # Simulate processing time | æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
        time.sleep(0.01)
        if i >= 20:
            break
    no_prefetch_time = time.time() - start_time
    
    # With prefetching | æœ‰é¢„å–
    print("Testing with prefetching:")
    loader_with_prefetch = DataLoader(
        dataset, batch_size=32, num_workers=2, 
        prefetch_factor=2, persistent_workers=True
    )
    
    start_time = time.time()
    for i, (batch_data, batch_labels) in enumerate(loader_with_prefetch):
        # Simulate processing time | æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
        time.sleep(0.01)
        if i >= 20:
            break
    prefetch_time = time.time() - start_time
    
    print(f"\nPrefetching Results:")
    print(f"Without prefetching: {no_prefetch_time:.4f}s")
    print(f"With prefetching: {prefetch_time:.4f}s")
    
    if no_prefetch_time > 0:
        speedup = no_prefetch_time / prefetch_time
        print(f"Prefetching speedup: {speedup:.2f}x")

# Run demonstrations | è¿è¡Œæ¼”ç¤º
benchmark_io_strategies()
demonstrate_prefetching()
```

### 10.4.3 GPUs and Other Accelerators | GPUå’Œå…¶ä»–åŠ é€Ÿå™¨

GPUs have become the workhorse of deep learning due to their massive parallel processing capabilities. Understanding GPU architecture and memory management is crucial for optimal performance.

ç”±äºGPUå…·æœ‰å¤§è§„æ¨¡å¹¶è¡Œå¤„ç†èƒ½åŠ›ï¼Œå®ƒä»¬å·²æˆä¸ºæ·±åº¦å­¦ä¹ çš„ä¸»åŠ›ã€‚ç†è§£GPUæ¶æ„å’Œå†…å­˜ç®¡ç†å¯¹äºæœ€ä½³æ€§èƒ½è‡³å…³é‡è¦ã€‚

```python
import torch
import gc

def comprehensive_gpu_analysis():
    """
    Comprehensive analysis of GPU capabilities and limitations
    GPUèƒ½åŠ›å’Œé™åˆ¶çš„ç»¼åˆåˆ†æ
    """
    print("=== Comprehensive GPU Analysis | GPUç»¼åˆåˆ†æ ===")
    
    if not torch.cuda.is_available():
        print("âŒ CUDA not available")
        return analyze_alternative_accelerators()
    
    device_count = torch.cuda.device_count()
    print(f"ğŸ”¢ Available GPUs: {device_count}")
    
    for i in range(device_count):
        analyze_single_gpu(i)

def analyze_single_gpu(device_id):
    """
    Analyze a single GPU in detail
    è¯¦ç»†åˆ†æå•ä¸ªGPU
    """
    print(f"\n=== GPU {device_id} Analysis | GPU {device_id} åˆ†æ ===")
    
    # Get device properties | è·å–è®¾å¤‡å±æ€§
    props = torch.cuda.get_device_properties(device_id)
    
    print(f"ğŸ“ Name: {props.name}")
    print(f"ğŸ§® Compute Capability: {props.major}.{props.minor}")
    print(f"ğŸ’¾ Total Memory: {props.total_memory / 1e9:.1f} GB")
    print(f"ğŸ”„ Multiprocessors: {props.multi_processor_count}")
    print(f"ğŸ§µ Max threads per MP: {props.max_threads_per_multi_processor}")
    print(f"ğŸ“¦ Max threads per block: {props.max_threads_per_block}")
    print(f"ğŸ”— Shared memory per block: {props.shared_memory_per_block / 1024:.1f} KB")
    print(f"ğŸ“‹ Max registers per block: {props.max_registers_per_block}")
    print(f"ğŸš€ Memory clock rate: {props.memory_clock_rate / 1e6:.1f} GHz")
    print(f"ğŸ“ Memory bus width: {props.memory_bus_width} bits")
    
    # Calculate theoretical performance | è®¡ç®—ç†è®ºæ€§èƒ½
    if props.major >= 7:  # Volta and newer
        cuda_cores_per_mp = 64
    elif props.major == 6:  # Pascal
        cuda_cores_per_mp = 64
    else:  # Older architectures
        cuda_cores_per_mp = 32
    
    total_cores = props.multi_processor_count * cuda_cores_per_mp
    base_clock_ghz = props.max_clock_rate / 1e6
    
    print(f"ğŸ¯ Estimated CUDA cores: {total_cores}")
    print(f"âš¡ Base clock rate: {base_clock_ghz:.2f} GHz")
    
    # Memory bandwidth calculation | å†…å­˜å¸¦å®½è®¡ç®—
    memory_bandwidth_gbps = (props.memory_clock_rate * 2 * props.memory_bus_width) / (8 * 1e9)
    print(f"ğŸ“Š Theoretical memory bandwidth: {memory_bandwidth_gbps:.0f} GB/s")

def demonstrate_gpu_memory_management():
    """
    Demonstrate advanced GPU memory management techniques
    æ¼”ç¤ºé«˜çº§GPUå†…å­˜ç®¡ç†æŠ€æœ¯
    """
    print("\n=== Advanced GPU Memory Management | é«˜çº§GPUå†…å­˜ç®¡ç† ===")
    
    if not torch.cuda.is_available():
        print("âŒ CUDA not available for memory management demo")
        return
    
    device = torch.device('cuda')
    
    def print_memory_stats(stage):
        """Print current memory statistics | æ‰“å°å½“å‰å†…å­˜ç»Ÿè®¡"""
        allocated = torch.cuda.memory_allocated() / 1e9
        reserved = torch.cuda.memory_reserved() / 1e9
        max_allocated = torch.cuda.max_memory_allocated() / 1e9
        print(f"{stage}:")
        print(f"  ğŸ“Š Allocated: {allocated:.2f} GB")
        print(f"  ğŸª Reserved: {reserved:.2f} GB")
        print(f"  ğŸ“ˆ Max allocated: {max_allocated:.2f} GB")
    
    # Initial state | åˆå§‹çŠ¶æ€
    print_memory_stats("Initial state | åˆå§‹çŠ¶æ€")
    
    # Allocate large tensors | åˆ†é…å¤§å¼ é‡
    print("\n1. Allocating large tensors | åˆ†é…å¤§å¼ é‡")
    tensors = []
    for i in range(5):
        tensor = torch.randn(1000, 1000, device=device)
        tensors.append(tensor)
        print(f"  Allocated tensor {i+1}: {tensor.numel() * 4 / 1e6:.1f} MB")
    
    print_memory_stats("After allocation | åˆ†é…å")
    
    # Delete some tensors | åˆ é™¤ä¸€äº›å¼ é‡
    print("\n2. Deleting tensors | åˆ é™¤å¼ é‡")
    del tensors[:3]  # Delete first 3 tensors | åˆ é™¤å‰3ä¸ªå¼ é‡
    gc.collect()     # Force garbage collection | å¼ºåˆ¶åƒåœ¾å›æ”¶
    
    print_memory_stats("After deletion (before cache clear) | åˆ é™¤åï¼ˆæ¸…é™¤ç¼“å­˜å‰ï¼‰")
    
    # Clear cache | æ¸…é™¤ç¼“å­˜
    torch.cuda.empty_cache()
    print_memory_stats("After cache clear | æ¸…é™¤ç¼“å­˜å")
    
    # Memory pool demonstration | å†…å­˜æ± æ¼”ç¤º
    print("\n3. Memory pool behavior | å†…å­˜æ± è¡Œä¸º")
    for i in range(3):
        temp_tensor = torch.randn(500, 500, device=device)
        print(f"  Created temporary tensor {i+1}")
        del temp_tensor
        print_memory_stats(f"  After temp tensor {i+1}")
    
    # Clean up | æ¸…ç†
    del tensors
    torch.cuda.empty_cache()
    torch.cuda.reset_peak_memory_stats()

def benchmark_memory_operations():
    """
    Benchmark different memory operations
    åŸºå‡†æµ‹è¯•ä¸åŒçš„å†…å­˜æ“ä½œ
    """
    print("\n=== Memory Operations Benchmark | å†…å­˜æ“ä½œåŸºå‡†æµ‹è¯• ===")
    
    if not torch.cuda.is_available():
        return
    
    device = torch.device('cuda')
    sizes_mb = [1, 10, 100, 500]  # Different tensor sizes in MB
    
    for size_mb in sizes_mb:
        print(f"\nTesting {size_mb} MB tensors:")
        
        # Calculate tensor dimensions | è®¡ç®—å¼ é‡ç»´åº¦
        num_elements = int(size_mb * 1024 * 1024 / 4)  # 4 bytes per float
        tensor_size = int(num_elements ** 0.5)  # Square tensor
        
        # 1. Allocation speed | åˆ†é…é€Ÿåº¦
        start_time = time.time()
        tensor = torch.randn(tensor_size, tensor_size, device=device)
        torch.cuda.synchronize()
        alloc_time = time.time() - start_time
        
        # 2. Copy speed (GPU to GPU) | å¤åˆ¶é€Ÿåº¦ï¼ˆGPUåˆ°GPUï¼‰
        start_time = time.time()
        tensor_copy = tensor.clone()
        torch.cuda.synchronize()
        copy_time = time.time() - start_time
        
        # 3. Transfer speed (GPU to CPU) | ä¼ è¾“é€Ÿåº¦ï¼ˆGPUåˆ°CPUï¼‰
        start_time = time.time()
        cpu_tensor = tensor.cpu()
        cpu_time = time.time() - start_time
        
        # 4. Transfer speed (CPU to GPU) | ä¼ è¾“é€Ÿåº¦ï¼ˆCPUåˆ°GPUï¼‰
        start_time = time.time()
        gpu_tensor = cpu_tensor.cuda()
        torch.cuda.synchronize()
        gpu_time = time.time() - start_time
        
        print(f"  Allocation: {alloc_time*1000:.2f} ms")
        print(f"  GPU copy: {copy_time*1000:.2f} ms")
        print(f"  GPUâ†’CPU: {cpu_time*1000:.2f} ms")
        print(f"  CPUâ†’GPU: {gpu_time*1000:.2f} ms")
        
        # Clean up | æ¸…ç†
        del tensor, tensor_copy, cpu_tensor, gpu_tensor
        torch.cuda.empty_cache()

def analyze_alternative_accelerators():
    """
    Analyze alternative accelerators when CUDA is not available
    å½“CUDAä¸å¯ç”¨æ—¶åˆ†ææ›¿ä»£åŠ é€Ÿå™¨
    """
    print("=== Alternative Accelerators | æ›¿ä»£åŠ é€Ÿå™¨ ===")
    
    # Check for Apple Metal Performance Shaders | æ£€æŸ¥Apple Metal Performance Shaders
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        print("âœ… Apple MPS (Metal Performance Shaders) available")
        device = torch.device('mps')
        
        # Test MPS performance | æµ‹è¯•MPSæ€§èƒ½
        size = 1000
        a = torch.randn(size, size, device=device)
        b = torch.randn(size, size, device=device)
        
        start_time = time.time()
        c = torch.mm(a, b)
        mps_time = time.time() - start_time
        
        print(f"MPS matrix multiplication ({size}x{size}): {mps_time*1000:.2f} ms")
    
    # CPU optimizations | CPUä¼˜åŒ–
    print(f"\nğŸ–¥ï¸ CPU Information:")
    print(f"  Threads available: {torch.get_num_threads()}")
    print(f"  BLAS backend: {torch.backends.openmp.is_available()}")
    print(f"  MKL available: {torch.backends.mkl.is_available()}")
    
    # Recommend optimizations | æ¨èä¼˜åŒ–
    print(f"\nğŸ’¡ Optimization Recommendations:")
    print(f"  âœ… Consider cloud GPU services (AWS, GCP, Azure)")
    print(f"  âœ… Optimize CPU code with proper threading")
    print(f"  âœ… Use quantization for model compression")
    print(f"  âœ… Consider model distillation for smaller models")

# Run all GPU analyses | è¿è¡Œæ‰€æœ‰GPUåˆ†æ
comprehensive_gpu_analysis()
demonstrate_gpu_memory_management()
benchmark_memory_operations()
```

---

## 10.5 Multi-GPU Training | å¤šGPUè®­ç»ƒ

### 10.5.1 Problem Splitting | é—®é¢˜åˆ†è§£

Multi-GPU training is like organizing a construction crew, where each worker handles different tasks.

å¤šGPUè®­ç»ƒå°±åƒç»„ç»‡ä¸€ä¸ªæ–½å·¥é˜Ÿï¼Œæ¯ä¸ªå·¥äººè´Ÿè´£ä¸åŒçš„ä»»åŠ¡ã€‚

**Data Parallelism vs Model Parallelism | æ•°æ®å¹¶è¡Œ vs æ¨¡å‹å¹¶è¡Œ**

```python
import torch
import torch.nn as nn
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

# Data Parallelism Example | æ•°æ®å¹¶è¡Œç¤ºä¾‹
class DataParallelModel(nn.Module):
    """
    Model designed for data parallel training
    ä¸ºæ•°æ®å¹¶è¡Œè®­ç»ƒè®¾è®¡çš„æ¨¡å‹
    """
    
    def __init__(self):
        super().__init__()
        self.layer1 = nn.Linear(1000, 512)
        self.layer2 = nn.Linear(512, 256)  
        self.layer3 = nn.Linear(256, 10)
        self.relu = nn.ReLU()
    
    def forward(self, x):
        x = self.relu(self.layer1(x))
        x = self.relu(self.layer2(x))
        return self.layer3(x)

# Model Parallelism Example | æ¨¡å‹å¹¶è¡Œç¤ºä¾‹  
class ModelParallelModel(nn.Module):
    """
    Model designed for model parallel training
    ä¸ºæ¨¡å‹å¹¶è¡Œè®­ç»ƒè®¾è®¡çš„æ¨¡å‹
    """
    
    def __init__(self):
        super().__init__()
        # First part on GPU 0 | ç¬¬ä¸€éƒ¨åˆ†åœ¨GPU 0
        self.layer1 = nn.Linear(1000, 512).cuda(0)
        self.layer2 = nn.Linear(512, 256).cuda(0)
        # Second part on GPU 1 | ç¬¬äºŒéƒ¨åˆ†åœ¨GPU 1
        self.layer3 = nn.Linear(256, 10).cuda(1)
        self.relu = nn.ReLU()
    
    def forward(self, x):
        # Compute on GPU 0 | åœ¨GPU 0ä¸Šè®¡ç®—
        x = x.cuda(0)
        x = self.relu(self.layer1(x))
        x = self.relu(self.layer2(x))
        
        # Move to GPU 1 and compute | ç§»åŠ¨åˆ°GPU 1å¹¶è®¡ç®—
        x = x.cuda(1)
        return self.layer3(x)
```

### 10.5.2 Data Parallelism | æ•°æ®å¹¶è¡Œ

**Distributed Data Parallel Implementation | åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œå®ç°**
```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from torch.nn.parallel import DistributedDataParallel as DDP
import torch.multiprocessing as mp

def setup_distributed(rank, world_size):
    """Initialize distributed training | åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ"""
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'
    
    # Initialize process group | åˆå§‹åŒ–è¿›ç¨‹ç»„
    dist.init_process_group("nccl", rank=rank, world_size=world_size)

def cleanup_distributed():
    """Clean up distributed environment | æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ"""
    dist.destroy_process_group()

def train_ddp(rank, world_size):
    """Distributed training function | åˆ†å¸ƒå¼è®­ç»ƒå‡½æ•°"""
    setup_distributed(rank, world_size)
    
    # Set device | è®¾ç½®è®¾å¤‡
    torch.cuda.set_device(rank)
    device = torch.device(f'cuda:{rank}')
    
    # Create model | åˆ›å»ºæ¨¡å‹
    model = DataParallelModel().to(device)
    model = DDP(model, device_ids=[rank])
    
    # Create optimizer | åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()
    
    # Create data | åˆ›å»ºæ•°æ®
    dataset_size = 1000
    data = torch.randn(dataset_size, 1000)
    labels = torch.randint(0, 10, (dataset_size,))
    dataset = TensorDataset(data, labels)
    
    # Distributed sampler | åˆ†å¸ƒå¼é‡‡æ ·å™¨
    sampler = torch.utils.data.distributed.DistributedSampler(
        dataset, num_replicas=world_size, rank=rank
    )
    
    dataloader = DataLoader(
        dataset, batch_size=32, sampler=sampler
    )
    
    # Training loop | è®­ç»ƒå¾ªç¯
    model.train()
    for epoch in range(5):
        sampler.set_epoch(epoch)  # Ensure different randomizations across epochs
        
        for batch_idx, (data, target) in enumerate(dataloader):
            data, target = data.to(device), target.to(device)
            
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            
            # Gradients automatically sync across GPUs | æ¢¯åº¦è‡ªåŠ¨åœ¨GPUé—´åŒæ­¥
            optimizer.step()
            
            if batch_idx % 10 == 0 and rank == 0:  # Only print on main process
                print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')
    
    cleanup_distributed()

def run_distributed_training():
    """Run distributed training | è¿è¡Œåˆ†å¸ƒå¼è®­ç»ƒ"""
    world_size = torch.cuda.device_count()
    if world_size < 2:
        print("Need at least 2 GPUs for distributed training")
        return
    
    # Launch with multiprocessing | ä½¿ç”¨å¤šè¿›ç¨‹å¯åŠ¨
    mp.spawn(train_ddp, args=(world_size,), nprocs=world_size, join=True)

# If multiple GPUs are available, run distributed training
if torch.cuda.device_count() > 1:
    run_distributed_training()
```

### 10.5.3 Data Synchronization | æ•°æ®åŒæ­¥

**Gradient Synchronization Mechanism | æ¢¯åº¦åŒæ­¥æœºåˆ¶**
```python
import torch
import torch.distributed as dist

def demonstrate_gradient_synchronization():
    """Demonstrate gradient synchronization process | æ¼”ç¤ºæ¢¯åº¦åŒæ­¥è¿‡ç¨‹"""
    
    # Simulate multi-GPU gradients | æ¨¡æ‹Ÿå¤šGPUæ¢¯åº¦
    def simulate_allreduce(gradients):
        """
        Simulate AllReduce operation for gradient synchronization
        æ¨¡æ‹Ÿæ¢¯åº¦åŒæ­¥çš„AllReduceæ“ä½œ
        """
        print("Simulating AllReduce operation:")
        
        for i, grad in enumerate(gradients):
            print(f"  GPU {i} gradient: {grad}")
        
        # AllReduce: sum then average | AllReduceï¼šæ±‚å’Œç„¶åå¹³å‡
        total_grad = sum(gradients)
        avg_grad = total_grad / len(gradients)
        
        print(f"\nSynchronized gradient: {avg_grad}")
        print("All GPUs now have the same gradient")
        
        return avg_grad
    
    # Example gradients from different GPUs | æ¥è‡ªä¸åŒGPUçš„ç¤ºä¾‹æ¢¯åº¦
    gradients = [
        torch.tensor([1.0, 2.0, 3.0]),  # GPU 0
        torch.tensor([2.0, 3.0, 1.0]),  # GPU 1  
        torch.tensor([3.0, 1.0, 2.0]),  # GPU 2
        torch.tensor([1.5, 2.5, 3.5])   # GPU 3
    ]
    
    synchronized = simulate_allreduce(gradients)

def implement_simple_allreduce():
    """Implement simple AllReduce | å®ç°ç®€å•çš„AllReduce"""
    
    class SimpleAllReduce:
        def __init__(self, tensors):
            self.tensors = tensors
        
        def reduce(self):
            # Calculate average of all tensors | è®¡ç®—æ‰€æœ‰å¼ é‡çš„å¹³å‡å€¼
            total = sum(self.tensors)
            avg = total / len(self.tensors)
            
            # Broadcast average to all positions | å°†å¹³å‡å€¼å¹¿æ’­åˆ°æ‰€æœ‰ä½ç½®
            return [avg.clone() for _ in self.tensors]
    
    # Test simple AllReduce | æµ‹è¯•ç®€å•AllReduce
    test_tensors = [
        torch.tensor([1.0, 2.0]),
        torch.tensor([3.0, 4.0]), 
        torch.tensor([5.0, 6.0])
    ]
    
    allreduce = SimpleAllReduce(test_tensors)
    result = allreduce.reduce()
    
    print("Before AllReduce:")
    for i, tensor in enumerate(test_tensors):
        print(f"  Tensor {i}: {tensor}")
    
    print("\nAfter AllReduce:")
    for i, tensor in enumerate(result):
        print(f"  Tensor {i}: {tensor}")

demonstrate_gradient_synchronization()
implement_simple_allreduce()
```

---

## 10.6 Concise Implementation for Multiple GPUs | å¤šGPUç®€æ´å®ç°

### 10.6.1 Using High-level APIs | ä½¿ç”¨é«˜çº§API

Modern deep learning frameworks provide concise multi-GPU training APIs, like using autopilot instead of manual driving.

ç°ä»£æ·±åº¦å­¦ä¹ æ¡†æ¶æä¾›äº†ç®€æ´çš„å¤šGPUè®­ç»ƒAPIï¼Œå°±åƒä½¿ç”¨è‡ªåŠ¨é©¾é©¶è€Œä¸æ˜¯æ‰‹åŠ¨é©¾é©¶ã€‚

```python
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
import pytorch_lightning as pl
from pytorch_lightning import Trainer

# PyTorch Lightning Implementation | PyTorch Lightningå®ç°
class LightningModel(pl.LightningModule):
    """
    Model designed for multi-GPU training using PyTorch Lightning
    ä¸ºå¤šGPUè®­ç»ƒè®¾è®¡çš„PyTorch Lightningæ¨¡å‹
    """
    
    def __init__(self):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(784, 512),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 10)
        )
        self.criterion = nn.CrossEntropyLoss()
    
    def forward(self, x):
        return self.model(x)
    
    def training_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = self.criterion(y_hat, y)
        self.log('train_loss', loss)
        return loss
    
    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=0.001)

def train_with_lightning():
    """Train with PyTorch Lightning | ä½¿ç”¨PyTorch Lightningè®­ç»ƒ"""
    # Create data | åˆ›å»ºæ•°æ®
    data = torch.randn(1000, 784)
    labels = torch.randint(0, 10, (1000,))
    dataset = TensorDataset(data, labels)
    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)
    
    # Create model | åˆ›å»ºæ¨¡å‹
    model = LightningModel()
    
    # Configure trainer | é…ç½®è®­ç»ƒå™¨
    trainer = Trainer(
        max_epochs=5,
        devices="auto",  # Auto detect GPU count | è‡ªåŠ¨æ£€æµ‹GPUæ•°é‡
        accelerator="auto",  # Auto select accelerator | è‡ªåŠ¨é€‰æ‹©åŠ é€Ÿå™¨
        strategy="ddp"  # Distributed data parallel | åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œ
    )
    
    # Start training | å¼€å§‹è®­ç»ƒ
    trainer.fit(model, dataloader)

# Native PyTorch Multi-GPU Implementation | åŸç”ŸPyTorchå¤šGPUå®ç°
def train_with_native_pytorch():
    """Train with native PyTorch | ä½¿ç”¨åŸç”ŸPyTorchè®­ç»ƒ"""
    
    # Check GPU availability | æ£€æŸ¥GPUå¯ç”¨æ€§
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # Create model | åˆ›å»ºæ¨¡å‹
    model = nn.Sequential(
        nn.Linear(784, 512),
        nn.ReLU(),
        nn.Linear(512, 256),
        nn.ReLU(),
        nn.Linear(256, 10)
    )
    
    # Multi-GPU wrapper | å¤šGPUåŒ…è£…
    if torch.cuda.device_count() > 1:
        print(f"Training with {torch.cuda.device_count()} GPUs")
        model = nn.DataParallel(model)
    
    model = model.to(device)
    
    # Create data and optimizer | åˆ›å»ºæ•°æ®å’Œä¼˜åŒ–å™¨
    data = torch.randn(1000, 784)
    labels = torch.randint(0, 10, (1000,))
    dataset = TensorDataset(data, labels)
    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)
    
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()
    
    # Training loop | è®­ç»ƒå¾ªç¯
    model.train()
    for epoch in range(5):
        total_loss = 0
        for batch_idx, (data_batch, target) in enumerate(dataloader):
            data_batch, target = data_batch.to(device), target.to(device)
            
            optimizer.zero_grad()
            output = model(data_batch)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            
            if batch_idx % 10 == 0:
                print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')
        
        print(f'Epoch {epoch} Average Loss: {total_loss/len(dataloader):.4f}')

# Run training examples | è¿è¡Œè®­ç»ƒç¤ºä¾‹
print("=== Native PyTorch Multi-GPU Training | åŸç”ŸPyTorchå¤šGPUè®­ç»ƒ ===")
train_with_native_pytorch()

# If PyTorch Lightning is installed, uncomment the following lines
# print("\n=== PyTorch Lightning Multi-GPU Training | PyTorch Lightningå¤šGPUè®­ç»ƒ ===")  
# train_with_lightning()
```

---

## 10.7 Parameter Servers | å‚æ•°æœåŠ¡å™¨

### 10.7.1 Distributed Training Architecture | åˆ†å¸ƒå¼è®­ç»ƒæ¶æ„

A parameter server is like a central bank that manages all the "money" (parameters), while branch offices (worker nodes) need to periodically synchronize their accounts.

å‚æ•°æœåŠ¡å™¨å°±åƒä¸€ä¸ªä¸­å¤®é“¶è¡Œï¼Œç®¡ç†ç€æ‰€æœ‰çš„"é’±"ï¼ˆå‚æ•°ï¼‰ï¼Œå„ä¸ªåˆ†æ”¯æœºæ„ï¼ˆå·¥ä½œèŠ‚ç‚¹ï¼‰éœ€è¦å®šæœŸåŒæ­¥è´¦ç›®ã€‚

```python
import torch
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.distributed.rpc import RRef, rpc
import threading
import time

class ParameterServer:
    """Parameter server implementation | å‚æ•°æœåŠ¡å™¨å®ç°"""
    
    def __init__(self, model_params):
        self.params = {}
        self.gradients = {}
        self.lock = threading.Lock()
        
        # Initialize parameters | åˆå§‹åŒ–å‚æ•°
        for name, param in model_params.items():
            self.params[name] = param.clone()
            self.gradients[name] = torch.zeros_like(param)
    
    def get_parameters(self):
        """Get current parameters | è·å–å½“å‰å‚æ•°"""
        with self.lock:
            return {name: param.clone() for name, param in self.params.items()}
    
    def update_parameters(self, worker_gradients, learning_rate=0.01):
        """Update parameters using gradients | ä½¿ç”¨æ¢¯åº¦æ›´æ–°å‚æ•°"""
        with self.lock:
            for name, grad in worker_gradients.items():
                # Accumulate gradients | ç´¯ç§¯æ¢¯åº¦
                self.gradients[name] += grad
                
                # Update parameters | æ›´æ–°å‚æ•°
                self.params[name] -= learning_rate * grad
    
    def reset_gradients(self):
        """Reset gradients | é‡ç½®æ¢¯åº¦"""
        with self.lock:
            for name in self.gradients:
                self.gradients[name].zero_()

class DistributedWorker:
    """Distributed worker node | åˆ†å¸ƒå¼å·¥ä½œèŠ‚ç‚¹"""
    
    def __init__(self, worker_id, model, parameter_server):
        self.worker_id = worker_id
        self.model = model
        self.parameter_server = parameter_server
        self.optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
    
    def train_step(self, data, target):
        """Execute one training step | æ‰§è¡Œä¸€æ­¥è®­ç»ƒ"""
        # Get latest parameters from server | ä»æœåŠ¡å™¨è·å–æœ€æ–°å‚æ•°
        server_params = self.parameter_server.get_parameters()
        
        # Update local model parameters | æ›´æ–°æœ¬åœ°æ¨¡å‹å‚æ•°
        with torch.no_grad():
            for name, param in self.model.named_parameters():
                if name in server_params:
                    param.copy_(server_params[name])
        
        # Forward and backward pass | å‰å‘å’Œåå‘ä¼ æ’­
        self.optimizer.zero_grad()
        output = self.model(data)
        loss = torch.nn.functional.cross_entropy(output, target)
        loss.backward()
        
        # Collect gradients | æ”¶é›†æ¢¯åº¦
        gradients = {}
        for name, param in self.model.named_parameters():
            if param.grad is not None:
                gradients[name] = param.grad.clone()
        
        # Send gradients to parameter server | å‘é€æ¢¯åº¦åˆ°å‚æ•°æœåŠ¡å™¨
        self.parameter_server.update_parameters(gradients)
        
        return loss.item()

def simulate_parameter_server_training():
    """Simulate parameter server training | æ¨¡æ‹Ÿå‚æ•°æœåŠ¡å™¨è®­ç»ƒ"""
    
    # Create simple model | åˆ›å»ºç®€å•æ¨¡å‹
    model = torch.nn.Linear(10, 1)
    
    # Initialize parameter server | åˆå§‹åŒ–å‚æ•°æœåŠ¡å™¨
    initial_params = {name: param.clone() for name, param in model.named_parameters()}
    param_server = ParameterServer(initial_params)
    
    # Create worker nodes | åˆ›å»ºå·¥ä½œèŠ‚ç‚¹
    num_workers = 3
    workers = []
    for i in range(num_workers):
        worker_model = torch.nn.Linear(10, 1)
        worker = DistributedWorker(i, worker_model, param_server)
        workers.append(worker)
    
    # Simulate training data | æ¨¡æ‹Ÿè®­ç»ƒæ•°æ®
    batch_size = 32
    num_batches = 20
    
    print("Starting parameter server training...")
    
    for epoch in range(5):
        epoch_loss = 0
        
        for batch in range(num_batches):
            # Generate random data | ç”Ÿæˆéšæœºæ•°æ®
            data = torch.randn(batch_size, 10)
            target = torch.randint(0, 2, (batch_size,)).float()
            
            # Each worker processes the same data | æ¯ä¸ªå·¥ä½œèŠ‚ç‚¹å¤„ç†ç›¸åŒçš„æ•°æ®
            batch_losses = []
            for worker in workers:
                loss = worker.train_step(data, target)
                batch_losses.append(loss)
            
            avg_loss = sum(batch_losses) / len(batch_losses)
            epoch_loss += avg_loss
            
            if batch % 5 == 0:
                print(f"Epoch {epoch}, Batch {batch}, Average Loss: {avg_loss:.4f}")
        
        # Reset gradient accumulation | é‡ç½®æ¢¯åº¦ç´¯ç§¯
        param_server.reset_gradients()
        
        print(f"Epoch {epoch} completed, Average Loss: {epoch_loss/num_batches:.4f}\n")

simulate_parameter_server_training()
```

### 10.7.2 Ring Synchronization | ç¯å½¢åŒæ­¥

**Ring AllReduce Algorithm | ç¯å½¢AllReduceç®—æ³•**
```python
import torch
import matplotlib.pyplot as plt

class RingAllReduce:
    """Ring AllReduce implementation | ç¯å½¢AllReduceå®ç°"""
    
    def __init__(self, tensors):
        self.tensors = tensors
        self.num_nodes = len(tensors)
        self.tensor_size = tensors[0].numel()
    
    def ring_allreduce(self):
        """Execute ring AllReduce | æ‰§è¡Œç¯å½¢AllReduce"""
        if self.num_nodes <= 1:
            return self.tensors
        
        # Split tensors into segments | å°†å¼ é‡åˆ†æˆæ®µ
        chunk_size = self.tensor_size // self.num_nodes
        
        print("Ring AllReduce Process:")
        print(f"Number of nodes: {self.num_nodes}")
        print(f"Chunk size: {chunk_size}")
        
        # Phase 1: Reduce-Scatter | é˜¶æ®µ1ï¼šReduce-Scatter
        print("\nPhase 1: Reduce-Scatter")
        for step in range(self.num_nodes - 1):
            for node_id in range(self.num_nodes):
                # Calculate send and receive nodes | è®¡ç®—å‘é€å’Œæ¥æ”¶èŠ‚ç‚¹
                send_to = (node_id + 1) % self.num_nodes
                recv_from = (node_id - 1) % self.num_nodes
                
                # Calculate current chunk | è®¡ç®—å½“å‰æ®µ
                chunk_id = (node_id - step) % self.num_nodes
                start_idx = chunk_id * chunk_size
                end_idx = min(start_idx + chunk_size, self.tensor_size)
                
                # Simulate communication: send data to next node | æ¨¡æ‹Ÿé€šä¿¡ï¼šå°†æ•°æ®å‘é€ç»™ä¸‹ä¸€ä¸ªèŠ‚ç‚¹
                if end_idx > start_idx:
                    chunk_data = self.tensors[node_id].view(-1)[start_idx:end_idx]
                    # Receiving node accumulates data | æ¥æ”¶èŠ‚ç‚¹ç´¯åŠ æ•°æ®
                    self.tensors[send_to].view(-1)[start_idx:end_idx] += chunk_data
                
            print(f"  Step {step + 1} completed")
        
        # Phase 2: All-Gather | é˜¶æ®µ2ï¼šAll-Gather
        print("\nPhase 2: All-Gather")
        for step in range(self.num_nodes - 1):
            for node_id in range(self.num_nodes):
                send_to = (node_id + 1) % self.num_nodes
                
                # Calculate current propagating chunk | è®¡ç®—å½“å‰ä¼ æ’­æ®µ
                chunk_id = (node_id - step + 1) % self.num_nodes
                start_idx = chunk_id * chunk_size
                end_idx = min(start_idx + chunk_size, self.tensor_size)
                
                # Propagate final result | ä¼ æ’­æœ€ç»ˆç»“æœ
                if end_idx > start_idx:
                    final_chunk = self.tensors[node_id].view(-1)[start_idx:end_idx].clone()
                    self.tensors[send_to].view(-1)[start_idx:end_idx] = final_chunk
            
            print(f"  Step {step + 1} completed")
        
        return self.tensors

def demonstrate_ring_allreduce():
    """Demonstrate ring AllReduce | æ¼”ç¤ºç¯å½¢AllReduce"""
    
    # Create test data | åˆ›å»ºæµ‹è¯•æ•°æ®
    num_nodes = 4
    tensor_size = 8
    
    original_tensors = [
        torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0]),
        torch.tensor([2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]),
        torch.tensor([3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]),
        torch.tensor([4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0])
    ]
    
    # Copy for ring AllReduce | å¤åˆ¶ç”¨äºç¯å½¢AllReduce
    ring_tensors = [t.clone() for t in original_tensors]
    
    print("Original tensors:")
    for i, tensor in enumerate(original_tensors):
        print(f"  Node {i}: {tensor}")
    
    # Execute ring AllReduce | æ‰§è¡Œç¯å½¢AllReduce
    ring_allreduce = RingAllReduce(ring_tensors)
    result = ring_allreduce.ring_allreduce()
    
    print(f"\nRing AllReduce result:")
    for i, tensor in enumerate(result):
        print(f"  Node {i}: {tensor}")
    
    # Verify result | éªŒè¯ç»“æœ
    expected = sum(original_tensors)
    print(f"\nExpected result (sum of all tensors): {expected}")
    print(f"Actual result matches expected: {'Yes' if torch.allclose(result[0], expected) else 'No'}")

demonstrate_ring_allreduce()
```

### 10.7.3 Key-Value Stores | é”®å€¼å­˜å‚¨

**Distributed Key-Value Store Implementation | åˆ†å¸ƒå¼é”®å€¼å­˜å‚¨å®ç°**
```python
import threading
import time
from collections import defaultdict
import torch

class DistributedKeyValueStore:
    """Distributed key-value store | åˆ†å¸ƒå¼é”®å€¼å­˜å‚¨"""
    
    def __init__(self):
        self.store = {}
        self.locks = defaultdict(threading.Lock)
        self.version = defaultdict(int)
        
    def put(self, key, value):
        """Store key-value pair | å­˜å‚¨é”®å€¼å¯¹"""
        with self.locks[key]:
            self.store[key] = value.clone() if torch.is_tensor(value) else value
            self.version[key] += 1
            return self.version[key]
    
    def get(self, key):
        """Get value | è·å–å€¼"""
        with self.locks[key]:
            if key in self.store:
                value = self.store[key]
                return value.clone() if torch.is_tensor(value) else value, self.version[key]
            else:
                return None, 0
    
    def add(self, key, value):
        """Atomic add operation | åŸå­åŠ æ³•æ“ä½œ"""
        with self.locks[key]:
            if key in self.store:
                if torch.is_tensor(self.store[key]) and torch.is_tensor(value):
                    self.store[key] += value
                else:
                    self.store[key] += value
            else:
                self.store[key] = value.clone() if torch.is_tensor(value) else value
            self.version[key] += 1
            return self.version[key]
    
    def get_all_keys(self):
        """Get all keys | è·å–æ‰€æœ‰é”®"""
        return list(self.store.keys())

def demonstrate_kv_store():
    """Demonstrate key-value store usage | æ¼”ç¤ºé”®å€¼å­˜å‚¨ä½¿ç”¨"""
    
    kv_store = DistributedKeyValueStore()
    
    # Simulate multiple worker nodes | æ¨¡æ‹Ÿå¤šä¸ªå·¥ä½œèŠ‚ç‚¹
    def worker_function(worker_id, store, num_iterations=5):
        for i in range(num_iterations):
            # Generate gradients | ç”Ÿæˆæ¢¯åº¦
            gradient = torch.randn(10) * (worker_id + 1)
            key = f"gradient_layer_{i}"
            
            # Add gradient to store | å°†æ¢¯åº¦æ·»åŠ åˆ°å­˜å‚¨
            version = store.add(key, gradient)
            print(f"Worker {worker_id}: Added gradient to {key}, version {version}")
            
            time.sleep(0.1)  # Simulate computation time | æ¨¡æ‹Ÿè®¡ç®—æ—¶é—´
    
    # Start multiple worker threads | å¯åŠ¨å¤šä¸ªå·¥ä½œçº¿ç¨‹
    threads = []
    num_workers = 3
    
    for worker_id in range(num_workers):
        thread = threading.Thread(
            target=worker_function, 
            args=(worker_id, kv_store)
        )
        threads.append(thread)
        thread.start()
    
    # Wait for all threads to complete | ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
    for thread in threads:
        thread.join()
    
    # Check final results | æŸ¥çœ‹æœ€ç»ˆç»“æœ
    print("\nFinal stored gradients:")
    for key in kv_store.get_all_keys():
        value, version = kv_store.get(key)
        print(f"{key}: Version {version}, Value {value[:3]}...")  # Only show first 3 elements

demonstrate_kv_store()
```

---

## Summary | æ€»ç»“

This chapter covered the fundamental aspects of computational performance in deep learning. Understanding these concepts is crucial for building efficient and scalable deep learning systems.

æœ¬ç« æ¶µç›–äº†æ·±åº¦å­¦ä¹ ä¸­è®¡ç®—æ€§èƒ½çš„åŸºæœ¬æ–¹é¢ã€‚ç†è§£è¿™äº›æ¦‚å¿µå¯¹äºæ„å»ºé«˜æ•ˆå’Œå¯æ‰©å±•çš„æ·±åº¦å­¦ä¹ ç³»ç»Ÿè‡³å…³é‡è¦ã€‚

### Key Takeaways | å…³é”®è¦ç‚¹

1. **Hybrid Programming | æ··åˆå¼ç¼–ç¨‹**
   - Combines flexibility of imperative programming with performance of symbolic programming
   - TorchScript enables seamless transition between modes
   - ç»“åˆäº†å‘½ä»¤å¼ç¼–ç¨‹çš„çµæ´»æ€§å’Œç¬¦å·å¼ç¼–ç¨‹çš„æ€§èƒ½
   - TorchScriptå¯ä»¥åœ¨æ¨¡å¼é—´æ— ç¼è½¬æ¢

2. **Asynchronous Computation | å¼‚æ­¥è®¡ç®—**
   - Enables better resource utilization through overlapping operations
   - Understanding synchronization points is crucial for optimization
   - é€šè¿‡é‡å æ“ä½œå®ç°æ›´å¥½çš„èµ„æºåˆ©ç”¨
   - ç†è§£åŒæ­¥ç‚¹å¯¹ä¼˜åŒ–è‡³å…³é‡è¦

3. **Automatic Parallelism | è‡ªåŠ¨å¹¶è¡ŒåŒ–**
   - GPU's parallel architecture is suitable for deep learning matrix operations
   - Data parallelism and model parallelism are two main parallelization strategies
   - GPUçš„å¹¶è¡Œæ¶æ„é€‚åˆæ·±åº¦å­¦ä¹ çš„çŸ©é˜µè¿ç®—
   - æ•°æ®å¹¶è¡Œå’Œæ¨¡å‹å¹¶è¡Œæ˜¯ä¸¤ç§ä¸»è¦çš„å¹¶è¡ŒåŒ–ç­–ç•¥

4. **Hardware Understanding | ç¡¬ä»¶ç†è§£**
   - Memory hierarchy affects performance significantly
   - GPU architecture is optimized for parallel computation
   - I/O optimization is crucial for large-scale training
   - å†…å­˜å±‚æ¬¡ç»“æ„æ˜¾è‘—å½±å“æ€§èƒ½
   - GPUæ¶æ„ä¸ºå¹¶è¡Œè®¡ç®—ä¼˜åŒ–
   - I/Oä¼˜åŒ–å¯¹å¤§è§„æ¨¡è®­ç»ƒè‡³å…³é‡è¦

5. **Multi-GPU Training | å¤šGPUè®­ç»ƒ**
   - Distributed data parallelism is the most common multi-GPU training method
   - Gradient synchronization is the core challenge of multi-GPU training
   - åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œæ˜¯æœ€å¸¸ç”¨çš„å¤šGPUè®­ç»ƒæ–¹æ³•
   - æ¢¯åº¦åŒæ­¥æ˜¯å¤šGPUè®­ç»ƒçš„æ ¸å¿ƒæŒ‘æˆ˜

6. **Parameter Servers | å‚æ•°æœåŠ¡å™¨**
   - Centralized parameter management is suitable for large-scale distributed training
   - Ring synchronization algorithms can reduce communication overhead
   - ä¸­å¿ƒåŒ–çš„å‚æ•°ç®¡ç†é€‚åˆå¤§è§„æ¨¡åˆ†å¸ƒå¼è®­ç»ƒ
   - ç¯å½¢åŒæ­¥ç­‰ç®—æ³•å¯ä»¥å‡å°‘é€šä¿¡å¼€é”€

### Practical Recommendations | å®è·µå»ºè®®

1. **Performance Tuning | æ€§èƒ½è°ƒä¼˜**
   - Use mixed precision training to reduce memory usage
   - Set appropriate batch size to balance memory and computation efficiency
   - Use data prefetching and parallel loading to improve I/O performance
   - ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒå‡å°‘å†…å­˜ä½¿ç”¨
   - åˆç†è®¾ç½®batch sizeå¹³è¡¡å†…å­˜å’Œè®¡ç®—æ•ˆç‡
   - åˆ©ç”¨æ•°æ®é¢„å–å’Œå¹¶è¡ŒåŠ è½½æå‡I/Oæ€§èƒ½

2. **Multi-GPU Usage | å¤šGPUä½¿ç”¨**
   - Prioritize official distributed training APIs
   - Pay attention to communication overhead, avoid frequent GPU-to-GPU data transfers
   - Choose appropriate parallel strategies based on model size
   - ä¼˜å…ˆä½¿ç”¨å®˜æ–¹æä¾›çš„åˆ†å¸ƒå¼è®­ç»ƒAPI
   - æ³¨æ„é€šä¿¡å¼€é”€ï¼Œé¿å…é¢‘ç¹çš„GPUé—´æ•°æ®ä¼ è¾“
   - æ ¹æ®æ¨¡å‹å¤§å°é€‰æ‹©åˆé€‚çš„å¹¶è¡Œç­–ç•¥

3. **Resource Management | èµ„æºç®¡ç†**
   - Monitor GPU memory usage, avoid memory overflow
   - Allocate computational resources reasonably, avoid resource contention
   - Use performance analysis tools to identify bottlenecks
   - ç›‘æ§GPUå†…å­˜ä½¿ç”¨ï¼Œé¿å…å†…å­˜æº¢å‡º
   - åˆç†åˆ†é…è®¡ç®—èµ„æºï¼Œé¿å…èµ„æºç«äº‰
   - ä½¿ç”¨æ€§èƒ½åˆ†æå·¥å…·è¯†åˆ«ç“¶é¢ˆ

By mastering these computational performance optimization techniques, you will be able to more effectively train and deploy deep learning models, fully leveraging the computational power of modern hardware.

é€šè¿‡æŒæ¡è¿™äº›è®¡ç®—æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯ï¼Œæ‚¨å°†èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è®­ç»ƒå’Œéƒ¨ç½²æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå……åˆ†å‘æŒ¥ç°ä»£ç¡¬ä»¶çš„è®¡ç®—èƒ½åŠ›ã€‚ 