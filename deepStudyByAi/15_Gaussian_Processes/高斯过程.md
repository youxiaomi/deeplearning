# 第十五章：高斯过程 (Gaussian Processes)

## 15.1 Introduction to Gaussian Processes (高斯过程简介)

### What is a Gaussian Process? (什么是高斯过程？)

A **Gaussian Process (GP)** is a powerful probabilistic machine learning model that provides a flexible framework for regression and classification tasks. Unlike traditional neural networks that learn specific parameter values, Gaussian processes define a **distribution over functions**. 

**高斯过程(GP)**是一种强大的概率机器学习模型，为回归和分类任务提供了灵活的框架。与学习特定参数值的传统神经网络不同，高斯过程定义了**函数上的分布**。

### Key Characteristics (主要特征)

1. **Probabilistic Nature**: GPs provide uncertainty estimates along with predictions
   **概率性质**：GP在提供预测的同时也给出不确定性估计

2. **Non-parametric**: The model complexity grows with the amount of data
   **非参数**：模型复杂度随数据量增长

3. **Kernel-based**: The similarity between data points is captured through kernel functions
   **基于核函数**：通过核函数捕捉数据点间的相似性

### Intuitive Understanding (直观理解)

Think of a Gaussian process as a "smart interpolation" method. Imagine you're trying to predict the temperature throughout a day based on a few measurements. A GP doesn't just draw a single line through your data points - instead, it considers **all possible smooth curves** that could fit your data and tells you how likely each one is.

将高斯过程想象为一种"智能插值"方法。假设你想根据几次测量来预测一天中的温度变化。GP不只是通过数据点画一条线，而是考虑**所有可能拟合数据的平滑曲线**，并告诉你每条曲线的可能性。

### Real-world Analogy (现实世界类比)

Consider a **weather forecasting scenario**:
- You have temperature readings from various weather stations (your training data)
- You want to predict temperature at locations where you don't have stations
- A GP would not only give you the most likely temperature but also tell you **how confident** it is about that prediction
- Areas close to weather stations would have high confidence (low uncertainty)
- Remote areas far from any station would have lower confidence (high uncertainty)

考虑一个**天气预报场景**：
- 你有来自各个气象站的温度读数（训练数据）
- 你想预测没有气象站的地点的温度
- GP不仅会给你最可能的温度，还会告诉你对该预测的**置信度**
- 靠近气象站的区域置信度高（不确定性低）
- 远离任何气象站的偏远地区置信度较低（不确定性高）

### 15.1.1 Summary (总结)

Gaussian processes represent a paradigm shift from traditional machine learning approaches. Instead of learning fixed parameters, they learn distributions over functions, providing both predictions and uncertainty quantification. This makes them particularly valuable in applications where understanding uncertainty is crucial.

高斯过程代表了从传统机器学习方法的范式转变。它们不学习固定参数，而是学习函数上的分布，既提供预测又提供不确定性量化。这使得它们在理解不确定性至关重要的应用中特别有价值。

### 15.1.2 Exercises (练习题)

1. **Conceptual Question**: Explain why Gaussian processes are called "non-parametric" models.
   **概念题**：解释为什么高斯过程被称为"非参数"模型。

2. **Comparison**: Compare and contrast Gaussian processes with linear regression in terms of flexibility and interpretability.
   **比较题**：在灵活性和可解释性方面比较高斯过程和线性回归。

3. **Application**: Give three real-world scenarios where uncertainty quantification would be more important than just getting a point prediction.
   **应用题**：给出三个现实世界场景，其中不确定性量化比仅获得点预测更重要。

---

## 15.2 Gaussian Process Priors (高斯过程先验)

### 15.2.1 Definition (定义)

A **Gaussian process** is formally defined as a collection of random variables, any finite number of which have a joint Gaussian distribution. Mathematically, we write:

**高斯过程**正式定义为随机变量的集合，其中任意有限数量的随机变量都具有联合高斯分布。数学上，我们写作：

$$f(x) \sim \mathcal{GP}(m(x), k(x, x'))$$

Where:
- $m(x)$ is the **mean function** (均值函数)
- $k(x, x')$ is the **covariance function** or **kernel** (协方差函数或核函数)

其中：
- $m(x)$ 是**均值函数**
- $k(x, x')$ 是**协方差函数**或**核函数**

### Key Properties (关键性质)

1. **Mean Function**: Represents our prior belief about the function's average behavior
   **均值函数**：表示我们对函数平均行为的先验信念

2. **Covariance Function**: Encodes our assumptions about how the function varies
   **协方差函数**：编码我们对函数如何变化的假设

### 15.2.2 A Simple Gaussian Process (简单的高斯过程)

Let's start with the simplest case: a zero-mean Gaussian process with a basic covariance function.

让我们从最简单的情况开始：具有基本协方差函数的零均值高斯过程。

#### Example: Zero-Mean GP (例子：零均值GP)

Consider $f(x) \sim \mathcal{GP}(0, k(x, x'))$ where the mean function is zero everywhere. The covariance function determines all the interesting behavior.

考虑 $f(x) \sim \mathcal{GP}(0, k(x, x'))$，其中均值函数处处为零。协方差函数决定所有有趣的行为。

**Simple Example**: 
If we have two input points $x_1$ and $x_2$, then:
$$\begin{pmatrix} f(x_1) \\ f(x_2) \end{pmatrix} \sim \mathcal{N}\left(\begin{pmatrix} 0 \\ 0 \end{pmatrix}, \begin{pmatrix} k(x_1, x_1) & k(x_1, x_2) \\ k(x_2, x_1) & k(x_2, x_2) \end{pmatrix}\right)$$

**简单例子**：
如果我们有两个输入点 $x_1$ 和 $x_2$，那么：

#### Intuitive Understanding (直观理解)

Think of drawing function samples from a GP like this:
1. Choose input points where you want to evaluate the function
2. Compute the covariance matrix using your kernel function
3. Sample from the resulting multivariate Gaussian distribution
4. Connect the dots to get a smooth function

将从GP中抽取函数样本想象为：
1. 选择要评估函数的输入点
2. 使用核函数计算协方差矩阵
3. 从得到的多元高斯分布中采样
4. 连接这些点得到平滑函数

### 15.2.3 From Weight Space to Function Space (从权重空间到函数空间)

Traditional neural networks work in **weight space** - they learn specific values for parameters. Gaussian processes work in **function space** - they consider distributions over entire functions.

传统神经网络在**权重空间**中工作——它们学习参数的特定值。高斯过程在**函数空间**中工作——它们考虑整个函数上的分布。

#### Weight Space Perspective (权重空间视角)

In a linear model: $f(x) = w^T \phi(x) + b$
- We learn specific values for weights $w$ and bias $b$
- Once learned, we get a single deterministic function

在线性模型中：$f(x) = w^T \phi(x) + b$
- 我们学习权重 $w$ 和偏置 $b$ 的特定值
- 一旦学习完成，我们得到单一确定性函数

#### Function Space Perspective (函数空间视角)

In a Gaussian process:
- We define a distribution over functions directly
- We can sample infinitely many different functions from this distribution
- Each function is equally valid under our prior assumptions

在高斯过程中：
- 我们直接定义函数上的分布
- 我们可以从该分布中采样无穷多个不同函数
- 在我们的先验假设下，每个函数都同样有效

#### Bayesian Linear Regression Connection (贝叶斯线性回归连接)

Interestingly, Bayesian linear regression with Gaussian priors on weights is equivalent to a specific Gaussian process! If we place a prior $w \sim \mathcal{N}(0, \Sigma_w)$ on weights, the resulting function distribution is:

有趣的是，对权重使用高斯先验的贝叶斯线性回归等价于特定的高斯过程！如果我们对权重设置先验 $w \sim \mathcal{N}(0, \Sigma_w)$，得到的函数分布是：

$$f(x) \sim \mathcal{GP}(0, \phi(x)^T \Sigma_w \phi(x'))$$

### 15.2.4 The Radial Basis Function (RBF) Kernel (径向基函数核)

The **RBF kernel** (also called the **squared exponential** or **Gaussian kernel**) is the most popular kernel in Gaussian processes:

**RBF核**（也称为**平方指数**或**高斯核**）是高斯过程中最流行的核：

$$k(x, x') = \sigma_f^2 \exp\left(-\frac{||x - x'||^2}{2l^2}\right)$$

Where:
- $\sigma_f^2$ is the **signal variance** (信号方差)
- $l$ is the **length scale** (长度尺度)

其中：
- $\sigma_f^2$ 是**信号方差**
- $l$ 是**长度尺度**

#### Understanding the Parameters (理解参数)

**Length Scale ($l$)**:
- **Small $l$**: Function values change rapidly; the GP produces "wiggly" functions
- **Large $l$**: Function values change slowly; the GP produces smooth functions

**长度尺度 ($l$)**：
- **小的 $l$**：函数值变化快速；GP产生"波动"函数
- **大的 $l$**：函数值变化缓慢；GP产生平滑函数

**Signal Variance ($\sigma_f^2$)**:
- Controls the overall magnitude of function variations
- Higher values allow for larger deviations from the mean

**信号方差 ($\sigma_f^2$)**：
- 控制函数变化的总体幅度
- 更高的值允许从均值更大的偏差

#### Real-world Analogy (现实世界类比)

Think of modeling **stock prices**:
- **Short length scale**: Prices are very sensitive to recent events (day trading)
- **Long length scale**: Prices follow long-term trends (value investing)
- **High signal variance**: Volatile stock with large price swings
- **Low signal variance**: Stable stock with small price movements

想象建模**股票价格**：
- **短长度尺度**：价格对近期事件非常敏感（日内交易）
- **长长度尺度**：价格遵循长期趋势（价值投资）
- **高信号方差**：价格大幅波动的不稳定股票
- **低信号方差**：价格小幅波动的稳定股票

#### Practical Example (实际例子)

```python
import numpy as np
import torch

def rbf_kernel(x1, x2, signal_var=1.0, length_scale=1.0):
    """
    Compute RBF kernel between x1 and x2
    计算x1和x2之间的RBF核
    """
    distance_squared = torch.sum((x1 - x2)**2)
    return signal_var * torch.exp(-distance_squared / (2 * length_scale**2))
```

### 15.2.5 The Neural Network Kernel (神经网络核)

The **neural network kernel** provides a connection between GPs and neural networks. For a single hidden layer with infinite width, the neural network kernel is:

**神经网络核**提供了GP和神经网络之间的连接。对于具有无限宽度的单隐藏层，神经网络核是：

$$k(x, x') = \frac{2}{\pi} \arcsin\left(\frac{2\tilde{x}^T\tilde{x}'}{\sqrt{(1 + 2\tilde{x}^T\tilde{x})(1 + 2\tilde{x}'^T\tilde{x}')}}\right)$$

Where $\tilde{x} = [1, x]$ (adding a bias term).

其中 $\tilde{x} = [1, x]$（添加偏置项）。

#### Key Insight (关键洞察)

As the width of a neural network approaches infinity, its behavior converges to a Gaussian process with the neural network kernel! This remarkable result connects two seemingly different approaches to machine learning.

当神经网络的宽度趋于无穷时，其行为收敛到具有神经网络核的高斯过程！这一显著结果连接了机器学习的两种看似不同的方法。

#### When to Use NN Kernel (何时使用NN核)

- When you want GP behavior similar to a shallow neural network
- For problems where you expect step-like or threshold behavior
- As a baseline when comparing GPs to actual neural networks

何时使用NN核：
- 当你想要类似浅层神经网络的GP行为时
- 对于期望阶跃或阈值行为的问题
- 作为比较GP与实际神经网络的基线

### 15.2.6 Summary (总结)

Gaussian process priors encode our assumptions about function behavior through:
1. **Mean functions**: Our belief about the function's average behavior
2. **Kernel functions**: Our assumptions about smoothness and correlation structure

Key kernels include:
- **RBF kernel**: For smooth, continuous functions
- **Neural network kernel**: Bridging GPs and neural networks

高斯过程先验通过以下方式编码我们对函数行为的假设：
1. **均值函数**：我们对函数平均行为的信念
2. **核函数**：我们对平滑性和相关结构的假设

关键核函数包括：
- **RBF核**：用于平滑连续函数
- **神经网络核**：连接GP和神经网络

### 15.2.7 Exercises (练习题)

1. **Mathematical**: Derive the covariance matrix for 3 points using the RBF kernel.
   **数学题**：使用RBF核推导3个点的协方差矩阵。

2. **Conceptual**: Explain why the neural network kernel connects infinite-width neural networks to Gaussian processes.
   **概念题**：解释为什么神经网络核将无限宽神经网络连接到高斯过程。

3. **Programming**: Implement a function to generate samples from a GP prior using the RBF kernel.
   **编程题**：实现一个函数，使用RBF核从GP先验生成样本。

---

## 15.3 Gaussian Process Inference (高斯过程推断)

### 15.3.1 Posterior Inference for Regression (回归的后验推断)

The power of Gaussian processes lies in their ability to update beliefs about functions based on observed data. Given training data $(X, y)$, we want to make predictions at new points $X_*$.

高斯过程的力量在于能够基于观察数据更新对函数的信念。给定训练数据$(X, y)$，我们想要在新点$X_*$处进行预测。

#### The Setup (设置)

Consider training inputs $X = \{x_1, x_2, ..., x_n\}$ with corresponding noisy observations:
$$y_i = f(x_i) + \epsilon_i$$

where $\epsilon_i \sim \mathcal{N}(0, \sigma_n^2)$ is Gaussian noise.

考虑训练输入 $X = \{x_1, x_2, ..., x_n\}$ 及其对应的噪声观测：
$$y_i = f(x_i) + \epsilon_i$$

其中 $\epsilon_i \sim \mathcal{N}(0, \sigma_n^2)$ 是高斯噪声。

#### Joint Distribution (联合分布)

The joint distribution of training outputs $y$ and test outputs $f_*$ is:

训练输出$y$和测试输出$f_*$的联合分布是：

$$\begin{pmatrix} y \\ f_* \end{pmatrix} \sim \mathcal{N}\left(0, \begin{pmatrix} K(X,X) + \sigma_n^2 I & K(X,X_*) \\ K(X_*,X) & K(X_*,X_*) \end{pmatrix}\right)$$

Where:
- $K(X,X)$ is the $n \times n$ covariance matrix between training points
- $K(X,X_*)$ is the $n \times m$ covariance matrix between training and test points
- $K(X_*,X_*)$ is the $m \times m$ covariance matrix between test points
- $\sigma_n^2 I$ accounts for observation noise

其中：
- $K(X,X)$ 是训练点之间的 $n \times n$ 协方差矩阵
- $K(X,X_*)$ 是训练点和测试点之间的 $n \times m$ 协方差矩阵
- $K(X_*,X_*)$ 是测试点之间的 $m \times m$ 协方差矩阵
- $\sigma_n^2 I$ 考虑了观测噪声

#### Posterior Predictive Distribution (后验预测分布)

Using the properties of multivariate Gaussians, the posterior predictive distribution is:

使用多元高斯的性质，后验预测分布是：

$$f_* | X, y, X_* \sim \mathcal{N}(\mu_*, \Sigma_*)$$

Where the **posterior mean** is:
$$\mu_* = K(X_*,X)[K(X,X) + \sigma_n^2 I]^{-1}y$$

And the **posterior covariance** is:
$$\Sigma_* = K(X_*,X_*) - K(X_*,X)[K(X,X) + \sigma_n^2 I]^{-1}K(X,X_*)$$

其中**后验均值**是：
其中**后验协方差**是：

### 15.3.2 Equations for Making Predictions and Learning Kernel Hyperparameters in GP Regression (GP回归中进行预测和学习核超参数的方程)

#### Prediction Equations (预测方程)

For a single test point $x_*$, the predictive mean and variance are:

对于单个测试点 $x_*$，预测均值和方差是：

**Predictive Mean (预测均值)**:
$$\mu(x_*) = k_*^T(K + \sigma_n^2 I)^{-1}y$$

**Predictive Variance (预测方差)**:
$$\sigma^2(x_*) = k(x_*, x_*) - k_*^T(K + \sigma_n^2 I)^{-1}k_*$$

Where $k_* = [k(x_*, x_1), k(x_*, x_2), ..., k(x_*, x_n)]^T$.

其中 $k_* = [k(x_*, x_1), k(x_*, x_2), ..., k(x_*, x_n)]^T$。

#### Learning Hyperparameters (学习超参数)

We optimize the **marginal likelihood** (evidence) with respect to kernel hyperparameters $\theta$:

我们相对于核超参数 $\theta$ 优化**边际似然**（证据）：

$$\log p(y|X, \theta) = -\frac{1}{2}y^T(K + \sigma_n^2 I)^{-1}y - \frac{1}{2}\log|K + \sigma_n^2 I| - \frac{n}{2}\log(2\pi)$$

The gradient with respect to hyperparameter $\theta_j$ is:

相对于超参数 $\theta_j$ 的梯度是：

$$\frac{\partial}{\partial \theta_j} \log p(y|X, \theta) = \frac{1}{2}y^T K^{-1} \frac{\partial K}{\partial \theta_j} K^{-1} y - \frac{1}{2}\text{tr}\left(K^{-1} \frac{\partial K}{\partial \theta_j}\right)$$

#### Computational Complexity (计算复杂度)

The main computational bottleneck is matrix inversion: $O(n^3)$ for $n$ training points. For large datasets, approximation methods are essential.

主要的计算瓶颈是矩阵求逆：对于 $n$ 个训练点需要 $O(n^3)$。对于大型数据集，近似方法是必要的。

### 15.3.3 Interpreting Equations for Learning and Predictions (解释学习和预测方程)

#### Understanding the Predictive Mean (理解预测均值)

The predictive mean $\mu(x_*) = k_*^T(K + \sigma_n^2 I)^{-1}y$ can be interpreted as:

预测均值 $\mu(x_*) = k_*^T(K + \sigma_n^2 I)^{-1}y$ 可以解释为：

1. **Weighted average**: A weighted combination of training outputs
   **加权平均**：训练输出的加权组合

2. **Weights depend on similarity**: Points similar to $x_*$ (high $k(x_*, x_i)$) get higher weights
   **权重取决于相似性**：与 $x_*$ 相似的点（高 $k(x_*, x_i)$）获得更高权重

3. **Smooth interpolation**: The result is a smooth function that passes close to training points
   **平滑插值**：结果是一个接近训练点的平滑函数

#### Understanding the Predictive Variance (理解预测方差)

The predictive variance $\sigma^2(x_*) = k(x_*, x_*) - k_*^T(K + \sigma_n^2 I)^{-1}k_*$ has two terms:

预测方差有两项：

1. **Prior variance**: $k(x_*, x_*)$ - our uncertainty before seeing data
   **先验方差**：$k(x_*, x_*)$ - 看到数据前的不确定性

2. **Information gain**: $k_*^T(K + \sigma_n^2 I)^{-1}k_*$ - reduction in uncertainty due to observations
   **信息增益**：$k_*^T(K + \sigma_n^2 I)^{-1}k_*$ - 由于观测而减少的不确定性

#### Intuitive Understanding (直观理解)

**Near training data**: High similarity means high weights in prediction, low uncertainty
**在训练数据附近**：高相似性意味着预测中的高权重，低不确定性

**Far from training data**: Low similarity means low weights, high uncertainty (approaching prior)
**远离训练数据**：低相似性意味着低权重，高不确定性（接近先验）

### 15.3.4 Worked Example from Scratch (从零开始的详细例子)

Let's work through a complete example step by step.

让我们逐步完成一个完整的例子。

#### Problem Setup (问题设置)

**Task**: Learn the function $f(x) = \sin(x)$ from noisy observations
**任务**：从噪声观测中学习函数 $f(x) = \sin(x)$

**Data**: 3 training points: $(0, 0.1)$, $(\pi/2, 0.9)$, $(\pi, 0.1)$
**数据**：3个训练点：$(0, 0.1)$，$(\pi/2, 0.9)$，$(\pi, 0.1)$

**Kernel**: RBF kernel with $\sigma_f^2 = 1$, $l = 1$, $\sigma_n^2 = 0.1$
**核函数**：RBF核，参数 $\sigma_f^2 = 1$，$l = 1$，$\sigma_n^2 = 0.1$

#### Step 1: Compute Kernel Matrices (计算核矩阵)

Training inputs: $X = [0, \pi/2, \pi]^T$
Training outputs: $y = [0.1, 0.9, 0.1]^T$

训练输入：$X = [0, \pi/2, \pi]^T$
训练输出：$y = [0.1, 0.9, 0.1]^T$

**Kernel matrix** $K$:
$$K_{ij} = \exp\left(-\frac{(x_i - x_j)^2}{2 \cdot 1^2}\right)$$

$$K = \begin{pmatrix}
1.0 & e^{-(\pi/2)^2/2} & e^{-\pi^2/2} \\
e^{-(\pi/2)^2/2} & 1.0 & e^{-(\pi/2)^2/2} \\
e^{-\pi^2/2} & e^{-(\pi/2)^2/2} & 1.0
\end{pmatrix} \approx \begin{pmatrix}
1.0 & 0.013 & 0.00006 \\
0.013 & 1.0 & 0.013 \\
0.00006 & 0.013 & 1.0
\end{pmatrix}$$

**Noisy kernel matrix**:
$$K + \sigma_n^2 I = \begin{pmatrix}
1.1 & 0.013 & 0.00006 \\
0.013 & 1.1 & 0.013 \\
0.00006 & 0.013 & 1.1
\end{pmatrix}$$

#### Step 2: Make Prediction at Test Point (在测试点进行预测)

Let's predict at $x_* = \pi/4$.

让我们在 $x_* = \pi/4$ 处进行预测。

**Kernel vector** $k_*$:
$$k_* = \begin{pmatrix}
e^{-(\pi/4)^2/2} \\
e^{-(\pi/4 - \pi/2)^2/2} \\
e^{-(\pi/4 - \pi)^2/2}
\end{pmatrix} \approx \begin{pmatrix}
0.47 \\
0.47 \\
0.055
\end{pmatrix}$$

**Predictive mean**:
$$\mu_* = k_*^T(K + \sigma_n^2 I)^{-1}y$$

**Predictive variance**:
$$\sigma_*^2 = k(x_*, x_*) - k_*^T(K + \sigma_n^2 I)^{-1}k_* = 1.0 - k_*^T(K + \sigma_n^2 I)^{-1}k_*$$

#### Implementation (实现)

```python
import numpy as np
from scipy.linalg import solve, cholesky

def rbf_kernel(x1, x2, signal_var=1.0, length_scale=1.0):
    """RBF kernel function"""
    dist_sq = np.sum((x1 - x2)**2)
    return signal_var * np.exp(-dist_sq / (2 * length_scale**2))

def gp_predict(X_train, y_train, x_test, noise_var=0.1):
    """Gaussian Process prediction"""
    n = len(X_train)
    
    # Compute kernel matrix
    K = np.zeros((n, n))
    for i in range(n):
        for j in range(n):
            K[i, j] = rbf_kernel(X_train[i], X_train[j])
    
    # Add noise to diagonal
    K_noise = K + noise_var * np.eye(n)
    
    # Compute kernel vector
    k_star = np.array([rbf_kernel(x_test, x_i) for x_i in X_train])
    
    # Solve linear system (more stable than inversion)
    # 解线性系统（比求逆更稳定）
    alpha = solve(K_noise, y_train)
    
    # Predictive mean
    mu_star = k_star.dot(alpha)
    
    # Predictive variance
    v = solve(K_noise, k_star)
    var_star = rbf_kernel(x_test, x_test) - k_star.dot(v)
    
    return mu_star, var_star

# Example usage
X_train = np.array([0, np.pi/2, np.pi])
y_train = np.array([0.1, 0.9, 0.1])
x_test = np.pi/4

mu, var = gp_predict(X_train, y_train, x_test)
print(f"Prediction at x={x_test:.2f}: μ={mu:.3f}, σ²={var:.3f}")
```

### 15.3.5 Making Life Easy with GPyTorch (使用GPyTorch简化生活)

**GPyTorch** is a modern library that makes Gaussian process modeling much easier and more scalable. It's built on PyTorch and provides automatic differentiation for hyperparameter optimization.

**GPyTorch**是一个现代库，使高斯过程建模更容易且更具可扩展性。它基于PyTorch构建，为超参数优化提供自动微分。

#### Basic GPyTorch Example (基本GPyTorch例子)

```python
import torch
import gpytorch

class ExactGPModel(gpytorch.models.ExactGP):
    def __init__(self, train_x, train_y, likelihood):
        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)
        
        # Mean function (constant mean)
        # 均值函数（常数均值）
        self.mean_module = gpytorch.means.ConstantMean()
        
        # Kernel function (RBF with outputscale)
        # 核函数（带输出尺度的RBF）
        self.covar_module = gpytorch.kernels.ScaleKernel(
            gpytorch.kernels.RBFKernel()
        )
    
    def forward(self, x):
        mean_x = self.mean_module(x)
        covar_x = self.covar_module(x)
        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)

# Training data
train_x = torch.tensor([0., np.pi/2, np.pi])
train_y = torch.tensor([0.1, 0.9, 0.1])

# Initialize likelihood and model
likelihood = gpytorch.likelihoods.GaussianLikelihood()
model = ExactGPModel(train_x, train_y, likelihood)

# Training mode
model.train()
likelihood.train()

# Optimize hyperparameters
optimizer = torch.optim.Adam(model.parameters(), lr=0.1)
mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)

for i in range(100):
    optimizer.zero_grad()
    output = model(train_x)
    loss = -mll(output, train_y)
    loss.backward()
    optimizer.step()

# Prediction mode
model.eval()
likelihood.eval()

# Make predictions
test_x = torch.linspace(0, 2*np.pi, 100)
with torch.no_grad():
    observed_pred = likelihood(model(test_x))
    mean = observed_pred.mean
    std = observed_pred.stddev
```

#### Advantages of GPyTorch (GPyTorch的优势)

1. **Automatic differentiation**: No need to manually compute gradients
   **自动微分**：无需手动计算梯度

2. **GPU acceleration**: Seamless GPU support for large-scale problems
   **GPU加速**：为大规模问题提供无缝GPU支持

3. **Modern kernels**: Rich library of kernel functions and compositions
   **现代核函数**：丰富的核函数库和组合

4. **Scalable algorithms**: Built-in support for variational inference and inducing points
   **可扩展算法**：内置对变分推断和诱导点的支持

### 15.3.6 Summary (总结)

Gaussian process inference provides a principled framework for:
1. **Making predictions** with uncertainty quantification
2. **Learning hyperparameters** via marginal likelihood optimization
3. **Interpreting results** through kernel similarity and uncertainty estimates

高斯过程推断为以下方面提供了原则性框架：
1. **进行预测**并量化不确定性
2. **学习超参数**通过边际似然优化
3. **解释结果**通过核相似性和不确定性估计

The key insight is that GPs provide both point predictions and confidence intervals, making them invaluable for decision-making under uncertainty.

关键洞察是GP既提供点预测又提供置信区间，使其在不确定性下的决策中无价。

### 15.3.7 Exercises (练习题)

1. **Mathematical**: Derive the predictive equations for GP regression starting from the joint Gaussian distribution.
   **数学题**：从联合高斯分布开始推导GP回归的预测方程。

2. **Programming**: Implement GP regression from scratch and compare results with GPyTorch on a toy dataset.
   **编程题**：从头实现GP回归，并在玩具数据集上与GPyTorch比较结果。

3. **Conceptual**: Explain why the predictive variance decreases near training points and increases far from them.
   **概念题**：解释为什么预测方差在训练点附近减少，远离训练点时增加。

4. **Application**: Design a GP model for predicting daily temperature using historical weather data. What kernel would you choose and why?
   **应用题**：设计一个GP模型，使用历史天气数据预测日温度。你会选择什么核函数，为什么？ 