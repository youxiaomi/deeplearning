# é«˜æ–¯è¿‡ç¨‹å®è·µé¡¹ç›®æ¦‚è¿°
# Gaussian Processes Practice Project Overview

**ä¸ç¡®å®šæ€§å»ºæ¨¡çš„è‰ºæœ¯ - è®©AIæ‹¥æœ‰"çŸ¥é“è‡ªå·±ä¸çŸ¥é“"çš„æ™ºæ…§**
**The Art of Uncertainty Modeling - Giving AI the Wisdom to "Know What It Doesn't Know"**

---

## ğŸ¯ é¡¹ç›®ç›®æ ‡ | Project Goals

é«˜æ–¯è¿‡ç¨‹æ˜¯æœºå™¨å­¦ä¹ ä¸­æœ€ä¼˜é›…çš„æ¦‚ç‡æ¨¡å‹ä¹‹ä¸€ï¼å®ƒä¸ä»…èƒ½è¿›è¡Œé¢„æµ‹ï¼Œè¿˜èƒ½é‡åŒ–é¢„æµ‹çš„ä¸ç¡®å®šæ€§ã€‚é€šè¿‡è¿™ä¸ªé¡¹ç›®ï¼Œä½ å°†æŒæ¡ï¼š
Gaussian Processes are one of the most elegant probabilistic models in machine learning! They not only make predictions but also quantify prediction uncertainty. Through this project, you will master:

- **é«˜æ–¯è¿‡ç¨‹å›å½’** | **Gaussian Process Regression**: éå‚æ•°è´å¶æ–¯å›å½’çš„å¼ºå¤§å·¥å…·
- **æ ¸å‡½æ•°è®¾è®¡** | **Kernel Function Design**: ç¼–ç å…ˆéªŒçŸ¥è¯†çš„æ•°å­¦è¯­è¨€
- **è´å¶æ–¯ä¼˜åŒ–** | **Bayesian Optimization**: é«˜æ•ˆçš„å…¨å±€ä¼˜åŒ–æ–¹æ³•
- **ä¸ç¡®å®šæ€§é‡åŒ–** | **Uncertainty Quantification**: å¯ä¿¡AIçš„æ ¸å¿ƒæŠ€æœ¯

## ğŸ”¬ ä¸ºä»€ä¹ˆé«˜æ–¯è¿‡ç¨‹å¦‚æ­¤é‡è¦ï¼Ÿ| Why are Gaussian Processes So Important?

**é«˜æ–¯è¿‡ç¨‹è®©AIæ‹¥æœ‰"è°¦é€Š"çš„æ™ºæ…§ï¼**
**Gaussian Processes give AI the wisdom of "humility"!**

åœ¨åŒ»ç–—è¯Šæ–­ä¸­ï¼ŒAIéœ€è¦çŸ¥é“è‡ªå·±çš„è¯Šæ–­æœ‰å¤šå¯é ï¼›åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼ŒAIéœ€è¦è¯†åˆ«æœªçŸ¥çš„å±é™©æƒ…å†µï¼›åœ¨é‡‘èæŠ•èµ„ä¸­ï¼ŒAIéœ€è¦è¯„ä¼°é¢„æµ‹çš„é£é™©ã€‚é«˜æ–¯è¿‡ç¨‹ä¸ºAIæä¾›äº†è¡¨è¾¾ä¸ç¡®å®šæ€§çš„æ•°å­¦æ¡†æ¶ï¼Œè¿™å¯¹äºæ„å»ºå¯ä¿¡ã€å®‰å…¨çš„AIç³»ç»Ÿè‡³å…³é‡è¦ã€‚

In medical diagnosis, AI needs to know how reliable its diagnosis is; in autonomous driving, AI needs to identify unknown dangerous situations; in financial investment, AI needs to assess prediction risks. Gaussian Processes provide AI with a mathematical framework for expressing uncertainty, which is crucial for building trustworthy and safe AI systems.

### é«˜æ–¯è¿‡ç¨‹çš„å‘å±•å†ç¨‹ | Evolution of Gaussian Processes
```
1940s: ç»´çº³è¿‡ç¨‹ç†è®º | Wiener Process Theory
1960s: å…‹é‡Œé‡‘æ’å€¼æ–¹æ³• | Kriging Interpolation Methods
1990s: æœºå™¨å­¦ä¹ ä¸­çš„GP | GP in Machine Learning
2000s: è´å¶æ–¯ä¼˜åŒ–å…´èµ· | Rise of Bayesian Optimization
2010s: æ·±åº¦é«˜æ–¯è¿‡ç¨‹ | Deep Gaussian Processes
2020s: å¯æ‰©å±•GPæ–¹æ³• | Scalable GP Methods
```

## ğŸ“š é¡¹ç›®ç»“æ„æ·±åº¦è§£æ | Deep Project Structure Analysis

### 01_é«˜æ–¯è¿‡ç¨‹åŸºç¡€ | Gaussian Process Fundamentals

**ç†è§£æ— ç©·ç»´çš„ä¼˜é›…æ•°å­¦ï¼**
**Understand the elegant mathematics of infinite dimensions!**

#### GPå›å½’å®ç° | GP Regression Implementation

**é¡¹ç›®æ ¸å¿ƒ | Project Core:**
é«˜æ–¯è¿‡ç¨‹å°†å‡½æ•°è§†ä¸ºéšæœºå˜é‡ï¼Œé€šè¿‡å‡å€¼å‡½æ•°å’Œåæ–¹å·®å‡½æ•°ï¼ˆæ ¸å‡½æ•°ï¼‰æ¥å®Œå…¨å®šä¹‰å‡½æ•°çš„åˆ†å¸ƒã€‚

Gaussian Processes treat functions as random variables, completely defining the distribution of functions through mean functions and covariance functions (kernel functions).

**æ•°å­¦åŸç† | Mathematical Principles:**

**é«˜æ–¯è¿‡ç¨‹å®šä¹‰ | Gaussian Process Definition:**
```
f(x) ~ GP(m(x), k(x, x'))
```

å…¶ä¸­ï¼š
- `m(x)`: å‡å€¼å‡½æ•° | mean function
- `k(x, x')`: æ ¸å‡½æ•°/åæ–¹å·®å‡½æ•° | kernel/covariance function

**é¢„æµ‹å…¬å¼ | Prediction Formula:**
```python
# ç»™å®šè®­ç»ƒæ•°æ® (X, y)ï¼Œé¢„æµ‹æ–°ç‚¹ x*
# Given training data (X, y), predict at new point x*

# åéªŒå‡å€¼ | Posterior mean
Î¼(x*) = k(x*, X) @ K^(-1) @ y

# åéªŒæ–¹å·® | Posterior variance  
ÏƒÂ²(x*) = k(x*, x*) - k(x*, X) @ K^(-1) @ k(X, x*)
```

**å®Œæ•´å®ç° | Complete Implementation:**
```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.linalg import cholesky, solve_triangular
from scipy.optimize import minimize
import warnings
warnings.filterwarnings('ignore')

class GaussianProcess:
    """
    é«˜æ–¯è¿‡ç¨‹å›å½’å®ç°
    Gaussian Process Regression Implementation
    """
    def __init__(self, kernel, noise_variance=1e-6, mean_function=None):
        self.kernel = kernel
        self.noise_variance = noise_variance
        self.mean_function = mean_function if mean_function else lambda x: np.zeros(x.shape[0])
        
        # è®­ç»ƒæ•°æ® | Training data
        self.X_train = None
        self.y_train = None
        self.K_inv = None
        self.L = None  # Choleskyåˆ†è§£ | Cholesky decomposition
        
    def fit(self, X_train, y_train):
        """
        è®­ç»ƒé«˜æ–¯è¿‡ç¨‹æ¨¡å‹
        Train Gaussian Process model
        """
        self.X_train = np.array(X_train)
        self.y_train = np.array(y_train)
        
        if self.X_train.ndim == 1:
            self.X_train = self.X_train.reshape(-1, 1)
        
        # è®¡ç®—åæ–¹å·®çŸ©é˜µ | Compute covariance matrix
        K = self.kernel(self.X_train, self.X_train)
        
        # æ·»åŠ å™ªå£°é¡¹ | Add noise term
        K += self.noise_variance * np.eye(K.shape[0])
        
        # Choleskyåˆ†è§£ç”¨äºæ•°å€¼ç¨³å®šæ€§ | Cholesky decomposition for numerical stability
        try:
            self.L = cholesky(K, lower=True)
        except np.linalg.LinAlgError:
            # å¦‚æœCholeskyåˆ†è§£å¤±è´¥ï¼Œæ·»åŠ æ›´å¤šå™ªå£° | If Cholesky fails, add more noise
            K += 1e-3 * np.eye(K.shape[0])
            self.L = cholesky(K, lower=True)
        
        # è®¡ç®— K^(-1) @ y ç”¨äºé¢„æµ‹ | Compute K^(-1) @ y for prediction
        alpha = solve_triangular(self.L, self.y_train, lower=True)
        self.alpha = solve_triangular(self.L.T, alpha, lower=False)
        
        return self
    
    def predict(self, X_test, return_std=True):
        """
        è¿›è¡Œé¢„æµ‹
        Make predictions
        """
        X_test = np.array(X_test)
        if X_test.ndim == 1:
            X_test = X_test.reshape(-1, 1)
        
        # è®¡ç®—æµ‹è¯•ç‚¹ä¸è®­ç»ƒç‚¹çš„åæ–¹å·® | Compute covariance between test and train points
        K_star = self.kernel(X_test, self.X_train)
        
        # åéªŒå‡å€¼ | Posterior mean
        mean_pred = K_star @ self.alpha
        
        if return_std:
            # è®¡ç®—åéªŒæ–¹å·® | Compute posterior variance
            v = solve_triangular(self.L, K_star.T, lower=True)
            
            # æµ‹è¯•ç‚¹ä¹‹é—´çš„åæ–¹å·® | Covariance between test points
            K_star_star = self.kernel(X_test, X_test)
            
            # åéªŒæ–¹å·® | Posterior variance
            var_pred = np.diag(K_star_star) - np.sum(v**2, axis=0)
            
            # ç¡®ä¿æ–¹å·®éè´Ÿ | Ensure variance is non-negative
            var_pred = np.maximum(var_pred, 0)
            std_pred = np.sqrt(var_pred)
            
            return mean_pred, std_pred
        
        return mean_pred
    
    def sample_posterior(self, X_test, n_samples=5):
        """
        ä»åéªŒåˆ†å¸ƒé‡‡æ ·å‡½æ•°
        Sample functions from posterior distribution
        """
        X_test = np.array(X_test)
        if X_test.ndim == 1:
            X_test = X_test.reshape(-1, 1)
        
        # è·å–åéªŒå‡å€¼å’Œåæ–¹å·® | Get posterior mean and covariance
        mean_pred, std_pred = self.predict(X_test, return_std=True)
        
        # è®¡ç®—å®Œæ•´çš„åéªŒåæ–¹å·®çŸ©é˜µ | Compute full posterior covariance matrix
        K_star = self.kernel(X_test, self.X_train)
        K_star_star = self.kernel(X_test, X_test)
        
        v = solve_triangular(self.L, K_star.T, lower=True)
        cov_pred = K_star_star - v.T @ v
        
        # æ·»åŠ å°çš„å™ªå£°ä»¥ç¡®ä¿æ•°å€¼ç¨³å®šæ€§ | Add small noise for numerical stability
        cov_pred += 1e-6 * np.eye(cov_pred.shape[0])
        
        # ä»å¤šå…ƒæ­£æ€åˆ†å¸ƒé‡‡æ · | Sample from multivariate normal
        samples = np.random.multivariate_normal(mean_pred, cov_pred, n_samples)
        
        return samples
    
    def log_marginal_likelihood(self):
        """
        è®¡ç®—å¯¹æ•°è¾¹é™…ä¼¼ç„¶
        Compute log marginal likelihood
        """
        if self.L is None:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ | Model not trained yet")
        
        # å¯¹æ•°è¾¹é™…ä¼¼ç„¶çš„ä¸‰ä¸ªç»„æˆéƒ¨åˆ† | Three components of log marginal likelihood
        # 1. æ•°æ®æ‹Ÿåˆé¡¹ | Data fit term
        alpha = solve_triangular(self.L, self.y_train, lower=True)
        data_fit = -0.5 * np.sum(alpha**2)
        
        # 2. å¤æ‚åº¦æƒ©ç½šé¡¹ | Complexity penalty term
        complexity_penalty = -np.sum(np.log(np.diag(self.L)))
        
        # 3. å½’ä¸€åŒ–å¸¸æ•° | Normalization constant
        normalization = -0.5 * len(self.y_train) * np.log(2 * np.pi)
        
        return data_fit + complexity_penalty + normalization

# æ ¸å‡½æ•°å®ç° | Kernel function implementations
class RBFKernel:
    """
    å¾„å‘åŸºå‡½æ•°æ ¸ï¼ˆé«˜æ–¯æ ¸ï¼‰
    Radial Basis Function (Gaussian) Kernel
    """
    def __init__(self, length_scale=1.0, signal_variance=1.0):
        self.length_scale = length_scale
        self.signal_variance = signal_variance
    
    def __call__(self, X1, X2):
        """
        è®¡ç®—æ ¸çŸ©é˜µ
        Compute kernel matrix
        """
        # è®¡ç®—å¹³æ–¹æ¬§å‡ é‡Œå¾—è·ç¦» | Compute squared Euclidean distances
        X1 = X1 / self.length_scale
        X2 = X2 / self.length_scale
        
        sqdist = np.sum(X1**2, axis=1).reshape(-1, 1) + np.sum(X2**2, axis=1) - 2 * np.dot(X1, X2.T)
        
        return self.signal_variance * np.exp(-0.5 * sqdist)

class MaternKernel:
    """
    MatÃ©rnæ ¸å‡½æ•°
    MatÃ©rn Kernel Function
    """
    def __init__(self, length_scale=1.0, signal_variance=1.0, nu=1.5):
        self.length_scale = length_scale
        self.signal_variance = signal_variance
        self.nu = nu
    
    def __call__(self, X1, X2):
        """
        è®¡ç®—MatÃ©rnæ ¸çŸ©é˜µ
        Compute MatÃ©rn kernel matrix
        """
        from scipy.special import gamma, kv
        
        # è®¡ç®—è·ç¦» | Compute distances
        X1_scaled = X1 / self.length_scale
        X2_scaled = X2 / self.length_scale
        
        dists = np.sqrt(np.sum((X1_scaled[:, np.newaxis, :] - X2_scaled[np.newaxis, :, :]) ** 2, axis=2))
        
        # MatÃ©rnæ ¸å…¬å¼ | MatÃ©rn kernel formula
        sqrt_2nu_times_r = np.sqrt(2 * self.nu) * dists
        
        # é¿å…åœ¨è·ç¦»ä¸º0æ—¶çš„æ•°å€¼é—®é¢˜ | Avoid numerical issues when distance is 0
        sqrt_2nu_times_r = np.maximum(sqrt_2nu_times_r, 1e-10)
        
        # MatÃ©rnæ ¸ | MatÃ©rn kernel
        kernel_matrix = (2 ** (1 - self.nu) / gamma(self.nu)) * \
                       (sqrt_2nu_times_r ** self.nu) * \
                       kv(self.nu, sqrt_2nu_times_r)
        
        # å¤„ç†è·ç¦»ä¸º0çš„æƒ…å†µ | Handle case when distance is 0
        kernel_matrix[dists == 0] = 1.0
        
        return self.signal_variance * kernel_matrix

class PeriodicKernel:
    """
    å‘¨æœŸæ ¸å‡½æ•°
    Periodic Kernel Function
    """
    def __init__(self, length_scale=1.0, signal_variance=1.0, period=1.0):
        self.length_scale = length_scale
        self.signal_variance = signal_variance
        self.period = period
    
    def __call__(self, X1, X2):
        """
        è®¡ç®—å‘¨æœŸæ ¸çŸ©é˜µ
        Compute periodic kernel matrix
        """
        # è®¡ç®—è·ç¦» | Compute distances
        dists = np.sqrt(np.sum((X1[:, np.newaxis, :] - X2[np.newaxis, :, :]) ** 2, axis=2))
        
        # å‘¨æœŸæ ¸å…¬å¼ | Periodic kernel formula
        kernel_matrix = np.exp(-2 * np.sin(np.pi * dists / self.period) ** 2 / self.length_scale ** 2)
        
        return self.signal_variance * kernel_matrix

# ç¤ºä¾‹ä½¿ç”¨ | Example usage
def gp_regression_demo():
    """
    é«˜æ–¯è¿‡ç¨‹å›å½’æ¼”ç¤º
    Gaussian Process Regression Demo
    """
    # ç”Ÿæˆç¤ºä¾‹æ•°æ® | Generate example data
    np.random.seed(42)
    X_train = np.array([1, 3, 5, 6, 7, 8]).reshape(-1, 1)
    y_train = np.array([1, 3, 2, 5, 4, 6]) + 0.1 * np.random.randn(6)
    
    # æµ‹è¯•ç‚¹ | Test points
    X_test = np.linspace(0, 10, 100).reshape(-1, 1)
    
    # åˆ›å»ºä¸åŒçš„æ ¸å‡½æ•° | Create different kernels
    kernels = {
        'RBF': RBFKernel(length_scale=1.0, signal_variance=1.0),
        'MatÃ©rn': MaternKernel(length_scale=1.0, signal_variance=1.0, nu=1.5),
        'Periodic': PeriodicKernel(length_scale=1.0, signal_variance=1.0, period=3.0)
    }
    
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    
    for i, (kernel_name, kernel) in enumerate(kernels.items()):
        # è®­ç»ƒé«˜æ–¯è¿‡ç¨‹ | Train Gaussian Process
        gp = GaussianProcess(kernel=kernel, noise_variance=0.01)
        gp.fit(X_train, y_train)
        
        # é¢„æµ‹ | Predict
        mean_pred, std_pred = gp.predict(X_test, return_std=True)
        
        # ä»åéªŒé‡‡æ · | Sample from posterior
        samples = gp.sample_posterior(X_test, n_samples=3)
        
        # ç»˜å›¾ | Plot
        ax = axes[i]
        
        # è®­ç»ƒæ•°æ® | Training data
        ax.scatter(X_train.flatten(), y_train, c='red', s=50, zorder=5, label='Training Data')
        
        # é¢„æµ‹å‡å€¼ | Prediction mean
        ax.plot(X_test.flatten(), mean_pred, 'blue', linewidth=2, label='Prediction Mean')
        
        # ä¸ç¡®å®šæ€§åŒºé—´ | Uncertainty interval
        ax.fill_between(X_test.flatten(), 
                       mean_pred - 2 * std_pred, 
                       mean_pred + 2 * std_pred, 
                       alpha=0.3, color='blue', label='95% Confidence')
        
        # åéªŒæ ·æœ¬ | Posterior samples
        for j, sample in enumerate(samples):
            ax.plot(X_test.flatten(), sample, '--', alpha=0.7, 
                   label=f'Sample {j+1}' if i == 0 else '')
        
        ax.set_title(f'{kernel_name} Kernel', fontsize=14, fontweight='bold')
        ax.set_xlabel('X')
        ax.set_ylabel('Y')
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # è®¡ç®—å¯¹æ•°è¾¹é™…ä¼¼ç„¶ | Compute log marginal likelihood
    for kernel_name, kernel in kernels.items():
        gp = GaussianProcess(kernel=kernel, noise_variance=0.01)
        gp.fit(X_train, y_train)
        lml = gp.log_marginal_likelihood()
        print(f"{kernel_name} Kernel - Log Marginal Likelihood: {lml:.4f}")

if __name__ == "__main__":
    gp_regression_demo()
```

#### æ ¸å‡½æ•°è®¾è®¡ | Kernel Function Design

**é¡¹ç›®ç‰¹è‰² | Project Features:**
æ ¸å‡½æ•°æ˜¯é«˜æ–¯è¿‡ç¨‹çš„çµé­‚ï¼Œå®ƒç¼–ç äº†å¯¹å‡½æ•°å½¢çŠ¶çš„å…ˆéªŒçŸ¥è¯†ã€‚è®¾è®¡åˆé€‚çš„æ ¸å‡½æ•°æ˜¯GPæˆåŠŸåº”ç”¨çš„å…³é”®ã€‚

Kernel functions are the soul of Gaussian Processes, encoding prior knowledge about function shapes. Designing appropriate kernel functions is key to successful GP applications.

**æ ¸å‡½æ•°çš„ç»„åˆä¸è®¾è®¡ | Kernel Combination and Design:**
```python
class CompositeKernel:
    """
    å¤åˆæ ¸å‡½æ•°ï¼šæ”¯æŒæ ¸å‡½æ•°çš„åŠ æ³•å’Œä¹˜æ³•ç»„åˆ
    Composite Kernel: Supports additive and multiplicative combinations of kernels
    """
    def __init__(self, kernels, operations):
        """
        Args:
            kernels: æ ¸å‡½æ•°åˆ—è¡¨ | List of kernel functions
            operations: æ“ä½œåˆ—è¡¨ ('+' æˆ– '*') | List of operations ('+' or '*')
        """
        self.kernels = kernels
        self.operations = operations
        
        if len(kernels) != len(operations) + 1:
            raise ValueError("æ ¸å‡½æ•°æ•°é‡åº”è¯¥æ¯”æ“ä½œæ•°é‡å¤š1")
    
    def __call__(self, X1, X2):
        """
        è®¡ç®—å¤åˆæ ¸çŸ©é˜µ
        Compute composite kernel matrix
        """
        result = self.kernels[0](X1, X2)
        
        for i, operation in enumerate(self.operations):
            kernel_matrix = self.kernels[i + 1](X1, X2)
            
            if operation == '+':
                result = result + kernel_matrix
            elif operation == '*':
                result = result * kernel_matrix
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ“ä½œ: {operation}")
        
        return result

class LocalPeriodicKernel:
    """
    å±€éƒ¨å‘¨æœŸæ ¸ï¼šRBFæ ¸ä¸å‘¨æœŸæ ¸çš„ä¹˜ç§¯ï¼Œç”¨äºå»ºæ¨¡å±€éƒ¨å‘¨æœŸæ€§
    Local Periodic Kernel: Product of RBF and Periodic kernels for local periodicity
    """
    def __init__(self, length_scale=1.0, signal_variance=1.0, period=1.0, local_length=1.0):
        self.rbf_kernel = RBFKernel(length_scale=local_length, signal_variance=1.0)
        self.periodic_kernel = PeriodicKernel(length_scale=length_scale, 
                                             signal_variance=1.0, period=period)
        self.signal_variance = signal_variance
    
    def __call__(self, X1, X2):
        """
        è®¡ç®—å±€éƒ¨å‘¨æœŸæ ¸
        Compute local periodic kernel
        """
        rbf_matrix = self.rbf_kernel(X1, X2)
        periodic_matrix = self.periodic_kernel(X1, X2)
        
        return self.signal_variance * rbf_matrix * periodic_matrix

class LinearKernel:
    """
    çº¿æ€§æ ¸å‡½æ•°
    Linear Kernel Function
    """
    def __init__(self, signal_variance=1.0, offset=0.0):
        self.signal_variance = signal_variance
        self.offset = offset
    
    def __call__(self, X1, X2):
        """
        è®¡ç®—çº¿æ€§æ ¸çŸ©é˜µ
        Compute linear kernel matrix
        """
        return self.signal_variance * ((X1 - self.offset) @ (X2 - self.offset).T)

class PolynomialKernel:
    """
    å¤šé¡¹å¼æ ¸å‡½æ•°
    Polynomial Kernel Function
    """
    def __init__(self, signal_variance=1.0, offset=1.0, degree=2):
        self.signal_variance = signal_variance
        self.offset = offset
        self.degree = degree
    
    def __call__(self, X1, X2):
        """
        è®¡ç®—å¤šé¡¹å¼æ ¸çŸ©é˜µ
        Compute polynomial kernel matrix
        """
        linear_kernel = (X1 @ X2.T) + self.offset
        return self.signal_variance * (linear_kernel ** self.degree)

def kernel_design_demo():
    """
    æ ¸å‡½æ•°è®¾è®¡æ¼”ç¤º
    Kernel Function Design Demo
    """
    # ç”Ÿæˆä¸åŒç±»å‹çš„æ•°æ® | Generate different types of data
    np.random.seed(42)
    
    # 1. è¶‹åŠ¿ + å‘¨æœŸæ•°æ® | Trend + Periodic data
    X = np.linspace(0, 10, 50).reshape(-1, 1)
    y_trend_periodic = 0.5 * X.flatten() + 2 * np.sin(2 * np.pi * X.flatten() / 3) + 0.2 * np.random.randn(50)
    
    # 2. å±€éƒ¨å‘¨æœŸæ•°æ® | Local periodic data
    y_local_periodic = np.exp(-0.1 * (X.flatten() - 5)**2) * np.sin(2 * np.pi * X.flatten()) + 0.1 * np.random.randn(50)
    
    # 3. å¤šé¡¹å¼æ•°æ® | Polynomial data
    y_polynomial = 0.1 * X.flatten()**2 - 0.05 * X.flatten()**3 + 0.3 * np.random.randn(50)
    
    datasets = [
        (y_trend_periodic, "Trend + Periodic"),
        (y_local_periodic, "Local Periodic"),
        (y_polynomial, "Polynomial")
    ]
    
    # è®¾è®¡å¯¹åº”çš„æ ¸å‡½æ•° | Design corresponding kernels
    kernels = [
        # è¶‹åŠ¿ + å‘¨æœŸï¼šçº¿æ€§æ ¸ + å‘¨æœŸæ ¸ | Trend + Periodic: Linear + Periodic
        CompositeKernel([
            LinearKernel(signal_variance=0.5),
            PeriodicKernel(length_scale=1.0, signal_variance=1.0, period=3.0)
        ], ['+']),
        
        # å±€éƒ¨å‘¨æœŸï¼šå±€éƒ¨å‘¨æœŸæ ¸ | Local Periodic: Local Periodic Kernel
        LocalPeriodicKernel(length_scale=2.0, signal_variance=1.0, period=1.0, local_length=3.0),
        
        # å¤šé¡¹å¼ï¼šRBFæ ¸ + å¤šé¡¹å¼æ ¸ | Polynomial: RBF + Polynomial
        CompositeKernel([
            RBFKernel(length_scale=2.0, signal_variance=0.5),
            PolynomialKernel(signal_variance=0.5, degree=3)
        ], ['+'])
    ]
    
    fig, axes = plt.subplots(3, 1, figsize=(12, 15))
    
    X_test = np.linspace(-1, 11, 100).reshape(-1, 1)
    
    for i, ((y_data, title), kernel) in enumerate(zip(datasets, kernels)):
        # è®­ç»ƒé«˜æ–¯è¿‡ç¨‹ | Train Gaussian Process
        gp = GaussianProcess(kernel=kernel, noise_variance=0.1)
        gp.fit(X, y_data)
        
        # é¢„æµ‹ | Predict
        mean_pred, std_pred = gp.predict(X_test, return_std=True)
        
        # ç»˜å›¾ | Plot
        ax = axes[i]
        
        # è®­ç»ƒæ•°æ® | Training data
        ax.scatter(X.flatten(), y_data, c='red', s=30, zorder=5, label='Training Data')
        
        # é¢„æµ‹å‡å€¼ | Prediction mean
        ax.plot(X_test.flatten(), mean_pred, 'blue', linewidth=2, label='GP Prediction')
        
        # ä¸ç¡®å®šæ€§åŒºé—´ | Uncertainty interval
        ax.fill_between(X_test.flatten(), 
                       mean_pred - 2 * std_pred, 
                       mean_pred + 2 * std_pred, 
                       alpha=0.3, color='blue', label='95% Confidence')
        
        ax.set_title(f'{title} - Custom Kernel Design', fontsize=14, fontweight='bold')
        ax.set_xlabel('X')
        ax.set_ylabel('Y')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # æ˜¾ç¤ºå¯¹æ•°è¾¹é™…ä¼¼ç„¶ | Show log marginal likelihood
        lml = gp.log_marginal_likelihood()
        ax.text(0.02, 0.98, f'Log ML: {lml:.2f}', transform=ax.transAxes, 
               verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat'))
    
    plt.tight_layout()
    plt.show()

if __name__ == "__main__":
    kernel_design_demo()
```

### 02_è´å¶æ–¯ä¼˜åŒ– | Bayesian Optimization

**é«˜æ•ˆçš„å…¨å±€ä¼˜åŒ–è‰ºæœ¯ï¼**
**The art of efficient global optimization!**

#### è¶…å‚æ•°ä¼˜åŒ– | Hyperparameter Optimization

**é¡¹ç›®ä»·å€¼ | Project Value:**
è´å¶æ–¯ä¼˜åŒ–ä½¿ç”¨é«˜æ–¯è¿‡ç¨‹æ¥å»ºæ¨¡ç›®æ ‡å‡½æ•°ï¼Œé€šè¿‡é‡‡é›†å‡½æ•°æ¥å¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨ï¼Œå®ç°é«˜æ•ˆçš„å…¨å±€ä¼˜åŒ–ã€‚

Bayesian Optimization uses Gaussian Processes to model objective functions, balancing exploration and exploitation through acquisition functions for efficient global optimization.

**è´å¶æ–¯ä¼˜åŒ–å®ç° | Bayesian Optimization Implementation:**
```python
from scipy.stats import norm
from scipy.optimize import minimize

class BayesianOptimizer:
    """
    è´å¶æ–¯ä¼˜åŒ–å™¨
    Bayesian Optimizer
    """
    def __init__(self, objective_function, bounds, kernel=None, 
                 acquisition='ei', xi=0.01, n_initial=5):
        """
        Args:
            objective_function: ç›®æ ‡å‡½æ•° | Objective function
            bounds: æœç´¢ç©ºé—´è¾¹ç•Œ | Search space bounds [(low, high), ...]
            kernel: é«˜æ–¯è¿‡ç¨‹æ ¸å‡½æ•° | GP kernel function
            acquisition: é‡‡é›†å‡½æ•°ç±»å‹ | Acquisition function type
            xi: æ¢ç´¢å‚æ•° | Exploration parameter
            n_initial: åˆå§‹éšæœºé‡‡æ ·æ•°é‡ | Number of initial random samples
        """
        self.objective_function = objective_function
        self.bounds = np.array(bounds)
        self.kernel = kernel if kernel else RBFKernel(length_scale=1.0)
        self.acquisition = acquisition
        self.xi = xi
        self.n_initial = n_initial
        
        # è§‚æµ‹æ•°æ® | Observed data
        self.X_observed = []
        self.y_observed = []
        
        # é«˜æ–¯è¿‡ç¨‹æ¨¡å‹ | Gaussian Process model
        self.gp = None
        
        # æœ€ä¼˜å€¼è®°å½• | Best value record
        self.best_x = None
        self.best_y = None
        self.optimization_history = []
    
    def _random_sampling(self, n_samples):
        """
        åœ¨æœç´¢ç©ºé—´å†…éšæœºé‡‡æ ·
        Random sampling within search space
        """
        samples = []
        for _ in range(n_samples):
            sample = []
            for low, high in self.bounds:
                sample.append(np.random.uniform(low, high))
            samples.append(sample)
        return np.array(samples)
    
    def _acquisition_function(self, X):
        """
        è®¡ç®—é‡‡é›†å‡½æ•°å€¼
        Compute acquisition function values
        """
        if len(self.X_observed) == 0:
            return np.ones(X.shape[0])
        
        # é¢„æµ‹å‡å€¼å’Œæ–¹å·® | Predict mean and variance
        mu, sigma = self.gp.predict(X, return_std=True)
        
        # é¿å…é™¤é›¶ | Avoid division by zero
        sigma = np.maximum(sigma, 1e-9)
        
        if self.acquisition == 'ei':
            # Expected Improvement | æœŸæœ›æ”¹è¿›
            return self._expected_improvement(mu, sigma)
        elif self.acquisition == 'pi':
            # Probability of Improvement | æ”¹è¿›æ¦‚ç‡
            return self._probability_of_improvement(mu, sigma)
        elif self.acquisition == 'ucb':
            # Upper Confidence Bound | ç½®ä¿¡ä¸Šç•Œ
            return self._upper_confidence_bound(mu, sigma)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„é‡‡é›†å‡½æ•°: {self.acquisition}")
    
    def _expected_improvement(self, mu, sigma):
        """
        æœŸæœ›æ”¹è¿›é‡‡é›†å‡½æ•°
        Expected Improvement acquisition function
        """
        if self.best_y is None:
            return np.ones_like(mu)
        
        # è®¡ç®—æ”¹è¿›é‡ | Compute improvement
        improvement = mu - self.best_y - self.xi
        Z = improvement / sigma
        
        # æœŸæœ›æ”¹è¿›å…¬å¼ | Expected improvement formula
        ei = improvement * norm.cdf(Z) + sigma * norm.pdf(Z)
        
        return ei
    
    def _probability_of_improvement(self, mu, sigma):
        """
        æ”¹è¿›æ¦‚ç‡é‡‡é›†å‡½æ•°
        Probability of Improvement acquisition function
        """
        if self.best_y is None:
            return np.ones_like(mu)
        
        # æ”¹è¿›æ¦‚ç‡å…¬å¼ | Probability of improvement formula
        Z = (mu - self.best_y - self.xi) / sigma
        pi = norm.cdf(Z)
        
        return pi
    
    def _upper_confidence_bound(self, mu, sigma, beta=2.0):
        """
        ç½®ä¿¡ä¸Šç•Œé‡‡é›†å‡½æ•°
        Upper Confidence Bound acquisition function
        """
        return mu + beta * sigma
    
    def _optimize_acquisition(self):
        """
        ä¼˜åŒ–é‡‡é›†å‡½æ•°æ‰¾åˆ°ä¸‹ä¸€ä¸ªé‡‡æ ·ç‚¹
        Optimize acquisition function to find next sampling point
        """
        def objective(x):
            # æœ€å°åŒ–è´Ÿé‡‡é›†å‡½æ•°å€¼ | Minimize negative acquisition function
            return -self._acquisition_function(x.reshape(1, -1))[0]
        
        best_x = None
        best_acquisition = -np.inf
        
        # å¤šèµ·ç‚¹ä¼˜åŒ– | Multi-start optimization
        n_restarts = 10
        for _ in range(n_restarts):
            # éšæœºèµ·ç‚¹ | Random starting point
            x0 = []
            for low, high in self.bounds:
                x0.append(np.random.uniform(low, high))
            x0 = np.array(x0)
            
            # ä¼˜åŒ– | Optimize
            result = minimize(objective, x0, method='L-BFGS-B', 
                            bounds=self.bounds)
            
            if result.success and -result.fun > best_acquisition:
                best_acquisition = -result.fun
                best_x = result.x
        
        return best_x
    
    def optimize(self, n_iterations=20, verbose=True):
        """
        æ‰§è¡Œè´å¶æ–¯ä¼˜åŒ–
        Perform Bayesian Optimization
        """
        if verbose:
            print("å¼€å§‹è´å¶æ–¯ä¼˜åŒ–...")
            print("Starting Bayesian Optimization...")
        
        # åˆå§‹éšæœºé‡‡æ · | Initial random sampling
        X_init = self._random_sampling(self.n_initial)
        
        for x in X_init:
            y = self.objective_function(x)
            self.X_observed.append(x)
            self.y_observed.append(y)
            
            # æ›´æ–°æœ€ä¼˜å€¼ | Update best value
            if self.best_y is None or y > self.best_y:
                self.best_y = y
                self.best_x = x.copy()
            
            self.optimization_history.append({
                'iteration': len(self.optimization_history),
                'x': x.copy(),
                'y': y,
                'best_y': self.best_y
            })
        
        # è´å¶æ–¯ä¼˜åŒ–ä¸»å¾ªç¯ | Main Bayesian optimization loop
        for iteration in range(n_iterations):
            # è®­ç»ƒé«˜æ–¯è¿‡ç¨‹ | Train Gaussian Process
            self.gp = GaussianProcess(kernel=self.kernel, noise_variance=1e-6)
            self.gp.fit(np.array(self.X_observed), np.array(self.y_observed))
            
            # ä¼˜åŒ–é‡‡é›†å‡½æ•° | Optimize acquisition function
            next_x = self._optimize_acquisition()
            
            if next_x is None:
                if verbose:
                    print("æ— æ³•æ‰¾åˆ°ä¸‹ä¸€ä¸ªé‡‡æ ·ç‚¹ï¼Œä¼˜åŒ–ç»ˆæ­¢")
                break
            
            # è¯„ä¼°ç›®æ ‡å‡½æ•° | Evaluate objective function
            next_y = self.objective_function(next_x)
            
            # æ›´æ–°è§‚æµ‹æ•°æ® | Update observed data
            self.X_observed.append(next_x)
            self.y_observed.append(next_y)
            
            # æ›´æ–°æœ€ä¼˜å€¼ | Update best value
            if next_y > self.best_y:
                self.best_y = next_y
                self.best_x = next_x.copy()
            
            # è®°å½•ä¼˜åŒ–å†å² | Record optimization history
            self.optimization_history.append({
                'iteration': len(self.optimization_history),
                'x': next_x.copy(),
                'y': next_y,
                'best_y': self.best_y
            })
            
            if verbose and (iteration + 1) % 5 == 0:
                print(f"Iteration {iteration + 1}, Best Y: {self.best_y:.6f}")
        
        if verbose:
            print(f"è´å¶æ–¯ä¼˜åŒ–å®Œæˆï¼æœ€ä¼˜å€¼: {self.best_y:.6f}")
            print(f"Bayesian Optimization completed! Best value: {self.best_y:.6f}")
        
        return self.best_x, self.best_y
    
    def plot_optimization_progress(self):
        """
        ç»˜åˆ¶ä¼˜åŒ–è¿›åº¦
        Plot optimization progress
        """
        iterations = [h['iteration'] for h in self.optimization_history]
        best_values = [h['best_y'] for h in self.optimization_history]
        current_values = [h['y'] for h in self.optimization_history]
        
        plt.figure(figsize=(12, 5))
        
        plt.subplot(1, 2, 1)
        plt.plot(iterations, current_values, 'o-', alpha=0.7, label='Current Values')
        plt.plot(iterations, best_values, 'r-', linewidth=2, label='Best Value')
        plt.xlabel('Iteration')
        plt.ylabel('Objective Value')
        plt.title('Optimization Progress')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # å¦‚æœæ˜¯ä¸€ç»´é—®é¢˜ï¼Œç»˜åˆ¶é‡‡é›†å‡½æ•° | If 1D problem, plot acquisition function
        if len(self.bounds) == 1:
            plt.subplot(1, 2, 2)
            
            # ç»˜åˆ¶ç›®æ ‡å‡½æ•°å’Œé«˜æ–¯è¿‡ç¨‹é¢„æµ‹ | Plot objective function and GP prediction
            x_plot = np.linspace(self.bounds[0, 0], self.bounds[0, 1], 200).reshape(-1, 1)
            
            if self.gp is not None:
                mu, sigma = self.gp.predict(x_plot, return_std=True)
                
                plt.plot(x_plot, mu, 'b-', label='GP Mean')
                plt.fill_between(x_plot.flatten(), mu - 2*sigma, mu + 2*sigma, 
                               alpha=0.3, color='blue', label='95% Confidence')
                
                # ç»˜åˆ¶é‡‡é›†å‡½æ•° | Plot acquisition function
                acquisition_values = self._acquisition_function(x_plot)
                plt.plot(x_plot, acquisition_values, 'g-', label='Acquisition Function')
            
            # ç»˜åˆ¶è§‚æµ‹ç‚¹ | Plot observed points
            X_obs = np.array(self.X_observed)
            y_obs = np.array(self.y_observed)
            plt.scatter(X_obs.flatten(), y_obs, c='red', s=50, zorder=5, label='Observations')
            
            plt.xlabel('X')
            plt.ylabel('Y')
            plt.title('Gaussian Process and Acquisition Function')
            plt.legend()
            plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()

# æµ‹è¯•å‡½æ•° | Test functions
def test_function_1d(x):
    """ä¸€ç»´æµ‹è¯•å‡½æ•°"""
    return -(x[0] - 0.3)**2 * np.sin(15 * x[0]) + 0.5

def test_function_2d(x):
    """äºŒç»´æµ‹è¯•å‡½æ•°ï¼ˆBraninå‡½æ•°ï¼‰"""
    x1, x2 = x[0], x[1]
    a = 1
    b = 5.1 / (4 * np.pi**2)
    c = 5 / np.pi
    r = 6
    s = 10
    t = 1 / (8 * np.pi)
    
    return -(a * (x2 - b * x1**2 + c * x1 - r)**2 + s * (1 - t) * np.cos(x1) + s)

def bayesian_optimization_demo():
    """
    è´å¶æ–¯ä¼˜åŒ–æ¼”ç¤º
    Bayesian Optimization Demo
    """
    print("=== ä¸€ç»´è´å¶æ–¯ä¼˜åŒ–æ¼”ç¤º ===")
    print("=== 1D Bayesian Optimization Demo ===")
    
    # ä¸€ç»´ä¼˜åŒ– | 1D optimization
    bounds_1d = [(-1, 1)]
    optimizer_1d = BayesianOptimizer(
        objective_function=test_function_1d,
        bounds=bounds_1d,
        acquisition='ei',
        n_initial=3
    )
    
    best_x_1d, best_y_1d = optimizer_1d.optimize(n_iterations=15, verbose=True)
    print(f"æœ€ä¼˜è§£: x = {best_x_1d[0]:.4f}, f(x) = {best_y_1d:.4f}")
    
    optimizer_1d.plot_optimization_progress()
    
    print("\n=== äºŒç»´è´å¶æ–¯ä¼˜åŒ–æ¼”ç¤º ===")
    print("=== 2D Bayesian Optimization Demo ===")
    
    # äºŒç»´ä¼˜åŒ– | 2D optimization
    bounds_2d = [(-5, 10), (0, 15)]
    optimizer_2d = BayesianOptimizer(
        objective_function=test_function_2d,
        bounds=bounds_2d,
        acquisition='ei',
        n_initial=5
    )
    
    best_x_2d, best_y_2d = optimizer_2d.optimize(n_iterations=25, verbose=True)
    print(f"æœ€ä¼˜è§£: x = [{best_x_2d[0]:.4f}, {best_x_2d[1]:.4f}], f(x) = {best_y_2d:.4f}")
    
    optimizer_2d.plot_optimization_progress()

if __name__ == "__main__":
    bayesian_optimization_demo()
```

#### ç¥ç»æ¶æ„æœç´¢ | Neural Architecture Search

**é¡¹ç›®å‰æ²¿ | Project Frontier:**
ç¥ç»æ¶æ„æœç´¢ä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–æ¥è‡ªåŠ¨è®¾è®¡ç¥ç»ç½‘ç»œæ¶æ„ï¼Œæ˜¯AutoMLçš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚

Neural Architecture Search uses Bayesian Optimization to automatically design neural network architectures, being an important component of AutoML.

**NASå®ç°æ¡†æ¶ | NAS Implementation Framework:**
```python
class NeuralArchitectureSearch:
    """
    ç¥ç»æ¶æ„æœç´¢ç³»ç»Ÿ
    Neural Architecture Search System
    """
    def __init__(self, search_space, performance_estimator, max_epochs=10):
        self.search_space = search_space
        self.performance_estimator = performance_estimator
        self.max_epochs = max_epochs
        
        # æ¶æ„ç¼–ç å™¨ | Architecture encoder
        self.architecture_encoder = self._create_architecture_encoder()
        
        # è´å¶æ–¯ä¼˜åŒ–å™¨ | Bayesian optimizer
        self.optimizer = None
        
    def _create_architecture_encoder(self):
        """
        åˆ›å»ºæ¶æ„ç¼–ç å™¨ï¼Œå°†æ¶æ„è½¬æ¢ä¸ºå‘é‡
        Create architecture encoder to convert architectures to vectors
        """
        # æœç´¢ç©ºé—´ç»´åº¦ | Search space dimensions
        encoding_dims = []
        
        for param_name, param_range in self.search_space.items():
            if isinstance(param_range, list):
                encoding_dims.append(len(param_range))
            else:
                encoding_dims.append(1)  # è¿ç»­å‚æ•°
        
        return encoding_dims
    
    def encode_architecture(self, architecture):
        """
        å°†æ¶æ„ç¼–ç ä¸ºå‘é‡
        Encode architecture as vector
        """
        encoded = []
        
        for param_name, param_range in self.search_space.items():
            value = architecture[param_name]
            
            if isinstance(param_range, list):
                # åˆ†ç±»å‚æ•°ï¼šone-hotç¼–ç  | Categorical parameter: one-hot encoding
                encoded.append(param_range.index(value))
            else:
                # è¿ç»­å‚æ•°ï¼šå½’ä¸€åŒ– | Continuous parameter: normalize
                low, high = param_range
                normalized = (value - low) / (high - low)
                encoded.append(normalized)
        
        return np.array(encoded)
    
    def decode_architecture(self, encoded_vector):
        """
        å°†å‘é‡è§£ç ä¸ºæ¶æ„
        Decode vector to architecture
        """
        architecture = {}
        idx = 0
        
        for param_name, param_range in self.search_space.items():
            if isinstance(param_range, list):
                # åˆ†ç±»å‚æ•° | Categorical parameter
                param_idx = int(round(encoded_vector[idx]))
                param_idx = max(0, min(len(param_range) - 1, param_idx))
                architecture[param_name] = param_range[param_idx]
            else:
                # è¿ç»­å‚æ•° | Continuous parameter
                low, high = param_range
                normalized_value = max(0, min(1, encoded_vector[idx]))
                architecture[param_name] = low + normalized_value * (high - low)
            
            idx += 1
        
        return architecture
    
    def objective_function(self, encoded_vector):
        """
        ç›®æ ‡å‡½æ•°ï¼šè¯„ä¼°æ¶æ„æ€§èƒ½
        Objective function: evaluate architecture performance
        """
        # è§£ç æ¶æ„ | Decode architecture
        architecture = self.decode_architecture(encoded_vector)
        
        # è¯„ä¼°æ€§èƒ½ | Evaluate performance
        performance = self.performance_estimator(architecture)
        
        return performance
    
    def search(self, n_iterations=50):
        """
        æ‰§è¡Œç¥ç»æ¶æ„æœç´¢
        Perform Neural Architecture Search
        """
        print("å¼€å§‹ç¥ç»æ¶æ„æœç´¢...")
        print("Starting Neural Architecture Search...")
        
        # è®¾ç½®æœç´¢è¾¹ç•Œ | Set search bounds
        bounds = []
        for param_name, param_range in self.search_space.items():
            if isinstance(param_range, list):
                bounds.append((0, len(param_range) - 1))
            else:
                bounds.append((0, 1))  # å½’ä¸€åŒ–èŒƒå›´
        
        # åˆ›å»ºè´å¶æ–¯ä¼˜åŒ–å™¨ | Create Bayesian optimizer
        self.optimizer = BayesianOptimizer(
            objective_function=self.objective_function,
            bounds=bounds,
            acquisition='ei',
            n_initial=10
        )
        
        # æ‰§è¡Œæœç´¢ | Perform search
        best_encoded, best_performance = self.optimizer.optimize(
            n_iterations=n_iterations, verbose=True
        )
        
        # è§£ç æœ€ä¼˜æ¶æ„ | Decode best architecture
        best_architecture = self.decode_architecture(best_encoded)
        
        print(f"æœç´¢å®Œæˆï¼æœ€ä¼˜æ¶æ„æ€§èƒ½: {best_performance:.4f}")
        print(f"Search completed! Best architecture performance: {best_performance:.4f}")
        print(f"æœ€ä¼˜æ¶æ„: {best_architecture}")
        print(f"Best architecture: {best_architecture}")
        
        return best_architecture, best_performance

# ç®€åŒ–çš„æ€§èƒ½ä¼°è®¡å™¨ç¤ºä¾‹ | Simplified performance estimator example
def simple_performance_estimator(architecture):
    """
    ç®€åŒ–çš„æ¶æ„æ€§èƒ½ä¼°è®¡å™¨
    Simplified architecture performance estimator
    """
    # æ¨¡æ‹Ÿè®­ç»ƒæ—¶é—´å’Œå‡†ç¡®ç‡çš„æƒè¡¡ | Simulate trade-off between training time and accuracy
    
    # åŸºç¡€å‡†ç¡®ç‡ | Base accuracy
    accuracy = 0.7
    
    # å±‚æ•°å½±å“ | Number of layers effect
    n_layers = architecture['n_layers']
    accuracy += 0.05 * min(n_layers, 5)  # æ›´å¤šå±‚æé«˜å‡†ç¡®ç‡ï¼Œä½†æœ‰ä¸Šé™
    
    # éšè—å•å…ƒæ•°å½±å“ | Hidden units effect
    hidden_units = architecture['hidden_units']
    accuracy += 0.1 * (hidden_units / 512)  # æ›´å¤šå•å…ƒæé«˜å‡†ç¡®ç‡
    
    # å­¦ä¹ ç‡å½±å“ | Learning rate effect
    learning_rate = architecture['learning_rate']
    optimal_lr = 0.001
    lr_penalty = abs(np.log10(learning_rate) - np.log10(optimal_lr)) * 0.02
    accuracy -= lr_penalty
    
    # æ¿€æ´»å‡½æ•°å½±å“ | Activation function effect
    activation_bonus = {
        'relu': 0.02,
        'tanh': 0.01,
        'sigmoid': 0.005
    }
    accuracy += activation_bonus.get(architecture['activation'], 0)
    
    # æ·»åŠ éšæœºå™ªå£°æ¨¡æ‹Ÿå®éªŒä¸ç¡®å®šæ€§ | Add random noise to simulate experimental uncertainty
    noise = np.random.normal(0, 0.01)
    accuracy += noise
    
    return max(0, min(1, accuracy))

def nas_demo():
    """
    ç¥ç»æ¶æ„æœç´¢æ¼”ç¤º
    Neural Architecture Search Demo
    """
    # å®šä¹‰æœç´¢ç©ºé—´ | Define search space
    search_space = {
        'n_layers': [2, 3, 4, 5, 6],  # å±‚æ•°é€‰æ‹©
        'hidden_units': (64, 512),    # éšè—å•å…ƒæ•°èŒƒå›´
        'learning_rate': (1e-4, 1e-2), # å­¦ä¹ ç‡èŒƒå›´
        'activation': ['relu', 'tanh', 'sigmoid']  # æ¿€æ´»å‡½æ•°é€‰æ‹©
    }
    
    # åˆ›å»ºNASç³»ç»Ÿ | Create NAS system
    nas = NeuralArchitectureSearch(
        search_space=search_space,
        performance_estimator=simple_performance_estimator
    )
    
    # æ‰§è¡Œæœç´¢ | Perform search
    best_arch, best_perf = nas.search(n_iterations=30)
    
    # å¯è§†åŒ–æœç´¢è¿‡ç¨‹ | Visualize search process
    nas.optimizer.plot_optimization_progress()

if __name__ == "__main__":
    nas_demo()
```

---

**ğŸ¯ é¡¹ç›®å®Œæˆæ£€æŸ¥æ¸…å• | Project Completion Checklist:**

### ç†è®ºç†è§£ | Theoretical Understanding
- [ ] æ·±å…¥ç†è§£é«˜æ–¯è¿‡ç¨‹çš„æ•°å­¦åŸºç¡€å’Œæ¦‚ç‡è§£é‡Š
- [ ] æŒæ¡ä¸åŒæ ¸å‡½æ•°çš„ç‰¹æ€§å’Œé€‚ç”¨åœºæ™¯
- [ ] ç†è§£è´å¶æ–¯ä¼˜åŒ–çš„åŸç†å’Œé‡‡é›†å‡½æ•°è®¾è®¡
- [ ] èƒ½å¤Ÿåˆ†æGPæ¨¡å‹çš„ä¸ç¡®å®šæ€§é‡åŒ–èƒ½åŠ›

### ç¼–ç¨‹å®ç° | Programming Implementation
- [ ] ä»é›¶å®ç°é«˜æ–¯è¿‡ç¨‹å›å½’ç®—æ³•
- [ ] è®¾è®¡å’Œç»„åˆå„ç§æ ¸å‡½æ•°
- [ ] å®ç°å®Œæ•´çš„è´å¶æ–¯ä¼˜åŒ–æ¡†æ¶
- [ ] æŒæ¡æ•°å€¼ç¨³å®šæ€§å’Œè®¡ç®—æ•ˆç‡çš„ä¼˜åŒ–æŠ€å·§

### å®é™…åº”ç”¨ | Practical Applications
- [ ] åœ¨çœŸå®æ•°æ®ä¸Šè·å¾—è‰¯å¥½çš„å›å½’å’Œä¼˜åŒ–æ•ˆæœ
- [ ] æˆåŠŸåº”ç”¨åˆ°è¶…å‚æ•°ä¼˜åŒ–é—®é¢˜
- [ ] ç†è§£å¹¶å¤„ç†é«˜ç»´å’Œå¤æ‚ä¼˜åŒ–åœºæ™¯
- [ ] åˆ†ææ¨¡å‹çš„ä¸ç¡®å®šæ€§å’Œç½®ä¿¡åŒºé—´

**è®°ä½**: é«˜æ–¯è¿‡ç¨‹æ˜¯æœºå™¨å­¦ä¹ ä¸­æœ€ä¼˜é›…çš„æ¦‚ç‡æ¨¡å‹ï¼Œå®ƒæ•™ä¼šAI"çŸ¥é“è‡ªå·±ä¸çŸ¥é“"çš„è°¦é€Šæ™ºæ…§ã€‚é€šè¿‡è¿™ä¸ªé¡¹ç›®ï¼Œä½ å°†æŒæ¡ä¸ç¡®å®šæ€§å»ºæ¨¡å’Œè´å¶æ–¯ä¼˜åŒ–çš„æ ¸å¿ƒæŠ€æœ¯ï¼

**Remember**: Gaussian Processes are the most elegant probabilistic models in machine learning, teaching AI the humble wisdom of "knowing what it doesn't know". Through this project, you will master the core technologies of uncertainty modeling and Bayesian optimization! 