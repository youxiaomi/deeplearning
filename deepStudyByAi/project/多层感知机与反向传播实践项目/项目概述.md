# å¤šå±‚æ„ŸçŸ¥æœºä¸åå‘ä¼ æ’­å®è·µé¡¹ç›®æ¦‚è¿°
# Multi-layer Perceptron and Backpropagation Practice Project Overview

**æ·±åº¦å­¦ä¹ æ ¸å¿ƒç®—æ³•çš„å®è·µä¹‹æ—…**
**A Practical Journey Through Core Deep Learning Algorithms**

---

## ğŸ¯ é¡¹ç›®ç›®æ ‡ | Project Goals

é€šè¿‡è¿™ä¸ªé¡¹ç›®ï¼Œä½ å°†æŒæ¡æ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒç®—æ³•ï¼š
Through this project, you will master the core algorithms of deep learning:

- **åå‘ä¼ æ’­ç®—æ³•** | **Backpropagation Algorithm**: æ·±åº¦å­¦ä¹ è®­ç»ƒçš„æ ¸å¿ƒæœºåˆ¶
- **å¤šå±‚ç¥ç»ç½‘ç»œ** | **Multi-layer Neural Networks**: æ„å»ºå¤æ‚çš„éçº¿æ€§æ¨¡å‹
- **æ¢¯åº¦ä¼˜åŒ–** | **Gradient Optimization**: ç†è§£å„ç§ä¼˜åŒ–å™¨çš„å·¥ä½œåŸç†
- **å®é™…åº”ç”¨** | **Real Applications**: å°†ç†è®ºåº”ç”¨åˆ°å¤æ‚çš„å®é™…é—®é¢˜

## ğŸ”¬ ä¸ºä»€ä¹ˆè¿™ä¸ªé¡¹ç›®å¦‚æ­¤é‡è¦ï¼Ÿ| Why is This Project So Important?

**åå‘ä¼ æ’­ç®—æ³•æ˜¯æ·±åº¦å­¦ä¹ çš„çµé­‚ï¼**
**Backpropagation is the soul of deep learning!**

å°±åƒå­¦ä¹ å¼€è½¦å¿…é¡»ç†è§£å‘åŠ¨æœºåŸç†ä¸€æ ·ï¼Œæƒ³è¦ç²¾é€šæ·±åº¦å­¦ä¹ ï¼Œä½ å¿…é¡»æ·±å…¥ç†è§£åå‘ä¼ æ’­ã€‚è¿™ä¸ªç®—æ³•è§£å†³äº†ä¸€ä¸ªçœ‹ä¼¼ä¸å¯èƒ½çš„é—®é¢˜ï¼šåœ¨æœ‰æ•°ç™¾ä¸‡ä¸ªå‚æ•°çš„å¤æ‚ç½‘ç»œä¸­ï¼Œå¦‚ä½•çŸ¥é“æ¯ä¸ªå‚æ•°å¯¹æœ€ç»ˆç»“æœçš„å½±å“ï¼Ÿ

Just like learning to drive requires understanding engine principles, mastering deep learning requires deep understanding of backpropagation. This algorithm solves a seemingly impossible problem: in complex networks with millions of parameters, how do we know each parameter's impact on the final result?

## ğŸ“š é¡¹ç›®ç»“æ„ | Project Structure

### 01_åå‘ä¼ æ’­ç®—æ³•å®ç° | Backpropagation Algorithm Implementation

**æ ¸å¿ƒå†…å®¹ | Core Content:**

1. **åå‘ä¼ æ’­æ•°å­¦æ¨å¯¼ä¸å®ç° | Backpropagation Mathematical Derivation and Implementation**
   - é“¾å¼æ³•åˆ™è¯¦è§£ | Chain rule explanation
   - æ¢¯åº¦è®¡ç®—å…¬å¼æ¨å¯¼ | Gradient calculation formula derivation
   - ä»é›¶å®ç°åå‘ä¼ æ’­ | Implementing backpropagation from scratch
   - æ•°å€¼æ¢¯åº¦éªŒè¯ | Numerical gradient verification

2. **æ¢¯åº¦è®¡ç®—éªŒè¯ | Gradient Calculation Verification**
   - æ•°å€¼å¾®åˆ† vs è§£æå¾®åˆ† | Numerical vs analytical differentiation
   - æ¢¯åº¦æ£€æŸ¥æ–¹æ³• | Gradient checking methods
   - å¸¸è§æ¢¯åº¦é”™è¯¯è°ƒè¯• | Common gradient error debugging

**ç±»æ¯”ç†è§£ | Analogical Understanding:**
åå‘ä¼ æ’­å°±åƒå·¥å‚è´¨æ£€ï¼šå½“æœ€ç»ˆäº§å“æœ‰é—®é¢˜æ—¶ï¼Œéœ€è¦é€†å‘è¿½è¸ªæ¯ä¸ªå·¥åºçš„è´£ä»»ï¼Œæ‰¾å‡ºéœ€è¦æ”¹è¿›çš„ç¯èŠ‚ã€‚
Backpropagation is like factory quality control: when the final product has issues, we need to trace backwards through each process to find areas that need improvement.

**å­¦ä¹ ç›®æ ‡ | Learning Objectives:**
- ä»æ•°å­¦åŸç†åˆ°ä»£ç å®ç°å®Œå…¨æŒæ¡åå‘ä¼ æ’­ | Complete mastery of backpropagation from mathematical principles to code implementation
- èƒ½å¤Ÿè°ƒè¯•å’ŒéªŒè¯æ¢¯åº¦è®¡ç®—çš„æ­£ç¡®æ€§ | Ability to debug and verify gradient calculation correctness

### 02_MLPæ¶æ„è®¾è®¡ | MLP Architecture Design

**æ ¸å¿ƒå†…å®¹ | Core Content:**

1. **ç¥ç»ç½‘ç»œæ¶æ„è®¾è®¡ | Neural Network Architecture Design**
   - å±‚æ•°é€‰æ‹©åŸåˆ™ | Layer number selection principles
   - ç¥ç»å…ƒæ•°é‡ç¡®å®š | Neuron count determination
   - æ¿€æ´»å‡½æ•°é€‰æ‹©ç­–ç•¥ | Activation function selection strategy
   - ç½‘ç»œåˆå§‹åŒ–æ–¹æ³• | Network initialization methods

2. **å‰å‘ä¼ æ’­å®ç° | Forward Propagation Implementation**
   - çŸ©é˜µè¿ç®—ä¼˜åŒ– | Matrix operation optimization
   - æ¿€æ´»å‡½æ•°å®ç° | Activation function implementation
   - æ‰¹å¤„ç†è®¡ç®— | Batch processing computation

**è®¾è®¡åŸåˆ™ | Design Principles:**
- **æ·±åº¦ vs å®½åº¦**: ä»€ä¹ˆæ—¶å€™å¢åŠ å±‚æ•°ï¼Œä»€ä¹ˆæ—¶å€™å¢åŠ æ¯å±‚ç¥ç»å…ƒï¼Ÿ
- **Depth vs Width**: When to add layers, when to add neurons per layer?
- **æ¿€æ´»å‡½æ•°é€‰æ‹©**: ä¸åŒåœºæ™¯ä¸‹çš„æœ€ä½³é€‰æ‹©ç­–ç•¥
- **Activation Function Choice**: Optimal selection strategies for different scenarios

**å­¦ä¹ ç›®æ ‡ | Learning Objectives:**
- æŒæ¡ç¥ç»ç½‘ç»œæ¶æ„è®¾è®¡çš„åŸºæœ¬åŸåˆ™ | Master basic principles of neural network architecture design
- ç†è§£ä¸åŒç»„ä»¶å¯¹ç½‘ç»œæ€§èƒ½çš„å½±å“ | Understand the impact of different components on network performance

### 03_æŸå¤±å‡½æ•°ä¸ä¼˜åŒ–å™¨ | Loss Functions and Optimizers

**æ ¸å¿ƒå†…å®¹ | Core Content:**

1. **æŸå¤±å‡½æ•°å®ç° | Loss Function Implementation**
   - å‡æ–¹è¯¯å·®(MSE) | Mean Squared Error
   - äº¤å‰ç†µæŸå¤± | Cross-entropy Loss
   - è‡ªå®šä¹‰æŸå¤±å‡½æ•° | Custom loss functions
   - æŸå¤±å‡½æ•°å¯è§†åŒ– | Loss function visualization

2. **ä¼˜åŒ–å™¨å¯¹æ¯” | Optimizer Comparison**
   - SGD vs Adam vs RMSprop | SGD vs Adam vs RMSprop
   - å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥ | Learning rate scheduling strategies
   - åŠ¨é‡æœºåˆ¶ç†è§£ | Momentum mechanism understanding
   - è‡ªé€‚åº”å­¦ä¹ ç‡ | Adaptive learning rates

**ä¼˜åŒ–å™¨ç±»æ¯” | Optimizer Analogies:**
- **SGD**: åƒç›²äººæ‘¸è±¡ï¼Œæ¯æ¬¡åªçœ‹ä¸€ä¸ªæ ·æœ¬ï¼Œå®¹æ˜“è¢«å™ªå£°è¯¯å¯¼
- **SGD**: Like a blind person feeling an elephant, only looking at one sample at a time, easily misled by noise
- **Adam**: åƒæœ‰ç»éªŒçš„ç™»å±±è€…ï¼Œæ—¢è€ƒè™‘å½“å‰æ–¹å‘ï¼Œä¹Ÿå‚è€ƒå†å²ç»éªŒ
- **Adam**: Like an experienced mountaineer, considering both current direction and historical experience

**å­¦ä¹ ç›®æ ‡ | Learning Objectives:**
- ç†è§£ä¸åŒæŸå¤±å‡½æ•°çš„é€‚ç”¨åœºæ™¯ | Understand applicable scenarios for different loss functions
- æŒæ¡å„ç§ä¼˜åŒ–å™¨çš„ç‰¹ç‚¹å’Œä½¿ç”¨æ–¹æ³• | Master characteristics and usage of various optimizers

### 04_å®Œæ•´é¡¹ç›®å®ç° | Complete Project Implementation

#### MNISTæ‰‹å†™æ•°å­—è¯†åˆ«å¢å¼ºç‰ˆ | Enhanced MNIST Handwritten Digit Recognition

**é¡¹ç›®æè¿° | Project Description:**
åœ¨ç»å…¸MNISTé¡¹ç›®åŸºç¡€ä¸Šï¼Œæ·±å…¥æ¢ç´¢ç½‘ç»œè®¾è®¡çš„æ¯ä¸ªç»†èŠ‚ï¼Œä»ç®€å•çš„MLPåˆ°å¤æ‚çš„æ·±å±‚ç½‘ç»œã€‚
Building on the classic MNIST project, deeply explore every detail of network design, from simple MLP to complex deep networks.

**æŠ€æœ¯è¦ç‚¹ | Technical Points:**
- **ç½‘ç»œæ¶æ„å¯¹æ¯”**: 2å±‚ vs 3å±‚ vs 5å±‚ç½‘ç»œæ€§èƒ½å¯¹æ¯”
- **Network Architecture Comparison**: Performance comparison of 2-layer vs 3-layer vs 5-layer networks
- **è¿‡æ‹Ÿåˆåˆ†æ**: è¯†åˆ«å’Œè§£å†³è¿‡æ‹Ÿåˆé—®é¢˜
- **Overfitting Analysis**: Identifying and solving overfitting problems
- **æ­£åˆ™åŒ–æŠ€æœ¯**: Dropout, L1/L2æ­£åˆ™åŒ–çš„å®é™…åº”ç”¨
- **Regularization Techniques**: Practical application of Dropout, L1/L2 regularization
- **æ€§èƒ½ä¼˜åŒ–**: è®­ç»ƒé€Ÿåº¦å’Œå‡†ç¡®ç‡çš„å¹³è¡¡
- **Performance Optimization**: Balancing training speed and accuracy

#### å¤šåˆ†ç±»é—®é¢˜è§£å†³æ–¹æ¡ˆ | Multi-class Classification Solution

**é¡¹ç›®æè¿° | Project Description:**
è§£å†³çœŸå®ä¸–ç•Œçš„å¤šåˆ†ç±»é—®é¢˜ï¼Œå¦‚é¸¢å°¾èŠ±åˆ†ç±»ã€åŠ¨ç‰©è¯†åˆ«ç­‰ï¼Œå­¦ä¼šå¤„ç†ä¸å¹³è¡¡æ•°æ®é›†ã€‚
Solve real-world multi-class problems like iris classification, animal recognition, learning to handle imbalanced datasets.

**åº”ç”¨åœºæ™¯ | Application Scenarios:**
- **æ–‡æœ¬åˆ†ç±»**: æ–°é—»ç±»åˆ«åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æ
- **Text Classification**: News category classification, sentiment analysis
- **å›¾åƒè¯†åˆ«**: åŠ¨ç‰©åˆ†ç±»ã€ç‰©ä½“è¯†åˆ«
- **Image Recognition**: Animal classification, object recognition
- **åŒ»ç–—è¯Šæ–­**: ç–¾ç—…åˆ†ç±»ã€ç—‡çŠ¶è¯†åˆ«
- **Medical Diagnosis**: Disease classification, symptom recognition

**æŠ€æœ¯æŒ‘æˆ˜ | Technical Challenges:**
- **ç±»åˆ«ä¸å¹³è¡¡**: å¦‚ä½•å¤„ç†æ ·æœ¬æ•°é‡å·®å¼‚å·¨å¤§çš„æƒ…å†µ
- **Class Imbalance**: How to handle cases with huge sample quantity differences
- **ç‰¹å¾å·¥ç¨‹**: å¦‚ä½•æå–å’Œé€‰æ‹©æœ‰æ•ˆç‰¹å¾
- **Feature Engineering**: How to extract and select effective features
- **è¯„ä¼°æŒ‡æ ‡**: å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°çš„æƒè¡¡
- **Evaluation Metrics**: Balancing accuracy, precision, recall, F1-score

## ğŸ› ï¸ æŠ€æœ¯æ ˆ | Technology Stack

### æ ¸å¿ƒåº“ | Core Libraries
```python
# æ•°å€¼è®¡ç®— | Numerical Computing
import numpy as np
import pandas as pd

# æ·±åº¦å­¦ä¹ æ¡†æ¶ | Deep Learning Framework
import torch
import torch.nn as nn
import torch.optim as optim

# å¯è§†åŒ– | Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# æ•°æ®å¤„ç† | Data Processing
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix
```

### å¼€å‘å·¥å…· | Development Tools
- **ä»£ç ç¼–è¾‘å™¨**: VS Code, PyCharm
- **å®éªŒè·Ÿè¸ª**: TensorBoard, Weights & Biases
- **ç‰ˆæœ¬æ§åˆ¶**: Git
- **æ–‡æ¡£å·¥å…·**: Jupyter Notebook

## ğŸ“ˆ å­¦ä¹ è·¯å¾„ | Learning Path

### ç¬¬1å‘¨ï¼šåå‘ä¼ æ’­æ·±åº¦ç†è§£ | Week 1: Deep Understanding of Backpropagation
- [ ] æ‰‹å·¥æ¨å¯¼åå‘ä¼ æ’­å…¬å¼
- [ ] å®ç°ç®€å•çš„ä¸¤å±‚ç½‘ç»œ
- [ ] éªŒè¯æ¢¯åº¦è®¡ç®—æ­£ç¡®æ€§
- [ ] åˆ†æä¸åŒæ¿€æ´»å‡½æ•°çš„æ¢¯åº¦ç‰¹æ€§

### ç¬¬2å‘¨ï¼šMLPæ¶æ„è®¾è®¡ | Week 2: MLP Architecture Design
- [ ] è®¾è®¡ä¸åŒæ·±åº¦çš„ç½‘ç»œ
- [ ] å¯¹æ¯”ä¸åŒæ¿€æ´»å‡½æ•°çš„æ•ˆæœ
- [ ] å®ç°æ‰¹é‡å½’ä¸€åŒ–
- [ ] åˆ†æç½‘ç»œå®¹é‡ä¸æ€§èƒ½çš„å…³ç³»

### ç¬¬3å‘¨ï¼šä¼˜åŒ–ç®—æ³•å®è·µ | Week 3: Optimization Algorithm Practice
- [ ] å®ç°SGDã€Adamç­‰ä¼˜åŒ–å™¨
- [ ] å¯¹æ¯”ä¸åŒä¼˜åŒ–å™¨çš„æ”¶æ•›ç‰¹æ€§
- [ ] å®ç°å­¦ä¹ ç‡è°ƒåº¦
- [ ] åˆ†æåŠ¨é‡å¯¹è®­ç»ƒçš„å½±å“

### ç¬¬4å‘¨ï¼šå®Œæ•´é¡¹ç›®å®ç° | Week 4: Complete Project Implementation
- [ ] å®ŒæˆMNISTå¢å¼ºç‰ˆé¡¹ç›®
- [ ] å®ç°å¤šåˆ†ç±»è§£å†³æ–¹æ¡ˆ
- [ ] æ€§èƒ½è°ƒä¼˜å’Œå¯¹æ¯”åˆ†æ
- [ ] é¡¹ç›®æ€»ç»“å’Œç»éªŒåˆ†äº«

## ğŸ’¡ æ ¸å¿ƒæ¦‚å¿µæ·±åº¦è§£æ | Deep Analysis of Core Concepts

### åå‘ä¼ æ’­çš„æœ¬è´¨ | The Essence of Backpropagation

**é—®é¢˜**: åœ¨æœ‰åƒä¸‡ä¸ªå‚æ•°çš„ç½‘ç»œä¸­ï¼Œå¦‚ä½•çŸ¥é“è°ƒæ•´å“ªä¸ªå‚æ•°ï¼Ÿ
**Question**: In a network with tens of millions of parameters, how do we know which parameter to adjust?

**ç­”æ¡ˆ**: åå‘ä¼ æ’­é€šè¿‡é“¾å¼æ³•åˆ™ï¼Œè®¡ç®—æŸå¤±å‡½æ•°å¯¹æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦ï¼Œå‘Šè¯‰æˆ‘ä»¬ï¼š
**Answer**: Backpropagation calculates the gradient of the loss function with respect to each parameter through the chain rule, telling us:

1. **æ–¹å‘**: å‚æ•°åº”è¯¥å¢å¤§è¿˜æ˜¯å‡å°
2. **Direction**: Should the parameter increase or decrease
3. **å¹…åº¦**: å‚æ•°è°ƒæ•´çš„é‡è¦æ€§
4. **Magnitude**: The importance of parameter adjustment

### æ¢¯åº¦æ¶ˆå¤±ä¸æ¢¯åº¦çˆ†ç‚¸ | Gradient Vanishing and Exploding

**æ¢¯åº¦æ¶ˆå¤± | Gradient Vanishing:**
æ·±å±‚ç½‘ç»œä¸­ï¼Œæ¢¯åº¦åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­é€å±‚è¡°å‡ï¼Œå¯¼è‡´å‰å±‚å‚æ•°å‡ ä¹ä¸æ›´æ–°ã€‚
In deep networks, gradients decay layer by layer during backpropagation, causing early layers to barely update.

**è§£å†³æ–¹æ¡ˆ | Solutions:**
- ä½¿ç”¨ReLUç­‰æ¿€æ´»å‡½æ•°
- æ®‹å·®è¿æ¥(Residual Connections)
- æ‰¹é‡å½’ä¸€åŒ–(Batch Normalization)
- æ¢¯åº¦è£å‰ª(Gradient Clipping)

## ğŸ¯ é¡¹ç›®è¯„ä¼°æ ‡å‡† | Project Evaluation Criteria

### ç†è®ºæŒæ¡ | Theoretical Mastery (30%)
- [ ] èƒ½å¤Ÿæ‰‹å·¥æ¨å¯¼åå‘ä¼ æ’­å…¬å¼
- [ ] ç†è§£ä¸åŒä¼˜åŒ–å™¨çš„å·¥ä½œåŸç†
- [ ] æŒæ¡æ¿€æ´»å‡½æ•°çš„æ•°å­¦ç‰¹æ€§
- [ ] ç†è§£æ­£åˆ™åŒ–çš„æ•°å­¦åŸç†

### ç¼–ç¨‹å®ç° | Programming Implementation (40%)
- [ ] ä»é›¶å®ç°å®Œæ•´çš„MLP
- [ ] ä»£ç ç»“æ„æ¸…æ™°ï¼Œæ³¨é‡Šè¯¦ç»†
- [ ] èƒ½å¤Ÿè°ƒè¯•æ¢¯åº¦è®¡ç®—é”™è¯¯
- [ ] å®ç°é«˜æ•ˆçš„çŸ©é˜µè¿ç®—

### é¡¹ç›®åº”ç”¨ | Project Application (30%)
- [ ] åœ¨å¤šä¸ªæ•°æ®é›†ä¸ŠéªŒè¯ç®—æ³•
- [ ] èƒ½å¤Ÿåˆ†æå’Œè§£å†³è¿‡æ‹Ÿåˆé—®é¢˜
- [ ] è¿›è¡Œè¯¦ç»†çš„æ€§èƒ½å¯¹æ¯”åˆ†æ
- [ ] æå‡ºåˆç†çš„æ”¹è¿›å»ºè®®

## ğŸš€ é«˜çº§æŒ‘æˆ˜ | Advanced Challenges

### æŒ‘æˆ˜1ï¼šè‡ªå®šä¹‰å±‚å®ç° | Challenge 1: Custom Layer Implementation
å®ç°è‡ªå·±çš„å…¨è¿æ¥å±‚ã€æ¿€æ´»å‡½æ•°å±‚ï¼Œç†è§£PyTorchçš„autogradæœºåˆ¶ã€‚
Implement your own fully connected layer and activation function layer, understand PyTorch's autograd mechanism.

### æŒ‘æˆ˜2ï¼šä¼˜åŒ–å™¨æ€§èƒ½å¯¹æ¯” | Challenge 2: Optimizer Performance Comparison
åœ¨ç›¸åŒç½‘ç»œæ¶æ„ä¸‹ï¼Œè¯¦ç»†å¯¹æ¯”SGDã€Adamã€RMSpropç­‰ä¼˜åŒ–å™¨çš„æ”¶æ•›é€Ÿåº¦å’Œæœ€ç»ˆæ€§èƒ½ã€‚
Under the same network architecture, detailed comparison of convergence speed and final performance of SGD, Adam, RMSprop optimizers.

### æŒ‘æˆ˜3ï¼šç½‘ç»œæ¶æ„æœç´¢ | Challenge 3: Network Architecture Search
è®¾è®¡å®éªŒè‡ªåŠ¨å¯»æ‰¾æœ€ä¼˜çš„ç½‘ç»œæ¶æ„ï¼ˆå±‚æ•°ã€ç¥ç»å…ƒæ•°é‡ç­‰ï¼‰ã€‚
Design experiments to automatically find optimal network architecture (number of layers, neurons, etc.).

---

**å…³é”®æé†’ | Key Reminder**: 
åå‘ä¼ æ’­ä¸åªæ˜¯ä¸€ä¸ªç®—æ³•ï¼Œå®ƒæ˜¯æ·±åº¦å­¦ä¹ çš„æ€ç»´æ–¹å¼ã€‚æŒæ¡äº†åå‘ä¼ æ’­ï¼Œä½ å°±æŒæ¡äº†æ·±åº¦å­¦ä¹ çš„ç²¾é«“ï¼
Backpropagation is not just an algorithm, it's the way of thinking in deep learning. Master backpropagation, and you master the essence of deep learning! 