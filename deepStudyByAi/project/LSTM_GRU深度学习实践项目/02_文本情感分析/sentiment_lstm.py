"""
ä½¿ç”¨LSTMè¿›è¡Œæ–‡æœ¬æƒ…æ„Ÿåˆ†æ - ç†è§£è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„åºåˆ—å»ºæ¨¡
Text Sentiment Analysis using LSTM - Understanding Sequence Modeling in NLP

è¿™ä¸ªé¡¹ç›®æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨LSTMæ¥ç†è§£æ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘ï¼Œå°±åƒæ•™è®¡ç®—æœºè¯»æ‡‚äººç±»çš„å–œæ€’å“€ä¹ã€‚
This project demonstrates how to use LSTM to understand text sentiment, 
like teaching computers to read human emotions.
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import re
import jieba
from collections import Counter
import sys
import os

# æ·»åŠ ä¸Šçº§ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils.data_utils import set_random_seed, TextProcessor


class SentimentDataset(Dataset):
    """
    æƒ…æ„Ÿåˆ†ææ•°æ®é›†
    Sentiment Analysis Dataset
    
    è¿™ä¸ªç±»å°±åƒä¸€ä¸ªæ™ºèƒ½çš„æ–‡æœ¬æ•´ç†å‘˜ï¼ŒæŠŠåŸå§‹æ–‡æœ¬è½¬æ¢æˆæ¨¡å‹èƒ½ç†è§£çš„æ•°å­—åºåˆ—ã€‚
    This class is like a smart text organizer that converts raw text into numerical sequences 
    that models can understand.
    """
    
    def __init__(self, texts, labels, text_processor, max_length=100):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        Initialize dataset
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨ | List of texts
            labels: æ ‡ç­¾åˆ—è¡¨ | List of labels  
            text_processor: æ–‡æœ¬å¤„ç†å™¨ | Text processor
            max_length: æœ€å¤§åºåˆ—é•¿åº¦ | Maximum sequence length
        """
        self.texts = texts
        self.labels = labels
        self.text_processor = text_processor
        self.max_length = max_length
        
        # å°†æ–‡æœ¬è½¬æ¢ä¸ºåºåˆ—
        # Convert texts to sequences
        self.sequences = []
        for text in texts:
            sequence = self.text_processor.text_to_sequence(text)
            self.sequences.append(sequence)
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        sequence = self.sequences[idx]
        label = self.labels[idx]
        
        # å¡«å……æˆ–æˆªæ–­åºåˆ—åˆ°å›ºå®šé•¿åº¦
        # Pad or truncate sequence to fixed length
        if len(sequence) > self.max_length:
            sequence = sequence[:self.max_length]
        else:
            sequence = sequence + [0] * (self.max_length - len(sequence))
        
        return torch.LongTensor(sequence), torch.LongTensor([label])


class SentimentLSTM(nn.Module):
    """
    åŸºäºLSTMçš„æƒ…æ„Ÿåˆ†ææ¨¡å‹
    LSTM-based Sentiment Analysis Model
    
    è¿™ä¸ªæ¨¡å‹å°±åƒä¸€ä¸ªç†è§£æ–‡å­—æƒ…æ„Ÿçš„ä¸“å®¶ï¼Œé€šè¿‡å­¦ä¹ å•è¯çš„é¡ºåºå’Œç»„åˆæ¥åˆ¤æ–­æƒ…æ„Ÿã€‚
    This model is like an expert in understanding text emotions, 
    learning from word sequences and combinations to judge sentiment.
    """
    
    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, 
                 n_layers=2, dropout=0.3, bidirectional=True):
        """
        åˆå§‹åŒ–LSTMæ¨¡å‹
        Initialize LSTM model
        
        Args:
            vocab_size: è¯æ±‡è¡¨å¤§å° | Vocabulary size
            embed_dim: è¯åµŒå…¥ç»´åº¦ | Word embedding dimension
            hidden_dim: éšè—å±‚ç»´åº¦ | Hidden layer dimension
            output_dim: è¾“å‡ºç»´åº¦ | Output dimension
            n_layers: LSTMå±‚æ•° | Number of LSTM layers
            dropout: Dropoutæ¯”ç‡ | Dropout ratio
            bidirectional: æ˜¯å¦åŒå‘ | Whether bidirectional
        """
        super(SentimentLSTM, self).__init__()
        
        self.hidden_dim = hidden_dim
        self.n_layers = n_layers
        self.bidirectional = bidirectional
        
        # è¯åµŒå…¥å±‚ - å°†å•è¯IDè½¬æ¢ä¸ºå‘é‡è¡¨ç¤º
        # Word embedding layer - convert word IDs to vector representations
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        
        # LSTMå±‚ - æ ¸å¿ƒçš„åºåˆ—å¤„ç†å±‚
        # LSTM layer - core sequence processing layer
        self.lstm = nn.LSTM(embed_dim, hidden_dim, n_layers, 
                           dropout=dropout, batch_first=True, 
                           bidirectional=bidirectional)
        
        # è®¡ç®—æœ€ç»ˆçš„éšè—ç»´åº¦
        # Calculate final hidden dimension
        final_hidden_dim = hidden_dim * 2 if bidirectional else hidden_dim
        
        # åˆ†ç±»å™¨ - å°†LSTMè¾“å‡ºè½¬æ¢ä¸ºæƒ…æ„Ÿåˆ†ç±»
        # Classifier - convert LSTM output to sentiment classification
        self.classifier = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(final_hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, output_dim)
        )
        
        # åˆå§‹åŒ–æƒé‡
        # Initialize weights
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–æ¨¡å‹æƒé‡"""
        for name, param in self.named_parameters():
            if 'weight' in name and param.dim() > 1:
                nn.init.xavier_uniform_(param)
            elif 'bias' in name:
                nn.init.constant_(param, 0)
    
    def forward(self, x):
        """
        å‰å‘ä¼ æ’­
        Forward propagation
        
        Args:
            x: è¾“å…¥åºåˆ— [batch_size, seq_length] | Input sequence
            
        Returns:
            æƒ…æ„Ÿåˆ†ç±»è¾“å‡º | Sentiment classification output
        """
        # è¯åµŒå…¥
        # Word embedding
        embedded = self.embedding(x)  # [batch_size, seq_length, embed_dim]
        
        # LSTMå¤„ç†
        # LSTM processing
        lstm_out, (hidden, cell) = self.lstm(embedded)
        
        # ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡ºï¼ˆå¯¹äºåŒå‘LSTMï¼Œè¿æ¥ä¸¤ä¸ªæ–¹å‘ï¼‰
        # Use output from last time step (for bidirectional LSTM, concatenate both directions)
        if self.bidirectional:
            # åŒå‘LSTMçš„æœ€åéšè—çŠ¶æ€
            # Final hidden state of bidirectional LSTM
            final_hidden = torch.cat([hidden[-2], hidden[-1]], dim=1)
        else:
            final_hidden = hidden[-1]
        
        # åˆ†ç±»
        # Classification
        output = self.classifier(final_hidden)
        
        return output


def create_sample_data():
    """
    åˆ›å»ºç¤ºä¾‹æƒ…æ„Ÿåˆ†ææ•°æ®
    Create sample sentiment analysis data
    
    åœ¨å®é™…é¡¹ç›®ä¸­ï¼Œä½ å¯ä»¥ä½¿ç”¨çœŸå®çš„æ•°æ®é›†å¦‚IMDbç”µå½±è¯„è®ºæ•°æ®é›†ã€‚
    In real projects, you can use real datasets like IMDb movie review dataset.
    """
    print("ğŸ“ åˆ›å»ºç¤ºä¾‹æƒ…æ„Ÿåˆ†ææ•°æ® | Creating Sample Sentiment Analysis Data")
    
    # æ­£é¢æƒ…æ„Ÿçš„æ–‡æœ¬ç¤ºä¾‹
    # Positive sentiment text examples
    positive_texts = [
        "è¿™éƒ¨ç”µå½±å¤ªæ£’äº†ï¼Œæ¼”å‘˜è¡¨æ¼”å¾—å¾ˆå¥½",
        "æˆ‘éå¸¸å–œæ¬¢è¿™ä¸ªäº§å“ï¼Œè´¨é‡å¾ˆä¸é”™",
        "ä»Šå¤©å¿ƒæƒ…å¾ˆå¥½ï¼Œå¤©æ°”ä¹Ÿå¾ˆæ£’",
        "è¿™å®¶é¤å…çš„é£Ÿç‰©å¾ˆç¾å‘³ï¼ŒæœåŠ¡ä¹Ÿå¾ˆå‘¨åˆ°",
        "è¿™æœ¬ä¹¦å†™å¾—å¾ˆç²¾å½©ï¼Œæƒ…èŠ‚å¼•äººå…¥èƒœ",
        "è¿™ä¸ªè½¯ä»¶ç”¨èµ·æ¥å¾ˆæ–¹ä¾¿ï¼ŒåŠŸèƒ½å¾ˆå®ç”¨",
        "æ¼”å”±ä¼šå¤ªç²¾å½©äº†ï¼Œæ­Œæ‰‹å”±å¾—å¾ˆæ£’",
        "è¿™ä¸ªæ¸¸æˆå¾ˆæœ‰è¶£ï¼Œç”»é¢ä¹Ÿå¾ˆç²¾ç¾",
        "è€å¸ˆè®²è¯¾å¾ˆç”ŸåŠ¨ï¼Œå­¦åˆ°äº†å¾ˆå¤šçŸ¥è¯†",
        "è¿™æ¬¡æ—…è¡Œå¾ˆæ„‰å¿«ï¼Œé£æ™¯å¾ˆç¾ä¸½"
    ]
    
    # è´Ÿé¢æƒ…æ„Ÿçš„æ–‡æœ¬ç¤ºä¾‹
    # Negative sentiment text examples
    negative_texts = [
        "è¿™éƒ¨ç”µå½±å¾ˆæ— èŠï¼Œå‰§æƒ…æ‹–æ²“",
        "äº§å“è´¨é‡å¾ˆå·®ï¼Œä¸æ¨èè´­ä¹°",
        "ä»Šå¤©å¿ƒæƒ…å¾ˆç³Ÿç³•ï¼Œä»€ä¹ˆéƒ½ä¸é¡ºåˆ©",
        "è¿™å®¶é¤å…çš„æœåŠ¡å¾ˆå·®ï¼Œé£Ÿç‰©ä¹Ÿä¸å¥½åƒ",
        "è¿™æœ¬ä¹¦å†™å¾—å¾ˆä¹å‘³ï¼Œçœ‹ä¸ä¸‹å»",
        "è¿™ä¸ªè½¯ä»¶bugså¾ˆå¤šï¼Œä½“éªŒå¾ˆå·®",
        "æ¼”å”±ä¼šå¾ˆå¤±æœ›ï¼ŒéŸ³å“æ•ˆæœå¾ˆå·®",
        "è¿™ä¸ªæ¸¸æˆå¾ˆæ— èŠï¼Œæ“ä½œä¹Ÿå¾ˆå¤æ‚",
        "è¯¾ç¨‹å†…å®¹å¾ˆæ¯ç‡¥ï¼Œå¬ä¸æ‡‚",
        "è¿™æ¬¡æ—…è¡Œå¾ˆç³Ÿç³•ï¼Œé…’åº—æ¡ä»¶å¾ˆå·®"
    ]
    
    # æ‰©å±•æ•°æ® - ç”Ÿæˆæ›´å¤šæ ·æœ¬
    # Expand data - generate more samples
    extended_positive = positive_texts * 10  # é‡å¤10æ¬¡
    extended_negative = negative_texts * 10
    
    # ç»„åˆæ–‡æœ¬å’Œæ ‡ç­¾
    # Combine texts and labels
    texts = extended_positive + extended_negative
    labels = [1] * len(extended_positive) + [0] * len(extended_negative)  # 1=æ­£é¢, 0=è´Ÿé¢
    
    print(f"æ€»æ ·æœ¬æ•°: {len(texts)} (æ­£é¢: {len(extended_positive)}, è´Ÿé¢: {len(extended_negative)})")
    print(f"Total samples: {len(texts)} (Positive: {len(extended_positive)}, Negative: {len(extended_negative)})")
    
    return texts, labels


def train_sentiment_model():
    """
    è®­ç»ƒæƒ…æ„Ÿåˆ†ææ¨¡å‹
    Train sentiment analysis model
    """
    print("\nğŸš€ å¼€å§‹è®­ç»ƒLSTMæƒ…æ„Ÿåˆ†ææ¨¡å‹ | Starting LSTM Sentiment Analysis Model Training")
    print("=" * 70)
    
    set_random_seed(42)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device} | Using device: {device}")
    
    # åˆ›å»ºæ•°æ®
    # Create data
    texts, labels = create_sample_data()
    
    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    # Split train and test sets
    train_texts, test_texts, train_labels, test_labels = train_test_split(
        texts, labels, test_size=0.2, random_state=42, stratify=labels
    )
    
    print(f"è®­ç»ƒé›†å¤§å°: {len(train_texts)} | Training set size: {len(train_texts)}")
    print(f"æµ‹è¯•é›†å¤§å°: {len(test_texts)} | Test set size: {len(test_texts)}")
    
    # æ„å»ºè¯æ±‡è¡¨
    # Build vocabulary
    text_processor = TextProcessor(max_vocab_size=5000, min_freq=1)
    text_processor.build_vocab(train_texts, language='chinese')
    
    print(f"è¯æ±‡è¡¨å¤§å°: {text_processor.vocab_size} | Vocabulary size: {text_processor.vocab_size}")
    
    # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    # Create datasets and data loaders
    train_dataset = SentimentDataset(train_texts, train_labels, text_processor, max_length=50)
    test_dataset = SentimentDataset(test_texts, test_labels, text_processor, max_length=50)
    
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)
    
    # åˆ›å»ºæ¨¡å‹
    # Create model
    model = SentimentLSTM(
        vocab_size=text_processor.vocab_size,
        embed_dim=128,
        hidden_dim=64,
        output_dim=2,  # äºŒåˆ†ç±»ï¼šæ­£é¢å’Œè´Ÿé¢ | Binary classification: positive and negative
        n_layers=2,
        dropout=0.3,
        bidirectional=True
    ).to(device)
    
    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    print(f"Number of model parameters: {sum(p.numel() for p in model.parameters()):,}")
    
    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    # Loss function and optimizer
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.8)
    
    # è®­ç»ƒå¾ªç¯
    # Training loop
    num_epochs = 30
    train_losses = []
    train_accuracies = []
    
    print("\nå¼€å§‹è®­ç»ƒ... | Starting training...")
    
    for epoch in range(num_epochs):
        model.train()
        total_loss = 0
        correct_predictions = 0
        total_predictions = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.squeeze().to(device)
            
            # å‰å‘ä¼ æ’­
            # Forward propagation
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            
            # åå‘ä¼ æ’­
            # Backward propagation
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            # ç»Ÿè®¡
            # Statistics
            total_loss += loss.item()
            pred = output.argmax(dim=1)
            correct_predictions += (pred == target).sum().item()
            total_predictions += target.size(0)
        
        # è®¡ç®—è®­ç»ƒæŒ‡æ ‡
        # Calculate training metrics
        avg_loss = total_loss / len(train_loader)
        accuracy = correct_predictions / total_predictions
        
        train_losses.append(avg_loss)
        train_accuracies.append(accuracy)
        
        # å­¦ä¹ ç‡è°ƒåº¦
        # Learning rate scheduling
        scheduler.step()
        
        if (epoch + 1) % 5 == 0:
            print(f'è½®æ¬¡ {epoch+1}/{num_epochs}:')
            print(f'Epoch {epoch+1}/{num_epochs}:')
            print(f'  è®­ç»ƒæŸå¤±: {avg_loss:.4f} | Training Loss: {avg_loss:.4f}')
            print(f'  è®­ç»ƒå‡†ç¡®ç‡: {accuracy:.4f} | Training Accuracy: {accuracy:.4f}')
            print(f'  å­¦ä¹ ç‡: {scheduler.get_last_lr()[0]:.6f} | Learning Rate: {scheduler.get_last_lr()[0]:.6f}')
    
    # æµ‹è¯•æ¨¡å‹
    # Test model
    model.eval()
    test_predictions = []
    test_targets = []
    test_loss = 0
    
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.squeeze().to(device)
            output = model(data)
            
            test_loss += criterion(output, target).item()
            pred = output.argmax(dim=1)
            
            test_predictions.extend(pred.cpu().numpy())
            test_targets.extend(target.cpu().numpy())
    
    # è®¡ç®—æµ‹è¯•æŒ‡æ ‡
    # Calculate test metrics
    test_accuracy = accuracy_score(test_targets, test_predictions)
    avg_test_loss = test_loss / len(test_loader)
    
    print(f"\nğŸ“Š æœ€ç»ˆæµ‹è¯•ç»“æœ | Final Test Results:")
    print(f"æµ‹è¯•æŸå¤±: {avg_test_loss:.4f} | Test Loss: {avg_test_loss:.4f}")
    print(f"æµ‹è¯•å‡†ç¡®ç‡: {test_accuracy:.4f} | Test Accuracy: {test_accuracy:.4f}")
    
    # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
    # Detailed classification report
    print(f"\nğŸ“ˆ è¯¦ç»†åˆ†ç±»æŠ¥å‘Š | Detailed Classification Report:")
    print(classification_report(test_targets, test_predictions, 
                              target_names=['è´Ÿé¢ | Negative', 'æ­£é¢ | Positive']))
    
    # å¯è§†åŒ–ç»“æœ
    # Visualize results
    visualize_results(train_losses, train_accuracies, test_targets, test_predictions)
    
    return model, text_processor


def visualize_results(train_losses, train_accuracies, test_targets, test_predictions):
    """
    å¯è§†åŒ–è®­ç»ƒç»“æœ
    Visualize training results
    """
    plt.figure(figsize=(15, 10))
    
    # è®­ç»ƒæŸå¤±æ›²çº¿
    # Training loss curve
    plt.subplot(2, 3, 1)
    plt.plot(train_losses)
    plt.title('è®­ç»ƒæŸå¤±æ›²çº¿ | Training Loss Curve')
    plt.xlabel('è½®æ¬¡ | Epoch')
    plt.ylabel('æŸå¤± | Loss')
    plt.grid(True)
    
    # è®­ç»ƒå‡†ç¡®ç‡æ›²çº¿
    # Training accuracy curve
    plt.subplot(2, 3, 2)
    plt.plot(train_accuracies)
    plt.title('è®­ç»ƒå‡†ç¡®ç‡æ›²çº¿ | Training Accuracy Curve')
    plt.xlabel('è½®æ¬¡ | Epoch')
    plt.ylabel('å‡†ç¡®ç‡ | Accuracy')
    plt.grid(True)
    
    # æ··æ·†çŸ©é˜µ
    # Confusion matrix
    plt.subplot(2, 3, 3)
    cm = confusion_matrix(test_targets, test_predictions)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['è´Ÿé¢ | Negative', 'æ­£é¢ | Positive'],
                yticklabels=['è´Ÿé¢ | Negative', 'æ­£é¢ | Positive'])
    plt.title('æ··æ·†çŸ©é˜µ | Confusion Matrix')
    plt.ylabel('çœŸå®æ ‡ç­¾ | True Label')
    plt.xlabel('é¢„æµ‹æ ‡ç­¾ | Predicted Label')
    
    # é¢„æµ‹åˆ†å¸ƒ
    # Prediction distribution
    plt.subplot(2, 3, 4)
    labels = ['è´Ÿé¢ | Negative', 'æ­£é¢ | Positive']
    true_counts = [test_targets.count(0), test_targets.count(1)]
    pred_counts = [test_predictions.count(0), test_predictions.count(1)]
    
    x = np.arange(len(labels))
    width = 0.35
    
    plt.bar(x - width/2, true_counts, width, label='çœŸå® | True', alpha=0.7)
    plt.bar(x + width/2, pred_counts, width, label='é¢„æµ‹ | Predicted', alpha=0.7)
    
    plt.xlabel('ç±»åˆ« | Category')
    plt.ylabel('æ•°é‡ | Count')
    plt.title('é¢„æµ‹åˆ†å¸ƒå¯¹æ¯” | Prediction Distribution Comparison')
    plt.xticks(x, labels)
    plt.legend()
    
    # æ ·æœ¬é¢„æµ‹ç½®ä¿¡åº¦åˆ†å¸ƒ
    # Sample prediction confidence distribution
    plt.subplot(2, 3, 5)
    correct_mask = np.array(test_targets) == np.array(test_predictions)
    plt.hist([np.array(test_predictions)[correct_mask], 
              np.array(test_predictions)[~correct_mask]], 
             bins=2, alpha=0.7, label=['æ­£ç¡® | Correct', 'é”™è¯¯ | Incorrect'])
    plt.xlabel('é¢„æµ‹ç±»åˆ« | Predicted Category')
    plt.ylabel('æ ·æœ¬æ•° | Sample Count')
    plt.title('é¢„æµ‹æ­£ç¡®æ€§åˆ†å¸ƒ | Prediction Correctness Distribution')
    plt.legend()
    
    # æ¨¡å‹æ€§èƒ½æ€»ç»“
    # Model performance summary
    plt.subplot(2, 3, 6)
    metrics = ['å‡†ç¡®ç‡ | Accuracy', 'ç²¾ç¡®ç‡ | Precision', 'å¬å›ç‡ | Recall', 'F1åˆ†æ•° | F1-Score']
    from sklearn.metrics import precision_score, recall_score, f1_score
    
    values = [
        accuracy_score(test_targets, test_predictions),
        precision_score(test_targets, test_predictions, average='weighted'),
        recall_score(test_targets, test_predictions, average='weighted'),
        f1_score(test_targets, test_predictions, average='weighted')
    ]
    
    bars = plt.bar(metrics, values, color=['skyblue', 'lightgreen', 'lightcoral', 'lightyellow'])
    plt.title('æ¨¡å‹æ€§èƒ½æŒ‡æ ‡ | Model Performance Metrics')
    plt.ylabel('åˆ†æ•° | Score')
    plt.ylim(0, 1)
    
    # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°å€¼
    # Add values on bars
    for bar, value in zip(bars, values):
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{value:.3f}', ha='center', va='bottom')
    
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig('sentiment_lstm_results.png', dpi=300, bbox_inches='tight')
    plt.show()


def test_model_predictions(model, text_processor, device):
    """
    æµ‹è¯•æ¨¡å‹çš„å®é™…é¢„æµ‹æ•ˆæœ
    Test model's actual prediction performance
    """
    print("\nğŸ”® æµ‹è¯•æ¨¡å‹é¢„æµ‹æ•ˆæœ | Testing Model Prediction Performance")
    print("=" * 60)
    
    # æµ‹è¯•æ ·æœ¬
    # Test samples
    test_samples = [
        "è¿™éƒ¨ç”µå½±çœŸçš„å¾ˆæ£’ï¼Œæˆ‘å¼ºçƒˆæ¨è",
        "äº§å“è´¨é‡å¤ªå·®äº†ï¼Œå®Œå…¨ä¸å€¼å¾—è´­ä¹°",
        "ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œå¿ƒæƒ…ä¹Ÿå¾ˆæ„‰å¿«",
        "æœåŠ¡æ€åº¦å¾ˆå·®ï¼Œè®©äººå¾ˆä¸æ»¡æ„",
        "è¿™æœ¬ä¹¦å†™å¾—å¾ˆç²¾å½©ï¼Œå†…å®¹å¾ˆæœ‰è¶£",
        "è½¯ä»¶ç»å¸¸å´©æºƒï¼Œä½“éªŒå¾ˆç³Ÿç³•"
    ]
    
    model.eval()
    
    with torch.no_grad():
        for text in test_samples:
            # æ–‡æœ¬é¢„å¤„ç†
            # Text preprocessing
            sequence = text_processor.text_to_sequence(text)
            
            # å¡«å……åˆ°å›ºå®šé•¿åº¦
            # Pad to fixed length
            max_length = 50
            if len(sequence) > max_length:
                sequence = sequence[:max_length]
            else:
                sequence = sequence + [0] * (max_length - len(sequence))
            
            # è½¬æ¢ä¸ºå¼ é‡
            # Convert to tensor
            input_tensor = torch.LongTensor(sequence).unsqueeze(0).to(device)
            
            # é¢„æµ‹
            # Predict
            output = model(input_tensor)
            probabilities = torch.softmax(output, dim=1)
            predicted_class = output.argmax(dim=1).item()
            confidence = probabilities[0][predicted_class].item()
            
            sentiment = "æ­£é¢ | Positive" if predicted_class == 1 else "è´Ÿé¢ | Negative"
            
            print(f"æ–‡æœ¬: {text}")
            print(f"Text: {text}")
            print(f"é¢„æµ‹æƒ…æ„Ÿ: {sentiment}")
            print(f"Predicted sentiment: {sentiment}")
            print(f"ç½®ä¿¡åº¦: {confidence:.4f}")
            print(f"Confidence: {confidence:.4f}")
            print("-" * 40)


if __name__ == "__main__":
    print("ğŸ­ LSTMæ–‡æœ¬æƒ…æ„Ÿåˆ†æé¡¹ç›® | LSTM Text Sentiment Analysis Project")
    print("=" * 70)
    
    # è®­ç»ƒæ¨¡å‹
    # Train model
    model, text_processor = train_sentiment_model()
    
    # æµ‹è¯•é¢„æµ‹
    # Test predictions
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    test_model_predictions(model, text_processor, device)
    
    print("\nğŸ‰ æƒ…æ„Ÿåˆ†æé¡¹ç›®å®Œæˆï¼| Sentiment Analysis Project Completed!")
    print("ğŸ“š é€šè¿‡è¿™ä¸ªé¡¹ç›®ï¼Œä½ å­¦ä¼šäº†ï¼š")
    print("ğŸ“š Through this project, you learned:")
    print("1. å¦‚ä½•å°†æ–‡æœ¬è½¬æ¢ä¸ºLSTMå¯ä»¥å¤„ç†çš„åºåˆ—")
    print("   How to convert text into sequences that LSTM can process")
    print("2. åŒå‘LSTMåœ¨æ–‡æœ¬åˆ†ç±»ä¸­çš„åº”ç”¨")
    print("   Application of bidirectional LSTM in text classification")
    print("3. è¯åµŒå…¥å’Œåºåˆ—å»ºæ¨¡çš„ç»“åˆ")
    print("   Combination of word embeddings and sequence modeling")
    print("4. æƒ…æ„Ÿåˆ†æçš„å®Œæ•´æµç¨‹å’Œè¯„ä¼°æ–¹æ³•")
    print("   Complete process and evaluation methods for sentiment analysis") 