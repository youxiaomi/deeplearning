"""
ä»é›¶å®ç°GRU - æ·±å…¥ç†è§£é—¨æ§å¾ªç¯å•å…ƒ
GRU Implementation from Scratch - Deep Understanding of Gated Recurrent Units

GRUæ˜¯LSTMçš„ç®€åŒ–ç‰ˆæœ¬ï¼Œåªæœ‰ä¸¤ä¸ªé—¨ä½†æ•ˆæœç›¸è¿‘ï¼Œå°±åƒç”¨æ›´ç®€å•çš„æœºåˆ¶å®ç°ç±»ä¼¼çš„è®°å¿†åŠŸèƒ½ã€‚
GRU is a simplified version of LSTM with only two gates but similar performance, 
like achieving similar memory functionality with simpler mechanisms.
"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from typing import Tuple, Optional
import sys
import os

# æ·»åŠ ä¸Šçº§ç›®å½•åˆ°è·¯å¾„ä»¥å¯¼å…¥å·¥å…·å‡½æ•°
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils.data_utils import set_random_seed


class GRUCellFromScratch:
    """
    ä»é›¶å®ç°çš„GRUå•å…ƒ
    GRU Cell implemented from scratch
    
    GRUæ¯”LSTMæ›´ç®€å•ï¼Œåªæœ‰ä¸¤ä¸ªé—¨ï¼š
    - é‡ç½®é—¨ï¼šå†³å®šå¿˜è®°å¤šå°‘è¿‡å»ä¿¡æ¯
    - æ›´æ–°é—¨ï¼šå†³å®šä¿ç•™å¤šå°‘è¿‡å»ä¿¡æ¯å’Œæ¥å—å¤šå°‘æ–°ä¿¡æ¯
    
    GRU is simpler than LSTM, with only two gates:
    - Reset gate: decides how much past information to forget
    - Update gate: decides how much past information to keep and how much new information to accept
    """
    
    def __init__(self, input_size: int, hidden_size: int, device: str = 'cpu'):
        """
        åˆå§‹åŒ–GRUå•å…ƒå‚æ•°
        Initialize GRU cell parameters
        """
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.device = device
        
        self._init_weights()
        
    def _init_weights(self):
        """
        åˆå§‹åŒ–æƒé‡å’Œåç½®
        Initialize weights and biases
        
        GRUåªéœ€è¦3ç»„æƒé‡ï¼šé‡ç½®é—¨ã€æ›´æ–°é—¨ã€å€™é€‰çŠ¶æ€
        GRU only needs 3 sets of weights: reset gate, update gate, candidate state
        """
        std = 1.0 / np.sqrt(self.hidden_size)
        
        # é‡ç½®é—¨æƒé‡ | Reset gate weights
        self.W_r = torch.randn(self.hidden_size, self.input_size + self.hidden_size,
                              dtype=torch.float32, device=self.device) * std
        self.b_r = torch.zeros(self.hidden_size, dtype=torch.float32, device=self.device)
        
        # æ›´æ–°é—¨æƒé‡ | Update gate weights  
        self.W_z = torch.randn(self.hidden_size, self.input_size + self.hidden_size,
                              dtype=torch.float32, device=self.device) * std
        self.b_z = torch.zeros(self.hidden_size, dtype=torch.float32, device=self.device)
        
        # å€™é€‰çŠ¶æ€æƒé‡ | Candidate state weights
        self.W_h = torch.randn(self.hidden_size, self.input_size + self.hidden_size,
                              dtype=torch.float32, device=self.device) * std
        self.b_h = torch.zeros(self.hidden_size, dtype=torch.float32, device=self.device)
        
        self.parameters = [self.W_r, self.b_r, self.W_z, self.b_z, self.W_h, self.b_h]
        
        for param in self.parameters:
            param.requires_grad_(True)
    
    def forward(self, x: torch.Tensor, h_prev: torch.Tensor) -> torch.Tensor:
        """
        GRUå‰å‘ä¼ æ’­
        GRU forward propagation
        
        GRUçš„è®¡ç®—è¿‡ç¨‹æ›´ç®€æ´ï¼š
        1. è®¡ç®—é‡ç½®é—¨å’Œæ›´æ–°é—¨
        2. ä½¿ç”¨é‡ç½®é—¨è®¡ç®—å€™é€‰çŠ¶æ€
        3. ä½¿ç”¨æ›´æ–°é—¨ç»„åˆæ—§çŠ¶æ€å’Œæ–°çŠ¶æ€
        
        GRU computation is more concise:
        1. Compute reset and update gates
        2. Use reset gate to compute candidate state
        3. Use update gate to combine old and new states
        """
        # ç»„åˆè¾“å…¥
        combined = torch.cat([x, h_prev], dim=1)
        
        # æ­¥éª¤1ï¼šé‡ç½®é—¨ - "æˆ‘åº”è¯¥å¿˜è®°å¤šå°‘è¿‡å»çš„ä¿¡æ¯ï¼Ÿ"
        # Step 1: Reset gate - "How much past information should I forget?"
        # r_t = Ïƒ(W_r Â· [h_{t-1}, x_t] + b_r)
        r_t = torch.sigmoid(combined @ self.W_r.T + self.b_r)
        
        # æ­¥éª¤2ï¼šæ›´æ–°é—¨ - "æˆ‘åº”è¯¥æ›´æ–°å¤šå°‘çŠ¶æ€ï¼Ÿ"
        # Step 2: Update gate - "How much state should I update?"
        # z_t = Ïƒ(W_z Â· [h_{t-1}, x_t] + b_z)
        z_t = torch.sigmoid(combined @ self.W_z.T + self.b_z)
        
        # æ­¥éª¤3ï¼šå€™é€‰çŠ¶æ€ - "æ–°çš„å€™é€‰çŠ¶æ€æ˜¯ä»€ä¹ˆï¼Ÿ"
        # Step 3: Candidate state - "What is the new candidate state?"
        # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨é‡ç½®é—¨æ§åˆ¶çš„æ—§çŠ¶æ€
        # Note: Here we use reset gate controlled old state
        reset_combined = torch.cat([x, r_t * h_prev], dim=1)
        h_tilde = torch.tanh(reset_combined @ self.W_h.T + self.b_h)
        
        # æ­¥éª¤4ï¼šæœ€ç»ˆçŠ¶æ€ - "ç»„åˆæ—§çŠ¶æ€å’Œæ–°çŠ¶æ€"
        # Step 4: Final state - "Combine old and new states"
        # h_t = (1 - z_t) âŠ™ h_{t-1} + z_t âŠ™ hÌƒ_t
        # è¿™æ˜¯GRUçš„æ ¸å¿ƒåˆ›æ–°ï¼šä¸€ä¸ªé—¨åŒæ—¶æ§åˆ¶é—å¿˜å’Œè¾“å…¥
        # This is GRU's core innovation: one gate controls both forgetting and input
        h_t = (1 - z_t) * h_prev + z_t * h_tilde
        
        return h_t
    
    def init_hidden(self, batch_size: int) -> torch.Tensor:
        """åˆå§‹åŒ–éšè—çŠ¶æ€"""
        return torch.zeros(batch_size, self.hidden_size, device=self.device)


class GRUFromScratch(nn.Module):
    """å®Œæ•´çš„GRUç½‘ç»œå®ç°"""
    
    def __init__(self, input_size: int, hidden_size: int, num_layers: int = 1,
                 output_size: int = 1, dropout: float = 0.0):
        super(GRUFromScratch, self).__init__()
        
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.output_size = output_size
        
        # åˆ›å»ºGRUå•å…ƒå±‚
        self.gru_cells = nn.ModuleList()
        for i in range(num_layers):
            layer_input_size = input_size if i == 0 else hidden_size
            self.gru_cells.append(GRUCellFromScratch(layer_input_size, hidden_size))
        
        self.output_layer = nn.Linear(hidden_size, output_size)
        self.dropout = nn.Dropout(dropout) if dropout > 0 else None
    
    def forward(self, x: torch.Tensor, hidden: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        """å‰å‘ä¼ æ’­"""
        seq_len, batch_size, _ = x.size()
        
        # åˆå§‹åŒ–éšè—çŠ¶æ€
        if hidden is None:
            hidden_states = []
            for gru_cell in self.gru_cells:
                h_0 = gru_cell.init_hidden(batch_size)
                hidden_states.append(h_0)
        else:
            hidden_states = hidden
        
        outputs = []
        
        # å¯¹æ¯ä¸ªæ—¶é—´æ­¥è¿›è¡Œå¤„ç†
        for t in range(seq_len):
            x_t = x[t]
            
            # é€šè¿‡æ‰€æœ‰GRUå±‚
            for layer_idx, gru_cell in enumerate(self.gru_cells):
                h_t = gru_cell.forward(x_t, hidden_states[layer_idx])
                hidden_states[layer_idx] = h_t
                x_t = h_t
                
                if self.dropout is not None and layer_idx < len(self.gru_cells) - 1:
                    x_t = self.dropout(x_t)
            
            # é€šè¿‡è¾“å‡ºå±‚
            output_t = self.output_layer(x_t)
            outputs.append(output_t)
        
        outputs = torch.stack(outputs, dim=0)
        return outputs, hidden_states


def demonstrate_gru_gates():
    """æ¼”ç¤ºGRUé—¨æ§æœºåˆ¶"""
    print("ğŸ§  GRUé—¨æ§æœºåˆ¶æ¼”ç¤º | GRU Gating Mechanism Demonstration")
    print("=" * 60)
    
    set_random_seed(42)
    
    input_size = 3
    hidden_size = 4
    batch_size = 1
    
    gru_cell = GRUCellFromScratch(input_size, hidden_size)
    
    x = torch.tensor([[1.0, 0.5, -0.3]], dtype=torch.float32)
    h_prev = torch.tensor([[0.2, -0.1, 0.3, 0.0]], dtype=torch.float32)
    
    print(f"è¾“å…¥ x: {x.numpy()}")
    print(f"å‰ä¸€éšè—çŠ¶æ€ h_prev: {h_prev.numpy()}")
    print()
    
    # æ‰‹åŠ¨è®¡ç®—æ¯ä¸ªé—¨çš„å€¼
    combined = torch.cat([x, h_prev], dim=1)
    
    # é‡ç½®é—¨
    r_t = torch.sigmoid(combined @ gru_cell.W_r.T + gru_cell.b_r)
    print(f"ğŸ”„ é‡ç½®é—¨ r_t: {r_t.detach().numpy()}")
    print("   -> å€¼è¶Šæ¥è¿‘0è¡¨ç¤ºè¶Šè¦é‡ç½®ï¼ˆå¿˜è®°ï¼‰è¿‡å»ä¿¡æ¯")
    print("   -> Values closer to 0 mean more reset (forget) of past information")
    
    # æ›´æ–°é—¨
    z_t = torch.sigmoid(combined @ gru_cell.W_z.T + gru_cell.b_z)
    print(f"ğŸ”„ æ›´æ–°é—¨ z_t: {z_t.detach().numpy()}")
    print("   -> å€¼è¶Šæ¥è¿‘1è¡¨ç¤ºè¶Šè¦ä¿ç•™è¿‡å»ä¿¡æ¯ï¼Œè¶Šæ¥è¿‘0è¡¨ç¤ºè¶Šè¦ä½¿ç”¨æ–°ä¿¡æ¯")
    print("   -> Values closer to 1 mean more retention of past info, closer to 0 mean more use of new info")
    
    # å€™é€‰çŠ¶æ€
    reset_combined = torch.cat([x, r_t * h_prev], dim=1)
    h_tilde = torch.tanh(reset_combined @ gru_cell.W_h.T + gru_cell.b_h)
    print(f"âœ¨ å€™é€‰çŠ¶æ€ h_tilde: {h_tilde.detach().numpy()}")
    print("   -> åŸºäºé‡ç½®åçš„è¿‡å»ä¿¡æ¯å’Œå½“å‰è¾“å…¥è®¡ç®—çš„æ–°çŠ¶æ€")
    print("   -> New state computed based on reset past information and current input")
    
    # æœ€ç»ˆçŠ¶æ€
    h_t = (1 - z_t) * h_prev + z_t * h_tilde
    print(f"ğŸ¯ æ–°éšè—çŠ¶æ€ h_t: {h_t.detach().numpy()}")
    print("   -> æ›´æ–°é—¨æ§åˆ¶çš„è¿‡å»çŠ¶æ€å’Œå€™é€‰çŠ¶æ€çš„åŠ æƒç»„åˆ")
    print("   -> Weighted combination of past state and candidate state controlled by update gate")
    
    print("\n" + "=" * 60)
    print("ğŸ” GRU vs LSTM å…³é”®åŒºåˆ«ï¼š")
    print("ğŸ” Key differences between GRU and LSTM:")
    print("1. GRUåªæœ‰2ä¸ªé—¨ï¼ŒLSTMæœ‰3ä¸ªé—¨")
    print("   GRU has 2 gates, LSTM has 3 gates")
    print("2. GRUæ²¡æœ‰å•ç‹¬çš„ç»†èƒçŠ¶æ€ï¼ŒLSTMæœ‰")
    print("   GRU has no separate cell state, LSTM does")
    print("3. GRUçš„æ›´æ–°é—¨åŒæ—¶æ§åˆ¶é—å¿˜å’Œè¾“å…¥")
    print("   GRU's update gate controls both forgetting and input")


def train_gru_sequence():
    """è®­ç»ƒGRUè¿›è¡Œåºåˆ—é¢„æµ‹"""
    print("\nğŸ“ˆ GRUåºåˆ—é¢„æµ‹è®­ç»ƒæ¼”ç¤º | GRU Sequence Prediction Training Demo")
    print("=" * 60)
    
    set_random_seed(42)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # ç”Ÿæˆæ›´å¤æ‚çš„åºåˆ—æ•°æ® - ç»„åˆæ­£å¼¦æ³¢
    seq_length = 50
    num_samples = 1000
    
    t = np.linspace(0, 8*np.pi, num_samples)
    data = np.sin(t) + 0.5*np.sin(2*t) + 0.2*np.random.randn(num_samples)
    
    # åˆ›å»ºåºåˆ—æ•°æ®
    X, y = [], []
    for i in range(len(data) - seq_length):
        X.append(data[i:i+seq_length])
        y.append(data[i+seq_length])
    
    X = np.array(X)
    y = np.array(y)
    
    X = torch.FloatTensor(X).unsqueeze(-1).to(device)
    y = torch.FloatTensor(y).unsqueeze(-1).to(device)
    X = X.transpose(0, 1)
    
    # åˆ›å»ºGRUæ¨¡å‹
    model = GRUFromScratch(
        input_size=1,
        hidden_size=32,
        num_layers=2,
        output_size=1,
        dropout=0.1
    ).to(device)
    
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    # è®­ç»ƒ
    num_epochs = 100
    losses = []
    
    print("å¼€å§‹è®­ç»ƒGRU...")
    
    for epoch in range(num_epochs):
        model.train()
        
        outputs, _ = model(X)
        predictions = outputs[-1]
        loss = criterion(predictions, y)
        
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        
        losses.append(loss.item())
        
        if (epoch + 1) % 20 == 0:
            print(f'è½®æ¬¡ {epoch+1}/{num_epochs}, æŸå¤±: {loss.item():.6f}')
    
    # æµ‹è¯•å’Œå¯è§†åŒ–
    model.eval()
    with torch.no_grad():
        test_outputs, _ = model(X)
        test_predictions = test_outputs[-1].cpu().numpy()
        actual_values = y.cpu().numpy()
    
    plt.figure(figsize=(15, 5))
    
    plt.subplot(1, 2, 1)
    plt.plot(losses)
    plt.title('GRUè®­ç»ƒæŸå¤± | GRU Training Loss')
    plt.xlabel('è½®æ¬¡ | Epoch')
    plt.ylabel('æŸå¤± | Loss')
    plt.grid(True)
    
    plt.subplot(1, 2, 2)
    indices = range(200)
    plt.plot(indices, actual_values[:200], 'b-', label='å®é™…å€¼ | Actual', alpha=0.7)
    plt.plot(indices, test_predictions[:200], 'r-', label='é¢„æµ‹å€¼ | Predicted', alpha=0.7)
    plt.title('GRUé¢„æµ‹ç»“æœ | GRU Prediction Results')
    plt.legend()
    plt.grid(True)
    
    plt.tight_layout()
    plt.savefig('gru_training_results.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return model, losses


if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹GRUä»é›¶å®ç°æ¼”ç¤º | Starting GRU From Scratch Demonstration")
    print("=" * 80)
    
    # æ¼”ç¤ºGRUé—¨æ§æœºåˆ¶
    demonstrate_gru_gates()
    
    # è®­ç»ƒGRUåºåˆ—é¢„æµ‹
    model, losses = train_gru_sequence()
    
    print("\nğŸ‰ GRUæ¼”ç¤ºå®Œæˆï¼")
    print("ğŸ‰ GRU demonstration completed!")
    print("\nğŸ“š GRUçš„ä¼˜åŠ¿ï¼š")
    print("ğŸ“š Advantages of GRU:")
    print("1. å‚æ•°æ›´å°‘ï¼Œè®­ç»ƒæ›´å¿«")
    print("   Fewer parameters, faster training")
    print("2. ç»“æ„æ›´ç®€å•ï¼Œæ˜“äºç†è§£")
    print("   Simpler structure, easier to understand")  
    print("3. åœ¨å¾ˆå¤šä»»åŠ¡ä¸Šæ•ˆæœæ¥è¿‘LSTM")
    print("   Performance close to LSTM on many tasks") 