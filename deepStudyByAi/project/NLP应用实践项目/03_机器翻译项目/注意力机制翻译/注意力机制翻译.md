# 注意力机制机器翻译
# Attention Mechanism Machine Translation

**让机器学会"专注"翻译 - 深入多头注意力机制**
**Making machines learn to "focus" on translation - Deep dive into multi-head attention**

---

## 🎯 注意力机制的核心思想 | Core Ideas of Attention Mechanism

注意力机制解决了传统Seq2Seq的"信息瓶颈"问题。就像人类翻译时会重点关注源语言中的特定部分，注意力机制让解码器在生成每个词时都能"看到"并"专注于"源序列的不同部分。

Attention mechanism solves the "information bottleneck" problem of traditional Seq2Seq. Just like humans focus on specific parts of the source language when translating, attention mechanism allows the decoder to "see" and "focus on" different parts of the source sequence when generating each word.

### 注意力的直观理解 | Intuitive Understanding of Attention

**翻译示例 | Translation Example:**
```
英文: "The cat sat on the mat"
中文: "猫坐在垫子上"

在翻译"猫"时，注意力主要集中在"cat"
When translating "猫", attention focuses mainly on "cat"

在翻译"坐"时，注意力主要集中在"sat" 
When translating "坐", attention focuses mainly on "sat"

在翻译"垫子"时，注意力主要集中在"mat"
When translating "垫子", attention focuses mainly on "mat"
```

## 🔬 多头注意力机制 | Multi-Head Attention Mechanism

### 1. 缩放点积注意力 | Scaled Dot-Product Attention

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

def scaled_dot_product_attention(query, key, value, mask=None, dropout=None):
    """
    缩放点积注意力
    Scaled Dot-Product Attention
    
    Attention(Q,K,V) = softmax(QK^T/√d_k)V
    """
    d_k = query.size(-1)
    
    # 计算注意力分数 | Compute attention scores
    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)
    
    # 应用掩码 | Apply mask
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)
    
    # 计算注意力权重 | Compute attention weights
    attention_weights = F.softmax(scores, dim=-1)
    
    # 应用dropout | Apply dropout
    if dropout is not None:
        attention_weights = dropout(attention_weights)
    
    # 计算输出 | Compute output
    output = torch.matmul(attention_weights, value)
    
    return output, attention_weights


class MultiHeadAttention(nn.Module):
    """
    多头注意力机制
    Multi-Head Attention Mechanism
    """
    def __init__(self, d_model, num_heads, dropout=0.1):
        super(MultiHeadAttention, self).__init__()
        
        assert d_model % num_heads == 0
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        # 线性变换层 | Linear transformation layers
        self.w_q = nn.Linear(d_model, d_model)
        self.w_k = nn.Linear(d_model, d_model)
        self.w_v = nn.Linear(d_model, d_model)
        self.w_o = nn.Linear(d_model, d_model)
        
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, query, key, value, mask=None):
        """
        多头注意力前向传播
        Multi-head attention forward pass
        """
        batch_size = query.size(0)
        
        # 1. 线性变换 | Linear transformations
        Q = self.w_q(query)
        K = self.w_k(key)
        V = self.w_v(value)
        
        # 2. 重塑为多头形式 | Reshape for multi-head
        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        
        # 3. 应用注意力 | Apply attention
        attention_output, attention_weights = scaled_dot_product_attention(
            Q, K, V, mask, self.dropout
        )
        
        # 4. 合并多头 | Concatenate heads
        attention_output = attention_output.transpose(1, 2).contiguous().view(
            batch_size, -1, self.d_model
        )
        
        # 5. 最终线性变换 | Final linear transformation
        output = self.w_o(attention_output)
        
        return output, attention_weights
```

### 2. Transformer编码器 | Transformer Encoder

```python
class TransformerEncoder(nn.Module):
    """
    Transformer编码器层
    Transformer Encoder Layer
    """
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super(TransformerEncoder, self).__init__()
        
        # 多头自注意力 | Multi-head self-attention
        self.self_attention = MultiHeadAttention(d_model, num_heads, dropout)
        
        # 前馈网络 | Feed-forward network
        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model)
        )
        
        # 层归一化 | Layer normalization
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x, mask=None):
        """
        编码器前向传播
        Encoder forward pass
        """
        # 自注意力 + 残差连接 | Self-attention + residual connection
        attn_output, _ = self.self_attention(x, x, x, mask)
        x = self.norm1(x + self.dropout(attn_output))
        
        # 前馈网络 + 残差连接 | Feed-forward + residual connection
        ff_output = self.feed_forward(x)
        x = self.norm2(x + self.dropout(ff_output))
        
        return x


class TransformerDecoder(nn.Module):
    """
    Transformer解码器层
    Transformer Decoder Layer
    """
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super(TransformerDecoder, self).__init__()
        
        # 掩码自注意力 | Masked self-attention
        self.self_attention = MultiHeadAttention(d_model, num_heads, dropout)
        
        # 交叉注意力 | Cross attention
        self.cross_attention = MultiHeadAttention(d_model, num_heads, dropout)
        
        # 前馈网络 | Feed-forward network
        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model)
        )
        
        # 层归一化 | Layer normalization
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)
        
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):
        """
        解码器前向传播
        Decoder forward pass
        """
        # 掩码自注意力 | Masked self-attention
        attn_output, _ = self.self_attention(x, x, x, tgt_mask)
        x = self.norm1(x + self.dropout(attn_output))
        
        # 交叉注意力 | Cross attention
        cross_attn_output, attention_weights = self.cross_attention(
            x, encoder_output, encoder_output, src_mask
        )
        x = self.norm2(x + self.dropout(cross_attn_output))
        
        # 前馈网络 | Feed-forward network
        ff_output = self.feed_forward(x)
        x = self.norm3(x + self.dropout(ff_output))
        
        return x, attention_weights
```

### 3. 完整的Transformer翻译模型 | Complete Transformer Translation Model

```python
class TransformerTranslator(nn.Module):
    """
    基于Transformer的完整翻译模型
    Complete Transformer-based Translation Model
    """
    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, 
                 num_heads=8, num_encoder_layers=6, num_decoder_layers=6, 
                 d_ff=2048, max_seq_length=5000, dropout=0.1):
        super(TransformerTranslator, self).__init__()
        
        self.d_model = d_model
        
        # 词嵌入和位置编码 | Word embedding and positional encoding
        self.src_embedding = nn.Embedding(src_vocab_size, d_model)
        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)
        self.positional_encoding = PositionalEncoding(d_model, dropout, max_seq_length)
        
        # 编码器和解码器层 | Encoder and decoder layers
        self.encoder_layers = nn.ModuleList([
            TransformerEncoder(d_model, num_heads, d_ff, dropout)
            for _ in range(num_encoder_layers)
        ])
        
        self.decoder_layers = nn.ModuleList([
            TransformerDecoder(d_model, num_heads, d_ff, dropout)
            for _ in range(num_decoder_layers)
        ])
        
        # 输出投影层 | Output projection layer
        self.output_projection = nn.Linear(d_model, tgt_vocab_size)
        
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, src, tgt, src_mask=None, tgt_mask=None):
        """
        前向传播
        Forward pass
        """
        # 编码器 | Encoder
        src_embedded = self.src_embedding(src) * math.sqrt(self.d_model)
        src_embedded = self.positional_encoding(src_embedded)
        
        encoder_output = src_embedded
        for encoder_layer in self.encoder_layers:
            encoder_output = encoder_layer(encoder_output, src_mask)
        
        # 解码器 | Decoder
        tgt_embedded = self.tgt_embedding(tgt) * math.sqrt(self.d_model)
        tgt_embedded = self.positional_encoding(tgt_embedded)
        
        decoder_output = tgt_embedded
        attention_weights = []
        
        for decoder_layer in self.decoder_layers:
            decoder_output, attn_weights = decoder_layer(
                decoder_output, encoder_output, src_mask, tgt_mask
            )
            attention_weights.append(attn_weights)
        
        # 输出投影 | Output projection
        output = self.output_projection(decoder_output)
        
        return output, attention_weights


class PositionalEncoding(nn.Module):
    """
    位置编码
    Positional Encoding
    """
    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super(PositionalEncoding, self).__init__()
        
        self.dropout = nn.Dropout(p=dropout)
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        x = x + self.pe[:x.size(0), :]
        return self.dropout(x)
```

## 🚀 高级技术：预训练翻译模型 | Advanced: Pre-trained Translation Models

### 使用预训练模型进行翻译 | Using Pre-trained Models for Translation

```python
from transformers import MarianMTModel, MarianTokenizer

class PretrainedTranslator:
    """
    基于预训练模型的翻译器
    Pre-trained Model-based Translator
    """
    def __init__(self, model_name='Helsinki-NLP/opus-mt-en-zh'):
        # 加载预训练的翻译模型
        # Load pre-trained translation model
        self.tokenizer = MarianTokenizer.from_pretrained(model_name)
        self.model = MarianMTModel.from_pretrained(model_name)
        
        print(f"已加载预训练模型: {model_name}")
    
    def translate(self, texts, max_length=512):
        """
        翻译文本
        Translate texts
        """
        if isinstance(texts, str):
            texts = [texts]
        
        # 编码输入
        # Encode inputs
        inputs = self.tokenizer(texts, return_tensors="pt", padding=True, truncation=True, max_length=max_length)
        
        # 生成翻译
        # Generate translations
        with torch.no_grad():
            outputs = self.model.generate(**inputs, max_length=max_length, num_beams=4, early_stopping=True)
        
        # 解码输出
        # Decode outputs
        translations = [self.tokenizer.decode(output, skip_special_tokens=True) for output in outputs]
        
        if len(translations) == 1:
            return translations[0]
        return translations
    
    def translate_with_scores(self, text, num_return_sequences=3):
        """
        翻译并返回置信度分数
        Translate and return confidence scores
        """
        inputs = self.tokenizer(text, return_tensors="pt")
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_length=512,
                num_beams=5,
                num_return_sequences=num_return_sequences,
                return_dict_in_generate=True,
                output_scores=True
            )
        
        translations = []
        for i, sequence in enumerate(outputs.sequences):
            translation = self.tokenizer.decode(sequence, skip_special_tokens=True)
            score = outputs.sequences_scores[i].item()
            translations.append({
                'translation': translation,
                'score': score
            })
        
        return translations


def main_demo():
    """
    演示不同翻译方法
    Demonstrate different translation methods
    """
    print("=== 机器翻译系统演示 | Machine Translation System Demo ===")
    
    # 1. 预训练模型翻译
    print("\n1. 预训练模型翻译 | Pre-trained Model Translation:")
    translator = PretrainedTranslator()
    
    test_sentences = [
        "Hello, how are you today?",
        "Machine learning is changing the world.",
        "The weather is beautiful today."
    ]
    
    for sentence in test_sentences:
        translation = translator.translate(sentence)
        print(f"EN: {sentence}")
        print(f"ZH: {translation}")
        print()
    
    # 2. 带置信度的翻译
    print("\n2. 带置信度的翻译 | Translation with Confidence:")
    text = "Artificial intelligence will transform our future."
    results = translator.translate_with_scores(text, num_return_sequences=3)
    
    print(f"原文: {text}")
    print("翻译候选:")
    for i, result in enumerate(results):
        print(f"{i+1}. {result['translation']} (分数: {result['score']:.4f})")


if __name__ == "__main__":
    main_demo()
```

## 📊 翻译质量评估 | Translation Quality Evaluation

### 自动评估指标 | Automatic Evaluation Metrics

```python
def comprehensive_evaluation(references, hypotheses):
    """
    综合评估翻译质量
    Comprehensive translation quality evaluation
    """
    from sacrebleu import corpus_bleu
    from rouge_score import rouge_scorer
    
    results = {}
    
    # BLEU分数 | BLEU score
    bleu = corpus_bleu(hypotheses, [references])
    results['BLEU'] = bleu.score
    
    # ROUGE分数 | ROUGE score
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    rouge_scores = []
    
    for ref, hyp in zip(references, hypotheses):
        scores = scorer.score(ref, hyp)
        rouge_scores.append(scores)
    
    # 平均ROUGE分数 | Average ROUGE scores
    results['ROUGE-1'] = sum(score['rouge1'].fmeasure for score in rouge_scores) / len(rouge_scores)
    results['ROUGE-2'] = sum(score['rouge2'].fmeasure for score in rouge_scores) / len(rouge_scores)
    results['ROUGE-L'] = sum(score['rougeL'].fmeasure for score in rouge_scores) / len(rouge_scores)
    
    return results
```

通过这个注意力机制翻译项目，你将深入理解现代机器翻译的核心技术 - 注意力机制和Transformer架构，掌握构建高质量翻译系统的关键技能！

Through this attention mechanism translation project, you will deeply understand the core technologies of modern machine translation - attention mechanism and Transformer architecture, and master the key skills for building high-quality translation systems! 