# åºåˆ—åˆ°åºåˆ—æœºå™¨ç¿»è¯‘
# Sequence-to-Sequence Machine Translation

**è®©æœºå™¨è·¨è¶Šè¯­è¨€éšœç¢ - æ„å»ºæ™ºèƒ½ç¿»è¯‘ç³»ç»Ÿ**
**Making machines cross language barriers - Building intelligent translation systems**

---

## ğŸŒ ä»€ä¹ˆæ˜¯åºåˆ—åˆ°åºåˆ—ç¿»è¯‘ï¼Ÿ| What is Sequence-to-Sequence Translation?

åºåˆ—åˆ°åºåˆ—(Seq2Seq)ç¿»è¯‘å°±åƒç»™æœºå™¨é…å¤‡äº†"è¯­è¨€è½¬æ¢å™¨"ã€‚å®ƒæ¥æ”¶ä¸€ç§è¯­è¨€çš„å¥å­ä½œä¸ºè¾“å…¥ï¼Œç„¶åç”Ÿæˆå¦ä¸€ç§è¯­è¨€çš„å¯¹åº”ç¿»è¯‘ã€‚è¿™ä¸æ˜¯ç®€å•çš„è¯æ±‡æ›¿æ¢ï¼Œè€Œæ˜¯éœ€è¦ç†è§£æºè¯­è¨€çš„è¯­ä¹‰ï¼Œç„¶åç”¨ç›®æ ‡è¯­è¨€é‡æ–°è¡¨è¾¾ã€‚

Sequence-to-sequence (Seq2Seq) translation is like equipping machines with a "language converter". It takes a sentence in one language as input, then generates the corresponding translation in another language. This is not simple word substitution, but requires understanding the semantics of the source language and then re-expressing it in the target language.

### æœºå™¨ç¿»è¯‘çš„æ¼”è¿›å†ç¨‹ | Evolution of Machine Translation

```
1940s-1980s: åŸºäºè§„åˆ™çš„ç¿»è¯‘ | Rule-based Translation
- æ‰‹å·¥ç¼–å†™è¯­æ³•è§„åˆ™å’Œè¯å…¸
- Manually written grammar rules and dictionaries

1990s-2000s: ç»Ÿè®¡æœºå™¨ç¿»è¯‘ | Statistical Machine Translation  
- åŸºäºå¤§é‡å¹³è¡Œè¯­æ–™çš„ç»Ÿè®¡æ¨¡å‹
- Statistical models based on large parallel corpora

2010s: ç¥ç»æœºå™¨ç¿»è¯‘ | Neural Machine Translation
- ç«¯åˆ°ç«¯çš„æ·±åº¦å­¦ä¹ æ–¹æ³•
- End-to-end deep learning approaches

2020s+: å¤§è§„æ¨¡é¢„è®­ç»ƒç¿»è¯‘æ¨¡å‹ | Large-scale Pretrained Translation Models
- Transformeræ¶æ„ + æµ·é‡æ•°æ®
- Transformer architecture + massive data
```

### Seq2Seqçš„æ ¸å¿ƒæ€æƒ³ | Core Ideas of Seq2Seq

**ç¼–ç å™¨-è§£ç å™¨æ¶æ„ (Encoder-Decoder Architecture)**

```python
class Seq2SeqFramework:
    """
    åºåˆ—åˆ°åºåˆ—æ¡†æ¶çš„æ¦‚å¿µæ¨¡å‹
    Conceptual model of Seq2Seq framework
    """
    def __init__(self):
        # ç¼–ç å™¨ï¼šå°†æºè¯­è¨€åºåˆ—ç¼–ç ä¸ºå‘é‡è¡¨ç¤º
        # Encoder: encode source language sequence to vector representation
        self.encoder = LanguageEncoder()
        
        # è§£ç å™¨ï¼šä»å‘é‡è¡¨ç¤ºç”Ÿæˆç›®æ ‡è¯­è¨€åºåˆ—
        # Decoder: generate target language sequence from vector representation
        self.decoder = LanguageDecoder()
    
    def translate(self, source_sentence):
        """
        ç¿»è¯‘è¿‡ç¨‹ | Translation process
        """
        # æ­¥éª¤1: ç¼–ç æºè¯­è¨€
        # Step 1: Encode source language
        context_vector = self.encoder.encode(source_sentence)
        
        # æ­¥éª¤2: è§£ç åˆ°ç›®æ ‡è¯­è¨€
        # Step 2: Decode to target language
        target_sentence = self.decoder.decode(context_vector)
        
        return target_sentence
```

## ğŸ”¬ Seq2Seqæ¨¡å‹æŠ€æœ¯æ¶æ„ | Seq2Seq Model Technical Architecture

### 1. ç»å…¸RNN-based Seq2Seq

#### åŸºç¡€ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ | Basic Encoder-Decoder Model

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import random

class Encoder(nn.Module):
    """
    RNNç¼–ç å™¨
    RNN Encoder
    
    å°†æºè¯­è¨€åºåˆ—ç¼–ç ä¸ºå›ºå®šå¤§å°çš„ä¸Šä¸‹æ–‡å‘é‡
    Encode source language sequence to fixed-size context vector
    """
    def __init__(self, input_size, hidden_size, num_layers=2, dropout=0.1):
        super(Encoder, self).__init__()
        
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # è¯åµŒå…¥å±‚ | Word embedding layer
        self.embedding = nn.Embedding(input_size, hidden_size)
        
        # LSTMç¼–ç å™¨ | LSTM encoder
        self.lstm = nn.LSTM(
            hidden_size, 
            hidden_size, 
            num_layers, 
            dropout=dropout,
            batch_first=True,
            bidirectional=True
        )
        
        self.dropout = nn.Dropout(dropout)
        
        # åŒå‘LSTMçš„è¾“å‡ºéœ€è¦å‹ç¼© | Bidirectional LSTM output needs compression
        self.hidden_projection = nn.Linear(hidden_size * 2, hidden_size)
        self.cell_projection = nn.Linear(hidden_size * 2, hidden_size)
    
    def forward(self, source_sequence, source_lengths):
        """
        ç¼–ç æºåºåˆ—
        Encode source sequence
        
        Args:
            source_sequence: [batch_size, max_src_len]
            source_lengths: [batch_size] æ¯ä¸ªåºåˆ—çš„å®é™…é•¿åº¦
        """
        batch_size = source_sequence.size(0)
        
        # è¯åµŒå…¥ | Word embedding
        embedded = self.embedding(source_sequence)
        embedded = self.dropout(embedded)
        
        # æ‰“åŒ…åºåˆ—ä»¥å¤„ç†ä¸åŒé•¿åº¦ | Pack sequences to handle different lengths
        packed_embedded = nn.utils.rnn.pack_padded_sequence(
            embedded, source_lengths, batch_first=True, enforce_sorted=False
        )
        
        # LSTMç¼–ç  | LSTM encoding
        packed_outputs, (hidden, cell) = self.lstm(packed_embedded)
        
        # è§£åŒ…åºåˆ— | Unpack sequences
        encoder_outputs, _ = nn.utils.rnn.pad_packed_sequence(
            packed_outputs, batch_first=True
        )
        
        # å¤„ç†åŒå‘LSTMçš„éšçŠ¶æ€ | Process bidirectional LSTM hidden states
        # hidden: [num_layers * 2, batch_size, hidden_size]
        # é‡æ–°ç»„ç»‡ä¸º [num_layers, batch_size, hidden_size * 2]
        hidden = hidden.view(self.num_layers, 2, batch_size, self.hidden_size)
        hidden = torch.cat([hidden[:, 0, :, :], hidden[:, 1, :, :]], dim=2)
        
        cell = cell.view(self.num_layers, 2, batch_size, self.hidden_size)
        cell = torch.cat([cell[:, 0, :, :], cell[:, 1, :, :]], dim=2)
        
        # æŠ•å½±åˆ°è§£ç å™¨çš„éšçŠ¶æ€å¤§å° | Project to decoder hidden state size
        hidden = self.hidden_projection(hidden)
        cell = self.cell_projection(cell)
        
        return encoder_outputs, (hidden, cell)


class Decoder(nn.Module):
    """
    RNNè§£ç å™¨ï¼ˆæ— æ³¨æ„åŠ›æœºåˆ¶ï¼‰
    RNN Decoder (without attention mechanism)
    """
    def __init__(self, output_size, hidden_size, num_layers=2, dropout=0.1):
        super(Decoder, self).__init__()
        
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.num_layers = num_layers
        
        # è¯åµŒå…¥å±‚ | Word embedding layer
        self.embedding = nn.Embedding(output_size, hidden_size)
        
        # LSTMè§£ç å™¨ | LSTM decoder
        self.lstm = nn.LSTM(
            hidden_size,
            hidden_size,
            num_layers,
            dropout=dropout,
            batch_first=True
        )
        
        # è¾“å‡ºæŠ•å½±å±‚ | Output projection layer
        self.out = nn.Linear(hidden_size, output_size)
        
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, input_token, hidden_state, cell_state):
        """
        è§£ç å•ä¸ªæ—¶é—´æ­¥
        Decode single time step
        
        Args:
            input_token: [batch_size, 1] å½“å‰è¾“å…¥token
            hidden_state: [num_layers, batch_size, hidden_size]
            cell_state: [num_layers, batch_size, hidden_size]
        """
        # è¯åµŒå…¥ | Word embedding
        embedded = self.embedding(input_token)
        embedded = self.dropout(embedded)
        
        # LSTMå‰å‘ä¼ æ’­ | LSTM forward pass
        output, (hidden, cell) = self.lstm(embedded, (hidden_state, cell_state))
        
        # è¾“å‡ºé¢„æµ‹ | Output prediction
        prediction = self.out(output)
        
        return prediction, hidden, cell


class BasicSeq2Seq(nn.Module):
    """
    åŸºç¡€åºåˆ—åˆ°åºåˆ—æ¨¡å‹
    Basic Sequence-to-Sequence Model
    """
    def __init__(self, encoder, decoder, device):
        super(BasicSeq2Seq, self).__init__()
        
        self.encoder = encoder
        self.decoder = decoder
        self.device = device
    
    def forward(self, source, target, source_lengths, teacher_forcing_ratio=0.5):
        """
        å‰å‘ä¼ æ’­
        Forward pass
        
        Args:
            source: [batch_size, src_len] æºåºåˆ—
            target: [batch_size, trg_len] ç›®æ ‡åºåˆ— 
            source_lengths: [batch_size] æºåºåˆ—é•¿åº¦
            teacher_forcing_ratio: æ•™å¸ˆå¼ºåˆ¶æ¯”ç‡
        """
        batch_size = source.shape[0]
        target_len = target.shape[1]
        target_vocab_size = self.decoder.output_size
        
        # å­˜å‚¨è§£ç å™¨è¾“å‡º | Store decoder outputs
        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(self.device)
        
        # ç¼–ç æºåºåˆ— | Encode source sequence
        encoder_outputs, (hidden, cell) = self.encoder(source, source_lengths)
        
        # è§£ç å™¨çš„ç¬¬ä¸€ä¸ªè¾“å…¥æ˜¯SOS token
        # First input to decoder is SOS token
        input_token = target[:, 0].unsqueeze(1)  # [batch_size, 1]
        
        for t in range(1, target_len):
            # è§£ç ä¸€æ­¥
            # Decode one step
            output, hidden, cell = self.decoder(input_token, hidden, cell)
            outputs[:, t] = output.squeeze(1)
            
            # å†³å®šä¸‹ä¸€ä¸ªè¾“å…¥ï¼šæ•™å¸ˆå¼ºåˆ¶ vs æ¨¡å‹é¢„æµ‹
            # Decide next input: teacher forcing vs model prediction
            teacher_force = random.random() < teacher_forcing_ratio
            top1 = output.argmax(2)
            input_token = target[:, t].unsqueeze(1) if teacher_force else top1
        
        return outputs
    
    def translate(self, source, source_length, max_length=50, sos_token=1, eos_token=2):
        """
        ç¿»è¯‘å•ä¸ªå¥å­ï¼ˆæ¨ç†æ¨¡å¼ï¼‰
        Translate single sentence (inference mode)
        """
        self.eval()
        
        with torch.no_grad():
            # ç¼–ç 
            # Encoding
            encoder_outputs, (hidden, cell) = self.encoder(source, source_length)
            
            # åˆå§‹åŒ–è§£ç 
            # Initialize decoding
            input_token = torch.tensor([[sos_token]]).to(self.device)
            outputs = [sos_token]
            
            for _ in range(max_length):
                output, hidden, cell = self.decoder(input_token, hidden, cell)
                predicted = output.argmax(2).item()
                outputs.append(predicted)
                
                if predicted == eos_token:
                    break
                
                input_token = torch.tensor([[predicted]]).to(self.device)
        
        return outputs
```

### 2. æ³¨æ„åŠ›æœºåˆ¶å¢å¼ºçš„Seq2Seq | Attention-Enhanced Seq2Seq

**è§£å†³ä¿¡æ¯ç“¶é¢ˆé—®é¢˜ | Solving Information Bottleneck Problem**

åŸºç¡€Seq2Seqæ¨¡å‹çš„é—®é¢˜æ˜¯å°†æ•´ä¸ªæºåºåˆ—å‹ç¼©åˆ°ä¸€ä¸ªå›ºå®šå¤§å°çš„å‘é‡ä¸­ï¼Œè¿™ä¼šå¯¼è‡´ä¿¡æ¯ä¸¢å¤±ã€‚æ³¨æ„åŠ›æœºåˆ¶å…è®¸è§£ç å™¨åœ¨æ¯ä¸ªæ—¶é—´æ­¥è®¿é—®ç¼–ç å™¨çš„æ‰€æœ‰éšçŠ¶æ€ã€‚

The problem with basic Seq2Seq models is compressing the entire source sequence into a fixed-size vector, which leads to information loss. Attention mechanism allows the decoder to access all encoder hidden states at each time step.

```python
class AttentionMechanism(nn.Module):
    """
    æ³¨æ„åŠ›æœºåˆ¶
    Attention Mechanism
    
    è®¡ç®—è§£ç å™¨å½“å‰çŠ¶æ€ä¸ç¼–ç å™¨æ‰€æœ‰çŠ¶æ€çš„æ³¨æ„åŠ›æƒé‡
    Compute attention weights between decoder current state and all encoder states
    """
    def __init__(self, hidden_size, method='general'):
        super(AttentionMechanism, self).__init__()
        
        self.method = method
        self.hidden_size = hidden_size
        
        if method == 'general':
            self.attn = nn.Linear(hidden_size, hidden_size)
        elif method == 'concat':
            self.attn = nn.Linear(hidden_size * 2, hidden_size)
            self.v = nn.Parameter(torch.FloatTensor(hidden_size))
    
    def forward(self, hidden, encoder_outputs, encoder_mask=None):
        """
        è®¡ç®—æ³¨æ„åŠ›æƒé‡å’Œä¸Šä¸‹æ–‡å‘é‡
        Compute attention weights and context vector
        
        Args:
            hidden: [batch_size, hidden_size] è§£ç å™¨å½“å‰éšçŠ¶æ€
            encoder_outputs: [batch_size, src_len, hidden_size] ç¼–ç å™¨è¾“å‡º
            encoder_mask: [batch_size, src_len] ç¼–ç å™¨æ©ç 
        """
        batch_size = encoder_outputs.size(0)
        src_len = encoder_outputs.size(1)
        
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        # Compute attention scores
        if self.method == 'general':
            # hidden: [batch_size, hidden_size] -> [batch_size, 1, hidden_size]
            hidden = hidden.unsqueeze(1)
            # è®¡ç®—ä¸æ‰€æœ‰ç¼–ç å™¨çŠ¶æ€çš„ç›¸ä¼¼åº¦
            # Compute similarity with all encoder states
            energy = torch.bmm(hidden, self.attn(encoder_outputs).transpose(1, 2))
            energy = energy.squeeze(1)  # [batch_size, src_len]
        
        elif self.method == 'concat':
            # æ‹¼æ¥éšçŠ¶æ€å’Œç¼–ç å™¨è¾“å‡º
            # Concatenate hidden state and encoder outputs
            hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)
            energy = torch.cat([hidden, encoder_outputs], dim=2)
            energy = torch.tanh(self.attn(energy))
            energy = torch.sum(self.v * energy, dim=2)  # [batch_size, src_len]
        
        # åº”ç”¨æ©ç 
        # Apply mask
        if encoder_mask is not None:
            energy = energy.masked_fill(encoder_mask == 0, -1e10)
        
        # è®¡ç®—æ³¨æ„åŠ›æƒé‡
        # Compute attention weights
        attention_weights = F.softmax(energy, dim=1)  # [batch_size, src_len]
        
        # è®¡ç®—ä¸Šä¸‹æ–‡å‘é‡
        # Compute context vector
        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)
        context = context.squeeze(1)  # [batch_size, hidden_size]
        
        return context, attention_weights


class AttentionDecoder(nn.Module):
    """
    å¸¦æ³¨æ„åŠ›æœºåˆ¶çš„è§£ç å™¨
    Decoder with Attention Mechanism
    """
    def __init__(self, output_size, hidden_size, num_layers=2, dropout=0.1, attention_method='general'):
        super(AttentionDecoder, self).__init__()
        
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.num_layers = num_layers
        
        # è¯åµŒå…¥å±‚ | Word embedding layer
        self.embedding = nn.Embedding(output_size, hidden_size)
        
        # æ³¨æ„åŠ›æœºåˆ¶ | Attention mechanism
        self.attention = AttentionMechanism(hidden_size, attention_method)
        
        # LSTMè§£ç å™¨ | LSTM decoder
        self.lstm = nn.LSTM(
            hidden_size * 2,  # åµŒå…¥ + ä¸Šä¸‹æ–‡å‘é‡
            hidden_size,
            num_layers,
            dropout=dropout,
            batch_first=True
        )
        
        # è¾“å‡ºå±‚ | Output layer
        self.concat = nn.Linear(hidden_size * 2, hidden_size)
        self.out = nn.Linear(hidden_size, output_size)
        
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, input_token, hidden_state, cell_state, encoder_outputs, encoder_mask=None):
        """
        å¸¦æ³¨æ„åŠ›çš„è§£ç æ­¥éª¤
        Decoding step with attention
        """
        # è¯åµŒå…¥ | Word embedding
        embedded = self.embedding(input_token)
        embedded = self.dropout(embedded)
        
        # è®¡ç®—æ³¨æ„åŠ›ä¸Šä¸‹æ–‡
        # Compute attention context
        # ä½¿ç”¨ä¸Šä¸€æ—¶åˆ»çš„éšçŠ¶æ€è®¡ç®—æ³¨æ„åŠ›
        # Use previous hidden state to compute attention
        context, attention_weights = self.attention(
            hidden_state[-1], encoder_outputs, encoder_mask
        )
        
        # æ‹¼æ¥åµŒå…¥å’Œä¸Šä¸‹æ–‡
        # Concatenate embedding and context
        lstm_input = torch.cat([embedded, context.unsqueeze(1)], dim=2)
        
        # LSTMå‰å‘ä¼ æ’­ | LSTM forward pass
        output, (hidden, cell) = self.lstm(lstm_input, (hidden_state, cell_state))
        
        # è¾“å‡ºå±‚è®¡ç®— | Output layer computation
        concat_input = torch.cat([output.squeeze(1), context], dim=1)
        concat_output = torch.tanh(self.concat(concat_input))
        prediction = self.out(concat_output).unsqueeze(1)
        
        return prediction, hidden, cell, attention_weights


class AttentionSeq2Seq(nn.Module):
    """
    å¸¦æ³¨æ„åŠ›æœºåˆ¶çš„åºåˆ—åˆ°åºåˆ—æ¨¡å‹
    Sequence-to-Sequence Model with Attention
    """
    def __init__(self, encoder, decoder, device):
        super(AttentionSeq2Seq, self).__init__()
        
        self.encoder = encoder
        self.decoder = decoder
        self.device = device
    
    def create_mask(self, src, src_lengths):
        """
        åˆ›å»ºæºåºåˆ—æ©ç 
        Create source sequence mask
        """
        batch_size, max_len = src.size()
        mask = torch.zeros(batch_size, max_len).to(self.device)
        
        for i, length in enumerate(src_lengths):
            mask[i, :length] = 1
        
        return mask.bool()
    
    def forward(self, source, target, source_lengths, teacher_forcing_ratio=0.5):
        """
        å‰å‘ä¼ æ’­
        Forward pass
        """
        batch_size = source.shape[0]
        target_len = target.shape[1]
        target_vocab_size = self.decoder.output_size
        
        # å­˜å‚¨è¾“å‡ºå’Œæ³¨æ„åŠ›æƒé‡
        # Store outputs and attention weights
        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(self.device)
        attentions = torch.zeros(batch_size, target_len, source.size(1)).to(self.device)
        
        # ç¼–ç  | Encoding
        encoder_outputs, (hidden, cell) = self.encoder(source, source_lengths)
        
        # åˆ›å»ºæºåºåˆ—æ©ç  | Create source sequence mask
        encoder_mask = self.create_mask(source, source_lengths)
        
        # è§£ç  | Decoding
        input_token = target[:, 0].unsqueeze(1)
        
        for t in range(1, target_len):
            output, hidden, cell, attention = self.decoder(
                input_token, hidden, cell, encoder_outputs, encoder_mask
            )
            
            outputs[:, t] = output.squeeze(1)
            attentions[:, t] = attention
            
            # æ•™å¸ˆå¼ºåˆ¶ | Teacher forcing
            teacher_force = random.random() < teacher_forcing_ratio
            top1 = output.argmax(2)
            input_token = target[:, t].unsqueeze(1) if teacher_force else top1
        
        return outputs, attentions
```

### 3. é«˜çº§æŠ€æœ¯ï¼šæŸæœç´¢è§£ç  | Advanced Technique: Beam Search Decoding

**è§£å†³è´ªå¿ƒè§£ç çš„å±€é™æ€§ | Solving Limitations of Greedy Decoding**

è´ªå¿ƒè§£ç åœ¨æ¯ä¸ªæ—¶é—´æ­¥é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„è¯ï¼Œä½†è¿™å¯èƒ½å¯¼è‡´æ¬¡ä¼˜çš„å…¨å±€è§£ã€‚æŸæœç´¢ä¿æŒå¤šä¸ªå€™é€‰åºåˆ—ï¼Œé€‰æ‹©å…¨å±€æœ€ä¼˜è§£ã€‚

Greedy decoding chooses the highest probability word at each time step, but this may lead to suboptimal global solutions. Beam search maintains multiple candidate sequences to find globally optimal solutions.

```python
class BeamSearchDecoder:
    """
    æŸæœç´¢è§£ç å™¨
    Beam Search Decoder
    
    åœ¨è§£ç è¿‡ç¨‹ä¸­ç»´æŠ¤top-kä¸ªå€™é€‰åºåˆ—
    Maintain top-k candidate sequences during decoding
    """
    def __init__(self, model, beam_size=5, max_length=50, sos_token=1, eos_token=2):
        self.model = model
        self.beam_size = beam_size
        self.max_length = max_length
        self.sos_token = sos_token
        self.eos_token = eos_token
    
    def search(self, source, source_length):
        """
        æ‰§è¡ŒæŸæœç´¢
        Execute beam search
        """
        device = source.device
        
        # ç¼–ç æºåºåˆ— | Encode source sequence
        with torch.no_grad():
            encoder_outputs, (hidden, cell) = self.model.encoder(source, source_length)
            encoder_mask = self.model.create_mask(source, source_length)
        
        # åˆå§‹åŒ–æŸ | Initialize beam
        # æ¯ä¸ªå€™é€‰åŒ…å«ï¼š(åºåˆ—, ç´¯ç§¯åˆ†æ•°, éšçŠ¶æ€, ç»†èƒçŠ¶æ€)
        # Each candidate contains: (sequence, cumulative score, hidden state, cell state)
        initial_hidden = hidden
        initial_cell = cell
        
        candidates = [(
            [self.sos_token],  # åºåˆ—
            0.0,               # ç´¯ç§¯logæ¦‚ç‡
            initial_hidden,    # éšçŠ¶æ€
            initial_cell       # ç»†èƒçŠ¶æ€
        )]
        
        finished_sequences = []
        
        for step in range(self.max_length):
            if not candidates:
                break
            
            # ä¸ºå½“å‰æ‰€æœ‰å€™é€‰ç”Ÿæˆä¸‹ä¸€ä¸ªè¯çš„åˆ†å¸ƒ
            # Generate next word distributions for all current candidates
            all_candidates = []
            
            for sequence, score, hidden, cell in candidates:
                if sequence[-1] == self.eos_token:
                    # å·²å®Œæˆçš„åºåˆ—ç›´æ¥åŠ å…¥ç»“æœ
                    # Add completed sequences directly to results
                    finished_sequences.append((sequence, score))
                    continue
                
                # å‡†å¤‡è§£ç å™¨è¾“å…¥
                # Prepare decoder input
                input_token = torch.tensor([[sequence[-1]]]).to(device)
                
                with torch.no_grad():
                    if hasattr(self.model.decoder, 'attention'):
                        # å¸¦æ³¨æ„åŠ›çš„è§£ç å™¨
                        # Decoder with attention
                        output, new_hidden, new_cell, _ = self.model.decoder(
                            input_token, hidden, cell, encoder_outputs, encoder_mask
                        )
                    else:
                        # åŸºç¡€è§£ç å™¨
                        # Basic decoder
                        output, new_hidden, new_cell = self.model.decoder(
                            input_token, hidden, cell
                        )
                
                # è·å–è¯æ±‡è¡¨ä¸Šçš„æ¦‚ç‡åˆ†å¸ƒ
                # Get probability distribution over vocabulary
                probs = F.log_softmax(output.squeeze(), dim=-1)
                
                # è·å–top-kä¸ªè¯
                # Get top-k words
                top_k_probs, top_k_indices = torch.topk(probs, self.beam_size)
                
                # ä¸ºæ¯ä¸ªå¯èƒ½çš„ä¸‹ä¸€ä¸ªè¯åˆ›å»ºæ–°å€™é€‰
                # Create new candidates for each possible next word
                for i in range(self.beam_size):
                    next_token = top_k_indices[i].item()
                    token_prob = top_k_probs[i].item()
                    
                    new_sequence = sequence + [next_token]
                    new_score = score + token_prob
                    
                    all_candidates.append((
                        new_sequence,
                        new_score,
                        new_hidden,
                        new_cell
                    ))
            
            # é€‰æ‹©top-kä¸ªå€™é€‰ç»§ç»­
            # Select top-k candidates to continue
            all_candidates.sort(key=lambda x: x[1] / len(x[0]), reverse=True)  # æŒ‰å¹³å‡åˆ†æ•°æ’åº
            candidates = all_candidates[:self.beam_size]
        
        # å°†æœªå®Œæˆçš„å€™é€‰ä¹ŸåŠ å…¥ç»“æœ
        # Add unfinished candidates to results
        for sequence, score, _, _ in candidates:
            finished_sequences.append((sequence, score))
        
        # æŒ‰åˆ†æ•°æ’åºå¹¶è¿”å›æœ€ä½³åºåˆ—
        # Sort by score and return best sequence
        if finished_sequences:
            finished_sequences.sort(key=lambda x: x[1] / len(x[0]), reverse=True)
            return finished_sequences[0][0]  # è¿”å›æœ€ä½³åºåˆ—
        else:
            return [self.sos_token, self.eos_token]  # é»˜è®¤è¿”å›
    
    def translate_batch(self, sources, source_lengths):
        """
        æ‰¹é‡ç¿»è¯‘
        Batch translation
        """
        translations = []
        
        for i in range(len(sources)):
            source = sources[i:i+1]  # å•ä¸ªæ ·æœ¬
            source_length = source_lengths[i:i+1]
            
            translation = self.search(source, source_length)
            translations.append(translation)
        
        return translations
```

### 4. è®­ç»ƒç­–ç•¥ä¼˜åŒ– | Training Strategy Optimization

#### è¯¾ç¨‹å­¦ä¹  (Curriculum Learning)

```python
class CurriculumLearningTrainer:
    """
    è¯¾ç¨‹å­¦ä¹ è®­ç»ƒå™¨
    Curriculum Learning Trainer
    
    ä»ç®€å•åˆ°å¤æ‚é€æ­¥è®­ç»ƒæ¨¡å‹
    Gradually train model from simple to complex
    """
    def __init__(self, model, optimizer, criterion, device):
        self.model = model
        self.optimizer = optimizer
        self.criterion = criterion
        self.device = device
    
    def sort_by_difficulty(self, dataset):
        """
        æŒ‰éš¾åº¦æ’åºæ•°æ®
        Sort data by difficulty
        """
        # æŒ‰å¥å­é•¿åº¦æ’åºï¼ˆç®€å•çš„éš¾åº¦åº¦é‡ï¼‰
        # Sort by sentence length (simple difficulty measure)
        dataset.sort(key=lambda x: len(x['source']) + len(x['target']))
        return dataset
    
    def create_curriculum_batches(self, dataset, batch_size, num_stages=5):
        """
        åˆ›å»ºè¯¾ç¨‹å­¦ä¹ æ‰¹æ¬¡
        Create curriculum learning batches
        """
        # å°†æ•°æ®åˆ†ä¸ºä¸åŒéš¾åº¦é˜¶æ®µ
        # Divide data into different difficulty stages
        data_per_stage = len(dataset) // num_stages
        stages = []
        
        for i in range(num_stages):
            start_idx = i * data_per_stage
            end_idx = (i + 1) * data_per_stage if i < num_stages - 1 else len(dataset)
            
            stage_data = dataset[start_idx:end_idx]
            stages.append(stage_data)
        
        return stages
    
    def train_with_curriculum(self, dataset, batch_size, epochs_per_stage=2):
        """
        ä½¿ç”¨è¯¾ç¨‹å­¦ä¹ è®­ç»ƒ
        Train with curriculum learning
        """
        # æŒ‰éš¾åº¦æ’åºå¹¶åˆ†é˜¶æ®µ
        # Sort by difficulty and divide into stages
        sorted_dataset = self.sort_by_difficulty(dataset)
        curriculum_stages = self.create_curriculum_batches(sorted_dataset, batch_size)
        
        for stage_idx, stage_data in enumerate(curriculum_stages):
            print(f"è®­ç»ƒé˜¶æ®µ {stage_idx + 1}/{len(curriculum_stages)}")
            print(f"Training stage {stage_idx + 1}/{len(curriculum_stages)}")
            
            # åœ¨å½“å‰é˜¶æ®µè®­ç»ƒ
            # Train on current stage
            for epoch in range(epochs_per_stage):
                self.train_epoch(stage_data, batch_size)
                
            # è¯„ä¼°å½“å‰é˜¶æ®µ
            # Evaluate current stage
            self.evaluate_stage(stage_data, batch_size)
    
    def train_epoch(self, data, batch_size):
        """
        è®­ç»ƒä¸€ä¸ªepoch
        Train one epoch
        """
        self.model.train()
        total_loss = 0
        
        # åˆ›å»ºæ‰¹æ¬¡
        # Create batches
        for i in range(0, len(data), batch_size):
            batch = data[i:i + batch_size]
            
            # å‡†å¤‡æ‰¹æ¬¡æ•°æ®
            # Prepare batch data
            sources, targets, src_lengths = self.prepare_batch(batch)
            
            # å‰å‘ä¼ æ’­
            # Forward pass
            self.optimizer.zero_grad()
            outputs = self.model(sources, targets, src_lengths)
            
            # è®¡ç®—æŸå¤±
            # Compute loss
            loss = self.compute_loss(outputs, targets)
            
            # åå‘ä¼ æ’­
            # Backward pass
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            self.optimizer.step()
            
            total_loss += loss.item()
        
        avg_loss = total_loss / (len(data) // batch_size)
        print(f"å¹³å‡æŸå¤± | Average loss: {avg_loss:.4f}")
    
    def prepare_batch(self, batch):
        """
        å‡†å¤‡æ‰¹æ¬¡æ•°æ®
        Prepare batch data
        """
        # å®ç°æ•°æ®é¢„å¤„ç†é€»è¾‘
        # Implement data preprocessing logic
        sources = []
        targets = []
        src_lengths = []
        
        for item in batch:
            sources.append(item['source'])
            targets.append(item['target'])
            src_lengths.append(len(item['source']))
        
        # è½¬æ¢ä¸ºtensorå¹¶padding
        # Convert to tensors and pad
        sources = self.pad_sequences(sources)
        targets = self.pad_sequences(targets)
        
        return sources.to(self.device), targets.to(self.device), src_lengths
    
    def pad_sequences(self, sequences):
        """
        åºåˆ—padding
        Sequence padding
        """
        max_len = max(len(seq) for seq in sequences)
        padded = torch.zeros(len(sequences), max_len, dtype=torch.long)
        
        for i, seq in enumerate(sequences):
            length = len(seq)
            padded[i, :length] = torch.tensor(seq, dtype=torch.long)
        
        return padded
    
    def compute_loss(self, outputs, targets):
        """
        è®¡ç®—æŸå¤±
        Compute loss
        """
        # é‡å¡‘è¾“å‡ºå’Œç›®æ ‡ä»¥è®¡ç®—æŸå¤±
        # Reshape outputs and targets to compute loss
        outputs = outputs[:, 1:].contiguous().view(-1, outputs.size(-1))
        targets = targets[:, 1:].contiguous().view(-1)
        
        return self.criterion(outputs, targets)
```

## ğŸ“Š è¯„ä¼°æŒ‡æ ‡ä¸è´¨é‡åˆ†æ | Evaluation Metrics and Quality Analysis

### BLEUåˆ†æ•°è®¡ç®— | BLEU Score Calculation

```python
from collections import Counter
import math

class BLEUEvaluator:
    """
    BLEUè¯„ä¼°å™¨
    BLEU Evaluator
    
    è®¡ç®—æœºå™¨ç¿»è¯‘çš„BLEUåˆ†æ•°
    Calculate BLEU scores for machine translation
    """
    
    def __init__(self, max_order=4):
        self.max_order = max_order
    
    def compute_bleu(self, reference_corpus, translation_corpus):
        """
        è®¡ç®—è¯­æ–™åº“çº§åˆ«çš„BLEUåˆ†æ•°
        Compute corpus-level BLEU score
        """
        matches_by_order = [0] * self.max_order
        possible_matches_by_order = [0] * self.max_order
        reference_length = 0
        translation_length = 0
        
        for references, translation in zip(reference_corpus, translation_corpus):
            reference_length += min(len(r) for r in references)
            translation_length += len(translation)
            
            # è®¡ç®—å„é˜¶n-gramåŒ¹é…
            # Compute n-gram matches for each order
            for order in range(1, self.max_order + 1):
                matches, possible_matches = self._compute_ngram_matches(
                    references, translation, order
                )
                matches_by_order[order - 1] += matches
                possible_matches_by_order[order - 1] += possible_matches
        
        # è®¡ç®—ç²¾ç¡®ç‡
        # Compute precisions
        precisions = []
        for i in range(self.max_order):
            if possible_matches_by_order[i] > 0:
                precisions.append(matches_by_order[i] / possible_matches_by_order[i])
            else:
                precisions.append(0.0)
        
        # è®¡ç®—ç®€çŸ­æƒ©ç½š
        # Compute brevity penalty
        if translation_length > reference_length:
            bp = 1.0
        else:
            bp = math.exp(1 - reference_length / translation_length)
        
        # è®¡ç®—BLEUåˆ†æ•°
        # Compute BLEU score
        if min(precisions) > 0:
            p_log_sum = sum(math.log(p) for p in precisions) / self.max_order
            geo_mean = math.exp(p_log_sum)
            bleu = bp * geo_mean
        else:
            bleu = 0.0
        
        return {
            'bleu': bleu,
            'precisions': precisions,
            'brevity_penalty': bp,
            'length_ratio': translation_length / reference_length,
            'translation_length': translation_length,
            'reference_length': reference_length
        }
    
    def _compute_ngram_matches(self, references, translation, order):
        """
        è®¡ç®—n-gramåŒ¹é…æ•°
        Compute n-gram matches
        """
        translation_ngrams = self._get_ngrams(translation, order)
        
        # åˆå¹¶æ‰€æœ‰å‚è€ƒç¿»è¯‘çš„n-gram
        # Merge n-grams from all reference translations
        reference_ngrams = Counter()
        for reference in references:
            ref_ngrams = self._get_ngrams(reference, order)
            for ngram in ref_ngrams:
                reference_ngrams[ngram] = max(reference_ngrams[ngram], ref_ngrams[ngram])
        
        # è®¡ç®—åŒ¹é…æ•°
        # Compute matches
        matches = 0
        for ngram in translation_ngrams:
            matches += min(translation_ngrams[ngram], reference_ngrams[ngram])
        
        possible_matches = sum(translation_ngrams.values())
        
        return matches, possible_matches
    
    def _get_ngrams(self, segment, order):
        """
        æå–n-gram
        Extract n-grams
        """
        ngram_counts = Counter()
        for i in range(0, len(segment) - order + 1):
            ngram = tuple(segment[i:i + order])
            ngram_counts[ngram] += 1
        return ngram_counts


def calculate_meteor_score(reference, hypothesis):
    """
    è®¡ç®—METEORåˆ†æ•°ï¼ˆç®€åŒ–ç‰ˆï¼‰
    Calculate METEOR score (simplified version)
    """
    # å®é™…å®ç°éœ€è¦ä½¿ç”¨METEORåº“æˆ–æ›´å¤æ‚çš„ç®—æ³•
    # Actual implementation requires METEOR library or more complex algorithms
    
    ref_words = set(reference.split())
    hyp_words = set(hypothesis.split())
    
    # è®¡ç®—ç²¾ç¡®ç‡å’Œå¬å›ç‡
    # Compute precision and recall
    if len(hyp_words) == 0:
        precision = 0
    else:
        precision = len(ref_words & hyp_words) / len(hyp_words)
    
    if len(ref_words) == 0:
        recall = 0
    else:
        recall = len(ref_words & hyp_words) / len(ref_words)
    
    # è®¡ç®—F1åˆ†æ•°
    # Compute F1 score
    if precision + recall == 0:
        f1 = 0
    else:
        f1 = 2 * precision * recall / (precision + recall)
    
    return f1
```

## ğŸ“ å®è·µé¡¹ç›®æŒ‡å¯¼ | Practical Project Guidance

### å®Œæ•´è®­ç»ƒæµç¨‹ | Complete Training Pipeline

```python
def main_training_pipeline():
    """
    å®Œæ•´çš„è®­ç»ƒæµç¨‹ç¤ºä¾‹
    Complete training pipeline example
    """
    # 1. æ•°æ®å‡†å¤‡ | Data preparation
    print("=== æ•°æ®å‡†å¤‡é˜¶æ®µ | Data Preparation ===")
    
    # åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
    # Load and preprocess data
    train_data = load_translation_data('train.txt')
    val_data = load_translation_data('val.txt')
    
    # æ„å»ºè¯æ±‡è¡¨
    # Build vocabulary
    src_vocab = build_vocabulary([item['source'] for item in train_data])
    tgt_vocab = build_vocabulary([item['target'] for item in train_data])
    
    print(f"æºè¯­è¨€è¯æ±‡è¡¨å¤§å°: {len(src_vocab)}")
    print(f"ç›®æ ‡è¯­è¨€è¯æ±‡è¡¨å¤§å°: {len(tgt_vocab)}")
    
    # 2. æ¨¡å‹æ„å»º | Model construction
    print("\n=== æ¨¡å‹æ„å»ºé˜¶æ®µ | Model Construction ===")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # åˆ›å»ºç¼–ç å™¨å’Œè§£ç å™¨
    # Create encoder and decoder
    encoder = Encoder(
        input_size=len(src_vocab),
        hidden_size=512,
        num_layers=2,
        dropout=0.1
    )
    
    decoder = AttentionDecoder(
        output_size=len(tgt_vocab),
        hidden_size=512,
        num_layers=2,
        dropout=0.1,
        attention_method='general'
    )
    
    # åˆ›å»ºå®Œæ•´æ¨¡å‹
    # Create complete model
    model = AttentionSeq2Seq(encoder, decoder, device).to(device)
    
    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters())}")
    
    # 3. è®­ç»ƒè®¾ç½® | Training setup
    print("\n=== è®­ç»ƒè®¾ç½®é˜¶æ®µ | Training Setup ===")
    
    criterion = nn.CrossEntropyLoss(ignore_index=0)  # å¿½ç•¥padding
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)
    
    # 4. è¯¾ç¨‹å­¦ä¹ è®­ç»ƒ | Curriculum learning training
    print("\n=== è®­ç»ƒé˜¶æ®µ | Training Phase ===")
    
    trainer = CurriculumLearningTrainer(model, optimizer, criterion, device)
    trainer.train_with_curriculum(train_data, batch_size=32, epochs_per_stage=3)
    
    # 5. æ¨¡å‹è¯„ä¼° | Model evaluation
    print("\n=== è¯„ä¼°é˜¶æ®µ | Evaluation Phase ===")
    
    # åˆ›å»ºæŸæœç´¢è§£ç å™¨
    # Create beam search decoder
    beam_decoder = BeamSearchDecoder(model, beam_size=5)
    
    # è¯„ä¼°BLEUåˆ†æ•°
    # Evaluate BLEU score
    evaluator = BLEUEvaluator()
    
    references = []
    translations = []
    
    for item in val_data[:100]:  # è¯„ä¼°å‰100ä¸ªæ ·æœ¬
        source = torch.tensor([item['source']]).to(device)
        source_length = torch.tensor([len(item['source'])])
        
        translation = beam_decoder.search(source, source_length)
        
        references.append([item['target']])
        translations.append(translation)
    
    bleu_result = evaluator.compute_bleu(references, translations)
    print(f"BLEUåˆ†æ•°: {bleu_result['bleu']:.4f}")
    
    # 6. æ¨¡å‹ä¿å­˜ | Model saving
    print("\n=== æ¨¡å‹ä¿å­˜é˜¶æ®µ | Model Saving ===")
    
    torch.save({
        'model_state_dict': model.state_dict(),
        'src_vocab': src_vocab,
        'tgt_vocab': tgt_vocab,
        'model_config': {
            'hidden_size': 512,
            'num_layers': 2,
            'dropout': 0.1
        }
    }, 'seq2seq_translation_model.pt')
    
    print("è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜ã€‚")
    print("Training completed! Model saved.")


def load_translation_data(file_path):
    """
    åŠ è½½ç¿»è¯‘æ•°æ®
    Load translation data
    """
    # ç®€åŒ–çš„æ•°æ®åŠ è½½ç¤ºä¾‹
    # Simplified data loading example
    data = []
    
    # ç¤ºä¾‹æ•°æ®æ ¼å¼ï¼šæºè¯­è¨€\tç›®æ ‡è¯­è¨€
    # Example data format: source_language\ttarget_language
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            parts = line.strip().split('\t')
            if len(parts) == 2:
                source = parts[0].split()
                target = parts[1].split()
                data.append({
                    'source': source,
                    'target': ['<sos>'] + target + ['<eos>']
                })
    
    return data


def build_vocabulary(sentences):
    """
    æ„å»ºè¯æ±‡è¡¨
    Build vocabulary
    """
    vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}
    
    for sentence in sentences:
        for word in sentence:
            if word not in vocab:
                vocab[word] = len(vocab)
    
    return vocab


if __name__ == "__main__":
    main_training_pipeline()
```

é€šè¿‡è¿™ä¸ªåºåˆ—åˆ°åºåˆ—ç¿»è¯‘é¡¹ç›®ï¼Œä½ å°†æŒæ¡ç¥ç»æœºå™¨ç¿»è¯‘çš„æ ¸å¿ƒæŠ€æœ¯ï¼Œå­¦ä¼šæ„å»ºä»åŸºç¡€åˆ°é«˜çº§çš„ç¿»è¯‘æ¨¡å‹ï¼Œä¸ºå¼€å‘å®ç”¨çš„ç¿»è¯‘ç³»ç»Ÿå¥ å®šåŸºç¡€ï¼

Through this sequence-to-sequence translation project, you will master the core technologies of neural machine translation, learn to build translation models from basic to advanced, and lay the foundation for developing practical translation systems! 