# çŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿ
# Knowledge Base Question Answering System

**è®©æœºå™¨è®¿é—®ç»“æ„åŒ–çŸ¥è¯†å›ç­”é—®é¢˜ - æ„å»ºæ™ºèƒ½çŸ¥è¯†é—®ç­”ç³»ç»Ÿ**
**Making machines access structured knowledge to answer questions - Building intelligent knowledge QA systems**

---

## ğŸ§  ä»€ä¹ˆæ˜¯çŸ¥è¯†åº“é—®ç­”ï¼Ÿ| What is Knowledge Base Question Answering?

çŸ¥è¯†åº“é—®ç­”(KBQA)å°±åƒç»™æœºå™¨é…å¤‡äº†ä¸€ä¸ª"ç™¾ç§‘å…¨ä¹¦å¤§è„‘"ã€‚ä¸ä¼ ç»Ÿçš„é˜…è¯»ç†è§£ä¸åŒï¼ŒKBQAéœ€è¦æœºå™¨ç†è§£è‡ªç„¶è¯­è¨€é—®é¢˜ï¼Œç„¶ååœ¨ç»“æ„åŒ–çš„çŸ¥è¯†åº“ä¸­æŸ¥æ‰¾å’Œæ¨ç†ï¼Œæœ€ç»ˆç»™å‡ºå‡†ç¡®ç­”æ¡ˆã€‚

Knowledge Base Question Answering (KBQA) is like equipping machines with an "encyclopedia brain". Unlike traditional reading comprehension, KBQA requires machines to understand natural language questions, then search and reason in structured knowledge bases to provide accurate answers.

### KBQA vs ä¼ ç»ŸQAçš„åŒºåˆ« | Differences between KBQA and Traditional QA

| ç‰¹æ€§ | ä¼ ç»Ÿé˜…è¯»ç†è§£QA | çŸ¥è¯†åº“QA |
|------|----------------|----------|
| **æ•°æ®æº** | éç»“æ„åŒ–æ–‡æœ¬ | ç»“æ„åŒ–çŸ¥è¯†å›¾è°± |
| **Data Source** | Unstructured text | Structured knowledge graph |
| **çŸ¥è¯†èŒƒå›´** | å•ç¯‡æ–‡æ¡£ | æ•´ä¸ªçŸ¥è¯†åº“ |
| **Knowledge Scope** | Single document | Entire knowledge base |
| **æ¨ç†èƒ½åŠ›** | å±€éƒ¨æ¨ç† | å¤šè·³å…¨å±€æ¨ç† |
| **Reasoning** | Local reasoning | Multi-hop global reasoning |
| **ç­”æ¡ˆç±»å‹** | æ–‡æœ¬ç‰‡æ®µ | å®ä½“ã€æ•°å€¼ã€å¸ƒå°”å€¼ |
| **Answer Type** | Text spans | Entities, numbers, boolean |

### çŸ¥è¯†åº“çš„ç»“æ„ | Structure of Knowledge Base

**çŸ¥è¯†å›¾è°±çš„åŸºæœ¬ç»„æˆ:**
**Basic Components of Knowledge Graph:**

```
å®ä½“ (Entity): ç°å®ä¸–ç•Œçš„å¯¹è±¡ï¼Œå¦‚"åŒ—äº¬"ã€"è‹¹æœå…¬å¸"
Entity: Real-world objects like "Beijing", "Apple Inc."

å…³ç³» (Relation): å®ä½“ä¹‹é—´çš„è”ç³»ï¼Œå¦‚"ä½äº"ã€"åˆ›å§‹äºº"
Relation: Connections between entities like "located_in", "founder"

å±æ€§ (Attribute): å®ä½“çš„ç‰¹å¾ï¼Œå¦‚"äººå£"ã€"æˆç«‹æ—¶é—´"
Attribute: Characteristics of entities like "population", "founded_date"

ä¸‰å…ƒç»„ (Triple): (å¤´å®ä½“, å…³ç³», å°¾å®ä½“)
Triple: (head_entity, relation, tail_entity)
```

**ç¤ºä¾‹çŸ¥è¯†å›¾è°±:**
**Example Knowledge Graph:**
```
(è‹¹æœå…¬å¸, åˆ›å§‹äºº, å²è’‚å¤«Â·ä¹”å¸ƒæ–¯)
(Apple Inc., founder, Steve Jobs)

(è‹¹æœå…¬å¸, æ€»éƒ¨ä½äº, åº“æ¯”è’‚è¯º)
(Apple Inc., headquarters, Cupertino)

(å²è’‚å¤«Â·ä¹”å¸ƒæ–¯, å‡ºç”Ÿæ—¥æœŸ, 1955-02-24)
(Steve Jobs, birth_date, 1955-02-24)

(åº“æ¯”è’‚è¯º, ä½äº, åŠ åˆ©ç¦å°¼äºšå·)
(Cupertino, located_in, California)
```

## ğŸ”¬ KBQAç³»ç»Ÿæ¶æ„ | KBQA System Architecture

### æ ¸å¿ƒç»„ä»¶ | Core Components

```python
class KBQASystem:
    """
    çŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿæ¶æ„
    Knowledge Base QA System Architecture
    """
    def __init__(self):
        # 1. é—®é¢˜ç†è§£æ¨¡å— | Question Understanding Module
        self.question_parser = QuestionParser()
        
        # 2. å®ä½“é“¾æ¥æ¨¡å— | Entity Linking Module  
        self.entity_linker = EntityLinker()
        
        # 3. å…³ç³»è¯†åˆ«æ¨¡å— | Relation Recognition Module
        self.relation_classifier = RelationClassifier()
        
        # 4. æŸ¥è¯¢ç”Ÿæˆæ¨¡å— | Query Generation Module
        self.query_generator = QueryGenerator()
        
        # 5. çŸ¥è¯†åº“æ£€ç´¢æ¨¡å— | Knowledge Base Retrieval Module
        self.kb_retriever = KnowledgeBaseRetriever()
        
        # 6. ç­”æ¡ˆç”Ÿæˆæ¨¡å— | Answer Generation Module
        self.answer_generator = AnswerGenerator()
    
    def answer_question(self, question):
        """
        å›ç­”é—®é¢˜çš„å®Œæ•´æµç¨‹
        Complete pipeline for answering questions
        """
        # æ­¥éª¤1: é—®é¢˜è§£æ
        # Step 1: Question parsing
        parsed_question = self.question_parser.parse(question)
        
        # æ­¥éª¤2: å®ä½“è¯†åˆ«å’Œé“¾æ¥
        # Step 2: Entity recognition and linking
        entities = self.entity_linker.link(parsed_question)
        
        # æ­¥éª¤3: å…³ç³»è¯†åˆ«
        # Step 3: Relation recognition
        relations = self.relation_classifier.classify(parsed_question, entities)
        
        # æ­¥éª¤4: æŸ¥è¯¢ç”Ÿæˆ
        # Step 4: Query generation
        queries = self.query_generator.generate(entities, relations, parsed_question)
        
        # æ­¥éª¤5: çŸ¥è¯†åº“æ£€ç´¢
        # Step 5: Knowledge base retrieval
        results = self.kb_retriever.retrieve(queries)
        
        # æ­¥éª¤6: ç­”æ¡ˆç”Ÿæˆ
        # Step 6: Answer generation
        answer = self.answer_generator.generate(results, parsed_question)
        
        return answer
```

### æŠ€æœ¯æŒ‘æˆ˜ä¸è§£å†³æ–¹æ¡ˆ | Technical Challenges and Solutions

#### 1. å®ä½“é“¾æ¥ (Entity Linking)

**æŒ‘æˆ˜**: è‡ªç„¶è¯­è¨€ä¸­çš„å®ä½“æåŠå¯èƒ½æœ‰æ­§ä¹‰æ€§
**Challenge**: Entity mentions in natural language can be ambiguous

```python
import torch
import torch.nn as nn
from transformers import BertModel, BertTokenizer

class EntityLinker:
    """
    å®ä½“é“¾æ¥å™¨
    Entity Linker
    
    å°†è‡ªç„¶è¯­è¨€ä¸­çš„å®ä½“æåŠé“¾æ¥åˆ°çŸ¥è¯†åº“ä¸­çš„æ ‡å‡†å®ä½“
    Link entity mentions in natural language to canonical entities in KB
    """
    
    def __init__(self, kb_entities, model_name='bert-base-uncased'):
        self.kb_entities = kb_entities  # çŸ¥è¯†åº“ä¸­çš„æ‰€æœ‰å®ä½“
        self.tokenizer = BertTokenizer.from_pretrained(model_name)
        self.bert = BertModel.from_pretrained(model_name)
        
        # é¢„è®¡ç®—æ‰€æœ‰å®ä½“çš„å‘é‡è¡¨ç¤º
        # Pre-compute vector representations for all entities
        self.entity_embeddings = self.compute_entity_embeddings()
        
        # ç›¸ä¼¼åº¦é˜ˆå€¼
        # Similarity threshold
        self.similarity_threshold = 0.7
    
    def compute_entity_embeddings(self):
        """
        é¢„è®¡ç®—å®ä½“åµŒå…¥
        Pre-compute entity embeddings
        """
        embeddings = {}
        
        for entity_id, entity_info in self.kb_entities.items():
            # ä½¿ç”¨å®ä½“åç§°å’Œæè¿°è®¡ç®—åµŒå…¥
            # Compute embedding using entity name and description
            text = f"{entity_info['name']} {entity_info.get('description', '')}"
            
            inputs = self.tokenizer(text, return_tensors='pt', max_length=128, truncation=True)
            with torch.no_grad():
                outputs = self.bert(**inputs)
                embedding = outputs.pooler_output.squeeze().numpy()
            
            embeddings[entity_id] = embedding
        
        return embeddings
    
    def link_entities(self, question):
        """
        åœ¨é—®é¢˜ä¸­é“¾æ¥å®ä½“
        Link entities in question
        """
        # é¦–å…ˆè¯†åˆ«å€™é€‰å®ä½“æåŠ
        # First identify candidate entity mentions
        mentions = self.extract_entity_mentions(question)
        
        linked_entities = []
        
        for mention in mentions:
            # è®¡ç®—æåŠçš„å‘é‡è¡¨ç¤º
            # Compute vector representation of mention
            mention_embedding = self.compute_mention_embedding(mention)
            
            # æ‰¾åˆ°æœ€ç›¸ä¼¼çš„å®ä½“
            # Find most similar entity
            best_entity = self.find_best_entity(mention_embedding)
            
            if best_entity:
                linked_entities.append({
                    'mention': mention['text'],
                    'entity_id': best_entity['id'],
                    'entity_name': best_entity['name'],
                    'confidence': best_entity['similarity'],
                    'start': mention['start'],
                    'end': mention['end']
                })
        
        return linked_entities
    
    def extract_entity_mentions(self, question):
        """
        æå–å®ä½“æåŠ
        Extract entity mentions
        """
        # ä½¿ç”¨NERæ¨¡å‹æå–å®ä½“
        # Use NER model to extract entities
        from transformers import pipeline
        
        ner = pipeline("ner", model="dbmdz/bert-large-cased-finetuned-conll03-english")
        entities = ner(question)
        
        mentions = []
        for entity in entities:
            mentions.append({
                'text': entity['word'],
                'start': entity['start'],
                'end': entity['end'],
                'type': entity['entity']
            })
        
        return mentions
    
    def compute_mention_embedding(self, mention):
        """
        è®¡ç®—æåŠçš„åµŒå…¥è¡¨ç¤º
        Compute embedding representation of mention
        """
        inputs = self.tokenizer(mention['text'], return_tensors='pt')
        with torch.no_grad():
            outputs = self.bert(**inputs)
            embedding = outputs.pooler_output.squeeze().numpy()
        
        return embedding
    
    def find_best_entity(self, mention_embedding):
        """
        æ‰¾åˆ°æœ€åŒ¹é…çš„å®ä½“
        Find best matching entity
        """
        best_similarity = 0
        best_entity = None
        
        for entity_id, entity_embedding in self.entity_embeddings.items():
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            # Compute cosine similarity
            similarity = self.cosine_similarity(mention_embedding, entity_embedding)
            
            if similarity > best_similarity and similarity > self.similarity_threshold:
                best_similarity = similarity
                best_entity = {
                    'id': entity_id,
                    'name': self.kb_entities[entity_id]['name'],
                    'similarity': similarity
                }
        
        return best_entity
    
    def cosine_similarity(self, vec1, vec2):
        """
        è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        Compute cosine similarity
        """
        import numpy as np
        
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        
        if norm1 == 0 or norm2 == 0:
            return 0
        
        return dot_product / (norm1 * norm2)
```

#### 2. å…³ç³»åˆ†ç±» (Relation Classification)

**æŒ‘æˆ˜**: ç†è§£é—®é¢˜ä¸­éšå«çš„å…³ç³»ç±»å‹
**Challenge**: Understanding implicit relation types in questions

```python
class RelationClassifier:
    """
    å…³ç³»åˆ†ç±»å™¨
    Relation Classifier
    
    è¯†åˆ«é—®é¢˜ä¸­æ¶‰åŠçš„å…³ç³»ç±»å‹
    Identify relation types involved in questions
    """
    
    def __init__(self, relation_vocab, model_name='bert-base-uncased'):
        self.relation_vocab = relation_vocab
        self.tokenizer = BertTokenizer.from_pretrained(model_name)
        self.model = self.build_relation_classifier(model_name)
        
        # å…³ç³»æ¨¡æ¿ | Relation templates
        self.relation_templates = {
            'birth_date': ['when was', 'birth', 'born'],
            'death_date': ['when did', 'death', 'died'],
            'location': ['where', 'located', 'place'],
            'founder': ['who founded', 'founder', 'established by'],
            'capital': ['capital of', 'capital city'],
            'population': ['population', 'how many people']
        }
    
    def build_relation_classifier(self, model_name):
        """
        æ„å»ºå…³ç³»åˆ†ç±»æ¨¡å‹
        Build relation classification model
        """
        class RelationBERT(nn.Module):
            def __init__(self, model_name, num_relations):
                super().__init__()
                self.bert = BertModel.from_pretrained(model_name)
                self.dropout = nn.Dropout(0.1)
                self.classifier = nn.Linear(self.bert.config.hidden_size, num_relations)
            
            def forward(self, input_ids, attention_mask):
                outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
                pooled_output = self.dropout(outputs.pooler_output)
                logits = self.classifier(pooled_output)
                return logits
        
        model = RelationBERT(model_name, len(self.relation_vocab))
        return model
    
    def classify_relations(self, question, entities):
        """
        åˆ†ç±»é—®é¢˜ä¸­çš„å…³ç³»
        Classify relations in question
        """
        # æ–¹æ³•1: åŸºäºæ¨¡æ¿çš„å…³ç³»è¯†åˆ«
        # Method 1: Template-based relation recognition
        template_relations = self.template_based_classification(question)
        
        # æ–¹æ³•2: åŸºäºæ·±åº¦å­¦ä¹ çš„å…³ç³»åˆ†ç±»
        # Method 2: Deep learning-based relation classification
        ml_relations = self.ml_based_classification(question, entities)
        
        # èåˆä¸¤ç§æ–¹æ³•çš„ç»“æœ
        # Combine results from both methods
        relations = self.combine_relation_predictions(template_relations, ml_relations)
        
        return relations
    
    def template_based_classification(self, question):
        """
        åŸºäºæ¨¡æ¿çš„å…³ç³»åˆ†ç±»
        Template-based relation classification
        """
        question_lower = question.lower()
        detected_relations = []
        
        for relation, templates in self.relation_templates.items():
            for template in templates:
                if template in question_lower:
                    detected_relations.append({
                        'relation': relation,
                        'confidence': 0.9,  # æ¨¡æ¿åŒ¹é…çš„é«˜ç½®ä¿¡åº¦
                        'method': 'template'
                    })
                    break
        
        return detected_relations
    
    def ml_based_classification(self, question, entities):
        """
        åŸºäºæœºå™¨å­¦ä¹ çš„å…³ç³»åˆ†ç±»
        Machine learning-based relation classification
        """
        # æ„é€ è¾“å…¥ï¼šé—®é¢˜ + å®ä½“
        # Construct input: question + entities
        entity_text = " ".join([entity['entity_name'] for entity in entities])
        input_text = f"{question} [SEP] {entity_text}"
        
        # ç¼–ç 
        # Encode
        inputs = self.tokenizer(
            input_text,
            return_tensors='pt',
            max_length=128,
            truncation=True,
            padding=True
        )
        
        # é¢„æµ‹
        # Predict
        with torch.no_grad():
            logits = self.model(**inputs)
            probabilities = torch.softmax(logits, dim=-1)
            
            # è·å–top-kå…³ç³»
            # Get top-k relations
            top_k = 3
            top_probs, top_indices = torch.topk(probabilities, k=top_k, dim=-1)
            
            relations = []
            for i in range(top_k):
                relation_id = top_indices[0][i].item()
                confidence = top_probs[0][i].item()
                
                if confidence > 0.3:  # ç½®ä¿¡åº¦é˜ˆå€¼
                    relation_name = list(self.relation_vocab.keys())[relation_id]
                    relations.append({
                        'relation': relation_name,
                        'confidence': confidence,
                        'method': 'ml'
                    })
        
        return relations
    
    def combine_relation_predictions(self, template_relations, ml_relations):
        """
        èåˆå…³ç³»é¢„æµ‹ç»“æœ
        Combine relation prediction results
        """
        # åˆ›å»ºå…³ç³»ç½®ä¿¡åº¦å­—å…¸
        # Create relation confidence dictionary
        relation_scores = defaultdict(float)
        
        # åŠ æƒèåˆ
        # Weighted fusion
        for rel in template_relations:
            relation_scores[rel['relation']] += rel['confidence'] * 0.7
        
        for rel in ml_relations:
            relation_scores[rel['relation']] += rel['confidence'] * 0.3
        
        # æ’åºå¹¶è¿”å›
        # Sort and return
        final_relations = []
        for relation, score in sorted(relation_scores.items(), key=lambda x: x[1], reverse=True):
            if score > 0.4:
                final_relations.append({
                    'relation': relation,
                    'confidence': score
                })
        
        return final_relations
```

#### 3. æŸ¥è¯¢ç”Ÿæˆ (Query Generation)

**æŒ‘æˆ˜**: å°†è‡ªç„¶è¯­è¨€é—®é¢˜è½¬æ¢ä¸ºç»“æ„åŒ–æŸ¥è¯¢
**Challenge**: Convert natural language questions to structured queries

```python
class SPARQLQueryGenerator:
    """
    SPARQLæŸ¥è¯¢ç”Ÿæˆå™¨
    SPARQL Query Generator
    
    å°†è‡ªç„¶è¯­è¨€é—®é¢˜è½¬æ¢ä¸ºSPARQLæŸ¥è¯¢
    Convert natural language questions to SPARQL queries
    """
    
    def __init__(self, knowledge_base):
        self.kb = knowledge_base
        self.query_templates = self.load_query_templates()
    
    def load_query_templates(self):
        """
        åŠ è½½æŸ¥è¯¢æ¨¡æ¿
        Load query templates
        """
        templates = {
            # å•è·³æŸ¥è¯¢æ¨¡æ¿ | Single-hop query templates
            'single_hop': {
                'attribute_query': """
                    SELECT ?answer WHERE {
                        <{entity}> <{relation}> ?answer .
                    }
                """,
                'reverse_query': """
                    SELECT ?answer WHERE {
                        ?answer <{relation}> <{entity}> .
                    }
                """
            },
            
            # å¤šè·³æŸ¥è¯¢æ¨¡æ¿ | Multi-hop query templates
            'multi_hop': {
                'two_hop': """
                    SELECT ?answer WHERE {
                        <{entity1}> <{relation1}> ?intermediate .
                        ?intermediate <{relation2}> ?answer .
                    }
                """,
                'three_hop': """
                    SELECT ?answer WHERE {
                        <{entity1}> <{relation1}> ?intermediate1 .
                        ?intermediate1 <{relation2}> ?intermediate2 .
                        ?intermediate2 <{relation3}> ?answer .
                    }
                """
            },
            
            # èšåˆæŸ¥è¯¢æ¨¡æ¿ | Aggregation query templates
            'aggregation': {
                'count': """
                    SELECT (COUNT(?answer) AS ?count) WHERE {
                        <{entity}> <{relation}> ?answer .
                    }
                """,
                'max': """
                    SELECT (MAX(?answer) AS ?max) WHERE {
                        <{entity}> <{relation}> ?answer .
                    }
                """
            }
        }
        
        return templates
    
    def generate_queries(self, entities, relations, question_type):
        """
        ç”ŸæˆSPARQLæŸ¥è¯¢
        Generate SPARQL queries
        """
        queries = []
        
        # æ ¹æ®é—®é¢˜ç±»å‹ç”Ÿæˆä¸åŒçš„æŸ¥è¯¢
        # Generate different queries based on question type
        if question_type == 'factual':
            queries.extend(self.generate_factual_queries(entities, relations))
        elif question_type == 'count':
            queries.extend(self.generate_count_queries(entities, relations))
        elif question_type == 'comparison':
            queries.extend(self.generate_comparison_queries(entities, relations))
        
        return queries
    
    def generate_factual_queries(self, entities, relations):
        """
        ç”Ÿæˆäº‹å®æ€§æŸ¥è¯¢
        Generate factual queries
        """
        queries = []
        
        for entity in entities:
            for relation in relations:
                # å•è·³æŸ¥è¯¢
                # Single-hop query
                query = self.query_templates['single_hop']['attribute_query'].format(
                    entity=entity['entity_id'],
                    relation=relation['relation']
                )
                queries.append({
                    'sparql': query,
                    'type': 'single_hop',
                    'entities': [entity],
                    'relations': [relation]
                })
                
                # åå‘æŸ¥è¯¢
                # Reverse query
                reverse_query = self.query_templates['single_hop']['reverse_query'].format(
                    entity=entity['entity_id'],
                    relation=relation['relation']
                )
                queries.append({
                    'sparql': reverse_query,
                    'type': 'reverse',
                    'entities': [entity],
                    'relations': [relation]
                })
        
        # å¤šè·³æŸ¥è¯¢
        # Multi-hop queries
        if len(entities) > 1 and len(relations) > 1:
            multi_hop_query = self.query_templates['multi_hop']['two_hop'].format(
                entity1=entities[0]['entity_id'],
                relation1=relations[0]['relation'],
                relation2=relations[1]['relation']
            )
            queries.append({
                'sparql': multi_hop_query,
                'type': 'multi_hop',
                'entities': entities[:1],
                'relations': relations[:2]
            })
        
        return queries
    
    def generate_count_queries(self, entities, relations):
        """
        ç”Ÿæˆè®¡æ•°æŸ¥è¯¢
        Generate count queries
        """
        queries = []
        
        for entity in entities:
            for relation in relations:
                query = self.query_templates['aggregation']['count'].format(
                    entity=entity['entity_id'],
                    relation=relation['relation']
                )
                queries.append({
                    'sparql': query,
                    'type': 'count',
                    'entities': [entity],
                    'relations': [relation]
                })
        
        return queries
    
    def execute_query(self, sparql_query):
        """
        æ‰§è¡ŒSPARQLæŸ¥è¯¢
        Execute SPARQL query
        """
        try:
            # åœ¨å®é™…ç³»ç»Ÿä¸­ï¼Œè¿™é‡Œä¼šè¿æ¥åˆ°çœŸå®çš„çŸ¥è¯†åº“
            # In actual systems, this would connect to a real knowledge base
            results = self.kb.query(sparql_query)
            return {
                'success': True,
                'results': results,
                'query': sparql_query
            }
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'query': sparql_query
            }
```

#### 4. å¤šè·³æ¨ç† (Multi-hop Reasoning)

**æŒ‘æˆ˜**: å¤„ç†éœ€è¦å¤šæ­¥æ¨ç†çš„å¤æ‚é—®é¢˜
**Challenge**: Handle complex questions requiring multi-step reasoning

```python
class MultiHopReasoner:
    """
    å¤šè·³æ¨ç†å™¨
    Multi-hop Reasoner
    
    å¤„ç†éœ€è¦å¤šæ­¥æ¨ç†çš„å¤æ‚é—®ç­”
    Handle complex QA requiring multi-step reasoning
    """
    
    def __init__(self, knowledge_base, max_hops=3):
        self.kb = knowledge_base
        self.max_hops = max_hops
        self.reasoning_cache = {}  # æ¨ç†ç¼“å­˜
    
    def reason(self, question, entities, target_type=None):
        """
        æ‰§è¡Œå¤šè·³æ¨ç†
        Execute multi-hop reasoning
        """
        # åˆå§‹åŒ–æ¨ç†çŠ¶æ€
        # Initialize reasoning state
        reasoning_paths = []
        
        for entity in entities:
            # ä»æ¯ä¸ªå®ä½“å¼€å§‹è¿›è¡Œå¹¿åº¦ä¼˜å…ˆæœç´¢
            # Start breadth-first search from each entity
            paths = self.bfs_reasoning(entity, target_type)
            reasoning_paths.extend(paths)
        
        # è¯„ä¼°å’Œæ’åºæ¨ç†è·¯å¾„
        # Evaluate and rank reasoning paths
        ranked_paths = self.rank_reasoning_paths(reasoning_paths, question)
        
        return ranked_paths
    
    def bfs_reasoning(self, start_entity, target_type=None):
        """
        å¹¿åº¦ä¼˜å…ˆæœç´¢æ¨ç†
        Breadth-first search reasoning
        """
        from collections import deque
        
        # åˆå§‹åŒ–æœç´¢é˜Ÿåˆ—
        # Initialize search queue
        queue = deque([(start_entity, [], 0)])  # (å®ä½“, è·¯å¾„, è·³æ•°)
        visited = set()
        reasoning_paths = []
        
        while queue and len(reasoning_paths) < 100:  # é™åˆ¶è·¯å¾„æ•°é‡
            current_entity, path, hops = queue.popleft()
            
            if hops >= self.max_hops:
                continue
            
            if current_entity['entity_id'] in visited:
                continue
            
            visited.add(current_entity['entity_id'])
            
            # è·å–å½“å‰å®ä½“çš„æ‰€æœ‰å…³ç³»
            # Get all relations of current entity
            relations = self.kb.get_entity_relations(current_entity['entity_id'])
            
            for relation, target_entities in relations.items():
                for target_entity in target_entities:
                    new_path = path + [(current_entity, relation, target_entity)]
                    
                    # å¦‚æœæ‰¾åˆ°ç›®æ ‡ç±»å‹çš„å®ä½“ï¼Œè®°å½•è·¯å¾„
                    # If target type entity found, record path
                    if target_type is None or self.kb.get_entity_type(target_entity) == target_type:
                        reasoning_paths.append({
                            'path': new_path,
                            'start_entity': start_entity,
                            'end_entity': target_entity,
                            'hops': hops + 1,
                            'confidence': self.calculate_path_confidence(new_path)
                        })
                    
                    # ç»§ç»­æœç´¢
                    # Continue searching
                    if hops + 1 < self.max_hops:
                        queue.append((
                            {'entity_id': target_entity, 'entity_name': self.kb.get_entity_name(target_entity)},
                            new_path,
                            hops + 1
                        ))
        
        return reasoning_paths
    
    def calculate_path_confidence(self, path):
        """
        è®¡ç®—æ¨ç†è·¯å¾„çš„ç½®ä¿¡åº¦
        Calculate confidence of reasoning path
        """
        if not path:
            return 0.0
        
        # åŸºäºè·¯å¾„é•¿åº¦çš„æƒ©ç½š
        # Penalty based on path length
        length_penalty = 0.8 ** len(path)
        
        # åŸºäºå…³ç³»é¢‘ç‡çš„å¥–åŠ±
        # Reward based on relation frequency
        relation_scores = []
        for _, relation, _ in path:
            relation_freq = self.kb.get_relation_frequency(relation)
            relation_scores.append(relation_freq)
        
        avg_relation_score = sum(relation_scores) / len(relation_scores)
        
        # ç»¼åˆç½®ä¿¡åº¦
        # Combined confidence
        confidence = length_penalty * avg_relation_score
        
        return confidence
    
    def rank_reasoning_paths(self, paths, question):
        """
        å¯¹æ¨ç†è·¯å¾„è¿›è¡Œæ’åº
        Rank reasoning paths
        """
        # è®¡ç®—æ¯æ¡è·¯å¾„ä¸é—®é¢˜çš„ç›¸å…³æ€§
        # Calculate relevance of each path to question
        for path in paths:
            relevance_score = self.calculate_question_relevance(path, question)
            path['total_score'] = path['confidence'] * relevance_score
        
        # æŒ‰æ€»åˆ†æ’åº
        # Sort by total score
        paths.sort(key=lambda x: x['total_score'], reverse=True)
        
        return paths[:10]  # è¿”å›top-10è·¯å¾„
    
    def calculate_question_relevance(self, path, question):
        """
        è®¡ç®—è·¯å¾„ä¸é—®é¢˜çš„ç›¸å…³æ€§
        Calculate relevance between path and question
        """
        # ç®€åŒ–çš„ç›¸å…³æ€§è®¡ç®—
        # Simplified relevance calculation
        question_words = set(question.lower().split())
        
        path_words = set()
        for entity1, relation, entity2 in path['path']:
            path_words.update(entity1['entity_name'].lower().split())
            path_words.update(relation.lower().split('_'))
            path_words.update(self.kb.get_entity_name(entity2).lower().split())
        
        # è®¡ç®—è¯æ±‡é‡å 
        # Calculate word overlap
        overlap = len(question_words & path_words)
        total_words = len(question_words | path_words)
        
        if total_words == 0:
            return 0.0
        
        return overlap / total_words
```

## ğŸ’» å®Œæ•´KBQAç³»ç»Ÿå®ç° | Complete KBQA System Implementation

### çŸ¥è¯†åº“æ¥å£ | Knowledge Base Interface

```python
class KnowledgeBase:
    """
    çŸ¥è¯†åº“æ¥å£
    Knowledge Base Interface
    
    æä¾›å¯¹ç»“æ„åŒ–çŸ¥è¯†çš„è®¿é—®æ¥å£
    Provides access interface to structured knowledge
    """
    
    def __init__(self, kb_path):
        self.kb_path = kb_path
        self.entities = {}
        self.relations = {}
        self.triples = []
        self.load_knowledge_base()
    
    def load_knowledge_base(self):
        """
        åŠ è½½çŸ¥è¯†åº“
        Load knowledge base
        """
        # åŠ è½½å®ä½“
        # Load entities
        with open(f"{self.kb_path}/entities.json", 'r', encoding='utf-8') as f:
            self.entities = json.load(f)
        
        # åŠ è½½å…³ç³»
        # Load relations
        with open(f"{self.kb_path}/relations.json", 'r', encoding='utf-8') as f:
            self.relations = json.load(f)
        
        # åŠ è½½ä¸‰å…ƒç»„
        # Load triples
        with open(f"{self.kb_path}/triples.json", 'r', encoding='utf-8') as f:
            self.triples = json.load(f)
        
        # æ„å»ºç´¢å¼•
        # Build indices
        self.build_indices()
    
    def build_indices(self):
        """
        æ„å»ºç´¢å¼•ä»¥åŠ é€ŸæŸ¥è¯¢
        Build indices for faster queries
        """
        self.entity_relations = defaultdict(lambda: defaultdict(list))
        self.relation_entities = defaultdict(list)
        
        for triple in self.triples:
            head, relation, tail = triple['head'], triple['relation'], triple['tail']
            
            # å®ä½“-å…³ç³»ç´¢å¼•
            # Entity-relation index
            self.entity_relations[head][relation].append(tail)
            self.entity_relations[tail][f"reverse_{relation}"].append(head)
            
            # å…³ç³»-å®ä½“ç´¢å¼•
            # Relation-entity index
            self.relation_entities[relation].append((head, tail))
    
    def get_entity_relations(self, entity_id):
        """
        è·å–å®ä½“çš„æ‰€æœ‰å…³ç³»
        Get all relations of an entity
        """
        return dict(self.entity_relations.get(entity_id, {}))
    
    def get_relation_entities(self, relation):
        """
        è·å–å…³ç³»æ¶‰åŠçš„æ‰€æœ‰å®ä½“å¯¹
        Get all entity pairs involved in a relation
        """
        return self.relation_entities.get(relation, [])
    
    def query_single_hop(self, entity_id, relation):
        """
        å•è·³æŸ¥è¯¢
        Single-hop query
        """
        return self.entity_relations.get(entity_id, {}).get(relation, [])
    
    def query_multi_hop(self, start_entity, relations):
        """
        å¤šè·³æŸ¥è¯¢
        Multi-hop query
        """
        current_entities = [start_entity]
        
        for relation in relations:
            next_entities = []
            for entity in current_entities:
                next_entities.extend(self.query_single_hop(entity, relation))
            current_entities = list(set(next_entities))
        
        return current_entities
    
    def get_entity_info(self, entity_id):
        """
        è·å–å®ä½“ä¿¡æ¯
        Get entity information
        """
        return self.entities.get(entity_id, {})
    
    def get_entity_name(self, entity_id):
        """
        è·å–å®ä½“åç§°
        Get entity name
        """
        return self.entities.get(entity_id, {}).get('name', entity_id)
    
    def get_entity_type(self, entity_id):
        """
        è·å–å®ä½“ç±»å‹
        Get entity type
        """
        return self.entities.get(entity_id, {}).get('type', 'unknown')
    
    def get_relation_frequency(self, relation):
        """
        è·å–å…³ç³»é¢‘ç‡
        Get relation frequency
        """
        return len(self.relation_entities.get(relation, []))
```

### é›†æˆç³»ç»Ÿ | Integrated System

```python
class CompleteKBQASystem:
    """
    å®Œæ•´çš„çŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿ
    Complete Knowledge Base QA System
    """
    
    def __init__(self, kb_path, model_configs=None):
        # åˆå§‹åŒ–çŸ¥è¯†åº“
        # Initialize knowledge base
        self.kb = KnowledgeBase(kb_path)
        
        # åˆå§‹åŒ–å„ç»„ä»¶
        # Initialize components
        self.entity_linker = EntityLinker(self.kb.entities)
        self.relation_classifier = RelationClassifier(self.kb.relations)
        self.query_generator = SPARQLQueryGenerator(self.kb)
        self.multi_hop_reasoner = MultiHopReasoner(self.kb)
        
        # é—®é¢˜ç±»å‹åˆ†ç±»å™¨
        # Question type classifier
        self.question_classifier = self.build_question_classifier()
    
    def build_question_classifier(self):
        """
        æ„å»ºé—®é¢˜ç±»å‹åˆ†ç±»å™¨
        Build question type classifier
        """
        # é—®é¢˜ç±»å‹æ¨¡æ¿
        # Question type templates
        templates = {
            'factual': ['what is', 'who is', 'where is', 'when was'],
            'count': ['how many', 'count of', 'number of'],
            'comparison': ['compare', 'difference', 'better than'],
            'yes_no': ['is', 'does', 'can', 'will'],
            'list': ['list', 'all', 'which']
        }
        
        return templates
    
    def classify_question_type(self, question):
        """
        åˆ†ç±»é—®é¢˜ç±»å‹
        Classify question type
        """
        question_lower = question.lower()
        
        for q_type, templates in self.question_classifier.items():
            for template in templates:
                if template in question_lower:
                    return q_type
        
        return 'factual'  # é»˜è®¤ç±»å‹
    
    def answer_question(self, question):
        """
        å›ç­”é—®é¢˜
        Answer question
        """
        try:
            # æ­¥éª¤1: é—®é¢˜ç±»å‹åˆ†ç±»
            # Step 1: Question type classification
            question_type = self.classify_question_type(question)
            
            # æ­¥éª¤2: å®ä½“é“¾æ¥
            # Step 2: Entity linking
            entities = self.entity_linker.link_entities(question)
            
            if not entities:
                return {
                    'answer': "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•è¯†åˆ«é—®é¢˜ä¸­çš„å®ä½“ã€‚",
                    'confidence': 0.0,
                    'reasoning': []
                }
            
            # æ­¥éª¤3: å…³ç³»åˆ†ç±»
            # Step 3: Relation classification
            relations = self.relation_classifier.classify_relations(question, entities)
            
            if not relations:
                return {
                    'answer': "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç†è§£é—®é¢˜ä¸­çš„å…³ç³»ã€‚",
                    'confidence': 0.0,
                    'reasoning': []
                }
            
            # æ­¥éª¤4: æŸ¥è¯¢ç”Ÿæˆå’Œæ‰§è¡Œ
            # Step 4: Query generation and execution
            answers = []
            
            if question_type in ['factual', 'yes_no']:
                answers = self.handle_factual_question(entities, relations)
            elif question_type == 'count':
                answers = self.handle_count_question(entities, relations)
            elif question_type == 'list':
                answers = self.handle_list_question(entities, relations)
            
            # å¦‚æœç®€å•æŸ¥è¯¢æ²¡æœ‰ç»“æœï¼Œå°è¯•å¤šè·³æ¨ç†
            # If simple queries have no results, try multi-hop reasoning
            if not answers:
                reasoning_paths = self.multi_hop_reasoner.reason(question, entities)
                if reasoning_paths:
                    best_path = reasoning_paths[0]
                    answers = [best_path['end_entity']]
            
            # æ ¼å¼åŒ–ç­”æ¡ˆ
            # Format answer
            if answers:
                formatted_answer = self.format_answer(answers, question_type)
                return {
                    'answer': formatted_answer,
                    'confidence': 0.8,
                    'entities': entities,
                    'relations': relations,
                    'reasoning': reasoning_paths[:3] if 'reasoning_paths' in locals() else []
                }
            else:
                return {
                    'answer': "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•åœ¨çŸ¥è¯†åº“ä¸­æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚",
                    'confidence': 0.0,
                    'reasoning': []
                }
                
        except Exception as e:
            return {
                'answer': f"å¤„ç†é—®é¢˜æ—¶å‡ºç°é”™è¯¯: {str(e)}",
                'confidence': 0.0,
                'reasoning': []
            }
    
    def handle_factual_question(self, entities, relations):
        """
        å¤„ç†äº‹å®æ€§é—®é¢˜
        Handle factual questions
        """
        answers = []
        
        for entity in entities:
            for relation in relations:
                # å•è·³æŸ¥è¯¢
                # Single-hop query
                results = self.kb.query_single_hop(entity['entity_id'], relation['relation'])
                answers.extend(results)
                
                # åå‘æŸ¥è¯¢
                # Reverse query
                reverse_results = self.kb.query_single_hop(entity['entity_id'], f"reverse_{relation['relation']}")
                answers.extend(reverse_results)
        
        return list(set(answers))  # å»é‡
    
    def handle_count_question(self, entities, relations):
        """
        å¤„ç†è®¡æ•°é—®é¢˜
        Handle count questions
        """
        total_count = 0
        
        for entity in entities:
            for relation in relations:
                results = self.kb.query_single_hop(entity['entity_id'], relation['relation'])
                total_count += len(results)
        
        return [str(total_count)]
    
    def handle_list_question(self, entities, relations):
        """
        å¤„ç†åˆ—è¡¨é—®é¢˜
        Handle list questions
        """
        all_answers = []
        
        for entity in entities:
            for relation in relations:
                results = self.kb.query_single_hop(entity['entity_id'], relation['relation'])
                all_answers.extend(results)
        
        return list(set(all_answers))  # å»é‡
    
    def format_answer(self, answers, question_type):
        """
        æ ¼å¼åŒ–ç­”æ¡ˆ
        Format answer
        """
        if not answers:
            return "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"
        
        # å°†å®ä½“IDè½¬æ¢ä¸ºå¯è¯»åç§°
        # Convert entity IDs to readable names
        readable_answers = []
        for answer in answers:
            if answer in self.kb.entities:
                readable_answers.append(self.kb.get_entity_name(answer))
            else:
                readable_answers.append(str(answer))
        
        if question_type == 'count':
            return f"æ•°é‡æ˜¯: {readable_answers[0]}"
        elif question_type == 'list':
            return "ç»“æœåŒ…æ‹¬: " + ", ".join(readable_answers[:10])  # é™åˆ¶æ˜¾ç¤ºæ•°é‡
        else:
            return readable_answers[0] if readable_answers else "æ²¡æœ‰æ‰¾åˆ°ç­”æ¡ˆã€‚"


def main():
    """
    æ¼”ç¤ºKBQAç³»ç»Ÿçš„ä½¿ç”¨
    Demonstrate KBQA system usage
    """
    # åˆ›å»ºç¤ºä¾‹çŸ¥è¯†åº“
    # Create example knowledge base
    kb_path = "example_kb"
    
    # åˆå§‹åŒ–KBQAç³»ç»Ÿ
    # Initialize KBQA system
    kbqa_system = CompleteKBQASystem(kb_path)
    
    # æµ‹è¯•é—®é¢˜
    # Test questions
    test_questions = [
        "è‹¹æœå…¬å¸çš„åˆ›å§‹äººæ˜¯è°ï¼Ÿ",
        "å²è’‚å¤«Â·ä¹”å¸ƒæ–¯ä»€ä¹ˆæ—¶å€™å‡ºç”Ÿçš„ï¼Ÿ",
        "æœ‰å¤šå°‘å®¶å…¬å¸çš„æ€»éƒ¨åœ¨åŠ åˆ©ç¦å°¼äºšï¼Ÿ",
        "åˆ—å‡ºæ‰€æœ‰ä½äºçº½çº¦çš„å¤§å­¦ã€‚"
    ]
    
    print("=== çŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿæ¼”ç¤º | KBQA System Demo ===")
    
    for question in test_questions:
        print(f"\né—®é¢˜ | Question: {question}")
        result = kbqa_system.answer_question(question)
        print(f"ç­”æ¡ˆ | Answer: {result['answer']}")
        print(f"ç½®ä¿¡åº¦ | Confidence: {result['confidence']:.2f}")
        
        if result['reasoning']:
            print("æ¨ç†è·¯å¾„ | Reasoning Path:")
            for i, path in enumerate(result['reasoning'][:2]):
                print(f"  è·¯å¾„{i+1}: {path}")


if __name__ == "__main__":
    main()
```

é€šè¿‡è¿™ä¸ªçŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿé¡¹ç›®ï¼Œä½ å°†å­¦ä¼šå¦‚ä½•è®©æœºå™¨ç†è§£ç»“æ„åŒ–çŸ¥è¯†ï¼Œå®ç°çœŸæ­£çš„æ™ºèƒ½é—®ç­”ï¼è¿™æ˜¯æ„å»ºä¸‹ä¸€ä»£AIåŠ©æ‰‹çš„å…³é”®æŠ€æœ¯ã€‚

Through this knowledge base QA system project, you will learn how to make machines understand structured knowledge and achieve truly intelligent question answering! This is a key technology for building next-generation AI assistants. 