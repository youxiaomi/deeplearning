# ä¸­æ–‡å‘½åå®ä½“è¯†åˆ«ç³»ç»Ÿ
# Chinese Named Entity Recognition System

**è®©æœºå™¨è¯†åˆ«ä¸­æ–‡æ–‡æœ¬ä¸­çš„äººåã€åœ°åã€æœºæ„åç­‰å…³é”®ä¿¡æ¯**
**Making machines identify key information like person names, locations, and organizations in Chinese text**

---

## ğŸ¯ ä»€ä¹ˆæ˜¯å‘½åå®ä½“è¯†åˆ«ï¼Ÿ| What is Named Entity Recognition?

å‘½åå®ä½“è¯†åˆ«(NER)å°±åƒç»™æ–‡æœ¬åš"æ ‡è®°"ï¼Œå¸®åŠ©æœºå™¨ç†è§£å“ªäº›è¯æ±‡æŒ‡ä»£ç‰¹å®šçš„å®ä½“ã€‚æƒ³è±¡ä¸€ä¸‹ä½ åœ¨çœ‹æ–°é—»æ—¶ä¼šè‡ªåŠ¨è¯†åˆ«"å°èŠ³"æ˜¯äººåã€"åŒ—äº¬"æ˜¯åœ°åã€"æ¸…åå¤§å­¦"æ˜¯æœºæ„åï¼Œè¿™å°±æ˜¯äººç±»å¤©ç„¶çš„NERèƒ½åŠ›ã€‚

Named Entity Recognition (NER) is like "tagging" text to help machines understand which words refer to specific entities. Imagine when you read news, you automatically recognize "xiaofang" as a person name, "Beijing" as a location, and "Tsinghua University" as an organization - this is humans' natural NER ability.

### ä¸ºä»€ä¹ˆä¸­æ–‡NERç‰¹åˆ«å›°éš¾ï¼Ÿ| Why is Chinese NER Particularly Difficult?

**1. æ²¡æœ‰ç©ºæ ¼åˆ†éš” | No Space Separation**
- è‹±æ–‡: "Barack Obama visited Beijing" â† å•è¯è¾¹ç•Œæ¸…æ™°
- English: Clear word boundaries
- ä¸­æ–‡: "å¥¥å·´é©¬è®¿é—®åŒ—äº¬" â† éœ€è¦å…ˆåˆ†è¯æ‰èƒ½è¯†åˆ«å®ä½“
- Chinese: Need word segmentation before entity recognition

**2. å¤šä¹‰æ€§é—®é¢˜ | Polysemy Issues**
```
"ç‹å»ºå›½" å¯èƒ½æ˜¯:
"Wang Jianguo" could be:
- äººå (Person): ç‹å»ºå›½å…ˆç”Ÿä»Šå¤©æ¥è®¿
- åŠ¨è¯çŸ­è¯­ (Verb phrase): ç‹å»ºå›½å®¶çš„å¸Œæœ›
```

**3. å®ä½“è¾¹ç•Œæ¨¡ç³Š | Fuzzy Entity Boundaries**
```
"åŒ—äº¬å¤§å­¦é™„å±ä¸­å­¦" 
"Beijing University Affiliated High School"
- æ˜¯ä¸€ä¸ªæœºæ„åï¼Ÿ| One organization name?
- è¿˜æ˜¯"åŒ—äº¬å¤§å­¦" + "é™„å±ä¸­å­¦"ï¼Ÿ| Or "Beijing University" + "Affiliated High School"?
```

## ğŸ§  æŠ€æœ¯åŸç†æ·±åº¦è§£æ | Deep Technical Principle Analysis

### BIOæ ‡æ³¨ä½“ç³» | BIO Tagging System

BIOæ ‡æ³¨å°±åƒç»™æ¯ä¸ªå­—ç¬¦è´´ä¸Š"èº«ä»½æ ‡ç­¾"ï¼š
BIO tagging is like putting "identity labels" on each character:

- **B-PER**: äººåçš„å¼€å§‹å­—ç¬¦ | Beginning of person name
- **I-PER**: äººåçš„å†…éƒ¨å­—ç¬¦ | Inside of person name  
- **B-LOC**: åœ°åçš„å¼€å§‹å­—ç¬¦ | Beginning of location name
- **I-LOC**: åœ°åçš„å†…éƒ¨å­—ç¬¦ | Inside of location name
- **B-ORG**: æœºæ„åçš„å¼€å§‹å­—ç¬¦ | Beginning of organization name
- **I-ORG**: æœºæ„åçš„å†…éƒ¨å­—ç¬¦ | Inside of organization name
- **O**: ä¸å±äºä»»ä½•å®ä½“ | Outside any entity

**æ ‡æ³¨ç¤ºä¾‹ | Tagging Example:**
```
è¾“å…¥æ–‡æœ¬: é©¬äº‘åˆ›ç«‹äº†é˜¿é‡Œå·´å·´å…¬å¸
Input text: Ma Yun founded Alibaba Company

å­—ç¬¦çº§æ ‡æ³¨:
Character-level tagging:
é©¬  B-PER
äº‘  I-PER  
åˆ›  O
ç«‹  O
äº†  O
é˜¿  B-ORG
é‡Œ  I-ORG
å·´  I-ORG
å·´  I-ORG
å…¬  I-ORG
å¸  I-ORG
```

### BERT + CRFæ¶æ„ | BERT + CRF Architecture

**ä¸ºä»€ä¹ˆé€‰æ‹©BERT + CRFç»„åˆï¼Ÿ| Why Choose BERT + CRF Combination?**

1. **BERTæä¾›è¯­ä¹‰ç†è§£** | **BERT Provides Semantic Understanding**
   - ç†è§£ä¸Šä¸‹æ–‡è¯­ä¹‰ï¼Œå¦‚"è‹¹æœ"åœ¨ä¸åŒè¯­å¢ƒä¸‹çš„å«ä¹‰
   - Understand contextual semantics, like different meanings of "apple" in various contexts

2. **CRFç¡®ä¿æ ‡æ³¨ä¸€è‡´æ€§** | **CRF Ensures Tagging Consistency**
   - é¿å…å‡ºç°"I-PER"åé¢ç›´æ¥è·Ÿ"B-ORG"è¿™æ ·çš„æ— æ•ˆåºåˆ—
   - Avoid invalid sequences like "I-PER" directly followed by "B-ORG"

### æ¨¡å‹è®­ç»ƒæµç¨‹ | Model Training Process

**ç¬¬ä¸€æ­¥ï¼šæ•°æ®é¢„å¤„ç† | Step 1: Data Preprocessing**
```python
def preprocess_chinese_text(text, entities):
    """
    ä¸­æ–‡æ–‡æœ¬é¢„å¤„ç†ï¼Œå°†æ–‡æœ¬è½¬æ¢ä¸ºå­—ç¬¦çº§åˆ«çš„æ ‡æ³¨
    Chinese text preprocessing, convert text to character-level annotations
    """
    # å°†æ–‡æœ¬è½¬ä¸ºå­—ç¬¦åˆ—è¡¨
    # Convert text to character list
    chars = list(text)
    
    # åˆå§‹åŒ–æ‰€æœ‰å­—ç¬¦ä¸ºOæ ‡ç­¾
    # Initialize all characters with O label
    labels = ['O'] * len(chars)
    
    # æ ¹æ®å®ä½“ä½ç½®è®¾ç½®BIOæ ‡ç­¾
    # Set BIO labels based on entity positions
    for entity in entities:
        start, end, entity_type = entity['start'], entity['end'], entity['type']
        labels[start] = f'B-{entity_type}'
        for i in range(start + 1, end):
            labels[i] = f'I-{entity_type}'
    
    return chars, labels
```

**ç¬¬äºŒæ­¥ï¼šBERTç‰¹å¾æå– | Step 2: BERT Feature Extraction**
```python
# BERTå°†æ¯ä¸ªå­—ç¬¦è½¬æ¢ä¸º768ç»´çš„å‘é‡
# BERT converts each character to a 768-dimensional vector
input_text = "é©¬äº‘åˆ›ç«‹äº†é˜¿é‡Œå·´å·´"
tokens = tokenizer(input_text, return_tensors='pt')
bert_output = bert_model(**tokens)

# è·å–æ¯ä¸ªå­—ç¬¦çš„å‘é‡è¡¨ç¤º
# Get vector representation for each character
char_embeddings = bert_output.last_hidden_state  # [batch_size, seq_len, 768]
```

**ç¬¬ä¸‰æ­¥ï¼šCRFåºåˆ—æ ‡æ³¨ | Step 3: CRF Sequence Labeling**
```python
# CRFå±‚ç¡®ä¿æ ‡æ³¨åºåˆ—çš„åˆç†æ€§
# CRF layer ensures rationality of tagging sequences
crf_scores = crf_layer(char_embeddings)
best_path = crf_layer.decode(crf_scores)  # ç»´ç‰¹æ¯”è§£ç æ‰¾æœ€ä¼˜è·¯å¾„
```

## ğŸ’» å®Œæ•´ä»£ç å®ç° | Complete Code Implementation

### æ•°æ®é›†å¤„ç†ç±» | Dataset Processing Class
```python
import torch
from torch.utils.data import Dataset
import json

class ChineseNERDataset(Dataset):
    """
    ä¸­æ–‡NERæ•°æ®é›†ç±»
    Chinese NER dataset class
    """
    def __init__(self, data_path, tokenizer, max_length=128):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.data = self.load_data(data_path)
        
        # æ ‡ç­¾æ˜ å°„
        # Label mapping
        self.label2id = {
            'O': 0, 'B-PER': 1, 'I-PER': 2,
            'B-LOC': 3, 'I-LOC': 4, 'B-ORG': 5, 'I-ORG': 6
        }
        self.id2label = {v: k for k, v in self.label2id.items()}
    
    def load_data(self, data_path):
        """åŠ è½½è®­ç»ƒæ•°æ® | Load training data"""
        with open(data_path, 'r', encoding='utf-8') as f:
            return [json.loads(line) for line in f]
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        text = item['text']
        entities = item.get('entities', [])
        
        # å­—ç¬¦çº§tokenization
        # Character-level tokenization
        chars = list(text)
        labels = ['O'] * len(chars)
        
        # è®¾ç½®å®ä½“æ ‡ç­¾
        # Set entity labels
        for entity in entities:
            start, end, entity_type = entity['start'], entity['end'], entity['type']
            if start < len(chars):
                labels[start] = f'B-{entity_type}'
                for i in range(start + 1, min(end, len(chars))):
                    labels[i] = f'I-{entity_type}'
        
        # BERTç¼–ç 
        # BERT encoding
        encoding = self.tokenizer(
            chars,
            is_split_into_words=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        # æ ‡ç­¾å¯¹é½
        # Label alignment
        word_ids = encoding.word_ids()
        aligned_labels = []
        for word_id in word_ids:
            if word_id is None:
                aligned_labels.append(-100)  # å¿½ç•¥ç‰¹æ®Štoken
            else:
                aligned_labels.append(self.label2id[labels[word_id]])
        
        return {
            'input_ids': encoding['input_ids'].squeeze(),
            'attention_mask': encoding['attention_mask'].squeeze(),
            'labels': torch.tensor(aligned_labels, dtype=torch.long)
        }
```

### NERæ¨¡å‹å®šä¹‰ | NER Model Definition
```python
import torch.nn as nn
from transformers import BertModel
from torchcrf import CRF

class ChineseNERModel(nn.Module):
    """
    ä¸­æ–‡å‘½åå®ä½“è¯†åˆ«æ¨¡å‹
    Chinese Named Entity Recognition Model
    """
    def __init__(self, bert_model_name, num_labels, dropout_rate=0.1):
        super(ChineseNERModel, self).__init__()
        
        # BERTç¼–ç å™¨
        # BERT encoder
        self.bert = BertModel.from_pretrained(bert_model_name)
        self.dropout = nn.Dropout(dropout_rate)
        
        # åˆ†ç±»å±‚
        # Classification layer
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)
        
        # CRFå±‚
        # CRF layer
        self.crf = CRF(num_labels, batch_first=True)
        
        self.num_labels = num_labels
    
    def forward(self, input_ids, attention_mask, labels=None):
        # BERTç‰¹å¾æå–
        # BERT feature extraction
        bert_output = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask
        )
        
        # è·å–åºåˆ—è¾“å‡ºå¹¶æ·»åŠ dropout
        # Get sequence output and add dropout
        sequence_output = self.dropout(bert_output.last_hidden_state)
        
        # åˆ†ç±»é¢„æµ‹
        # Classification prediction
        logits = self.classifier(sequence_output)
        
        # åˆ›å»ºCRFæ©ç ï¼ˆå¿½ç•¥paddingå’Œç‰¹æ®Štokenï¼‰
        # Create CRF mask (ignore padding and special tokens)
        mask = attention_mask.bool()
        
        if labels is not None:
            # è®­ç»ƒæ¨¡å¼ï¼šè®¡ç®—CRFæŸå¤±
            # Training mode: calculate CRF loss
            # è¿‡æ»¤æ‰æ ‡ç­¾ä¸º-100çš„ä½ç½®
            # Filter out positions with label -100
            active_loss = mask.view(-1)
            active_logits = logits.view(-1, self.num_labels)[active_loss]
            active_labels = labels.view(-1)[active_loss]
            
            # é‡å¡‘ä¸ºCRFæœŸæœ›çš„å½¢çŠ¶
            # Reshape to CRF expected shape
            batch_size, seq_len = labels.shape
            crf_labels = torch.full_like(labels, -1)
            crf_logits = logits.clone()
            
            for i in range(batch_size):
                valid_length = mask[i].sum().item()
                valid_labels = labels[i][:valid_length]
                # åªä¿ç•™é-100çš„æ ‡ç­¾
                valid_mask = valid_labels != -100
                if valid_mask.any():
                    crf_labels[i][:valid_mask.sum()] = valid_labels[valid_mask]
            
            # è®¡ç®—CRFæŸå¤±
            # Calculate CRF loss
            loss = -self.crf(crf_logits, crf_labels, mask=mask, reduction='mean')
            return loss, logits
        else:
            # æ¨ç†æ¨¡å¼ï¼šCRFè§£ç 
            # Inference mode: CRF decoding
            predictions = self.crf.decode(logits, mask=mask)
            return predictions
```

### è®­ç»ƒè„šæœ¬ | Training Script
```python
from torch.utils.data import DataLoader
from transformers import BertTokenizer, AdamW
from sklearn.metrics import classification_report
import numpy as np

def train_chinese_ner():
    """
    è®­ç»ƒä¸­æ–‡NERæ¨¡å‹
    Train Chinese NER model
    """
    # è®¾ç½®è®¾å¤‡
    # Set device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # åŠ è½½tokenizerå’Œæ•°æ®
    # Load tokenizer and data
    tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
    
    train_dataset = ChineseNERDataset('train.jsonl', tokenizer)
    val_dataset = ChineseNERDataset('val.jsonl', tokenizer)
    
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)
    
    # åˆå§‹åŒ–æ¨¡å‹
    # Initialize model
    model = ChineseNERModel('bert-base-chinese', num_labels=7)
    model.to(device)
    
    # ä¼˜åŒ–å™¨è®¾ç½®
    # Optimizer setup
    optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)
    
    # è®­ç»ƒå¾ªç¯
    # Training loop
    model.train()
    for epoch in range(3):
        total_loss = 0
        for batch in train_loader:
            # æ•°æ®è½¬ç§»åˆ°è®¾å¤‡
            # Move data to device
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            
            # å‰å‘ä¼ æ’­
            # Forward pass
            optimizer.zero_grad()
            loss, _ = model(input_ids, attention_mask, labels)
            
            # åå‘ä¼ æ’­
            # Backward pass
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
        
        print(f"Epoch {epoch + 1}/3, Average Loss: {total_loss / len(train_loader):.4f}")
        
        # éªŒè¯
        # Validation
        evaluate_model(model, val_loader, train_dataset.id2label, device)

def evaluate_model(model, val_loader, id2label, device):
    """
    è¯„ä¼°æ¨¡å‹æ€§èƒ½
    Evaluate model performance
    """
    model.eval()
    all_predictions = []
    all_labels = []
    
    with torch.no_grad():
        for batch in val_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            
            # è·å–é¢„æµ‹
            # Get predictions
            predictions = model(input_ids, attention_mask)
            
            # å¤„ç†æ‰¹æ¬¡ä¸­çš„æ¯ä¸ªæ ·æœ¬
            # Process each sample in batch
            for i, pred_seq in enumerate(predictions):
                # è·å–æœ‰æ•ˆé•¿åº¦
                # Get valid length
                valid_length = attention_mask[i].sum().item()
                
                # è½¬æ¢é¢„æµ‹å’ŒçœŸå®æ ‡ç­¾
                # Convert predictions and true labels
                pred_labels = [id2label[p] for p in pred_seq[:valid_length-2]]  # å»é™¤[CLS]å’Œ[SEP]
                true_labels = [id2label[l.item()] for l in labels[i][1:valid_length-1] if l.item() != -100]
                
                all_predictions.append(pred_labels)
                all_labels.append(true_labels)
    
    # ä½¿ç”¨seqevalè®¡ç®—F1åˆ†æ•°
    # Calculate F1 score using seqeval
    from seqeval.metrics import classification_report, f1_score
    
    print("åˆ†ç±»æŠ¥å‘Š | Classification Report:")
    print(classification_report(all_labels, all_predictions, digits=4))
    
    f1 = f1_score(all_labels, all_predictions)
    print(f"æ•´ä½“F1åˆ†æ•° | Overall F1 Score: {f1:.4f}")
```

## ğŸ” å®ä½“è¯†åˆ«ç¤ºä¾‹ | Entity Recognition Examples

### æ–°é—»æ–‡æœ¬å¤„ç† | News Text Processing
```python
def process_news_text():
    """
    å¤„ç†æ–°é—»æ–‡æœ¬ç¤ºä¾‹
    Process news text example
    """
    news_text = """
    æ®å¤®è§†æ–°é—»æŠ¥é“ï¼Œå›½å®¶é¢†å¯¼äººå°èŠ³ä»Šæ—¥åœ¨åŒ—äº¬äººæ°‘å¤§ä¼šå ‚ä¼šè§äº†ç¾å›½æ€»ç»Ÿæ‹œç™»ã€‚
    åŒæ–¹å°±ä¸­ç¾å…³ç³»å‘å±•ã€æ°”å€™å˜åŒ–ç­‰é—®é¢˜æ·±å…¥äº¤æ¢äº†æ„è§ã€‚
    """
    
    # é¢„æœŸè¯†åˆ«ç»“æœ
    # Expected recognition results
    expected_entities = [
        ("å¤®è§†æ–°é—»", "ORG"),
        ("å°èŠ³", "PER"), 
        ("åŒ—äº¬äººæ°‘å¤§ä¼šå ‚", "LOC"),
        ("ç¾å›½", "LOC"),
        ("æ‹œç™»", "PER")
    ]
    
    print("æ–°é—»æ–‡æœ¬å®ä½“è¯†åˆ« | News Text Entity Recognition:")
    print(f"è¾“å…¥æ–‡æœ¬ | Input Text: {news_text}")
    print("è¯†åˆ«çš„å®ä½“ | Recognized Entities:")
    for entity, entity_type in expected_entities:
        print(f"  {entity} -> {entity_type}")
```

### å•†ä¸šæ–‡æ¡£å¤„ç† | Business Document Processing
```python
def process_business_text():
    """
    å¤„ç†å•†ä¸šæ–‡æ¡£ç¤ºä¾‹  
    Process business document example
    """
    business_text = """
    è…¾è®¯ç§‘æŠ€æœ‰é™å…¬å¸ä¸é˜¿é‡Œå·´å·´é›†å›¢äº2023å¹´12æœˆåœ¨æ·±åœ³ç­¾ç½²æˆ˜ç•¥åˆä½œåè®®ã€‚
    è¯¥åè®®ç”±è…¾è®¯CEOé©¬åŒ–è…¾å’Œé˜¿é‡Œå·´å·´è‘£äº‹é•¿å¼ å‹‡å…±åŒç­¾ç½²ã€‚
    """
    
    expected_entities = [
        ("è…¾è®¯ç§‘æŠ€æœ‰é™å…¬å¸", "ORG"),
        ("é˜¿é‡Œå·´å·´é›†å›¢", "ORG"),
        ("æ·±åœ³", "LOC"),
        ("é©¬åŒ–è…¾", "PER"),
        ("å¼ å‹‡", "PER")
    ]
    
    print("å•†ä¸šæ–‡æ¡£å®ä½“è¯†åˆ« | Business Document Entity Recognition:")
    print(f"è¾“å…¥æ–‡æœ¬ | Input Text: {business_text}")
    print("è¯†åˆ«çš„å®ä½“ | Recognized Entities:")
    for entity, entity_type in expected_entities:
        print(f"  {entity} -> {entity_type}")
```

## ğŸš€ æ¨¡å‹ä¼˜åŒ–æŠ€å·§ | Model Optimization Tips

### æ•°æ®å¢å¼ºç­–ç•¥ | Data Augmentation Strategies

**1. å®ä½“æ›¿æ¢ | Entity Substitution**
```python
def entity_substitution_augmentation(text, entities):
    """
    å®ä½“æ›¿æ¢æ•°æ®å¢å¼º
    Entity substitution data augmentation
    """
    # æ„å»ºå®ä½“è¯å…¸
    # Build entity dictionary
    entity_dict = {
        'PER': ['å¼ ä¸‰', 'æå››', 'ç‹äº”', 'èµµå…­'],
        'LOC': ['åŒ—äº¬', 'ä¸Šæµ·', 'å¹¿å·', 'æ·±åœ³'], 
        'ORG': ['æ¸…åå¤§å­¦', 'åŒ—äº¬å¤§å­¦', 'å¤æ—¦å¤§å­¦', 'æµ™æ±Ÿå¤§å­¦']
    }
    
    augmented_texts = []
    
    for entity in entities:
        original_text = entity['text']
        entity_type = entity['type']
        
        # éšæœºé€‰æ‹©åŒç±»å‹å®ä½“è¿›è¡Œæ›¿æ¢
        # Randomly select same-type entity for replacement
        if entity_type in entity_dict:
            for replacement in entity_dict[entity_type]:
                new_text = text.replace(original_text, replacement)
                augmented_texts.append(new_text)
    
    return augmented_texts
```

**2. ä¸Šä¸‹æ–‡æ‰°åŠ¨ | Context Perturbation**
```python
def context_perturbation(text, entities):
    """
    ä¸Šä¸‹æ–‡æ‰°åŠ¨å¢å¼º
    Context perturbation augmentation
    """
    import random
    
    # åŒä¹‰è¯æ›¿æ¢è¯å…¸
    # Synonym replacement dictionary
    synonyms = {
        'è®¿é—®': ['æ‹œè®¿', 'æ¢è®¿', 'é€ è®¿'],
        'ä¼šè§': ['æ¥è§', 'ä¼šé¢', 'è§é¢'],
        'ç­¾ç½²': ['ç­¾è®¢', 'ç­¾çº¦', 'ç­¾å®š']
    }
    
    words = text.split()
    augmented_words = words.copy()
    
    for i, word in enumerate(words):
        if word in synonyms and random.random() < 0.3:
            augmented_words[i] = random.choice(synonyms[word])
    
    return ''.join(augmented_words)
```

### æ¨¡å‹é›†æˆæŠ€æœ¯ | Model Ensemble Techniques

**1. å¤šæ¨¡å‹æŠ•ç¥¨ | Multi-Model Voting**
```python
class NERModelEnsemble:
    """
    NERæ¨¡å‹é›†æˆç±»
    NER model ensemble class
    """
    def __init__(self, models):
        self.models = models
    
    def predict(self, text):
        """
        é›†æˆé¢„æµ‹æ–¹æ³•
        Ensemble prediction method
        """
        all_predictions = []
        
        # è·å–æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹
        # Get predictions from all models
        for model in self.models:
            pred = model.predict(text)
            all_predictions.append(pred)
        
        # æŠ•ç¥¨å†³å®šæœ€ç»ˆç»“æœ
        # Vote for final result
        final_prediction = self.vote_predictions(all_predictions)
        return final_prediction
    
    def vote_predictions(self, predictions):
        """
        æŠ•ç¥¨æœºåˆ¶
        Voting mechanism
        """
        from collections import Counter
        
        final_tags = []
        max_len = max(len(pred) for pred in predictions)
        
        for i in range(max_len):
            # æ”¶é›†è¯¥ä½ç½®æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹
            # Collect predictions from all models at this position
            position_votes = []
            for pred in predictions:
                if i < len(pred):
                    position_votes.append(pred[i])
            
            # å¤šæ•°æŠ•ç¥¨
            # Majority voting
            if position_votes:
                most_common = Counter(position_votes).most_common(1)[0][0]
                final_tags.append(most_common)
        
        return final_tags
```

## ğŸ“Š æ€§èƒ½è¯„ä¼°æŒ‡æ ‡ | Performance Evaluation Metrics

### å®ä½“çº§åˆ«è¯„ä¼° | Entity-Level Evaluation
```python
def calculate_entity_metrics(true_entities, pred_entities):
    """
    è®¡ç®—å®ä½“çº§åˆ«çš„è¯„ä¼°æŒ‡æ ‡
    Calculate entity-level evaluation metrics
    """
    # æå–å®ä½“é›†åˆ
    # Extract entity sets
    true_set = set((entity['start'], entity['end'], entity['type']) for entity in true_entities)
    pred_set = set((entity['start'], entity['end'], entity['type']) for entity in pred_entities)
    
    # è®¡ç®—ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°
    # Calculate precision, recall, F1 score
    tp = len(true_set & pred_set)  # çœŸæ­£ä¾‹ True Positives
    fp = len(pred_set - true_set)  # å‡æ­£ä¾‹ False Positives  
    fn = len(true_set - pred_set)  # å‡è´Ÿä¾‹ False Negatives
    
    precision = tp / (tp + fp) if tp + fp > 0 else 0
    recall = tp / (tp + fn) if tp + fn > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0
    
    return {
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'true_positives': tp,
        'false_positives': fp,
        'false_negatives': fn
    }

def detailed_error_analysis(true_entities, pred_entities):
    """
    è¯¦ç»†é”™è¯¯åˆ†æ
    Detailed error analysis
    """
    errors = {
        'missing_entities': [],      # æ¼æ£€å®ä½“ Missing entities
        'false_entities': [],        # è¯¯æ£€å®ä½“ False entities
        'boundary_errors': [],       # è¾¹ç•Œé”™è¯¯ Boundary errors
        'type_errors': []           # ç±»å‹é”™è¯¯ Type errors
    }
    
    true_positions = {(e['start'], e['end']): e for e in true_entities}
    pred_positions = {(e['start'], e['end']): e for e in pred_entities}
    
    # åˆ†æå„ç§é”™è¯¯ç±»å‹
    # Analyze various error types
    for pos, true_entity in true_positions.items():
        if pos not in pred_positions:
            errors['missing_entities'].append(true_entity)
        else:
            pred_entity = pred_positions[pos]
            if true_entity['type'] != pred_entity['type']:
                errors['type_errors'].append({
                    'text': true_entity['text'],
                    'true_type': true_entity['type'],
                    'pred_type': pred_entity['type']
                })
    
    for pos, pred_entity in pred_positions.items():
        if pos not in true_positions:
            errors['false_entities'].append(pred_entity)
    
    return errors
```

## ğŸ“ å­¦ä¹ å»ºè®® | Learning Recommendations

### å®è·µæ­¥éª¤ | Practice Steps

**ç¬¬1å¤©ï¼šç¯å¢ƒæ­å»º** | **Day 1: Environment Setup**
1. å®‰è£…å¿…è¦çš„PythonåŒ…ï¼štransformers, torch, seqeval
2. Install necessary Python packages: transformers, torch, seqeval
3. ä¸‹è½½ä¸­æ–‡BERTé¢„è®­ç»ƒæ¨¡å‹
4. Download Chinese BERT pretrained model

**ç¬¬2-3å¤©ï¼šæ•°æ®ç†è§£** | **Day 2-3: Data Understanding**
1. åˆ†æä¸­æ–‡NERæ•°æ®é›†çš„ç‰¹ç‚¹
2. Analyze characteristics of Chinese NER datasets
3. ç†è§£BIOæ ‡æ³¨ä½“ç³»
4. Understand BIO tagging system

**ç¬¬4-5å¤©ï¼šæ¨¡å‹å®ç°** | **Day 4-5: Model Implementation**
1. å®ç°BERT+CRFæ¨¡å‹
2. Implement BERT+CRF model
3. ç¼–å†™è®­ç»ƒå’Œè¯„ä¼°ä»£ç 
4. Write training and evaluation code

**ç¬¬6-7å¤©ï¼šå®éªŒä¼˜åŒ–** | **Day 6-7: Experiment Optimization**
1. å°è¯•ä¸åŒçš„è¶…å‚æ•°è®¾ç½®
2. Try different hyperparameter settings
3. å®ç°æ•°æ®å¢å¼ºæŠ€æœ¯
4. Implement data augmentation techniques

### å¸¸è§é—®é¢˜è§£ç­” | Frequently Asked Questions

**Q1: ä¸ºä»€ä¹ˆè¦ä½¿ç”¨å­—ç¬¦çº§åˆ«è€Œä¸æ˜¯è¯çº§åˆ«çš„tokenizationï¼Ÿ**
**Q1: Why use character-level instead of word-level tokenization?**

A: ä¸­æ–‡æ²¡æœ‰æ˜ç¡®çš„è¯è¾¹ç•Œï¼Œå­—ç¬¦çº§åˆ«å¯ä»¥é¿å…åˆ†è¯é”™è¯¯å¯¹NERçš„å½±å“ï¼ŒåŒæ—¶èƒ½æ›´å¥½åœ°å¤„ç†æœªç™»å½•è¯ã€‚
A: Chinese has no clear word boundaries. Character-level can avoid the impact of word segmentation errors on NER and better handle out-of-vocabulary words.

**Q2: CRFå±‚çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ**
**Q2: What is the role of the CRF layer?**

A: CRFç¡®ä¿è¾“å‡ºæ ‡ç­¾åºåˆ—çš„åˆç†æ€§ï¼Œé¿å…å‡ºç°"I-PERåé¢è·ŸB-ORG"è¿™æ ·çš„æ— æ•ˆè½¬ç§»ã€‚
A: CRF ensures the rationality of output label sequences, avoiding invalid transitions like "I-PER followed by B-ORG".

**Q3: å¦‚ä½•å¤„ç†åµŒå¥—å®ä½“ï¼Ÿ**
**Q3: How to handle nested entities?**

A: å¯ä»¥ä½¿ç”¨å¤šå±‚æ ‡æ³¨æˆ–è€…ç‰‡æ®µæ’åçš„æ–¹æ³•ï¼Œä¾‹å¦‚"åŒ—äº¬å¤§å­¦è®¡ç®—æœºå­¦é™¢"ä¸­çš„"åŒ—äº¬å¤§å­¦"å’Œ"è®¡ç®—æœºå­¦é™¢"ã€‚
A: You can use multi-layer annotation or span ranking methods, for example, "Beijing University" and "Computer Science College" in "Beijing University Computer Science College".

é€šè¿‡è¿™ä¸ªä¸­æ–‡NERç³»ç»Ÿé¡¹ç›®ï¼Œä½ å°†æ·±å…¥ç†è§£åºåˆ—æ ‡æ³¨ä»»åŠ¡çš„æ ¸å¿ƒæŠ€æœ¯ï¼ŒæŒæ¡BERTå’ŒCRFçš„ç»„åˆä½¿ç”¨ï¼Œä¸ºåç»­çš„NLPåº”ç”¨å¥ å®šåšå®åŸºç¡€ï¼

Through this Chinese NER system project, you will deeply understand the core technologies of sequence labeling tasks, master the combined use of BERT and CRF, and lay a solid foundation for subsequent NLP applications! 