# 中文命名实体识别系统
# Chinese Named Entity Recognition System

**让机器识别中文文本中的人名、地名、机构名等关键信息**
**Making machines identify key information like person names, locations, and organizations in Chinese text**

---

## 🎯 什么是命名实体识别？| What is Named Entity Recognition?

命名实体识别(NER)就像给文本做"标记"，帮助机器理解哪些词汇指代特定的实体。想象一下你在看新闻时会自动识别"小芳"是人名、"北京"是地名、"清华大学"是机构名，这就是人类天然的NER能力。

Named Entity Recognition (NER) is like "tagging" text to help machines understand which words refer to specific entities. Imagine when you read news, you automatically recognize "xiaofang" as a person name, "Beijing" as a location, and "Tsinghua University" as an organization - this is humans' natural NER ability.

### 为什么中文NER特别困难？| Why is Chinese NER Particularly Difficult?

**1. 没有空格分隔 | No Space Separation**
- 英文: "Barack Obama visited Beijing" ← 单词边界清晰
- English: Clear word boundaries
- 中文: "奥巴马访问北京" ← 需要先分词才能识别实体
- Chinese: Need word segmentation before entity recognition

**2. 多义性问题 | Polysemy Issues**
```
"王建国" 可能是:
"Wang Jianguo" could be:
- 人名 (Person): 王建国先生今天来访
- 动词短语 (Verb phrase): 王建国家的希望
```

**3. 实体边界模糊 | Fuzzy Entity Boundaries**
```
"北京大学附属中学" 
"Beijing University Affiliated High School"
- 是一个机构名？| One organization name?
- 还是"北京大学" + "附属中学"？| Or "Beijing University" + "Affiliated High School"?
```

## 🧠 技术原理深度解析 | Deep Technical Principle Analysis

### BIO标注体系 | BIO Tagging System

BIO标注就像给每个字符贴上"身份标签"：
BIO tagging is like putting "identity labels" on each character:

- **B-PER**: 人名的开始字符 | Beginning of person name
- **I-PER**: 人名的内部字符 | Inside of person name  
- **B-LOC**: 地名的开始字符 | Beginning of location name
- **I-LOC**: 地名的内部字符 | Inside of location name
- **B-ORG**: 机构名的开始字符 | Beginning of organization name
- **I-ORG**: 机构名的内部字符 | Inside of organization name
- **O**: 不属于任何实体 | Outside any entity

**标注示例 | Tagging Example:**
```
输入文本: 马云创立了阿里巴巴公司
Input text: Ma Yun founded Alibaba Company

字符级标注:
Character-level tagging:
马  B-PER
云  I-PER  
创  O
立  O
了  O
阿  B-ORG
里  I-ORG
巴  I-ORG
巴  I-ORG
公  I-ORG
司  I-ORG
```

### BERT + CRF架构 | BERT + CRF Architecture

**为什么选择BERT + CRF组合？| Why Choose BERT + CRF Combination?**

1. **BERT提供语义理解** | **BERT Provides Semantic Understanding**
   - 理解上下文语义，如"苹果"在不同语境下的含义
   - Understand contextual semantics, like different meanings of "apple" in various contexts

2. **CRF确保标注一致性** | **CRF Ensures Tagging Consistency**
   - 避免出现"I-PER"后面直接跟"B-ORG"这样的无效序列
   - Avoid invalid sequences like "I-PER" directly followed by "B-ORG"

### 模型训练流程 | Model Training Process

**第一步：数据预处理 | Step 1: Data Preprocessing**
```python
def preprocess_chinese_text(text, entities):
    """
    中文文本预处理，将文本转换为字符级别的标注
    Chinese text preprocessing, convert text to character-level annotations
    """
    # 将文本转为字符列表
    # Convert text to character list
    chars = list(text)
    
    # 初始化所有字符为O标签
    # Initialize all characters with O label
    labels = ['O'] * len(chars)
    
    # 根据实体位置设置BIO标签
    # Set BIO labels based on entity positions
    for entity in entities:
        start, end, entity_type = entity['start'], entity['end'], entity['type']
        labels[start] = f'B-{entity_type}'
        for i in range(start + 1, end):
            labels[i] = f'I-{entity_type}'
    
    return chars, labels
```

**第二步：BERT特征提取 | Step 2: BERT Feature Extraction**
```python
# BERT将每个字符转换为768维的向量
# BERT converts each character to a 768-dimensional vector
input_text = "马云创立了阿里巴巴"
tokens = tokenizer(input_text, return_tensors='pt')
bert_output = bert_model(**tokens)

# 获取每个字符的向量表示
# Get vector representation for each character
char_embeddings = bert_output.last_hidden_state  # [batch_size, seq_len, 768]
```

**第三步：CRF序列标注 | Step 3: CRF Sequence Labeling**
```python
# CRF层确保标注序列的合理性
# CRF layer ensures rationality of tagging sequences
crf_scores = crf_layer(char_embeddings)
best_path = crf_layer.decode(crf_scores)  # 维特比解码找最优路径
```

## 💻 完整代码实现 | Complete Code Implementation

### 数据集处理类 | Dataset Processing Class
```python
import torch
from torch.utils.data import Dataset
import json

class ChineseNERDataset(Dataset):
    """
    中文NER数据集类
    Chinese NER dataset class
    """
    def __init__(self, data_path, tokenizer, max_length=128):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.data = self.load_data(data_path)
        
        # 标签映射
        # Label mapping
        self.label2id = {
            'O': 0, 'B-PER': 1, 'I-PER': 2,
            'B-LOC': 3, 'I-LOC': 4, 'B-ORG': 5, 'I-ORG': 6
        }
        self.id2label = {v: k for k, v in self.label2id.items()}
    
    def load_data(self, data_path):
        """加载训练数据 | Load training data"""
        with open(data_path, 'r', encoding='utf-8') as f:
            return [json.loads(line) for line in f]
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        text = item['text']
        entities = item.get('entities', [])
        
        # 字符级tokenization
        # Character-level tokenization
        chars = list(text)
        labels = ['O'] * len(chars)
        
        # 设置实体标签
        # Set entity labels
        for entity in entities:
            start, end, entity_type = entity['start'], entity['end'], entity['type']
            if start < len(chars):
                labels[start] = f'B-{entity_type}'
                for i in range(start + 1, min(end, len(chars))):
                    labels[i] = f'I-{entity_type}'
        
        # BERT编码
        # BERT encoding
        encoding = self.tokenizer(
            chars,
            is_split_into_words=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        # 标签对齐
        # Label alignment
        word_ids = encoding.word_ids()
        aligned_labels = []
        for word_id in word_ids:
            if word_id is None:
                aligned_labels.append(-100)  # 忽略特殊token
            else:
                aligned_labels.append(self.label2id[labels[word_id]])
        
        return {
            'input_ids': encoding['input_ids'].squeeze(),
            'attention_mask': encoding['attention_mask'].squeeze(),
            'labels': torch.tensor(aligned_labels, dtype=torch.long)
        }
```

### NER模型定义 | NER Model Definition
```python
import torch.nn as nn
from transformers import BertModel
from torchcrf import CRF

class ChineseNERModel(nn.Module):
    """
    中文命名实体识别模型
    Chinese Named Entity Recognition Model
    """
    def __init__(self, bert_model_name, num_labels, dropout_rate=0.1):
        super(ChineseNERModel, self).__init__()
        
        # BERT编码器
        # BERT encoder
        self.bert = BertModel.from_pretrained(bert_model_name)
        self.dropout = nn.Dropout(dropout_rate)
        
        # 分类层
        # Classification layer
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)
        
        # CRF层
        # CRF layer
        self.crf = CRF(num_labels, batch_first=True)
        
        self.num_labels = num_labels
    
    def forward(self, input_ids, attention_mask, labels=None):
        # BERT特征提取
        # BERT feature extraction
        bert_output = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask
        )
        
        # 获取序列输出并添加dropout
        # Get sequence output and add dropout
        sequence_output = self.dropout(bert_output.last_hidden_state)
        
        # 分类预测
        # Classification prediction
        logits = self.classifier(sequence_output)
        
        # 创建CRF掩码（忽略padding和特殊token）
        # Create CRF mask (ignore padding and special tokens)
        mask = attention_mask.bool()
        
        if labels is not None:
            # 训练模式：计算CRF损失
            # Training mode: calculate CRF loss
            # 过滤掉标签为-100的位置
            # Filter out positions with label -100
            active_loss = mask.view(-1)
            active_logits = logits.view(-1, self.num_labels)[active_loss]
            active_labels = labels.view(-1)[active_loss]
            
            # 重塑为CRF期望的形状
            # Reshape to CRF expected shape
            batch_size, seq_len = labels.shape
            crf_labels = torch.full_like(labels, -1)
            crf_logits = logits.clone()
            
            for i in range(batch_size):
                valid_length = mask[i].sum().item()
                valid_labels = labels[i][:valid_length]
                # 只保留非-100的标签
                valid_mask = valid_labels != -100
                if valid_mask.any():
                    crf_labels[i][:valid_mask.sum()] = valid_labels[valid_mask]
            
            # 计算CRF损失
            # Calculate CRF loss
            loss = -self.crf(crf_logits, crf_labels, mask=mask, reduction='mean')
            return loss, logits
        else:
            # 推理模式：CRF解码
            # Inference mode: CRF decoding
            predictions = self.crf.decode(logits, mask=mask)
            return predictions
```

### 训练脚本 | Training Script
```python
from torch.utils.data import DataLoader
from transformers import BertTokenizer, AdamW
from sklearn.metrics import classification_report
import numpy as np

def train_chinese_ner():
    """
    训练中文NER模型
    Train Chinese NER model
    """
    # 设置设备
    # Set device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # 加载tokenizer和数据
    # Load tokenizer and data
    tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
    
    train_dataset = ChineseNERDataset('train.jsonl', tokenizer)
    val_dataset = ChineseNERDataset('val.jsonl', tokenizer)
    
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)
    
    # 初始化模型
    # Initialize model
    model = ChineseNERModel('bert-base-chinese', num_labels=7)
    model.to(device)
    
    # 优化器设置
    # Optimizer setup
    optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)
    
    # 训练循环
    # Training loop
    model.train()
    for epoch in range(3):
        total_loss = 0
        for batch in train_loader:
            # 数据转移到设备
            # Move data to device
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            
            # 前向传播
            # Forward pass
            optimizer.zero_grad()
            loss, _ = model(input_ids, attention_mask, labels)
            
            # 反向传播
            # Backward pass
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
        
        print(f"Epoch {epoch + 1}/3, Average Loss: {total_loss / len(train_loader):.4f}")
        
        # 验证
        # Validation
        evaluate_model(model, val_loader, train_dataset.id2label, device)

def evaluate_model(model, val_loader, id2label, device):
    """
    评估模型性能
    Evaluate model performance
    """
    model.eval()
    all_predictions = []
    all_labels = []
    
    with torch.no_grad():
        for batch in val_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            
            # 获取预测
            # Get predictions
            predictions = model(input_ids, attention_mask)
            
            # 处理批次中的每个样本
            # Process each sample in batch
            for i, pred_seq in enumerate(predictions):
                # 获取有效长度
                # Get valid length
                valid_length = attention_mask[i].sum().item()
                
                # 转换预测和真实标签
                # Convert predictions and true labels
                pred_labels = [id2label[p] for p in pred_seq[:valid_length-2]]  # 去除[CLS]和[SEP]
                true_labels = [id2label[l.item()] for l in labels[i][1:valid_length-1] if l.item() != -100]
                
                all_predictions.append(pred_labels)
                all_labels.append(true_labels)
    
    # 使用seqeval计算F1分数
    # Calculate F1 score using seqeval
    from seqeval.metrics import classification_report, f1_score
    
    print("分类报告 | Classification Report:")
    print(classification_report(all_labels, all_predictions, digits=4))
    
    f1 = f1_score(all_labels, all_predictions)
    print(f"整体F1分数 | Overall F1 Score: {f1:.4f}")
```

## 🔍 实体识别示例 | Entity Recognition Examples

### 新闻文本处理 | News Text Processing
```python
def process_news_text():
    """
    处理新闻文本示例
    Process news text example
    """
    news_text = """
    据央视新闻报道，国家领导人小芳今日在北京人民大会堂会见了美国总统拜登。
    双方就中美关系发展、气候变化等问题深入交换了意见。
    """
    
    # 预期识别结果
    # Expected recognition results
    expected_entities = [
        ("央视新闻", "ORG"),
        ("小芳", "PER"), 
        ("北京人民大会堂", "LOC"),
        ("美国", "LOC"),
        ("拜登", "PER")
    ]
    
    print("新闻文本实体识别 | News Text Entity Recognition:")
    print(f"输入文本 | Input Text: {news_text}")
    print("识别的实体 | Recognized Entities:")
    for entity, entity_type in expected_entities:
        print(f"  {entity} -> {entity_type}")
```

### 商业文档处理 | Business Document Processing
```python
def process_business_text():
    """
    处理商业文档示例  
    Process business document example
    """
    business_text = """
    腾讯科技有限公司与阿里巴巴集团于2023年12月在深圳签署战略合作协议。
    该协议由腾讯CEO马化腾和阿里巴巴董事长张勇共同签署。
    """
    
    expected_entities = [
        ("腾讯科技有限公司", "ORG"),
        ("阿里巴巴集团", "ORG"),
        ("深圳", "LOC"),
        ("马化腾", "PER"),
        ("张勇", "PER")
    ]
    
    print("商业文档实体识别 | Business Document Entity Recognition:")
    print(f"输入文本 | Input Text: {business_text}")
    print("识别的实体 | Recognized Entities:")
    for entity, entity_type in expected_entities:
        print(f"  {entity} -> {entity_type}")
```

## 🚀 模型优化技巧 | Model Optimization Tips

### 数据增强策略 | Data Augmentation Strategies

**1. 实体替换 | Entity Substitution**
```python
def entity_substitution_augmentation(text, entities):
    """
    实体替换数据增强
    Entity substitution data augmentation
    """
    # 构建实体词典
    # Build entity dictionary
    entity_dict = {
        'PER': ['张三', '李四', '王五', '赵六'],
        'LOC': ['北京', '上海', '广州', '深圳'], 
        'ORG': ['清华大学', '北京大学', '复旦大学', '浙江大学']
    }
    
    augmented_texts = []
    
    for entity in entities:
        original_text = entity['text']
        entity_type = entity['type']
        
        # 随机选择同类型实体进行替换
        # Randomly select same-type entity for replacement
        if entity_type in entity_dict:
            for replacement in entity_dict[entity_type]:
                new_text = text.replace(original_text, replacement)
                augmented_texts.append(new_text)
    
    return augmented_texts
```

**2. 上下文扰动 | Context Perturbation**
```python
def context_perturbation(text, entities):
    """
    上下文扰动增强
    Context perturbation augmentation
    """
    import random
    
    # 同义词替换词典
    # Synonym replacement dictionary
    synonyms = {
        '访问': ['拜访', '探访', '造访'],
        '会见': ['接见', '会面', '见面'],
        '签署': ['签订', '签约', '签定']
    }
    
    words = text.split()
    augmented_words = words.copy()
    
    for i, word in enumerate(words):
        if word in synonyms and random.random() < 0.3:
            augmented_words[i] = random.choice(synonyms[word])
    
    return ''.join(augmented_words)
```

### 模型集成技术 | Model Ensemble Techniques

**1. 多模型投票 | Multi-Model Voting**
```python
class NERModelEnsemble:
    """
    NER模型集成类
    NER model ensemble class
    """
    def __init__(self, models):
        self.models = models
    
    def predict(self, text):
        """
        集成预测方法
        Ensemble prediction method
        """
        all_predictions = []
        
        # 获取所有模型的预测
        # Get predictions from all models
        for model in self.models:
            pred = model.predict(text)
            all_predictions.append(pred)
        
        # 投票决定最终结果
        # Vote for final result
        final_prediction = self.vote_predictions(all_predictions)
        return final_prediction
    
    def vote_predictions(self, predictions):
        """
        投票机制
        Voting mechanism
        """
        from collections import Counter
        
        final_tags = []
        max_len = max(len(pred) for pred in predictions)
        
        for i in range(max_len):
            # 收集该位置所有模型的预测
            # Collect predictions from all models at this position
            position_votes = []
            for pred in predictions:
                if i < len(pred):
                    position_votes.append(pred[i])
            
            # 多数投票
            # Majority voting
            if position_votes:
                most_common = Counter(position_votes).most_common(1)[0][0]
                final_tags.append(most_common)
        
        return final_tags
```

## 📊 性能评估指标 | Performance Evaluation Metrics

### 实体级别评估 | Entity-Level Evaluation
```python
def calculate_entity_metrics(true_entities, pred_entities):
    """
    计算实体级别的评估指标
    Calculate entity-level evaluation metrics
    """
    # 提取实体集合
    # Extract entity sets
    true_set = set((entity['start'], entity['end'], entity['type']) for entity in true_entities)
    pred_set = set((entity['start'], entity['end'], entity['type']) for entity in pred_entities)
    
    # 计算精确率、召回率、F1分数
    # Calculate precision, recall, F1 score
    tp = len(true_set & pred_set)  # 真正例 True Positives
    fp = len(pred_set - true_set)  # 假正例 False Positives  
    fn = len(true_set - pred_set)  # 假负例 False Negatives
    
    precision = tp / (tp + fp) if tp + fp > 0 else 0
    recall = tp / (tp + fn) if tp + fn > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0
    
    return {
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'true_positives': tp,
        'false_positives': fp,
        'false_negatives': fn
    }

def detailed_error_analysis(true_entities, pred_entities):
    """
    详细错误分析
    Detailed error analysis
    """
    errors = {
        'missing_entities': [],      # 漏检实体 Missing entities
        'false_entities': [],        # 误检实体 False entities
        'boundary_errors': [],       # 边界错误 Boundary errors
        'type_errors': []           # 类型错误 Type errors
    }
    
    true_positions = {(e['start'], e['end']): e for e in true_entities}
    pred_positions = {(e['start'], e['end']): e for e in pred_entities}
    
    # 分析各种错误类型
    # Analyze various error types
    for pos, true_entity in true_positions.items():
        if pos not in pred_positions:
            errors['missing_entities'].append(true_entity)
        else:
            pred_entity = pred_positions[pos]
            if true_entity['type'] != pred_entity['type']:
                errors['type_errors'].append({
                    'text': true_entity['text'],
                    'true_type': true_entity['type'],
                    'pred_type': pred_entity['type']
                })
    
    for pos, pred_entity in pred_positions.items():
        if pos not in true_positions:
            errors['false_entities'].append(pred_entity)
    
    return errors
```

## 🎓 学习建议 | Learning Recommendations

### 实践步骤 | Practice Steps

**第1天：环境搭建** | **Day 1: Environment Setup**
1. 安装必要的Python包：transformers, torch, seqeval
2. Install necessary Python packages: transformers, torch, seqeval
3. 下载中文BERT预训练模型
4. Download Chinese BERT pretrained model

**第2-3天：数据理解** | **Day 2-3: Data Understanding**
1. 分析中文NER数据集的特点
2. Analyze characteristics of Chinese NER datasets
3. 理解BIO标注体系
4. Understand BIO tagging system

**第4-5天：模型实现** | **Day 4-5: Model Implementation**
1. 实现BERT+CRF模型
2. Implement BERT+CRF model
3. 编写训练和评估代码
4. Write training and evaluation code

**第6-7天：实验优化** | **Day 6-7: Experiment Optimization**
1. 尝试不同的超参数设置
2. Try different hyperparameter settings
3. 实现数据增强技术
4. Implement data augmentation techniques

### 常见问题解答 | Frequently Asked Questions

**Q1: 为什么要使用字符级别而不是词级别的tokenization？**
**Q1: Why use character-level instead of word-level tokenization?**

A: 中文没有明确的词边界，字符级别可以避免分词错误对NER的影响，同时能更好地处理未登录词。
A: Chinese has no clear word boundaries. Character-level can avoid the impact of word segmentation errors on NER and better handle out-of-vocabulary words.

**Q2: CRF层的作用是什么？**
**Q2: What is the role of the CRF layer?**

A: CRF确保输出标签序列的合理性，避免出现"I-PER后面跟B-ORG"这样的无效转移。
A: CRF ensures the rationality of output label sequences, avoiding invalid transitions like "I-PER followed by B-ORG".

**Q3: 如何处理嵌套实体？**
**Q3: How to handle nested entities?**

A: 可以使用多层标注或者片段排名的方法，例如"北京大学计算机学院"中的"北京大学"和"计算机学院"。
A: You can use multi-layer annotation or span ranking methods, for example, "Beijing University" and "Computer Science College" in "Beijing University Computer Science College".

通过这个中文NER系统项目，你将深入理解序列标注任务的核心技术，掌握BERT和CRF的组合使用，为后续的NLP应用奠定坚实基础！

Through this Chinese NER system project, you will deeply understand the core technologies of sequence labeling tasks, master the combined use of BERT and CRF, and lay a solid foundation for subsequent NLP applications! 