# Word2Vecè¯¦ç»†å®ç°æŒ‡å—
# Word2Vec Detailed Implementation Guide

**è¯å‘é‡é©å‘½çš„èµ·ç‚¹ - è®©è¯è¯­æ‹¥æœ‰æ•°å­¦è¡¨ç¤º**
**The Starting Point of Word Vector Revolution - Giving Words Mathematical Representation**

---

## ğŸ¯ é¡¹ç›®æ¦‚è¿° | Project Overview

Word2Vecä¸ä»…ä»…æ˜¯ä¸€ä¸ªç®—æ³•ï¼Œå®ƒæ˜¯NLPå†å²çš„è½¬æŠ˜ç‚¹ï¼2013å¹´ï¼ŒGoogleçš„Tomas Mikolovå›¢é˜Ÿæå‡ºäº†è¿™ä¸ªé©å‘½æ€§çš„æ–¹æ³•ï¼Œç¬¬ä¸€æ¬¡è®©è®¡ç®—æœºçœŸæ­£"ç†è§£"äº†è¯è¯­ä¹‹é—´çš„è¯­ä¹‰å…³ç³»ã€‚

Word2Vec is not just an algorithm, it's a turning point in NLP history! In 2013, Google's Tomas Mikolov team proposed this revolutionary method, making computers truly "understand" semantic relationships between words for the first time.

### æ ¸å¿ƒæ´å¯Ÿ | Core Insights
- **åˆ†å¸ƒå¼å‡è®¾**: è¯è¯­çš„å«ä¹‰ç”±å…¶ä¸Šä¸‹æ–‡å†³å®š
- **Distributional Hypothesis**: A word's meaning is determined by its context
- **ä½ç»´è¡¨ç¤º**: å°†é«˜ç»´ç¨€ç–çš„one-hotå‘é‡è½¬æ¢ä¸ºä½ç»´ç¨ å¯†å‘é‡
- **Low-dimensional Representation**: Transform high-dimensional sparse one-hot vectors to low-dimensional dense vectors
- **è¯­ä¹‰å…³ç³»**: å‘é‡è¿ç®—èƒ½å¤Ÿæ•æ‰è¯è¯­é—´çš„è¯­ä¹‰å…³ç³»
- **Semantic Relations**: Vector operations can capture semantic relationships between words

## ğŸ§  æ·±åº¦ç†è®ºè§£æ | Deep Theoretical Analysis

### åˆ†å¸ƒå¼å‡è®¾çš„æ•°å­¦è¡¨ç¤º | Mathematical Representation of Distributional Hypothesis

**æ ¸å¿ƒæ€æƒ³ | Core Idea:**
å¦‚æœä¸¤ä¸ªè¯ç»å¸¸å‡ºç°åœ¨ç›¸ä¼¼çš„ä¸Šä¸‹æ–‡ä¸­ï¼Œé‚£ä¹ˆå®ƒä»¬åœ¨è¯­ä¹‰ä¸Šæ˜¯ç›¸å…³çš„ã€‚

If two words often appear in similar contexts, they are semantically related.

**æ•°å­¦è¡¨è¾¾ | Mathematical Expression:**
```
P(w_o | w_c) = exp(u_o^T v_c) / Î£ exp(u_w^T v_c)
```

å…¶ä¸­ï¼š
- `w_c`: ä¸­å¿ƒè¯ | center word
- `w_o`: ä¸Šä¸‹æ–‡è¯ | context word  
- `v_c`: ä¸­å¿ƒè¯å‘é‡ | center word vector
- `u_o`: ä¸Šä¸‹æ–‡è¯å‘é‡ | context word vector

### Skip-gram vs CBOWæ¶æ„å¯¹æ¯” | Skip-gram vs CBOW Architecture Comparison

#### Skip-gramæ¨¡å‹ | Skip-gram Model
```
è¾“å…¥: ä¸­å¿ƒè¯ â†’ è¾“å‡º: ä¸Šä¸‹æ–‡è¯
Input: Center word â†’ Output: Context words

ä¾‹å­: "I love [deep] learning AI"
Example: "I love [deep] learning AI"

ç»™å®š"deep"ï¼Œé¢„æµ‹"I", "love", "learning", "AI"
Given "deep", predict "I", "love", "learning", "AI"
```

#### CBOWæ¨¡å‹ | CBOW Model  
```
è¾“å…¥: ä¸Šä¸‹æ–‡è¯ â†’ è¾“å‡º: ä¸­å¿ƒè¯
Input: Context words â†’ Output: Center word

ä¾‹å­: "I love [?] learning AI"
Example: "I love [?] learning AI"

ç»™å®š"I", "love", "learning", "AI"ï¼Œé¢„æµ‹"deep"
Given "I", "love", "learning", "AI", predict "deep"
```

## ğŸ› ï¸ å®Œæ•´å®ç°ä»£ç  | Complete Implementation Code

### ç¬¬ä¸€æ­¥: æ•°æ®é¢„å¤„ç† | Step 1: Data Preprocessing

```python
import numpy as np
import pandas as pd
from collections import Counter, defaultdict
import random
import math
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.manifold import TSNE
import torch
import torch.nn as nn
import torch.optim as optim

class Word2VecDataset:
    """
    Word2Vecæ•°æ®é¢„å¤„ç†ç±»
    Word2Vec Data Preprocessing Class
    """
    def __init__(self, min_count=5, window_size=5):
        self.min_count = min_count
        self.window_size = window_size
        self.word2idx = {}
        self.idx2word = {}
        self.word_counts = Counter()
        self.vocab_size = 0
        
    def build_vocab(self, sentences):
        """
        æ„å»ºè¯æ±‡è¡¨
        Build vocabulary
        """
        print("Building vocabulary...")
        
        # ç»Ÿè®¡è¯é¢‘ | Count word frequencies
        for sentence in tqdm(sentences):
            words = sentence.lower().split()
            self.word_counts.update(words)
        
        # è¿‡æ»¤ä½é¢‘è¯ | Filter low-frequency words
        filtered_words = {word: count for word, count in self.word_counts.items() 
                         if count >= self.min_count}
        
        # åˆ›å»ºè¯æ±‡æ˜ å°„ | Create vocabulary mapping
        vocab = list(filtered_words.keys())
        self.vocab_size = len(vocab)
        
        self.word2idx = {word: idx for idx, word in enumerate(vocab)}
        self.idx2word = {idx: word for word, idx in self.word2idx.items()}
        
        print(f"Vocabulary size: {self.vocab_size}")
        return vocab
    
    def create_skipgram_pairs(self, sentences):
        """
        åˆ›å»ºSkip-gramè®­ç»ƒå¯¹
        Create Skip-gram training pairs
        """
        print("Creating Skip-gram pairs...")
        
        pairs = []
        for sentence in tqdm(sentences):
            words = [word for word in sentence.lower().split() 
                    if word in self.word2idx]
            
            for i, center_word in enumerate(words):
                # å®šä¹‰ä¸Šä¸‹æ–‡çª—å£ | Define context window
                start = max(0, i - self.window_size)
                end = min(len(words), i + self.window_size + 1)
                
                for j in range(start, end):
                    if i != j:  # è·³è¿‡ä¸­å¿ƒè¯æœ¬èº« | Skip center word itself
                        context_word = words[j]
                        center_idx = self.word2idx[center_word]
                        context_idx = self.word2idx[context_word]
                        pairs.append((center_idx, context_idx))
        
        print(f"Generated {len(pairs)} training pairs")
        return pairs
    
    def create_cbow_pairs(self, sentences):
        """
        åˆ›å»ºCBOWè®­ç»ƒå¯¹
        Create CBOW training pairs
        """
        print("Creating CBOW pairs...")
        
        pairs = []
        for sentence in tqdm(sentences):
            words = [word for word in sentence.lower().split() 
                    if word in self.word2idx]
            
            for i, center_word in enumerate(words):
                # æ”¶é›†ä¸Šä¸‹æ–‡è¯ | Collect context words
                start = max(0, i - self.window_size)
                end = min(len(words), i + self.window_size + 1)
                
                context_words = []
                for j in range(start, end):
                    if i != j:
                        context_words.append(self.word2idx[words[j]])
                
                if len(context_words) > 0:
                    center_idx = self.word2idx[center_word]
                    pairs.append((context_words, center_idx))
        
        print(f"Generated {len(pairs)} training pairs")
        return pairs
```

### ç¬¬äºŒæ­¥: Skip-gramæ¨¡å‹å®ç° | Step 2: Skip-gram Model Implementation

```python
class SkipGramModel(nn.Module):
    """
    Skip-gramæ¨¡å‹å®ç°
    Skip-gram Model Implementation
    """
    def __init__(self, vocab_size, embedding_dim=100):
        super(SkipGramModel, self).__init__()
        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim
        
        # ä¸­å¿ƒè¯åµŒå…¥çŸ©é˜µ | Center word embedding matrix
        self.center_embeddings = nn.Embedding(vocab_size, embedding_dim)
        
        # ä¸Šä¸‹æ–‡è¯åµŒå…¥çŸ©é˜µ | Context word embedding matrix
        self.context_embeddings = nn.Embedding(vocab_size, embedding_dim)
        
        # åˆå§‹åŒ–æƒé‡ | Initialize weights
        self.init_weights()
    
    def init_weights(self):
        """
        åˆå§‹åŒ–åµŒå…¥æƒé‡
        Initialize embedding weights
        """
        initrange = 0.5 / self.embedding_dim
        self.center_embeddings.weight.data.uniform_(-initrange, initrange)
        self.context_embeddings.weight.data.uniform_(-initrange, initrange)
    
    def forward(self, center_words, context_words):
        """
        å‰å‘ä¼ æ’­
        Forward pass
        """
        # è·å–ä¸­å¿ƒè¯åµŒå…¥ | Get center word embeddings
        center_embeds = self.center_embeddings(center_words)  # [batch_size, embed_dim]
        
        # è·å–ä¸Šä¸‹æ–‡è¯åµŒå…¥ | Get context word embeddings
        context_embeds = self.context_embeddings(context_words)  # [batch_size, embed_dim]
        
        # è®¡ç®—å†…ç§¯ | Compute dot product
        scores = torch.sum(center_embeds * context_embeds, dim=1)  # [batch_size]
        
        return scores
    
    def get_word_embedding(self, word_idx):
        """
        è·å–è¯åµŒå…¥å‘é‡
        Get word embedding vector
        """
        return self.center_embeddings.weight[word_idx].detach().numpy()
```

### ç¬¬ä¸‰æ­¥: è´Ÿé‡‡æ ·ä¼˜åŒ– | Step 3: Negative Sampling Optimization

```python
class NegativeSamplingLoss(nn.Module):
    """
    è´Ÿé‡‡æ ·æŸå¤±å‡½æ•°
    Negative Sampling Loss Function
    """
    def __init__(self, vocab_size, num_negative_samples=5):
        super(NegativeSamplingLoss, self).__init__()
        self.vocab_size = vocab_size
        self.num_negative_samples = num_negative_samples
        
        # åˆ›å»ºè´Ÿé‡‡æ ·åˆ†å¸ƒ | Create negative sampling distribution
        self.noise_distribution = torch.ones(vocab_size)
        
    def set_noise_distribution(self, word_counts):
        """
        è®¾ç½®å™ªå£°åˆ†å¸ƒï¼ˆåŸºäºè¯é¢‘çš„3/4æ¬¡æ–¹ï¼‰
        Set noise distribution (based on word frequency to the power of 3/4)
        """
        counts = torch.FloatTensor(word_counts)
        self.noise_distribution = torch.pow(counts, 0.75)
        self.noise_distribution = self.noise_distribution / self.noise_distribution.sum()
    
    def forward(self, positive_scores, center_words, context_words):
        """
        è®¡ç®—è´Ÿé‡‡æ ·æŸå¤±
        Compute negative sampling loss
        """
        batch_size = positive_scores.size(0)
        
        # æ­£æ ·æœ¬æŸå¤± | Positive sample loss
        positive_loss = -torch.log(torch.sigmoid(positive_scores)).mean()
        
        # è´Ÿæ ·æœ¬é‡‡æ · | Negative sample sampling
        negative_words = torch.multinomial(
            self.noise_distribution, 
            batch_size * self.num_negative_samples, 
            replacement=True
        ).view(batch_size, self.num_negative_samples)
        
        # è®¡ç®—è´Ÿæ ·æœ¬åˆ†æ•° | Compute negative sample scores
        center_embeds = model.center_embeddings(center_words).unsqueeze(1)  # [batch, 1, embed]
        negative_embeds = model.context_embeddings(negative_words)  # [batch, neg_samples, embed]
        
        negative_scores = torch.sum(center_embeds * negative_embeds, dim=2)  # [batch, neg_samples]
        
        # è´Ÿæ ·æœ¬æŸå¤± | Negative sample loss
        negative_loss = -torch.log(torch.sigmoid(-negative_scores)).mean()
        
        return positive_loss + negative_loss

def create_negative_samples(positive_pairs, vocab_size, num_negative=5):
    """
    ä¸ºæ¯ä¸ªæ­£æ ·æœ¬åˆ›å»ºè´Ÿæ ·æœ¬
    Create negative samples for each positive sample
    """
    negative_pairs = []
    
    for center_word, context_word in positive_pairs:
        # ä¸ºæ¯ä¸ªæ­£æ ·æœ¬ç”Ÿæˆå¤šä¸ªè´Ÿæ ·æœ¬
        # Generate multiple negative samples for each positive sample
        for _ in range(num_negative):
            # éšæœºé€‰æ‹©ä¸€ä¸ªè¯ä½œä¸ºè´Ÿæ ·æœ¬
            # Randomly select a word as negative sample
            negative_word = random.randint(0, vocab_size - 1)
            # ç¡®ä¿è´Ÿæ ·æœ¬ä¸æ˜¯çœŸå®çš„ä¸Šä¸‹æ–‡è¯
            # Ensure negative sample is not the actual context word
            while negative_word == context_word:
                negative_word = random.randint(0, vocab_size - 1)
            
            negative_pairs.append((center_word, negative_word, 0))  # æ ‡ç­¾ä¸º0è¡¨ç¤ºè´Ÿæ ·æœ¬
    
    return negative_pairs
```

### ç¬¬å››æ­¥: è®­ç»ƒå¾ªç¯ | Step 4: Training Loop

```python
def train_word2vec(sentences, embedding_dim=100, window_size=5, 
                   min_count=5, num_epochs=5, learning_rate=0.001,
                   batch_size=512, num_negative_samples=5):
    """
    Word2Vecè®­ç»ƒä¸»å‡½æ•°
    Word2Vec Training Main Function
    """
    
    # 1. æ•°æ®é¢„å¤„ç† | Data preprocessing
    dataset = Word2VecDataset(min_count=min_count, window_size=window_size)
    vocab = dataset.build_vocab(sentences)
    
    # 2. åˆ›å»ºè®­ç»ƒæ•°æ® | Create training data
    skipgram_pairs = dataset.create_skipgram_pairs(sentences)
    
    # 3. åˆå§‹åŒ–æ¨¡å‹ | Initialize model
    model = SkipGramModel(dataset.vocab_size, embedding_dim)
    criterion = NegativeSamplingLoss(dataset.vocab_size, num_negative_samples)
    
    # è®¾ç½®å™ªå£°åˆ†å¸ƒ | Set noise distribution
    word_counts = [dataset.word_counts[word] for word in vocab]
    criterion.set_noise_distribution(word_counts)
    
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    
    # 4. è®­ç»ƒå¾ªç¯ | Training loop
    model.train()
    total_loss = 0
    
    for epoch in range(num_epochs):
        print(f"\nEpoch {epoch + 1}/{num_epochs}")
        
        # æ‰“ä¹±æ•°æ® | Shuffle data
        random.shuffle(skipgram_pairs)
        
        epoch_loss = 0
        num_batches = len(skipgram_pairs) // batch_size
        
        for i in tqdm(range(0, len(skipgram_pairs), batch_size)):
            batch_pairs = skipgram_pairs[i:i + batch_size]
            
            if len(batch_pairs) < batch_size:
                continue
            
            # å‡†å¤‡æ‰¹æ¬¡æ•°æ® | Prepare batch data
            center_words = torch.LongTensor([pair[0] for pair in batch_pairs])
            context_words = torch.LongTensor([pair[1] for pair in batch_pairs])
            
            # å‰å‘ä¼ æ’­ | Forward pass
            positive_scores = model(center_words, context_words)
            
            # è®¡ç®—æŸå¤± | Compute loss
            loss = criterion(positive_scores, center_words, context_words)
            
            # åå‘ä¼ æ’­ | Backward pass
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()
        
        avg_loss = epoch_loss / num_batches
        print(f"Average Loss: {avg_loss:.4f}")
        total_loss += avg_loss
    
    print(f"\nTraining completed! Average total loss: {total_loss/num_epochs:.4f}")
    
    return model, dataset

# ä½¿ç”¨ç¤ºä¾‹ | Usage Example
if __name__ == "__main__":
    # ç¤ºä¾‹è¯­æ–™ | Example corpus
    sentences = [
        "I love deep learning and natural language processing",
        "Machine learning is a subset of artificial intelligence",
        "Neural networks are inspired by biological neurons",
        "Word embeddings capture semantic relationships between words",
        "BERT and GPT are powerful language models",
        # æ·»åŠ æ›´å¤šå¥å­...
        # Add more sentences...
    ]
    
    # è®­ç»ƒæ¨¡å‹ | Train model
    model, dataset = train_word2vec(
        sentences=sentences,
        embedding_dim=100,
        window_size=5,
        min_count=1,  # ç”±äºç¤ºä¾‹æ•°æ®è¾ƒå°‘ï¼Œé™ä½æœ€å°é¢‘æ¬¡
        num_epochs=10,
        learning_rate=0.001,
        batch_size=32
    )
```

### ç¬¬äº”æ­¥: æ¨¡å‹è¯„ä¼°ä¸å¯è§†åŒ– | Step 5: Model Evaluation and Visualization

```python
class Word2VecEvaluator:
    """
    Word2Vecæ¨¡å‹è¯„ä¼°å™¨
    Word2Vec Model Evaluator
    """
    def __init__(self, model, dataset):
        self.model = model
        self.dataset = dataset
        self.word_vectors = self._extract_word_vectors()
    
    def _extract_word_vectors(self):
        """
        æå–æ‰€æœ‰è¯çš„å‘é‡è¡¨ç¤º
        Extract vector representations of all words
        """
        word_vectors = {}
        with torch.no_grad():
            for word, idx in self.dataset.word2idx.items():
                vector = self.model.get_word_embedding(idx)
                word_vectors[word] = vector
        return word_vectors
    
    def cosine_similarity(self, word1, word2):
        """
        è®¡ç®—ä¸¤ä¸ªè¯çš„ä½™å¼¦ç›¸ä¼¼åº¦
        Compute cosine similarity between two words
        """
        if word1 not in self.word_vectors or word2 not in self.word_vectors:
            return None
        
        vec1 = self.word_vectors[word1]
        vec2 = self.word_vectors[word2]
        
        cos_sim = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))
        return cos_sim
    
    def find_most_similar(self, word, top_k=5):
        """
        æ‰¾åˆ°ä¸ç»™å®šè¯æœ€ç›¸ä¼¼çš„è¯
        Find most similar words to a given word
        """
        if word not in self.word_vectors:
            return []
        
        target_vector = self.word_vectors[word]
        similarities = []
        
        for other_word, other_vector in self.word_vectors.items():
            if other_word != word:
                similarity = self.cosine_similarity(word, other_word)
                similarities.append((other_word, similarity))
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº | Sort by similarity
        similarities.sort(key=lambda x: x[1], reverse=True)
        return similarities[:top_k]
    
    def word_analogy(self, word_a, word_b, word_c, top_k=1):
        """
        è¯ç±»æ¯”ä»»åŠ¡: word_a is to word_b as word_c is to ?
        Word analogy task: word_a is to word_b as word_c is to ?
        
        ä¾‹å¦‚: king - man + woman = queen
        Example: king - man + woman = queen
        """
        if not all(word in self.word_vectors for word in [word_a, word_b, word_c]):
            return []
        
        # è®¡ç®—ç±»æ¯”å‘é‡ | Compute analogy vector
        vec_a = self.word_vectors[word_a]
        vec_b = self.word_vectors[word_b]
        vec_c = self.word_vectors[word_c]
        
        target_vector = vec_b - vec_a + vec_c
        
        # æ‰¾åˆ°æœ€æ¥è¿‘çš„è¯ | Find closest words
        similarities = []
        for word, vector in self.word_vectors.items():
            if word not in [word_a, word_b, word_c]:
                similarity = np.dot(target_vector, vector) / (
                    np.linalg.norm(target_vector) * np.linalg.norm(vector)
                )
                similarities.append((word, similarity))
        
        similarities.sort(key=lambda x: x[1], reverse=True)
        return similarities[:top_k]
    
    def visualize_embeddings(self, words=None, perplexity=30, n_iter=1000):
        """
        ä½¿ç”¨t-SNEå¯è§†åŒ–è¯åµŒå…¥
        Visualize word embeddings using t-SNE
        """
        if words is None:
            words = list(self.word_vectors.keys())[:50]  # å¯è§†åŒ–å‰50ä¸ªè¯
        
        # å‡†å¤‡æ•°æ® | Prepare data
        vectors = [self.word_vectors[word] for word in words if word in self.word_vectors]
        valid_words = [word for word in words if word in self.word_vectors]
        
        if len(vectors) < 2:
            print("Not enough vectors to visualize")
            return
        
        # t-SNEé™ç»´ | t-SNE dimensionality reduction
        tsne = TSNE(n_components=2, perplexity=min(perplexity, len(vectors)-1), 
                   n_iter=n_iter, random_state=42)
        vectors_2d = tsne.fit_transform(np.array(vectors))
        
        # ç»˜å›¾ | Plot
        plt.figure(figsize=(12, 8))
        plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1], alpha=0.7)
        
        # æ·»åŠ è¯æ ‡ç­¾ | Add word labels
        for i, word in enumerate(valid_words):
            plt.annotate(word, (vectors_2d[i, 0], vectors_2d[i, 1]), 
                        xytext=(5, 2), textcoords='offset points', 
                        ha='left', va='bottom', fontsize=10)
        
        plt.title('Word2Vec Embeddings Visualization (t-SNE)')
        plt.xlabel('Dimension 1')
        plt.ylabel('Dimension 2')
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()
    
    def evaluate_word_similarity(self, word_pairs_with_scores):
        """
        è¯„ä¼°è¯ç›¸ä¼¼åº¦ä»»åŠ¡
        Evaluate word similarity task
        
        word_pairs_with_scores: [(word1, word2, human_score), ...]
        """
        model_scores = []
        human_scores = []
        
        for word1, word2, human_score in word_pairs_with_scores:
            model_score = self.cosine_similarity(word1, word2)
            if model_score is not None:
                model_scores.append(model_score)
                human_scores.append(human_score)
        
        if len(model_scores) == 0:
            return 0.0
        
        # è®¡ç®—æ–¯çš®å°”æ›¼ç›¸å…³ç³»æ•° | Compute Spearman correlation
        from scipy.stats import spearmanr
        correlation, p_value = spearmanr(human_scores, model_scores)
        
        return correlation, p_value

# è¯„ä¼°ç¤ºä¾‹ | Evaluation Example
def evaluate_model():
    """
    æ¨¡å‹è¯„ä¼°ç¤ºä¾‹
    Model evaluation example
    """
    # å‡è®¾å·²ç»è®­ç»ƒå¥½äº†æ¨¡å‹ | Assume model is already trained
    evaluator = Word2VecEvaluator(model, dataset)
    
    # 1. è¯ç›¸ä¼¼åº¦æµ‹è¯• | Word similarity test
    print("=== Word Similarity Test ===")
    test_pairs = [
        ("deep", "learning"),
        ("machine", "artificial"),
        ("neural", "networks"),
        ("language", "processing")
    ]
    
    for word1, word2 in test_pairs:
        similarity = evaluator.cosine_similarity(word1, word2)
        if similarity is not None:
            print(f"Similarity({word1}, {word2}): {similarity:.4f}")
    
    # 2. æœ€ç›¸ä¼¼è¯æŸ¥æ‰¾ | Most similar words search
    print("\n=== Most Similar Words ===")
    test_words = ["learning", "neural", "language"]
    for word in test_words:
        if word in evaluator.word_vectors:
            similar_words = evaluator.find_most_similar(word, top_k=3)
            print(f"Most similar to '{word}': {similar_words}")
    
    # 3. è¯ç±»æ¯”æµ‹è¯• | Word analogy test
    print("\n=== Word Analogy Test ===")
    analogy_tests = [
        ("deep", "learning", "machine"),  # deep learning vs machine ?
        ("neural", "networks", "natural")  # neural networks vs natural ?
    ]
    
    for word_a, word_b, word_c in analogy_tests:
        result = evaluator.word_analogy(word_a, word_b, word_c, top_k=3)
        if result:
            print(f"{word_a}:{word_b} :: {word_c}:{result[0][0]} (score: {result[0][1]:.4f})")
    
    # 4. å¯è§†åŒ–è¯åµŒå…¥ | Visualize embeddings
    print("\n=== Visualizing Embeddings ===")
    important_words = ["deep", "learning", "machine", "neural", "networks", 
                      "language", "processing", "artificial", "intelligence"]
    evaluator.visualize_embeddings(words=important_words)

if __name__ == "__main__":
    evaluate_model()
```

## ğŸ¨ é«˜çº§åº”ç”¨ä¸æ‰©å±• | Advanced Applications and Extensions

### 1. å­è¯åµŒå…¥ (Subword Embeddings)

```python
class SubwordEmbedding:
    """
    å­è¯åµŒå…¥å®ç°ï¼Œå¤„ç†æœªç™»å½•è¯é—®é¢˜
    Subword embedding implementation to handle OOV words
    """
    def __init__(self, min_n=3, max_n=6):
        self.min_n = min_n
        self.max_n = max_n
        self.char_ngrams = {}
    
    def get_char_ngrams(self, word):
        """
        è·å–å•è¯çš„å­—ç¬¦n-gram
        Get character n-grams of a word
        """
        word = f"<{word}>"  # æ·»åŠ è¾¹ç•Œç¬¦å·
        ngrams = []
        
        for n in range(self.min_n, min(len(word), self.max_n) + 1):
            for i in range(len(word) - n + 1):
                ngrams.append(word[i:i+n])
        
        return ngrams
    
    def get_word_vector(self, word, word2vec_model):
        """
        é€šè¿‡å­è¯å‘é‡åˆæˆå•è¯å‘é‡
        Compose word vector from subword vectors
        """
        if word in word2vec_model.word_vectors:
            return word2vec_model.word_vectors[word]
        
        # å¯¹äºæœªç™»å½•è¯ï¼Œä½¿ç”¨å­è¯å‘é‡çš„å¹³å‡
        # For OOV words, use average of subword vectors
        ngrams = self.get_char_ngrams(word)
        vectors = []
        
        for ngram in ngrams:
            if ngram in word2vec_model.word_vectors:
                vectors.append(word2vec_model.word_vectors[ngram])
        
        if vectors:
            return np.mean(vectors, axis=0)
        else:
            # è¿”å›éšæœºå‘é‡ | Return random vector
            return np.random.normal(0, 0.1, word2vec_model.embedding_dim)
```

### 2. å±‚æ¬¡åŒ–Softmaxä¼˜åŒ–

```python
class HierarchicalSoftmax:
    """
    å±‚æ¬¡åŒ–Softmaxå®ç°
    Hierarchical Softmax Implementation
    """
    def __init__(self, vocab_size, embedding_dim):
        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim
        
        # æ„å»ºéœå¤«æ›¼æ ‘ | Build Huffman tree
        self.huffman_tree = self.build_huffman_tree()
    
    def build_huffman_tree(self):
        """
        æ„å»ºéœå¤«æ›¼æ ‘ä»¥å®ç°å±‚æ¬¡åŒ–softmax
        Build Huffman tree for hierarchical softmax
        """
        # ç®€åŒ–å®ç°ï¼šä½¿ç”¨å®Œå…¨äºŒå‰æ ‘
        # Simplified implementation: use complete binary tree
        import heapq
        
        # è¿™é‡Œåº”è¯¥æ ¹æ®è¯é¢‘æ„å»ºçœŸæ­£çš„éœå¤«æ›¼æ ‘
        # Here should build real Huffman tree based on word frequencies
        # ä¸ºç®€åŒ–ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç®€å•çš„äºŒå‰æ ‘ç»“æ„
        # For simplicity, we create a simple binary tree structure
        
        tree_nodes = {}
        for i in range(self.vocab_size * 2 - 1):
            tree_nodes[i] = {
                'vector': np.random.normal(0, 0.1, self.embedding_dim),
                'left': None,
                'right': None,
                'code': [],
                'path': []
            }
        
        return tree_nodes
    
    def get_path_and_code(self, word_idx):
        """
        è·å–è¯åˆ°æ ¹èŠ‚ç‚¹çš„è·¯å¾„å’Œç¼–ç 
        Get path and code from word to root node
        """
        # ç®€åŒ–å®ç°ï¼šè¿”å›é¢„å®šä¹‰çš„è·¯å¾„
        # Simplified implementation: return predefined path
        path = []
        code = []
        
        # è¿™é‡Œåº”è¯¥å®ç°çœŸæ­£çš„éœå¤«æ›¼ç¼–ç è·¯å¾„
        # Here should implement real Huffman coding path
        current = word_idx + self.vocab_size - 1
        while current > 0:
            parent = (current - 1) // 2
            if current % 2 == 1:  # å·¦å­æ ‘
                code.append(0)
            else:  # å³å­æ ‘
                code.append(1)
            path.append(parent)
            current = parent
        
        return path[::-1], code[::-1]
```

### 3. è¯å‘é‡è´¨é‡è¯„ä¼°

```python
def intrinsic_evaluation():
    """
    å†…åœ¨è¯„ä¼°ï¼šè¯å‘é‡è´¨é‡çš„å®šé‡åˆ†æ
    Intrinsic evaluation: quantitative analysis of word vector quality
    """
    
    # 1. è¯ç›¸ä¼¼åº¦æ•°æ®é›†è¯„ä¼°
    # Word similarity dataset evaluation
    wordsim353 = [
        ("love", "sex", 6.77),
        ("tiger", "cat", 7.35),
        ("computer", "keyboard", 7.62),
        # æ›´å¤šæ•°æ®...
        # More data...
    ]
    
    # 2. è¯ç±»æ¯”æ•°æ®é›†è¯„ä¼°
    # Word analogy dataset evaluation
    analogies = [
        ("man", "woman", "king", "queen"),
        ("good", "better", "bad", "worse"),
        ("go", "went", "take", "took"),
        # æ›´å¤šç±»æ¯”...
        # More analogies...
    ]
    
    return wordsim353, analogies

def extrinsic_evaluation(word_vectors, downstream_task):
    """
    å¤–åœ¨è¯„ä¼°ï¼šåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æ€§èƒ½
    Extrinsic evaluation: performance on downstream tasks
    """
    
    # ä½¿ç”¨è¯å‘é‡ä½œä¸ºç‰¹å¾è¿›è¡Œæ–‡æœ¬åˆ†ç±»
    # Use word vectors as features for text classification
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import accuracy_score
    
    # è¿™é‡Œåº”è¯¥å®ç°å…·ä½“çš„ä¸‹æ¸¸ä»»åŠ¡è¯„ä¼°
    # Here should implement specific downstream task evaluation
    pass
```

---

**ğŸ¯ é¡¹ç›®å®Œæˆæ£€æŸ¥æ¸…å• | Project Completion Checklist:**

### ç†è®ºç†è§£ | Theoretical Understanding
- [ ] æ·±å…¥ç†è§£åˆ†å¸ƒå¼å‡è®¾çš„æ•°å­¦åŸºç¡€
- [ ] æŒæ¡Skip-gramå’ŒCBOWçš„åŒºåˆ«å’Œé€‚ç”¨åœºæ™¯
- [ ] ç†è§£è´Ÿé‡‡æ ·å’Œå±‚æ¬¡åŒ–Softmaxçš„ä¼˜åŒ–åŸç†
- [ ] èƒ½å¤Ÿåˆ†æè¯å‘é‡çš„å‡ ä½•æ€§è´¨

### ç¼–ç¨‹å®ç° | Programming Implementation  
- [ ] ä»é›¶å®ç°å®Œæ•´çš„Word2Vecè®­ç»ƒæµç¨‹
- [ ] æ­£ç¡®å®ç°è´Ÿé‡‡æ ·ä¼˜åŒ–æŠ€æœ¯
- [ ] èƒ½å¤Ÿå¤„ç†å¤§è§„æ¨¡æ–‡æœ¬æ•°æ®
- [ ] å®ç°é«˜æ•ˆçš„è¯å‘é‡æ£€ç´¢å’Œç›¸ä¼¼åº¦è®¡ç®—

### å®éªŒåˆ†æ | Experimental Analysis
- [ ] åœ¨çœŸå®æ•°æ®é›†ä¸Šè®­ç»ƒé«˜è´¨é‡è¯å‘é‡
- [ ] è¿›è¡Œå…¨é¢çš„å†…åœ¨å’Œå¤–åœ¨è¯„ä¼°
- [ ] å¯è§†åŒ–å’Œåˆ†æè¯å‘é‡çš„è¯­ä¹‰ç»“æ„
- [ ] å¯¹æ¯”ä¸åŒè¶…å‚æ•°è®¾ç½®çš„å½±å“

### åº”ç”¨æ‰©å±• | Application Extensions
- [ ] å®ç°å­è¯åµŒå…¥å¤„ç†æœªç™»å½•è¯
- [ ] æ¢ç´¢è·¨è¯­è¨€è¯å‘é‡å¯¹é½
- [ ] å°†è¯å‘é‡åº”ç”¨åˆ°å®é™…NLPä»»åŠ¡
- [ ] ä¼˜åŒ–æ¨¡å‹ä»¥æå‡è®­ç»ƒå’Œæ¨ç†æ•ˆç‡

**è®°ä½**: Word2Vecä¸ä»…æ˜¯ä¸€ä¸ªæŠ€æœ¯ï¼Œæ›´æ˜¯ç†è§£è¯­è¨€åˆ†å¸ƒå¼è¡¨ç¤ºçš„é’¥åŒ™ã€‚æŒæ¡äº†Word2Vecï¼Œä½ å°±ç†è§£äº†ç°ä»£NLPçš„åŸºç¡€ï¼

**Remember**: Word2Vec is not just a technique, but a key to understanding distributed representation of language. Master Word2Vec, and you understand the foundation of modern NLP! 