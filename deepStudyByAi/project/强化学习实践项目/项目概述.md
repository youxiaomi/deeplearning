# 强化学习实践项目概述
# Reinforcement Learning Practice Project Overview

**让AI学会决策 - 从游戏到现实世界的智能决策**
**Teaching AI to Make Decisions - From Games to Real-world Intelligent Decision Making**

---

## 🎯 项目目标 | Project Goals

强化学习是AI最激动人心的领域之一！它让机器能够通过与环境交互来学习最优决策策略。通过这个项目，你将掌握：
Reinforcement Learning is one of the most exciting fields in AI! It enables machines to learn optimal decision-making strategies through interaction with environments. Through this project, you will master:

- **马尔可夫决策过程** | **Markov Decision Processes**: 决策问题的数学建模
- **价值函数与策略** | **Value Functions & Policies**: 评估和改进决策策略
- **深度强化学习** | **Deep Reinforcement Learning**: 结合深度学习的强化学习
- **实际应用** | **Real Applications**: 游戏AI、自动驾驶、推荐系统等

## 🔬 为什么强化学习如此重要？| Why is Reinforcement Learning So Important?

**强化学习正在创造真正的人工智能！**
**Reinforcement Learning is creating true artificial intelligence!**

从AlphaGo击败世界围棋冠军，到自动驾驶汽车的路径规划，从个性化推荐的策略优化，到机器人的灵活控制，强化学习让AI拥有了"智能决策"的能力。这不仅仅是算法，更是通向通用人工智能的关键路径。

From AlphaGo defeating world Go champions to path planning in autonomous vehicles, from strategy optimization in personalized recommendations to flexible robot control, reinforcement learning gives AI the ability to make "intelligent decisions". This is not just algorithms, but a key path to artificial general intelligence.

### 强化学习的发展历程 | Evolution of Reinforcement Learning
```
1950s: 动态规划基础 | Dynamic Programming Foundations
1980s: 时间差分学习 | Temporal Difference Learning
1990s: Q学习算法 | Q-Learning Algorithm
2000s: 策略梯度方法 | Policy Gradient Methods
2010s: 深度强化学习 | Deep Reinforcement Learning
2020s: 多智能体与元学习 | Multi-Agent & Meta-Learning
```

## 📚 项目结构深度解析 | Deep Project Structure Analysis

### 01_基础强化学习 | Basic Reinforcement Learning

**理解智能决策的数学基础！**
**Understand the mathematical foundations of intelligent decision-making!**

#### Q学习算法实现 | Q-Learning Algorithm Implementation

**项目核心 | Project Core:**
Q学习是强化学习的经典算法，通过学习状态-动作价值函数来找到最优策略。

Q-learning is a classic reinforcement learning algorithm that finds optimal policies by learning state-action value functions.

**数学原理 | Mathematical Principles:**

**贝尔曼方程 | Bellman Equation:**
```
Q*(s, a) = R(s, a) + γ * max Q*(s', a')
```

**Q学习更新规则 | Q-Learning Update Rule:**
```
Q(s, a) ← Q(s, a) + α[R + γ * max Q(s', a') - Q(s, a)]
```

其中：
- `s`: 当前状态 | current state
- `a`: 当前动作 | current action
- `R`: 即时奖励 | immediate reward
- `γ`: 折扣因子 | discount factor
- `α`: 学习率 | learning rate

**完整实现 | Complete Implementation:**
```python
import numpy as np
import matplotlib.pyplot as plt
from collections import defaultdict
import random

class QLearningAgent:
    """
    Q学习智能体实现
    Q-Learning Agent Implementation
    """
    def __init__(self, num_states, num_actions, learning_rate=0.1, 
                 discount_factor=0.95, epsilon=0.1):
        self.num_states = num_states
        self.num_actions = num_actions
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = epsilon
        
        # 初始化Q表 | Initialize Q-table
        self.q_table = np.zeros((num_states, num_actions))
        
        # 记录训练历史 | Record training history
        self.episode_rewards = []
        self.episode_steps = []
    
    def choose_action(self, state, training=True):
        """
        选择动作：ε-贪婪策略
        Choose action: ε-greedy policy
        """
        if training and random.random() < self.epsilon:
            # 探索：随机选择动作 | Exploration: random action
            return random.randint(0, self.num_actions - 1)
        else:
            # 利用：选择Q值最大的动作 | Exploitation: action with max Q-value
            return np.argmax(self.q_table[state])
    
    def update_q_table(self, state, action, reward, next_state, done):
        """
        更新Q表
        Update Q-table
        """
        current_q = self.q_table[state, action]
        
        if done:
            # 终止状态，没有未来奖励 | Terminal state, no future reward
            target_q = reward
        else:
            # 使用贝尔曼方程计算目标Q值 | Use Bellman equation for target Q-value
            max_next_q = np.max(self.q_table[next_state])
            target_q = reward + self.discount_factor * max_next_q
        
        # Q学习更新 | Q-learning update
        self.q_table[state, action] += self.learning_rate * (target_q - current_q)
    
    def train(self, environment, num_episodes=1000):
        """
        训练智能体
        Train the agent
        """
        print(f"开始训练Q学习智能体，共{num_episodes}轮...")
        print(f"Starting Q-Learning training for {num_episodes} episodes...")
        
        for episode in range(num_episodes):
            state = environment.reset()
            total_reward = 0
            steps = 0
            
            while True:
                # 选择动作 | Choose action
                action = self.choose_action(state)
                
                # 执行动作 | Execute action
                next_state, reward, done, info = environment.step(action)
                
                # 更新Q表 | Update Q-table
                self.update_q_table(state, action, reward, next_state, done)
                
                # 更新状态和统计 | Update state and statistics
                state = next_state
                total_reward += reward
                steps += 1
                
                if done:
                    break
            
            # 记录本轮结果 | Record episode results
            self.episode_rewards.append(total_reward)
            self.episode_steps.append(steps)
            
            # 动态调整探索率 | Dynamically adjust exploration rate
            if episode % 100 == 0:
                self.epsilon = max(0.01, self.epsilon * 0.995)
                avg_reward = np.mean(self.episode_rewards[-100:])
                print(f"Episode {episode}, Average Reward: {avg_reward:.2f}, Epsilon: {self.epsilon:.3f}")
        
        print("训练完成！| Training completed!")
        return self.q_table
    
    def evaluate(self, environment, num_episodes=100):
        """
        评估智能体性能
        Evaluate agent performance
        """
        print(f"评估智能体性能，共{num_episodes}轮...")
        print(f"Evaluating agent performance for {num_episodes} episodes...")
        
        eval_rewards = []
        
        for episode in range(num_episodes):
            state = environment.reset()
            total_reward = 0
            
            while True:
                # 使用贪婪策略（不探索）| Use greedy policy (no exploration)
                action = self.choose_action(state, training=False)
                state, reward, done, info = environment.step(action)
                total_reward += reward
                
                if done:
                    break
            
            eval_rewards.append(total_reward)
        
        avg_reward = np.mean(eval_rewards)
        std_reward = np.std(eval_rewards)
        
        print(f"评估结果 | Evaluation Results:")
        print(f"平均奖励 | Average Reward: {avg_reward:.2f} ± {std_reward:.2f}")
        
        return eval_rewards
    
    def visualize_training(self):
        """
        可视化训练过程
        Visualize training process
        """
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
        
        # 奖励曲线 | Reward curve
        window_size = 100
        smoothed_rewards = []
        for i in range(len(self.episode_rewards)):
            start = max(0, i - window_size)
            smoothed_rewards.append(np.mean(self.episode_rewards[start:i+1]))
        
        ax1.plot(smoothed_rewards)
        ax1.set_title('Training Rewards (Smoothed)')
        ax1.set_xlabel('Episode')
        ax1.set_ylabel('Average Reward')
        ax1.grid(True)
        
        # 步数曲线 | Steps curve
        smoothed_steps = []
        for i in range(len(self.episode_steps)):
            start = max(0, i - window_size)
            smoothed_steps.append(np.mean(self.episode_steps[start:i+1]))
        
        ax2.plot(smoothed_steps)
        ax2.set_title('Episode Steps (Smoothed)')
        ax2.set_xlabel('Episode')
        ax2.set_ylabel('Average Steps')
        ax2.grid(True)
        
        plt.tight_layout()
        plt.show()
        
        return fig
```

#### 策略梯度方法 | Policy Gradient Methods

**项目特色 | Project Features:**
策略梯度直接优化策略函数，适用于连续动作空间和随机策略。

Policy gradient directly optimizes the policy function, suitable for continuous action spaces and stochastic policies.

**REINFORCE算法实现 | REINFORCE Algorithm Implementation:**
```python
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.distributions import Categorical

class PolicyNetwork(nn.Module):
    """
    策略网络：将状态映射到动作概率分布
    Policy Network: Maps states to action probability distributions
    """
    def __init__(self, state_size, action_size, hidden_size=128):
        super(PolicyNetwork, self).__init__()
        self.fc1 = nn.Linear(state_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, action_size)
        
    def forward(self, state):
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        action_probs = F.softmax(self.fc3(x), dim=-1)
        return action_probs

class REINFORCEAgent:
    """
    REINFORCE算法实现
    REINFORCE Algorithm Implementation
    """
    def __init__(self, state_size, action_size, learning_rate=1e-3, gamma=0.99):
        self.state_size = state_size
        self.action_size = action_size
        self.gamma = gamma
        
        # 策略网络 | Policy network
        self.policy_net = PolicyNetwork(state_size, action_size)
        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=learning_rate)
        
        # 存储轨迹 | Store trajectory
        self.states = []
        self.actions = []
        self.rewards = []
        self.log_probs = []
        
        # 训练历史 | Training history
        self.episode_rewards = []
    
    def choose_action(self, state):
        """
        根据当前策略选择动作
        Choose action according to current policy
        """
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        action_probs = self.policy_net(state_tensor)
        
        # 创建分布并采样 | Create distribution and sample
        distribution = Categorical(action_probs)
        action = distribution.sample()
        
        # 存储用于训练 | Store for training
        self.log_probs.append(distribution.log_prob(action))
        
        return action.item()
    
    def store_transition(self, state, action, reward):
        """
        存储状态转移
        Store state transition
        """
        self.states.append(state)
        self.actions.append(action)
        self.rewards.append(reward)
    
    def calculate_returns(self):
        """
        计算回报（从后往前）
        Calculate returns (backwards)
        """
        returns = []
        R = 0
        
        # 从终止状态往回计算 | Calculate backwards from terminal state
        for reward in reversed(self.rewards):
            R = reward + self.gamma * R
            returns.insert(0, R)
        
        return returns
    
    def update_policy(self):
        """
        更新策略网络
        Update policy network
        """
        if len(self.rewards) == 0:
            return
        
        # 计算回报 | Calculate returns
        returns = self.calculate_returns()
        returns = torch.FloatTensor(returns)
        
        # 标准化回报 | Normalize returns
        returns = (returns - returns.mean()) / (returns.std() + 1e-8)
        
        # 计算策略损失 | Calculate policy loss
        policy_loss = []
        for log_prob, R in zip(self.log_probs, returns):
            policy_loss.append(-log_prob * R)
        
        policy_loss = torch.cat(policy_loss).sum()
        
        # 反向传播 | Backpropagation
        self.optimizer.zero_grad()
        policy_loss.backward()
        self.optimizer.step()
        
        # 清空轨迹 | Clear trajectory
        self.states.clear()
        self.actions.clear()
        self.rewards.clear()
        self.log_probs.clear()
    
    def train(self, environment, num_episodes=2000):
        """
        训练策略网络
        Train policy network
        """
        print(f"开始REINFORCE训练，共{num_episodes}轮...")
        print(f"Starting REINFORCE training for {num_episodes} episodes...")
        
        for episode in range(num_episodes):
            state = environment.reset()
            total_reward = 0
            
            while True:
                # 选择动作 | Choose action
                action = self.choose_action(state)
                
                # 执行动作 | Execute action
                next_state, reward, done, info = environment.step(action)
                
                # 存储转移 | Store transition
                self.store_transition(state, action, reward)
                
                state = next_state
                total_reward += reward
                
                if done:
                    break
            
            # 更新策略 | Update policy
            self.update_policy()
            
            # 记录奖励 | Record reward
            self.episode_rewards.append(total_reward)
            
            # 打印进度 | Print progress
            if episode % 100 == 0:
                avg_reward = np.mean(self.episode_rewards[-100:])
                print(f"Episode {episode}, Average Reward: {avg_reward:.2f}")
        
        print("REINFORCE训练完成！| REINFORCE training completed!")
```

### 02_深度强化学习 | Deep Reinforcement Learning

**将深度学习的力量注入强化学习！**
**Inject the power of deep learning into reinforcement learning!**

#### DQN深度Q网络 | DQN Deep Q-Network

**项目突破 | Project Breakthrough:**
DQN是深度强化学习的里程碑，解决了Q学习在高维状态空间的问题。

DQN is a milestone in deep reinforcement learning, solving Q-learning problems in high-dimensional state spaces.

**核心创新 | Core Innovations:**

1. **经验回放 | Experience Replay**
2. **目标网络 | Target Network**
3. **双重DQN | Double DQN**
4. **优先经验回放 | Prioritized Experience Replay**

**完整DQN实现 | Complete DQN Implementation:**
```python
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import random
from collections import deque, namedtuple

# 经验元组 | Experience tuple
Experience = namedtuple('Experience', 
                       ['state', 'action', 'reward', 'next_state', 'done'])

class DQNNetwork(nn.Module):
    """
    深度Q网络架构
    Deep Q-Network Architecture
    """
    def __init__(self, state_size, action_size, hidden_size=512):
        super(DQNNetwork, self).__init__()
        
        self.fc1 = nn.Linear(state_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, hidden_size)
        self.fc4 = nn.Linear(hidden_size, action_size)
        
        # 权重初始化 | Weight initialization
        self.apply(self.init_weights)
    
    def init_weights(self, layer):
        if isinstance(layer, nn.Linear):
            torch.nn.init.xavier_uniform_(layer.weight)
            layer.bias.data.fill_(0.01)
    
    def forward(self, state):
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        q_values = self.fc4(x)
        return q_values

class ReplayBuffer:
    """
    经验回放缓冲区
    Experience Replay Buffer
    """
    def __init__(self, capacity=100000):
        self.buffer = deque(maxlen=capacity)
    
    def push(self, state, action, reward, next_state, done):
        """添加经验 | Add experience"""
        experience = Experience(state, action, reward, next_state, done)
        self.buffer.append(experience)
    
    def sample(self, batch_size):
        """采样批次经验 | Sample batch of experiences"""
        experiences = random.sample(self.buffer, batch_size)
        
        states = torch.FloatTensor([e.state for e in experiences])
        actions = torch.LongTensor([e.action for e in experiences])
        rewards = torch.FloatTensor([e.reward for e in experiences])
        next_states = torch.FloatTensor([e.next_state for e in experiences])
        dones = torch.BoolTensor([e.done for e in experiences])
        
        return states, actions, rewards, next_states, dones
    
    def __len__(self):
        return len(self.buffer)

class DQNAgent:
    """
    DQN智能体实现
    DQN Agent Implementation
    """
    def __init__(self, state_size, action_size, learning_rate=1e-4, 
                 gamma=0.99, epsilon=1.0, epsilon_decay=0.995, 
                 epsilon_min=0.01, memory_size=100000, batch_size=64):
        
        self.state_size = state_size
        self.action_size = action_size
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min
        self.batch_size = batch_size
        
        # 设备选择 | Device selection
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # 神经网络 | Neural networks
        self.q_network = DQNNetwork(state_size, action_size).to(self.device)
        self.target_network = DQNNetwork(state_size, action_size).to(self.device)
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)
        
        # 经验回放 | Experience replay
        self.memory = ReplayBuffer(memory_size)
        
        # 训练统计 | Training statistics
        self.episode_rewards = []
        self.losses = []
        self.update_target_every = 1000  # 目标网络更新频率
        self.step_count = 0
    
    def act(self, state, training=True):
        """
        选择动作
        Choose action
        """
        if training and random.random() <= self.epsilon:
            return random.choice(range(self.action_size))
        
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        q_values = self.q_network(state_tensor)
        return q_values.cpu().data.numpy().argmax()
    
    def remember(self, state, action, reward, next_state, done):
        """
        存储经验
        Store experience
        """
        self.memory.push(state, action, reward, next_state, done)
    
    def replay(self):
        """
        经验回放训练
        Experience replay training
        """
        if len(self.memory) < self.batch_size:
            return
        
        # 采样批次数据 | Sample batch data
        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)
        
        states = states.to(self.device)
        actions = actions.to(self.device)
        rewards = rewards.to(self.device)
        next_states = next_states.to(self.device)
        dones = dones.to(self.device)
        
        # 当前Q值 | Current Q-values
        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))
        
        # 下一状态的最大Q值（使用目标网络）| Max Q-values for next states (using target network)
        with torch.no_grad():
            next_q_values = self.target_network(next_states).max(1)[0].detach()
            target_q_values = rewards + (self.gamma * next_q_values * ~dones)
        
        # 计算损失 | Calculate loss
        loss = F.mse_loss(current_q_values.squeeze(), target_q_values)
        
        # 反向传播 | Backpropagation
        self.optimizer.zero_grad()
        loss.backward()
        
        # 梯度裁剪 | Gradient clipping
        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)
        
        self.optimizer.step()
        
        # 记录损失 | Record loss
        self.losses.append(loss.item())
        
        # 更新目标网络 | Update target network
        self.step_count += 1
        if self.step_count % self.update_target_every == 0:
            self.update_target_network()
        
        # 衰减探索率 | Decay exploration rate
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
    
    def update_target_network(self):
        """
        更新目标网络
        Update target network
        """
        self.target_network.load_state_dict(self.q_network.state_dict())
    
    def train(self, environment, num_episodes=2000):
        """
        训练DQN智能体
        Train DQN agent
        """
        print(f"开始DQN训练，共{num_episodes}轮...")
        print(f"Starting DQN training for {num_episodes} episodes...")
        
        for episode in range(num_episodes):
            state = environment.reset()
            total_reward = 0
            steps = 0
            
            while True:
                # 选择动作 | Choose action
                action = self.act(state)
                
                # 执行动作 | Execute action
                next_state, reward, done, info = environment.step(action)
                
                # 存储经验 | Store experience
                self.remember(state, action, reward, next_state, done)
                
                # 经验回放学习 | Experience replay learning
                self.replay()
                
                state = next_state
                total_reward += reward
                steps += 1
                
                if done:
                    break
            
            # 记录本轮结果 | Record episode results
            self.episode_rewards.append(total_reward)
            
            # 打印进度 | Print progress
            if episode % 100 == 0:
                avg_reward = np.mean(self.episode_rewards[-100:])
                avg_loss = np.mean(self.losses[-100:]) if self.losses else 0
                print(f"Episode {episode}")
                print(f"  Average Reward: {avg_reward:.2f}")
                print(f"  Average Loss: {avg_loss:.4f}")
                print(f"  Epsilon: {self.epsilon:.3f}")
                print(f"  Memory Size: {len(self.memory)}")
        
        print("DQN训练完成！| DQN training completed!")
        return self.q_network
```

#### Actor-Critic算法 | Actor-Critic Algorithm

**项目价值 | Project Value:**
Actor-Critic结合了价值函数和策略函数的优势，是现代强化学习的核心方法。

Actor-Critic combines the advantages of value functions and policy functions, being a core method in modern reinforcement learning.

**A2C算法实现 | A2C Algorithm Implementation:**
```python
class ActorCriticNetwork(nn.Module):
    """
    Actor-Critic网络架构
    Actor-Critic Network Architecture
    """
    def __init__(self, state_size, action_size, hidden_size=256):
        super(ActorCriticNetwork, self).__init__()
        
        # 共享特征层 | Shared feature layers
        self.shared_fc1 = nn.Linear(state_size, hidden_size)
        self.shared_fc2 = nn.Linear(hidden_size, hidden_size)
        
        # Actor分支（策略网络）| Actor branch (policy network)
        self.actor_fc = nn.Linear(hidden_size, hidden_size)
        self.actor_output = nn.Linear(hidden_size, action_size)
        
        # Critic分支（价值网络）| Critic branch (value network)
        self.critic_fc = nn.Linear(hidden_size, hidden_size)
        self.critic_output = nn.Linear(hidden_size, 1)
        
    def forward(self, state):
        # 共享特征提取 | Shared feature extraction
        x = F.relu(self.shared_fc1(state))
        x = F.relu(self.shared_fc2(x))
        
        # Actor输出（动作概率）| Actor output (action probabilities)
        actor_x = F.relu(self.actor_fc(x))
        action_probs = F.softmax(self.actor_output(actor_x), dim=-1)
        
        # Critic输出（状态价值）| Critic output (state value)
        critic_x = F.relu(self.critic_fc(x))
        state_value = self.critic_output(critic_x)
        
        return action_probs, state_value

class A2CAgent:
    """
    Advantage Actor-Critic智能体
    Advantage Actor-Critic Agent
    """
    def __init__(self, state_size, action_size, learning_rate=1e-3, 
                 gamma=0.99, value_loss_coef=0.5, entropy_coef=0.01):
        
        self.state_size = state_size
        self.action_size = action_size
        self.gamma = gamma
        self.value_loss_coef = value_loss_coef
        self.entropy_coef = entropy_coef
        
        # 设备选择 | Device selection
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # Actor-Critic网络 | Actor-Critic network
        self.network = ActorCriticNetwork(state_size, action_size).to(self.device)
        self.optimizer = optim.Adam(self.network.parameters(), lr=learning_rate)
        
        # 存储轨迹 | Store trajectory
        self.states = []
        self.actions = []
        self.rewards = []
        self.values = []
        self.log_probs = []
        self.entropies = []
        
        # 训练历史 | Training history
        self.episode_rewards = []
        self.actor_losses = []
        self.critic_losses = []
    
    def act(self, state):
        """
        选择动作并计算价值
        Choose action and compute value
        """
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        action_probs, state_value = self.network(state_tensor)
        
        # 创建分布 | Create distribution
        distribution = Categorical(action_probs)
        action = distribution.sample()
        
        # 计算熵（用于鼓励探索）| Calculate entropy (for exploration)
        entropy = distribution.entropy()
        
        # 存储用于训练 | Store for training
        self.log_probs.append(distribution.log_prob(action))
        self.entropies.append(entropy)
        self.values.append(state_value)
        
        return action.item(), state_value.item()
    
    def store_transition(self, state, action, reward):
        """
        存储状态转移
        Store state transition
        """
        self.states.append(state)
        self.actions.append(action)
        self.rewards.append(reward)
    
    def calculate_advantages(self, next_value=0):
        """
        计算优势函数
        Calculate advantage function
        """
        returns = []
        advantages = []
        
        # 计算回报 | Calculate returns
        R = next_value
        for reward in reversed(self.rewards):
            R = reward + self.gamma * R
            returns.insert(0, R)
        
        returns = torch.FloatTensor(returns).to(self.device)
        values = torch.cat(self.values).to(self.device)
        
        # 计算优势 A(s,a) = R - V(s) | Calculate advantage A(s,a) = R - V(s)
        advantages = returns - values
        
        return returns, advantages
    
    def update_network(self, next_value=0):
        """
        更新网络参数
        Update network parameters
        """
        if len(self.rewards) == 0:
            return
        
        # 计算优势 | Calculate advantages
        returns, advantages = self.calculate_advantages(next_value)
        
        # 标准化优势 | Normalize advantages
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        # 计算损失 | Calculate losses
        log_probs = torch.cat(self.log_probs).to(self.device)
        entropies = torch.cat(self.entropies).to(self.device)
        
        # Actor损失（策略梯度）| Actor loss (policy gradient)
        actor_loss = -(log_probs * advantages.detach()).mean()
        
        # Critic损失（价值函数）| Critic loss (value function)
        critic_loss = F.mse_loss(torch.cat(self.values).to(self.device), returns)
        
        # 熵损失（鼓励探索）| Entropy loss (encourage exploration)
        entropy_loss = -entropies.mean()
        
        # 总损失 | Total loss
        total_loss = (actor_loss + 
                     self.value_loss_coef * critic_loss + 
                     self.entropy_coef * entropy_loss)
        
        # 反向传播 | Backpropagation
        self.optimizer.zero_grad()
        total_loss.backward()
        
        # 梯度裁剪 | Gradient clipping
        torch.nn.utils.clip_grad_norm_(self.network.parameters(), 0.5)
        
        self.optimizer.step()
        
        # 记录损失 | Record losses
        self.actor_losses.append(actor_loss.item())
        self.critic_losses.append(critic_loss.item())
        
        # 清空轨迹 | Clear trajectory
        self.clear_trajectory()
    
    def clear_trajectory(self):
        """清空轨迹数据 | Clear trajectory data"""
        self.states.clear()
        self.actions.clear()
        self.rewards.clear()
        self.values.clear()
        self.log_probs.clear()
        self.entropies.clear()
    
    def train(self, environment, num_episodes=2000):
        """
        训练A2C智能体
        Train A2C agent
        """
        print(f"开始A2C训练，共{num_episodes}轮...")
        print(f"Starting A2C training for {num_episodes} episodes...")
        
        for episode in range(num_episodes):
            state = environment.reset()
            total_reward = 0
            
            while True:
                # 选择动作 | Choose action
                action, value = self.act(state)
                
                # 执行动作 | Execute action
                next_state, reward, done, info = environment.step(action)
                
                # 存储转移 | Store transition
                self.store_transition(state, action, reward)
                
                state = next_state
                total_reward += reward
                
                if done:
                    # 回合结束，更新网络 | Episode ended, update network
                    self.update_network(next_value=0)
                    break
            
            # 记录奖励 | Record reward
            self.episode_rewards.append(total_reward)
            
            # 打印进度 | Print progress
            if episode % 100 == 0:
                avg_reward = np.mean(self.episode_rewards[-100:])
                avg_actor_loss = np.mean(self.actor_losses[-100:]) if self.actor_losses else 0
                avg_critic_loss = np.mean(self.critic_losses[-100:]) if self.critic_losses else 0
                
                print(f"Episode {episode}")
                print(f"  Average Reward: {avg_reward:.2f}")
                print(f"  Actor Loss: {avg_actor_loss:.4f}")
                print(f"  Critic Loss: {avg_critic_loss:.4f}")
        
        print("A2C训练完成！| A2C training completed!")
```

## 🎯 实际应用场景 | Real-world Application Scenarios

### 🎮 游戏AI开发 | Game AI Development

```python
class GameAI:
    """
    游戏AI系统
    Game AI System
    """
    def __init__(self, game_type="atari"):
        self.game_type = game_type
        self.agent = None
        
    def create_agent_for_game(self, game_name):
        """
        为特定游戏创建智能体
        Create agent for specific game
        """
        if game_name in ["Pong", "Breakout", "SpaceInvaders"]:
            # 使用DQN处理Atari游戏
            self.agent = DQNAgent(state_size=84*84*4, action_size=4)
        elif game_name in ["CartPole", "MountainCar"]:
            # 使用A2C处理经典控制问题
            self.agent = A2CAgent(state_size=4, action_size=2)
        
        return self.agent
```

### 🚗 自动驾驶决策 | Autonomous Driving Decision

```python
class AutonomousDrivingAgent:
    """
    自动驾驶决策系统
    Autonomous driving decision system
    """
    def __init__(self):
        # 状态：速度、位置、周围车辆信息等
        # State: speed, position, surrounding vehicle info, etc.
        self.state_size = 20
        
        # 动作：加速、减速、变道等
        # Actions: accelerate, decelerate, change lanes, etc.
        self.action_size = 5
        
        self.agent = A2CAgent(self.state_size, self.action_size)
    
    def make_driving_decision(self, traffic_state):
        """
        做出驾驶决策
        Make driving decision
        """
        action, _ = self.agent.act(traffic_state)
        return self.interpret_action(action)
    
    def interpret_action(self, action):
        """解释动作含义 | Interpret action meaning"""
        actions = ["maintain_speed", "accelerate", "decelerate", 
                  "change_left", "change_right"]
        return actions[action]
```

---

**🎯 项目完成检查清单 | Project Completion Checklist:**

### 理论理解 | Theoretical Understanding
- [ ] 深入理解马尔可夫决策过程的数学框架
- [ ] 掌握价值函数、策略函数和优势函数的概念
- [ ] 理解探索与利用的平衡策略
- [ ] 掌握策略梯度定理和Actor-Critic方法

### 算法实现 | Algorithm Implementation
- [ ] 从零实现Q学习和SARSA算法
- [ ] 实现DQN及其改进版本（Double DQN、Dueling DQN）
- [ ] 实现策略梯度方法（REINFORCE、A2C、PPO）
- [ ] 掌握经验回放和目标网络等关键技术

### 实际应用 | Practical Applications
- [ ] 在经典控制任务上获得良好性能
- [ ] 成功训练游戏AI（如Atari游戏）
- [ ] 实现多智能体强化学习系统
- [ ] 将RL应用到实际问题（推荐、自动驾驶等）

**记住**: 强化学习是让AI学会决策的关键技术。通过这个项目，你将掌握从简单的Q学习到复杂的深度强化学习的完整技术栈！

**Remember**: Reinforcement learning is the key technology for teaching AI to make decisions. Through this project, you will master the complete technology stack from simple Q-learning to complex deep reinforcement learning! 