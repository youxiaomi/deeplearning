# Chapter 20: Appendix: Tools for Deep Learning 
# ç¬¬20ç« ï¼šé™„å½•ï¼šæ·±åº¦å­¦ä¹ å·¥å…·

## Overview æ¦‚è¿°

Deep learning has revolutionized the field of artificial intelligence, but mastering it requires not only understanding the theoretical concepts but also becoming proficient with the practical tools and platforms that make implementation possible. This appendix provides a comprehensive guide to the essential tools, platforms, and environments that every deep learning practitioner should know.

æ·±åº¦å­¦ä¹ å·²ç»å½»åº•æ”¹é©äº†äººå·¥æ™ºèƒ½é¢†åŸŸï¼Œä½†è¦æŒæ¡å®ƒä¸ä»…éœ€è¦ç†è§£ç†è®ºæ¦‚å¿µï¼Œè¿˜éœ€è¦ç†Ÿç»ƒæŒæ¡ä½¿å®ç°æˆä¸ºå¯èƒ½çš„å®ç”¨å·¥å…·å’Œå¹³å°ã€‚è¿™ä¸ªé™„å½•ä¸ºæ¯ä¸ªæ·±åº¦å­¦ä¹ ä»ä¸šè€…åº”è¯¥äº†è§£çš„åŸºæœ¬å·¥å…·ã€å¹³å°å’Œç¯å¢ƒæä¾›äº†å…¨é¢çš„æŒ‡å—ã€‚

In this chapter, we will explore various development environments, from local Jupyter notebooks to cloud-based solutions like Amazon SageMaker, AWS EC2, and Google Colab. We'll also discuss hardware considerations, collaboration tools, and utility functions that can significantly enhance your deep learning workflow.

åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ¢ç´¢å„ç§å¼€å‘ç¯å¢ƒï¼Œä»æœ¬åœ°Jupyterç¬”è®°æœ¬åˆ°åŸºäºäº‘çš„è§£å†³æ–¹æ¡ˆï¼Œå¦‚Amazon SageMakerã€AWS EC2å’ŒGoogle Colabã€‚æˆ‘ä»¬è¿˜å°†è®¨è®ºç¡¬ä»¶è€ƒè™‘å› ç´ ã€åä½œå·¥å…·å’Œå¯ä»¥æ˜¾è‘—å¢å¼ºæ·±åº¦å­¦ä¹ å·¥ä½œæµç¨‹çš„å®ç”¨å‡½æ•°ã€‚

## 20.1 Using Jupyter Notebooks ä½¿ç”¨Jupyterç¬”è®°æœ¬

Jupyter Notebooks have become the de facto standard for interactive data science and machine learning development. They provide an excellent environment for experimenting with code, visualizing data, and documenting your thought process all in one place.

Jupyterç¬”è®°æœ¬å·²æˆä¸ºäº¤äº’å¼æ•°æ®ç§‘å­¦å’Œæœºå™¨å­¦ä¹ å¼€å‘çš„äº‹å®æ ‡å‡†ã€‚å®ƒä»¬ä¸ºåœ¨ä¸€ä¸ªåœ°æ–¹è¯•éªŒä»£ç ã€å¯è§†åŒ–æ•°æ®å’Œè®°å½•æ€ç»´è¿‡ç¨‹æä¾›äº†ä¼˜ç§€çš„ç¯å¢ƒã€‚

### 20.1.1 Editing and Running the Code Locally æœ¬åœ°ç¼–è¾‘å’Œè¿è¡Œä»£ç 

#### Installation and Setup å®‰è£…å’Œè®¾ç½®

To get started with Jupyter notebooks locally, you need to install the necessary software. The easiest way is through Anaconda, which comes with Jupyter pre-installed, or you can install it via pip.

è¦åœ¨æœ¬åœ°å¼€å§‹ä½¿ç”¨Jupyterç¬”è®°æœ¬ï¼Œä½ éœ€è¦å®‰è£…å¿…è¦çš„è½¯ä»¶ã€‚æœ€ç®€å•çš„æ–¹æ³•æ˜¯é€šè¿‡Anacondaï¼Œå®ƒé¢„è£…äº†Jupyterï¼Œæˆ–è€…ä½ å¯ä»¥é€šè¿‡pipå®‰è£…å®ƒã€‚

**Method 1: Using Anaconda (Recommended) æ–¹æ³•1ï¼šä½¿ç”¨Anacondaï¼ˆæ¨èï¼‰**

```bash
# Download and install Anaconda from https://www.anaconda.com/
# ä» https://www.anaconda.com/ ä¸‹è½½å¹¶å®‰è£…Anaconda

# Launch Jupyter Notebook
# å¯åŠ¨Jupyterç¬”è®°æœ¬
jupyter notebook
```

**Method 2: Using pip æ–¹æ³•2ï¼šä½¿ç”¨pip**

```bash
# Install Jupyter
# å®‰è£…Jupyter
pip install jupyter

# Install additional packages for deep learning
# å®‰è£…æ·±åº¦å­¦ä¹ çš„é¢å¤–åŒ…
pip install torch torchvision matplotlib numpy pandas scikit-learn

# Launch Jupyter Notebook
# å¯åŠ¨Jupyterç¬”è®°æœ¬
jupyter notebook
```

#### Creating Your First Notebook åˆ›å»ºä½ çš„ç¬¬ä¸€ä¸ªç¬”è®°æœ¬

When you launch Jupyter, it opens in your web browser. You can create a new notebook by clicking "New" â†’ "Python 3". Let's create a simple deep learning example:

å½“ä½ å¯åŠ¨Jupyteræ—¶ï¼Œå®ƒåœ¨ç½‘é¡µæµè§ˆå™¨ä¸­æ‰“å¼€ã€‚ä½ å¯ä»¥é€šè¿‡ç‚¹å‡»"æ–°å»º"â†’"Python 3"æ¥åˆ›å»ºæ–°ç¬”è®°æœ¬ã€‚è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç®€å•çš„æ·±åº¦å­¦ä¹ ç¤ºä¾‹ï¼š

```python
# Cell 1: Import necessary libraries
# å•å…ƒæ ¼1ï¼šå¯¼å…¥å¿…è¦çš„åº“
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt

print("PyTorch version:", torch.__version__)
print("CUDA available:", torch.cuda.is_available())
```

```python
# Cell 2: Create a simple neural network
# å•å…ƒæ ¼2ï¼šåˆ›å»ºä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œ
class SimpleNet(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(SimpleNet, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

# Create model instance
# åˆ›å»ºæ¨¡å‹å®ä¾‹
model = SimpleNet(input_size=10, hidden_size=20, output_size=1)
print(model)
```

#### Best Practices for Local Development æœ¬åœ°å¼€å‘çš„æœ€ä½³å®è·µ

1. **Organize your notebooks** ç»„ç»‡ä½ çš„ç¬”è®°æœ¬
   - Create separate folders for different projects
   - ä¸ºä¸åŒé¡¹ç›®åˆ›å»ºå•ç‹¬çš„æ–‡ä»¶å¤¹
   - Use descriptive filenames with dates or version numbers
   - ä½¿ç”¨å¸¦æœ‰æ—¥æœŸæˆ–ç‰ˆæœ¬å·çš„æè¿°æ€§æ–‡ä»¶å

2. **Use virtual environments** ä½¿ç”¨è™šæ‹Ÿç¯å¢ƒ
   - Keep dependencies isolated between projects
   - ä¿æŒé¡¹ç›®é—´ä¾èµ–å…³ç³»çš„éš”ç¦»
   ```bash
   # Create virtual environment
   # åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
   conda create -n deeplearning python=3.8
   conda activate deeplearning
   ```

3. **Document your code** è®°å½•ä½ çš„ä»£ç 
   - Use markdown cells to explain your thought process
   - ä½¿ç”¨markdownå•å…ƒæ ¼è§£é‡Šä½ çš„æ€ç»´è¿‡ç¨‹
   - Include comments in code cells
   - åœ¨ä»£ç å•å…ƒæ ¼ä¸­åŒ…å«æ³¨é‡Š

### 20.1.2 Advanced Options é«˜çº§é€‰é¡¹

#### Jupyter Extensions ï¿½jupyteræ‰©å±•

Jupyter extensions can significantly enhance your productivity. Here are some essential ones:

Jupyteræ‰©å±•å¯ä»¥æ˜¾è‘—æé«˜ä½ çš„ç”Ÿäº§åŠ›ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å¿…è¦çš„æ‰©å±•ï¼š

```bash
# Install Jupyter extensions
# å®‰è£…Jupyteræ‰©å±•
pip install jupyter_contrib_nbextensions
jupyter contrib nbextension install --user

# Enable useful extensions
# å¯ç”¨æœ‰ç”¨çš„æ‰©å±•
jupyter nbextension enable --py widgetsnbextension
```

**Popular Extensions çƒ­é—¨æ‰©å±•ï¼š**

1. **Variable Inspector** å˜é‡æ£€æŸ¥å™¨
   - Shows all variables in your namespace
   - æ˜¾ç¤ºå‘½åç©ºé—´ä¸­çš„æ‰€æœ‰å˜é‡
   - Useful for debugging and understanding data structures
   - å¯¹è°ƒè¯•å’Œç†è§£æ•°æ®ç»“æ„å¾ˆæœ‰ç”¨

2. **Table of Contents** ç›®å½•
   - Automatically generates a table of contents from markdown headers
   - ä»markdownæ ‡é¢˜è‡ªåŠ¨ç”Ÿæˆç›®å½•
   - Makes navigation easier in long notebooks
   - ä½¿é•¿ç¬”è®°æœ¬ä¸­çš„å¯¼èˆªæ›´å®¹æ˜“

3. **Code Folding** ä»£ç æŠ˜å 
   - Allows you to collapse code cells
   - å…è®¸ä½ æŠ˜å ä»£ç å•å…ƒæ ¼
   - Helps with organization and focus
   - æœ‰åŠ©äºç»„ç»‡å’Œä¸“æ³¨

#### Magic Commands é­”æ³•å‘½ä»¤

Jupyter notebooks support magic commands that provide powerful functionality:

Jupyterç¬”è®°æœ¬æ”¯æŒæä¾›å¼ºå¤§åŠŸèƒ½çš„é­”æ³•å‘½ä»¤ï¼š

```python
# Time execution of a cell
# è®¡æ—¶å•å…ƒæ ¼çš„æ‰§è¡Œæ—¶é—´
%%time
import time
time.sleep(1)
print("This took about 1 second")

# Profile memory usage
# åˆ†æå†…å­˜ä½¿ç”¨æƒ…å†µ
%load_ext memory_profiler
%memit torch.randn(1000, 1000)

# Load external Python file
# åŠ è½½å¤–éƒ¨Pythonæ–‡ä»¶
# %load external_script.py

# Show matplotlib plots inline
# å†…è”æ˜¾ç¤ºmatplotlibå›¾è¡¨
%matplotlib inline
```

#### GPU Configuration GPUé…ç½®

When working with deep learning models locally, GPU acceleration is crucial:

åœ¨æœ¬åœ°å¤„ç†æ·±åº¦å­¦ä¹ æ¨¡å‹æ—¶ï¼ŒGPUåŠ é€Ÿæ˜¯è‡³å…³é‡è¦çš„ï¼š

```python
# Check GPU availability and configure device
# æ£€æŸ¥GPUå¯ç”¨æ€§å¹¶é…ç½®è®¾å¤‡
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

# Move model to GPU
# å°†æ¨¡å‹ç§»åŠ¨åˆ°GPU
model = model.to(device)
```

### 20.1.3 Summary æ€»ç»“

Jupyter notebooks provide an ideal environment for deep learning experimentation and development. They combine code execution, visualization, and documentation in a single interface, making them perfect for iterative development and research.

Jupyterç¬”è®°æœ¬ä¸ºæ·±åº¦å­¦ä¹ å®éªŒå’Œå¼€å‘æä¾›äº†ç†æƒ³çš„ç¯å¢ƒã€‚å®ƒä»¬åœ¨å•ä¸ªç•Œé¢ä¸­ç»“åˆäº†ä»£ç æ‰§è¡Œã€å¯è§†åŒ–å’Œæ–‡æ¡£ï¼Œä½¿å…¶éå¸¸é€‚åˆè¿­ä»£å¼€å‘å’Œç ”ç©¶ã€‚

Key advantages include:
ä¸»è¦ä¼˜åŠ¿åŒ…æ‹¬ï¼š
- Interactive development and immediate feedback
- äº¤äº’å¼å¼€å‘å’Œå³æ—¶åé¦ˆ
- Rich output formatting (plots, tables, images)
- ä¸°å¯Œçš„è¾“å‡ºæ ¼å¼ï¼ˆå›¾è¡¨ã€è¡¨æ ¼ã€å›¾åƒï¼‰
- Easy sharing and collaboration
- æ˜“äºåˆ†äº«å’Œåä½œ
- Extensive ecosystem of extensions
- æ‰©å±•çš„å¹¿æ³›ç”Ÿæ€ç³»ç»Ÿ

### 20.1.4 Exercises ç»ƒä¹ 

1. **Basic Setup Exercise** åŸºæœ¬è®¾ç½®ç»ƒä¹ 
   - Install Jupyter notebook on your local machine
   - åœ¨æœ¬åœ°æœºå™¨ä¸Šå®‰è£…Jupyterç¬”è®°æœ¬
   - Create a new notebook and verify PyTorch installation
   - åˆ›å»ºæ–°ç¬”è®°æœ¬å¹¶éªŒè¯PyTorchå®‰è£…
   - Implement a simple linear regression model
   - å®ç°ä¸€ä¸ªç®€å•çš„çº¿æ€§å›å½’æ¨¡å‹

2. **Extension Practice** æ‰©å±•ç»ƒä¹ 
   - Install and configure at least 3 Jupyter extensions
   - å®‰è£…å¹¶é…ç½®è‡³å°‘3ä¸ªJupyteræ‰©å±•
   - Use magic commands to profile a deep learning training loop
   - ä½¿ç”¨é­”æ³•å‘½ä»¤åˆ†ææ·±åº¦å­¦ä¹ è®­ç»ƒå¾ªç¯

3. **Documentation Challenge** æ–‡æ¡£æŒ‘æˆ˜
   - Create a well-documented notebook that explains a deep learning concept
   - åˆ›å»ºä¸€ä¸ªè§£é‡Šæ·±åº¦å­¦ä¹ æ¦‚å¿µçš„è¯¦ç»†æ–‡æ¡£ç¬”è®°æœ¬
   - Include mathematical formulas, code examples, and visualizations
   - åŒ…æ‹¬æ•°å­¦å…¬å¼ã€ä»£ç ç¤ºä¾‹å’Œå¯è§†åŒ–

## 20.2 Using Amazon SageMaker ä½¿ç”¨Amazon SageMaker

Amazon SageMaker is a fully managed service that provides every developer and data scientist with the ability to build, train, and deploy machine learning models quickly. It eliminates the heavy lifting from each step of the machine learning process to make it easier to develop high-quality models.

Amazon SageMakeræ˜¯ä¸€é¡¹å®Œå…¨æ‰˜ç®¡çš„æœåŠ¡ï¼Œä¸ºæ¯ä¸ªå¼€å‘è€…å’Œæ•°æ®ç§‘å­¦å®¶æä¾›å¿«é€Ÿæ„å»ºã€è®­ç»ƒå’Œéƒ¨ç½²æœºå™¨å­¦ä¹ æ¨¡å‹çš„èƒ½åŠ›ã€‚å®ƒæ¶ˆé™¤äº†æœºå™¨å­¦ä¹ è¿‡ç¨‹æ¯ä¸ªæ­¥éª¤çš„ç¹é‡å·¥ä½œï¼Œä½¿å¼€å‘é«˜è´¨é‡æ¨¡å‹å˜å¾—æ›´å®¹æ˜“ã€‚

### 20.2.1 Signing Up æ³¨å†Œ

To get started with Amazon SageMaker, you need an AWS account. Here's the step-by-step process:

è¦å¼€å§‹ä½¿ç”¨Amazon SageMakerï¼Œä½ éœ€è¦ä¸€ä¸ªAWSè´¦æˆ·ã€‚ä»¥ä¸‹æ˜¯é€æ­¥è¿‡ç¨‹ï¼š

#### Step 1: Create AWS Account æ­¥éª¤1ï¼šåˆ›å»ºAWSè´¦æˆ·

1. Go to https://aws.amazon.com/
   è®¿é—® https://aws.amazon.com/
2. Click "Create an AWS Account"
   ç‚¹å‡»"åˆ›å»ºAWSè´¦æˆ·"
3. Provide your email, password, and AWS account name
   æä¾›ä½ çš„é‚®ç®±ã€å¯†ç å’ŒAWSè´¦æˆ·å
4. Add payment information (required for verification)
   æ·»åŠ ä»˜æ¬¾ä¿¡æ¯ï¼ˆéªŒè¯æ‰€éœ€ï¼‰
5. Verify your identity via phone
   é€šè¿‡ç”µè¯éªŒè¯èº«ä»½

#### Step 2: Access SageMaker æ­¥éª¤2ï¼šè®¿é—®SageMaker

```bash
# Once logged into AWS Console
# ç™»å½•AWSæ§åˆ¶å°å
# 1. Search for "SageMaker" in the AWS services search bar
# 1. åœ¨AWSæœåŠ¡æœç´¢æ ä¸­æœç´¢"SageMaker"
# 2. Click on "Amazon SageMaker"
# 2. ç‚¹å‡»"Amazon SageMaker"
# 3. You'll be taken to the SageMaker dashboard
# 3. ä½ å°†è¢«å¸¦åˆ°SageMakerä»ªè¡¨æ¿
```

#### Step 3: Understanding Pricing æ­¥éª¤3ï¼šäº†è§£å®šä»·

SageMaker pricing is based on usage:
SageMakerå®šä»·åŸºäºä½¿ç”¨æƒ…å†µï¼š

- **Notebook instances**: Charged per hour of usage
- **ç¬”è®°æœ¬å®ä¾‹**ï¼šæŒ‰ä½¿ç”¨å°æ—¶æ”¶è´¹
- **Training**: Charged per second of training time
- **è®­ç»ƒ**ï¼šæŒ‰è®­ç»ƒæ—¶é—´ç§’è®¡è´¹
- **Inference**: Charged per hour for hosted endpoints
- **æ¨ç†**ï¼šæ‰˜ç®¡ç«¯ç‚¹æŒ‰å°æ—¶æ”¶è´¹

*Important*: Always stop instances when not in use to avoid unnecessary charges.
*é‡è¦æç¤º*ï¼šä¸ä½¿ç”¨æ—¶åŠ¡å¿…åœæ­¢å®ä¾‹ä»¥é¿å…ä¸å¿…è¦çš„è´¹ç”¨ã€‚ 

### 20.2.2 Creating a SageMaker Instance åˆ›å»ºSageMakerå®ä¾‹

Once you have access to SageMaker, creating a notebook instance is straightforward. A notebook instance is a machine learning compute instance running the Jupyter Notebook App, where you can prepare and process data, write code to train models, deploy models, and test or validate your models.

ä¸€æ—¦ä½ å¯ä»¥è®¿é—®SageMakerï¼Œåˆ›å»ºç¬”è®°æœ¬å®ä¾‹å°±å¾ˆç®€å•äº†ã€‚ç¬”è®°æœ¬å®ä¾‹æ˜¯è¿è¡ŒJupyterç¬”è®°æœ¬åº”ç”¨ç¨‹åºçš„æœºå™¨å­¦ä¹ è®¡ç®—å®ä¾‹ï¼Œä½ å¯ä»¥åœ¨å…¶ä¸­å‡†å¤‡å’Œå¤„ç†æ•°æ®ã€ç¼–å†™ä»£ç æ¥è®­ç»ƒæ¨¡å‹ã€éƒ¨ç½²æ¨¡å‹ä»¥åŠæµ‹è¯•æˆ–éªŒè¯æ¨¡å‹ã€‚

#### Step-by-Step Instance Creation é€æ­¥åˆ›å»ºå®ä¾‹

```bash
# Navigate to SageMaker Console
# å¯¼èˆªåˆ°SageMakeræ§åˆ¶å°
# 1. In AWS Console, go to SageMaker service
# 1. åœ¨AWSæ§åˆ¶å°ä¸­ï¼Œè½¬åˆ°SageMakeræœåŠ¡
# 2. Click "Notebook instances" in the left sidebar
# 2. åœ¨å·¦ä¾§è¾¹æ ç‚¹å‡»"ç¬”è®°æœ¬å®ä¾‹"
# 3. Click "Create notebook instance"
# 3. ç‚¹å‡»"åˆ›å»ºç¬”è®°æœ¬å®ä¾‹"
```

#### Instance Configuration å®ä¾‹é…ç½®

When creating a SageMaker notebook instance, you need to configure several important settings:

åˆ›å»ºSageMakerç¬”è®°æœ¬å®ä¾‹æ—¶ï¼Œä½ éœ€è¦é…ç½®å‡ ä¸ªé‡è¦è®¾ç½®ï¼š

**1. Basic Settings åŸºæœ¬è®¾ç½®**

```python
# Instance configuration parameters
# å®ä¾‹é…ç½®å‚æ•°
instance_config = {
    "notebook_instance_name": "my-deep-learning-notebook",  # å®ä¾‹åç§°
    "instance_type": "ml.t3.medium",  # å®ä¾‹ç±»å‹ï¼ˆCPUå®ä¾‹ï¼Œé€‚åˆå¼€å§‹ï¼‰
    "role_arn": "arn:aws:iam::account:role/service-role/AmazonSageMaker-ExecutionRole"
}

# For GPU instances (more expensive but faster for training)
# GPUå®ä¾‹ï¼ˆæ›´æ˜‚è´µä½†è®­ç»ƒæ›´å¿«ï¼‰
gpu_instance_config = {
    "instance_type": "ml.p3.2xlarge",  # GPUå®ä¾‹ç±»å‹
    "volume_size": 20  # å­˜å‚¨å¤§å°ï¼ˆGBï¼‰
}
```

**2. IAM Role Configuration IAMè§’è‰²é…ç½®**

SageMaker needs permissions to access other AWS services. You can either create a new role or use an existing one:

SageMakeréœ€è¦æƒé™æ¥è®¿é—®å…¶ä»–AWSæœåŠ¡ã€‚ä½ å¯ä»¥åˆ›å»ºæ–°è§’è‰²æˆ–ä½¿ç”¨ç°æœ‰è§’è‰²ï¼š

```python
# Creating a SageMaker execution role
# åˆ›å»ºSageMakeræ‰§è¡Œè§’è‰²
import boto3

def create_sagemaker_role():
    """
    Create IAM role for SageMaker with necessary permissions
    ä¸ºSageMakeråˆ›å»ºå…·æœ‰å¿…è¦æƒé™çš„IAMè§’è‰²
    """
    iam = boto3.client('iam')
    
    # Define trust policy
    # å®šä¹‰ä¿¡ä»»ç­–ç•¥
    trust_policy = {
        "Version": "2012-10-17",
        "Statement": [
            {
                "Effect": "Allow",
                "Principal": {"Service": "sagemaker.amazonaws.com"},
                "Action": "sts:AssumeRole"
            }
        ]
    }
    
    # Create role
    # åˆ›å»ºè§’è‰²
    role_name = "SageMakerExecutionRole"
    
    try:
        response = iam.create_role(
            RoleName=role_name,
            AssumeRolePolicyDocument=str(trust_policy),
            Description="Role for SageMaker notebook instances"
        )
        print(f"åˆ›å»ºè§’è‰²æˆåŠŸ: {response['Role']['Arn']}")
        return response['Role']['Arn']
    except Exception as e:
        print(f"åˆ›å»ºè§’è‰²å¤±è´¥: {e}")
        return None
```

#### Instance Types and Pricing å®ä¾‹ç±»å‹å’Œå®šä»·

Understanding different instance types helps you choose the right one for your needs:

äº†è§£ä¸åŒçš„å®ä¾‹ç±»å‹æœ‰åŠ©äºä½ é€‰æ‹©é€‚åˆéœ€æ±‚çš„å®ä¾‹ï¼š

```python
# SageMaker instance types comparison
# SageMakerå®ä¾‹ç±»å‹æ¯”è¾ƒ
instance_types = {
    "ml.t3.medium": {
        "vcpu": 2,
        "memory_gb": 4,
        "gpu": 0,
        "price_per_hour": 0.0464,
        "use_case": "å¼€å‘å’Œæµ‹è¯•"
    },
    "ml.m5.large": {
        "vcpu": 2,
        "memory_gb": 8,
        "gpu": 0,
        "price_per_hour": 0.096,
        "use_case": "ä¸­ç­‰è®¡ç®—éœ€æ±‚"
    },
    "ml.p3.2xlarge": {
        "vcpu": 8,
        "memory_gb": 61,
        "gpu": 1,  # Tesla V100
        "price_per_hour": 3.06,
        "use_case": "GPUåŠ é€Ÿè®­ç»ƒ"
    },
    "ml.p4d.24xlarge": {
        "vcpu": 96,
        "memory_gb": 1152,
        "gpu": 8,  # A100
        "price_per_hour": 32.77,
        "use_case": "å¤§è§„æ¨¡æ·±åº¦å­¦ä¹ "
    }
}

# Display comparison
# æ˜¾ç¤ºæ¯”è¾ƒ
print("SageMakerå®ä¾‹ç±»å‹æ¯”è¾ƒ:")
print("="*80)
print(f"{'ç±»å‹':<15} {'vCPU':<5} {'å†…å­˜(GB)':<8} {'GPU':<3} {'æ¯å°æ—¶ä»·æ ¼($)':<12} {'ç”¨é€”'}")
print("-"*80)

for instance_type, specs in instance_types.items():
    print(f"{instance_type:<15} {specs['vcpu']:<5} {specs['memory_gb']:<8} "
          f"{specs['gpu']:<3} {specs['price_per_hour']:<12} {specs['use_case']}")
```

### 20.2.3 Running and Stopping an Instance è¿è¡Œå’Œåœæ­¢å®ä¾‹

Managing your SageMaker instances properly is crucial for cost control and efficient workflow. Unlike EC2 instances, SageMaker notebook instances are designed to be started and stopped as needed.

æ­£ç¡®ç®¡ç†SageMakerå®ä¾‹å¯¹äºæˆæœ¬æ§åˆ¶å’Œé«˜æ•ˆå·¥ä½œæµç¨‹è‡³å…³é‡è¦ã€‚ä¸EC2å®ä¾‹ä¸åŒï¼ŒSageMakerç¬”è®°æœ¬å®ä¾‹è®¾è®¡ä¸ºæ ¹æ®éœ€è¦å¯åŠ¨å’Œåœæ­¢ã€‚

#### Starting an Instance å¯åŠ¨å®ä¾‹

```python
import boto3
import time

def start_notebook_instance(instance_name):
    """
    Start a SageMaker notebook instance
    å¯åŠ¨SageMakerç¬”è®°æœ¬å®ä¾‹
    """
    sagemaker = boto3.client('sagemaker')
    
    try:
        # Check current status
        # æ£€æŸ¥å½“å‰çŠ¶æ€
        response = sagemaker.describe_notebook_instance(
            NotebookInstanceName=instance_name
        )
        current_status = response['NotebookInstanceStatus']
        
        if current_status == 'InService':
            print(f"å®ä¾‹ {instance_name} å·²ç»åœ¨è¿è¡Œ")
            return response['Url']
        elif current_status == 'Stopped':
            # Start the instance
            # å¯åŠ¨å®ä¾‹
            sagemaker.start_notebook_instance(
                NotebookInstanceName=instance_name
            )
            print(f"æ­£åœ¨å¯åŠ¨å®ä¾‹ {instance_name}...")
            
            # Wait for instance to be ready
            # ç­‰å¾…å®ä¾‹å‡†å¤‡å°±ç»ª
            while True:
                response = sagemaker.describe_notebook_instance(
                    NotebookInstanceName=instance_name
                )
                status = response['NotebookInstanceStatus']
                print(f"å½“å‰çŠ¶æ€: {status}")
                
                if status == 'InService':
                    print("å®ä¾‹å¯åŠ¨æˆåŠŸ!")
                    return response['Url']
                elif status == 'Failed':
                    print("å®ä¾‹å¯åŠ¨å¤±è´¥!")
                    return None
                
                time.sleep(30)  # Wait 30 seconds before checking again
                
    except Exception as e:
        print(f"å¯åŠ¨å®ä¾‹æ—¶å‡ºé”™: {e}")
        return None

# Example usage
# ä½¿ç”¨ç¤ºä¾‹
# notebook_url = start_notebook_instance("my-deep-learning-notebook")
# if notebook_url:
#     print(f"ç¬”è®°æœ¬URL: {notebook_url}")
```

#### Stopping an Instance åœæ­¢å®ä¾‹

**Important**: Always stop your instances when you're not using them to avoid unnecessary charges.

**é‡è¦æç¤º**ï¼šä¸ä½¿ç”¨æ—¶åŠ¡å¿…åœæ­¢å®ä¾‹ï¼Œä»¥é¿å…ä¸å¿…è¦çš„è´¹ç”¨ã€‚

```python
def stop_notebook_instance(instance_name):
    """
    Stop a SageMaker notebook instance
    åœæ­¢SageMakerç¬”è®°æœ¬å®ä¾‹
    """
    sagemaker = boto3.client('sagemaker')
    
    try:
        # Check current status
        # æ£€æŸ¥å½“å‰çŠ¶æ€
        response = sagemaker.describe_notebook_instance(
            NotebookInstanceName=instance_name
        )
        current_status = response['NotebookInstanceStatus']
        
        if current_status == 'Stopped':
            print(f"å®ä¾‹ {instance_name} å·²ç»åœæ­¢")
        elif current_status == 'InService':
            # Stop the instance
            # åœæ­¢å®ä¾‹
            sagemaker.stop_notebook_instance(
                NotebookInstanceName=instance_name
            )
            print(f"æ­£åœ¨åœæ­¢å®ä¾‹ {instance_name}...")
            
            # Wait for instance to stop
            # ç­‰å¾…å®ä¾‹åœæ­¢
            while True:
                response = sagemaker.describe_notebook_instance(
                    NotebookInstanceName=instance_name
                )
                status = response['NotebookInstanceStatus']
                print(f"å½“å‰çŠ¶æ€: {status}")
                
                if status == 'Stopped':
                    print("å®ä¾‹åœæ­¢æˆåŠŸ!")
                    break
                elif status == 'Failed':
                    print("åœæ­¢å®ä¾‹å¤±è´¥!")
                    break
                
                time.sleep(30)
                
    except Exception as e:
        print(f"åœæ­¢å®ä¾‹æ—¶å‡ºé”™: {e}")

# Automated stop scheduler
# è‡ªåŠ¨åœæ­¢è°ƒåº¦å™¨
def schedule_auto_stop(instance_name, hours=2):
    """
    Schedule automatic stop of instance after specified hours
    åœ¨æŒ‡å®šå°æ—¶åå®‰æ’è‡ªåŠ¨åœæ­¢å®ä¾‹
    """
    import threading
    
    def auto_stop():
        time.sleep(hours * 3600)  # Convert hours to seconds
        print(f"è‡ªåŠ¨åœæ­¢å®ä¾‹ {instance_name} (è¿è¡Œäº† {hours} å°æ—¶)")
        stop_notebook_instance(instance_name)
    
    # Start auto-stop timer in background
    # åœ¨åå°å¯åŠ¨è‡ªåŠ¨åœæ­¢è®¡æ—¶å™¨
    timer_thread = threading.Thread(target=auto_stop)
    timer_thread.daemon = True
    timer_thread.start()
    
    print(f"å·²è®¾ç½® {hours} å°æ—¶åè‡ªåŠ¨åœæ­¢å®ä¾‹")
```

#### Instance Lifecycle Management å®ä¾‹ç”Ÿå‘½å‘¨æœŸç®¡ç†

```python
def get_instance_metrics(instance_name, days=7):
    """
    Get usage metrics for cost analysis
    è·å–ç”¨äºæˆæœ¬åˆ†æçš„ä½¿ç”¨æŒ‡æ ‡
    """
    import boto3
    from datetime import datetime, timedelta
    
    cloudwatch = boto3.client('cloudwatch')
    
    # Calculate time range
    # è®¡ç®—æ—¶é—´èŒƒå›´
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(days=days)
    
    # Get CPU utilization metrics
    # è·å–CPUåˆ©ç”¨ç‡æŒ‡æ ‡
    response = cloudwatch.get_metric_statistics(
        Namespace='AWS/SageMaker',
        MetricName='CPUUtilization',
        Dimensions=[
            {
                'Name': 'NotebookInstanceName',
                'Value': instance_name
            }
        ],
        StartTime=start_time,
        EndTime=end_time,
        Period=3600,  # 1 hour intervals
        Statistics=['Average', 'Maximum']
    )
    
    # Calculate estimated costs
    # è®¡ç®—ä¼°ç®—æˆæœ¬
    instance_hours = len(response['Datapoints'])
    estimated_cost = instance_hours * 0.0464  # ml.t3.medium price
    
    print(f"å®ä¾‹ {instance_name} æœ€è¿‘ {days} å¤©çš„ä½¿ç”¨æƒ…å†µ:")
    print(f"è¿è¡Œå°æ—¶æ•°: {instance_hours}")
    print(f"ä¼°ç®—æˆæœ¬: ${estimated_cost:.2f}")
    print(f"å¹³å‡CPUåˆ©ç”¨ç‡: {sum(dp['Average'] for dp in response['Datapoints'])/len(response['Datapoints']):.1f}%" if response['Datapoints'] else "æ— æ•°æ®")

# Cost optimization recommendations
# æˆæœ¬ä¼˜åŒ–å»ºè®®
def analyze_instance_usage(instance_name):
    """
    Analyze instance usage and provide cost optimization recommendations
    åˆ†æå®ä¾‹ä½¿ç”¨æƒ…å†µå¹¶æä¾›æˆæœ¬ä¼˜åŒ–å»ºè®®
    """
    sagemaker = boto3.client('sagemaker')
    
    # Get instance details
    # è·å–å®ä¾‹è¯¦æƒ…
    response = sagemaker.describe_notebook_instance(
        NotebookInstanceName=instance_name
    )
    
    instance_type = response['InstanceType']
    creation_time = response['CreationTime']
    last_modified = response['LastModifiedTime']
    
    # Calculate age
    # è®¡ç®—ä½¿ç”¨æ—¶é•¿
    age = datetime.now(creation_time.tzinfo) - creation_time
    
    print(f"å®ä¾‹åˆ†ææŠ¥å‘Š:")
    print(f"å®ä¾‹ç±»å‹: {instance_type}")
    print(f"åˆ›å»ºæ—¶é—´: {creation_time}")
    print(f"å®ä¾‹å¹´é¾„: {age.days} å¤©")
    
    # Provide recommendations
    # æä¾›å»ºè®®
    if age.days > 30 and instance_type.startswith('ml.p'):
        print("ğŸ’¡ å»ºè®®: GPUå®ä¾‹å·²ä½¿ç”¨è¶…è¿‡30å¤©ï¼Œè€ƒè™‘æ˜¯å¦éœ€è¦é™çº§åˆ°CPUå®ä¾‹ä»¥èŠ‚çœæˆæœ¬")
    elif age.days > 7 and 'large' in instance_type:
        print("ğŸ’¡ å»ºè®®: æ£€æŸ¥æ˜¯å¦å¯ä»¥ä½¿ç”¨æ›´å°çš„å®ä¾‹ç±»å‹")
    
    print("ğŸ’¡ æˆæœ¬ä¼˜åŒ–æç¤º:")
    print("- ä¸ä½¿ç”¨æ—¶ç«‹å³åœæ­¢å®ä¾‹")
    print("- å®šæœŸå¤‡ä»½é‡è¦ç¬”è®°æœ¬åˆ°S3")
    print("- è€ƒè™‘ä½¿ç”¨SageMaker Studioä½œä¸ºç°ä»£æ›¿ä»£æ–¹æ¡ˆ")
```

### 20.2.4 Updating Notebooks æ›´æ–°ç¬”è®°æœ¬

Keeping your SageMaker environment up to date is important for security, performance, and access to the latest features. There are several ways to update and manage your notebooks effectively.

ä¿æŒSageMakerç¯å¢ƒæ›´æ–°å¯¹äºå®‰å…¨æ€§ã€æ€§èƒ½å’Œè®¿é—®æœ€æ–°åŠŸèƒ½å¾ˆé‡è¦ã€‚æœ‰å‡ ç§æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°æ›´æ–°å’Œç®¡ç†ç¬”è®°æœ¬ã€‚

#### Git Integration Gité›†æˆ

SageMaker supports Git integration, allowing you to clone repositories directly into your notebook instance:

SageMakeræ”¯æŒGité›†æˆï¼Œå…è®¸ä½ ç›´æ¥å°†å­˜å‚¨åº“å…‹éš†åˆ°ç¬”è®°æœ¬å®ä¾‹ä¸­ï¼š

```python
# Setting up Git repository in SageMaker
# åœ¨SageMakerä¸­è®¾ç½®Gitå­˜å‚¨åº“
import subprocess
import os

def setup_git_repository(repo_url, local_path="/home/ec2-user/SageMaker"):
    """
    Clone a Git repository to SageMaker notebook instance
    å°†Gitå­˜å‚¨åº“å…‹éš†åˆ°SageMakerç¬”è®°æœ¬å®ä¾‹
    """
    try:
        # Change to SageMaker directory
        # åˆ‡æ¢åˆ°SageMakerç›®å½•
        os.chdir(local_path)
        
        # Clone repository
        # å…‹éš†å­˜å‚¨åº“
        result = subprocess.run(
            ["git", "clone", repo_url],
            capture_output=True,
            text=True
        )
        
        if result.returncode == 0:
            print(f"æˆåŠŸå…‹éš†å­˜å‚¨åº“: {repo_url}")
            print(f"æœ¬åœ°è·¯å¾„: {local_path}")
        else:
            print(f"å…‹éš†å¤±è´¥: {result.stderr}")
            
    except Exception as e:
        print(f"è®¾ç½®Gitå­˜å‚¨åº“æ—¶å‡ºé”™: {e}")

# Example: Clone a deep learning course repository
# ç¤ºä¾‹ï¼šå…‹éš†æ·±åº¦å­¦ä¹ è¯¾ç¨‹å­˜å‚¨åº“
# setup_git_repository("https://github.com/d2l-ai/d2l-en.git")
```

#### Package Management åŒ…ç®¡ç†

Managing Python packages in SageMaker requires understanding the environment structure:

åœ¨SageMakerä¸­ç®¡ç†PythonåŒ…éœ€è¦äº†è§£ç¯å¢ƒç»“æ„ï¼š

```python
# Package installation and management
# åŒ…å®‰è£…å’Œç®¡ç†
import sys
import subprocess

def install_packages(packages):
    """
    Install Python packages in SageMaker environment
    åœ¨SageMakerç¯å¢ƒä¸­å®‰è£…PythonåŒ…
    """
    for package in packages:
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", package])
            print(f"âœ… æˆåŠŸå®‰è£…: {package}")
        except subprocess.CalledProcessError as e:
            print(f"âŒ å®‰è£…å¤±è´¥ {package}: {e}")

# Essential packages for deep learning
# æ·±åº¦å­¦ä¹ å¿…éœ€åŒ…
essential_packages = [
    "torch",
    "torchvision",
    "transformers",
    "datasets",
    "matplotlib",
    "seaborn",
    "scikit-learn",
    "pandas",
    "numpy",
    "tensorboard"
]

# Install packages
# å®‰è£…åŒ…
# install_packages(essential_packages)

# Check installed packages
# æ£€æŸ¥å·²å®‰è£…çš„åŒ…
def list_installed_packages():
    """
    List all installed packages with versions
    åˆ—å‡ºæ‰€æœ‰å·²å®‰è£…åŒ…åŠå…¶ç‰ˆæœ¬
    """
    result = subprocess.run([sys.executable, "-m", "pip", "list"], 
                          capture_output=True, text=True)
    print("å·²å®‰è£…çš„åŒ…:")
    print(result.stdout)

# Create requirements.txt for reproducibility
# åˆ›å»ºrequirements.txtä»¥ç¡®ä¿å¯é‡å¤æ€§
def create_requirements_file():
    """
    Create requirements.txt file for environment reproducibility
    åˆ›å»ºrequirements.txtæ–‡ä»¶ä»¥ç¡®ä¿ç¯å¢ƒå¯é‡å¤æ€§
    """
    result = subprocess.run([sys.executable, "-m", "pip", "freeze"], 
                          capture_output=True, text=True)
    
    with open("requirements.txt", "w") as f:
        f.write(result.stdout)
    
    print("å·²åˆ›å»º requirements.txt æ–‡ä»¶")
    print("ä½¿ç”¨ 'pip install -r requirements.txt' æ¥é‡æ–°åˆ›å»ºç¯å¢ƒ")
```

#### Notebook Lifecycle Configuration ç¬”è®°æœ¬ç”Ÿå‘½å‘¨æœŸé…ç½®

Lifecycle configurations allow you to automate setup tasks when instances start:

ç”Ÿå‘½å‘¨æœŸé…ç½®å…è®¸ä½ åœ¨å®ä¾‹å¯åŠ¨æ—¶è‡ªåŠ¨åŒ–è®¾ç½®ä»»åŠ¡ï¼š

```bash
#!/bin/bash
# Lifecycle configuration script
# ç”Ÿå‘½å‘¨æœŸé…ç½®è„šæœ¬

# This script runs when the notebook instance starts
# æ­¤è„šæœ¬åœ¨ç¬”è®°æœ¬å®ä¾‹å¯åŠ¨æ—¶è¿è¡Œ

set -e

# Update system packages
# æ›´æ–°ç³»ç»ŸåŒ…
sudo yum update -y

# Install additional system dependencies
# å®‰è£…é¢å¤–çš„ç³»ç»Ÿä¾èµ–
sudo yum install -y htop tree

# Install conda packages
# å®‰è£…condaåŒ…
conda install -y -c conda-forge jupyterlab-git

# Install pip packages
# å®‰è£…pipåŒ…
pip install --upgrade pip
pip install torch torchvision torchaudio
pip install transformers datasets
pip install wandb tensorboard

# Set up Jupyter extensions
# è®¾ç½®Jupyteræ‰©å±•
jupyter labextension install @jupyter-widgets/jupyterlab-manager

# Create common directories
# åˆ›å»ºå¸¸ç”¨ç›®å½•
mkdir -p /home/ec2-user/SageMaker/data
mkdir -p /home/ec2-user/SageMaker/models
mkdir -p /home/ec2-user/SageMaker/notebooks

# Set up Git configuration (replace with your details)
# è®¾ç½®Gité…ç½®ï¼ˆæ›¿æ¢ä¸ºä½ çš„è¯¦ç»†ä¿¡æ¯ï¼‰
git config --global user.name "Your Name"
git config --global user.email "your.email@example.com"

echo "Lifecycle configuration completed successfully!"
echo "ç”Ÿå‘½å‘¨æœŸé…ç½®æˆåŠŸå®Œæˆ!"
```

#### Environment Management ç¯å¢ƒç®¡ç†

```python
# Environment setup and management
# ç¯å¢ƒè®¾ç½®å’Œç®¡ç†
import json
import boto3

def create_lifecycle_config(config_name, script_content):
    """
    Create a lifecycle configuration for SageMaker
    ä¸ºSageMakeråˆ›å»ºç”Ÿå‘½å‘¨æœŸé…ç½®
    """
    sagemaker = boto3.client('sagemaker')
    
    try:
        response = sagemaker.create_notebook_instance_lifecycle_config(
            NotebookInstanceLifecycleConfigName=config_name,
            OnStart=[
                {
                    'Content': script_content
                }
            ]
        )
        print(f"æˆåŠŸåˆ›å»ºç”Ÿå‘½å‘¨æœŸé…ç½®: {config_name}")
        return response['NotebookInstanceLifecycleConfigArn']
    except Exception as e:
        print(f"åˆ›å»ºç”Ÿå‘½å‘¨æœŸé…ç½®å¤±è´¥: {e}")
        return None

# Environment monitoring
# ç¯å¢ƒç›‘æ§
def monitor_environment():
    """
    Monitor the current environment status
    ç›‘æ§å½“å‰ç¯å¢ƒçŠ¶æ€
    """
    import psutil
    import GPUtil
    
    # CPU and Memory info
    # CPUå’Œå†…å­˜ä¿¡æ¯
    cpu_percent = psutil.cpu_percent(interval=1)
    memory = psutil.virtual_memory()
    disk = psutil.disk_usage('/')
    
    print("ç¯å¢ƒçŠ¶æ€ç›‘æ§:")
    print("="*50)
    print(f"CPU ä½¿ç”¨ç‡: {cpu_percent:.1f}%")
    print(f"å†…å­˜ä½¿ç”¨ç‡: {memory.percent:.1f}% ({memory.used/1024**3:.1f}GB / {memory.total/1024**3:.1f}GB)")
    print(f"ç£ç›˜ä½¿ç”¨ç‡: {disk.percent:.1f}% ({disk.used/1024**3:.1f}GB / {disk.total/1024**3:.1f}GB)")
    
    # GPU info (if available)
    # GPUä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    try:
        gpus = GPUtil.getGPUs()
        if gpus:
            print(f"GPU æ•°é‡: {len(gpus)}")
            for i, gpu in enumerate(gpus):
                print(f"GPU {i}: {gpu.name}")
                print(f"  æ˜¾å­˜ä½¿ç”¨ç‡: {gpu.memoryUtil*100:.1f}%")
                print(f"  GPUä½¿ç”¨ç‡: {gpu.load*100:.1f}%")
                print(f"  æ¸©åº¦: {gpu.temperature}Â°C")
    except:
        print("æœªæ£€æµ‹åˆ°GPUæˆ–GPUtilä¸å¯ç”¨")

# Run monitoring
# è¿è¡Œç›‘æ§
# monitor_environment()
```

### 20.2.5 Summary æ€»ç»“

Amazon SageMaker provides a powerful, managed environment for deep learning development. Here are the key takeaways:

Amazon SageMakerä¸ºæ·±åº¦å­¦ä¹ å¼€å‘æä¾›äº†å¼ºå¤§çš„æ‰˜ç®¡ç¯å¢ƒã€‚ä»¥ä¸‹æ˜¯å…³é”®è¦ç‚¹ï¼š

**Advantages ä¼˜åŠ¿:**
- Fully managed Jupyter notebook environment å®Œå…¨æ‰˜ç®¡çš„Jupyterç¬”è®°æœ¬ç¯å¢ƒ
- Easy scalability from CPU to GPU instances ä»CPUåˆ°GPUå®ä¾‹çš„è½»æ¾æ‰©å±•
- Integrated with AWS ecosystem AWSç”Ÿæ€ç³»ç»Ÿé›†æˆ
- Built-in security and compliance features å†…ç½®å®‰å…¨å’Œåˆè§„åŠŸèƒ½
- No infrastructure management required æ— éœ€åŸºç¡€è®¾æ–½ç®¡ç†

**Best Practices æœ€ä½³å®è·µ:**
- Always stop instances when not in use ä¸ä½¿ç”¨æ—¶åŠ¡å¿…åœæ­¢å®ä¾‹
- Use lifecycle configurations for consistent environments ä½¿ç”¨ç”Ÿå‘½å‘¨æœŸé…ç½®ç¡®ä¿ç¯å¢ƒä¸€è‡´æ€§
- Implement proper IAM roles and policies å®æ–½é€‚å½“çš„IAMè§’è‰²å’Œç­–ç•¥
- Regular backup of important notebooks å®šæœŸå¤‡ä»½é‡è¦ç¬”è®°æœ¬
- Monitor costs and usage patterns ç›‘æ§æˆæœ¬å’Œä½¿ç”¨æ¨¡å¼

**Cost Optimization æˆæœ¬ä¼˜åŒ–:**
- Start with smaller instances and scale up as needed ä»è¾ƒå°å®ä¾‹å¼€å§‹ï¼Œæ ¹æ®éœ€è¦æ‰©å±•
- Use Spot instances for training jobs ä½¿ç”¨Spotå®ä¾‹è¿›è¡Œè®­ç»ƒä½œä¸š
- Leverage S3 for data storage instead of instance storage åˆ©ç”¨S3è¿›è¡Œæ•°æ®å­˜å‚¨è€Œä¸æ˜¯å®ä¾‹å­˜å‚¨
- Set up billing alerts and usage monitoring è®¾ç½®è´¦å•è­¦æŠ¥å’Œä½¿ç”¨ç›‘æ§

### 20.2.6 Exercises ç»ƒä¹ 

1. **Basic Setup Exercise åŸºæœ¬è®¾ç½®ç»ƒä¹ **
   - Create your first SageMaker notebook instance åˆ›å»ºä½ çš„ç¬¬ä¸€ä¸ªSageMakerç¬”è®°æœ¬å®ä¾‹
   - Install PyTorch and verify GPU availability å®‰è£…PyTorchå¹¶éªŒè¯GPUå¯ç”¨æ€§
   - Run a simple neural network training example è¿è¡Œç®€å•çš„ç¥ç»ç½‘ç»œè®­ç»ƒç¤ºä¾‹

2. **Cost Management Exercise æˆæœ¬ç®¡ç†ç»ƒä¹ **
   - Set up CloudWatch billing alerts è®¾ç½®CloudWatchè´¦å•è­¦æŠ¥
   - Create a script to automatically stop instances after inactivity åˆ›å»ºè„šæœ¬åœ¨ä¸æ´»åŠ¨åè‡ªåŠ¨åœæ­¢å®ä¾‹
   - Compare costs between different instance types æ¯”è¾ƒä¸åŒå®ä¾‹ç±»å‹çš„æˆæœ¬

3. **Environment Configuration Exercise ç¯å¢ƒé…ç½®ç»ƒä¹ **
   - Create a lifecycle configuration script åˆ›å»ºç”Ÿå‘½å‘¨æœŸé…ç½®è„šæœ¬
   - Set up Git integration with your favorite repository è®¾ç½®ä¸ä½ å–œæ¬¢çš„å­˜å‚¨åº“çš„Gité›†æˆ
   - Create a custom conda environment for your project ä¸ºä½ çš„é¡¹ç›®åˆ›å»ºè‡ªå®šä¹‰condaç¯å¢ƒ

## 20.3 Using AWS EC2 Instances ä½¿ç”¨AWS EC2å®ä¾‹

Amazon Elastic Compute Cloud (EC2) provides scalable computing capacity in the cloud. Unlike SageMaker, EC2 gives you complete control over the computing environment, making it ideal when you need custom configurations or want to optimize costs for long-running workloads.

Amazonå¼¹æ€§è®¡ç®—äº‘ï¼ˆEC2ï¼‰åœ¨äº‘ä¸­æä¾›å¯æ‰©å±•çš„è®¡ç®—èƒ½åŠ›ã€‚ä¸SageMakerä¸åŒï¼ŒEC2è®©ä½ å®Œå…¨æ§åˆ¶è®¡ç®—ç¯å¢ƒï¼Œè¿™ä½¿å…¶åœ¨éœ€è¦è‡ªå®šä¹‰é…ç½®æˆ–æƒ³è¦ä¸ºé•¿æ—¶é—´è¿è¡Œçš„å·¥ä½œè´Ÿè½½ä¼˜åŒ–æˆæœ¬æ—¶éå¸¸ç†æƒ³ã€‚

Think of EC2 as renting a virtual computer in the cloud - you get to choose its specifications, install whatever software you need, and configure it exactly how you want.

æŠŠEC2æƒ³è±¡æˆåœ¨äº‘ä¸­ç§Ÿç”¨è™šæ‹Ÿè®¡ç®—æœºâ€”â€”ä½ å¯ä»¥é€‰æ‹©å…¶è§„æ ¼ã€å®‰è£…æ‰€éœ€çš„ä»»ä½•è½¯ä»¶ï¼Œå¹¶å®Œå…¨æŒ‰ç…§ä½ çš„éœ€è¦é…ç½®å®ƒã€‚

### 20.3.1 Creating and Running an EC2 Instance åˆ›å»ºå’Œè¿è¡ŒEC2å®ä¾‹

Creating an EC2 instance for deep learning requires careful consideration of instance types, storage, networking, and security configurations.

ä¸ºæ·±åº¦å­¦ä¹ åˆ›å»ºEC2å®ä¾‹éœ€è¦ä»”ç»†è€ƒè™‘å®ä¾‹ç±»å‹ã€å­˜å‚¨ã€ç½‘ç»œå’Œå®‰å…¨é…ç½®ã€‚

#### Step 1: Choosing the Right Instance Type æ­¥éª¤1ï¼šé€‰æ‹©æ­£ç¡®çš„å®ä¾‹ç±»å‹

```python
# EC2 Instance types for deep learning
# ç”¨äºæ·±åº¦å­¦ä¹ çš„EC2å®ä¾‹ç±»å‹
ec2_instances = {
    # CPU-optimized instances for development
    # ç”¨äºå¼€å‘çš„CPUä¼˜åŒ–å®ä¾‹
    "t3.large": {
        "vcpu": 2,
        "memory_gb": 8,
        "network": "Up to 5 Gbps",
        "price_per_hour": 0.0832,
        "use_case": "å¼€å‘å’Œå°è§„æ¨¡å®éªŒ"
    },
    "m5.xlarge": {
        "vcpu": 4,
        "memory_gb": 16,
        "network": "Up to 10 Gbps",
        "price_per_hour": 0.192,
        "use_case": "ä¸­ç­‰è®¡ç®—éœ€æ±‚"
    },
    
    # GPU instances for training
    # ç”¨äºè®­ç»ƒçš„GPUå®ä¾‹
    "p3.2xlarge": {
        "vcpu": 8,
        "memory_gb": 61,
        "gpu": "1x Tesla V100 (16GB)",
        "gpu_memory": "16 GB",
        "price_per_hour": 3.06,
        "use_case": "å•GPUè®­ç»ƒ"
    },
    "p3.8xlarge": {
        "vcpu": 32,
        "memory_gb": 244,
        "gpu": "4x Tesla V100 (16GB each)",
        "gpu_memory": "64 GB total",
        "price_per_hour": 12.24,
        "use_case": "å¤šGPUè®­ç»ƒ"
    },
    "p4d.24xlarge": {
        "vcpu": 96,
        "memory_gb": 1152,
        "gpu": "8x A100 (40GB each)",
        "gpu_memory": "320 GB total",
        "price_per_hour": 32.77,
        "use_case": "å¤§è§„æ¨¡åˆ†å¸ƒå¼è®­ç»ƒ"
    },
    
    # Cost-effective GPU instances
    # æ€§ä»·æ¯”é«˜çš„GPUå®ä¾‹
    "g4dn.xlarge": {
        "vcpu": 4,
        "memory_gb": 16,
        "gpu": "1x T4 (16GB)",
        "gpu_memory": "16 GB",
        "price_per_hour": 0.526,
        "use_case": "æ¨ç†å’Œè½»é‡çº§è®­ç»ƒ"
    }
}

# Display comparison
# æ˜¾ç¤ºæ¯”è¾ƒ
print("EC2æ·±åº¦å­¦ä¹ å®ä¾‹ç±»å‹æ¯”è¾ƒ:")
print("="*100)
print(f"{'å®ä¾‹ç±»å‹':<12} {'vCPU':<5} {'å†…å­˜(GB)':<8} {'GPU':<25} {'æ¯å°æ—¶($)':<10} {'æ¨èç”¨é€”'}")
print("-"*100)

for instance, specs in ec2_instances.items():
    gpu_info = specs.get('gpu', 'None')
    print(f"{instance:<12} {specs['vcpu']:<5} {specs['memory_gb']:<8} "
          f"{gpu_info:<25} {specs['price_per_hour']:<10} {specs['use_case']}")
```

#### Step 2: Launching an EC2 Instance æ­¥éª¤2ï¼šå¯åŠ¨EC2å®ä¾‹

```python
import boto3
import time

def launch_ec2_instance(instance_type="g4dn.xlarge", key_name="my-key-pair"):
    """
    Launch an EC2 instance optimized for deep learning
    å¯åŠ¨é’ˆå¯¹æ·±åº¦å­¦ä¹ ä¼˜åŒ–çš„EC2å®ä¾‹
    """
    ec2 = boto3.client('ec2')
    
    # Deep Learning AMI (Amazon Machine Image)
    # æ·±åº¦å­¦ä¹ AMIï¼ˆAmazonæœºå™¨æ˜ åƒï¼‰
    # This AMI comes pre-installed with popular ML frameworks
    # æ­¤AMIé¢„è£…äº†æµè¡Œçš„æœºå™¨å­¦ä¹ æ¡†æ¶
    ami_id = "ami-0c94855ba95b798c7"  # Deep Learning AMI (Ubuntu 20.04)
    
    # Security group configuration
    # å®‰å…¨ç»„é…ç½®
    security_group_config = {
        'GroupName': 'deep-learning-sg',
        'Description': 'Security group for deep learning instances',
        'IpPermissions': [
            {
                'IpProtocol': 'tcp',
                'FromPort': 22,
                'ToPort': 22,
                'IpRanges': [{'CidrIp': '0.0.0.0/0', 'Description': 'SSH access'}]
            },
            {
                'IpProtocol': 'tcp',
                'FromPort': 8888,
                'ToPort': 8888,
                'IpRanges': [{'CidrIp': '0.0.0.0/0', 'Description': 'Jupyter Notebook'}]
            },
            {
                'IpProtocol': 'tcp',
                'FromPort': 6006,
                'ToPort': 6006,
                'IpRanges': [{'CidrIp': '0.0.0.0/0', 'Description': 'TensorBoard'}]
            }
        ]
    }
    
    try:
        # Create security group
        # åˆ›å»ºå®‰å…¨ç»„
        try:
            sg_response = ec2.create_security_group(**security_group_config)
            security_group_id = sg_response['GroupId']
            print(f"åˆ›å»ºå®‰å…¨ç»„: {security_group_id}")
        except ec2.exceptions.ClientError as e:
            if 'already exists' in str(e):
                # Get existing security group
                # è·å–ç°æœ‰å®‰å…¨ç»„
                sg_response = ec2.describe_security_groups(
                    GroupNames=[security_group_config['GroupName']]
                )
                security_group_id = sg_response['SecurityGroups'][0]['GroupId']
                print(f"ä½¿ç”¨ç°æœ‰å®‰å…¨ç»„: {security_group_id}")
            else:
                raise e
        
        # Launch instance
        # å¯åŠ¨å®ä¾‹
        response = ec2.run_instances(
            ImageId=ami_id,
            MinCount=1,
            MaxCount=1,
            InstanceType=instance_type,
            KeyName=key_name,
            SecurityGroupIds=[security_group_id],
            BlockDeviceMappings=[
                {
                    'DeviceName': '/dev/sda1',
                    'Ebs': {
                        'VolumeType': 'gp3',
                        'VolumeSize': 100,  # 100 GB storage
                        'DeleteOnTermination': True
                    }
                }
            ],
            TagSpecifications=[
                {
                    'ResourceType': 'instance',
                    'Tags': [
                        {'Key': 'Name', 'Value': 'Deep Learning Instance'},
                        {'Key': 'Purpose', 'Value': 'ML Training'}
                    ]
                }
            ]
        )
        
        instance_id = response['Instances'][0]['InstanceId']
        print(f"å¯åŠ¨å®ä¾‹: {instance_id}")
        
        # Wait for instance to be running
        # ç­‰å¾…å®ä¾‹è¿è¡Œ
        print("ç­‰å¾…å®ä¾‹å¯åŠ¨...")
        waiter = ec2.get_waiter('instance_running')
        waiter.wait(InstanceIds=[instance_id])
        
        # Get instance details
        # è·å–å®ä¾‹è¯¦æƒ…
        instances = ec2.describe_instances(InstanceIds=[instance_id])
        instance = instances['Reservations'][0]['Instances'][0]
        public_ip = instance.get('PublicIpAddress')
        
        print(f"å®ä¾‹å¯åŠ¨æˆåŠŸ!")
        print(f"å®ä¾‹ID: {instance_id}")
        print(f"å…¬ç½‘IP: {public_ip}")
        print(f"SSHè¿æ¥: ssh -i {key_name}.pem ubuntu@{public_ip}")
        
        return {
            'instance_id': instance_id,
            'public_ip': public_ip,
            'instance_type': instance_type
        }
        
    except Exception as e:
        print(f"å¯åŠ¨å®ä¾‹å¤±è´¥: {e}")
        return None

# Example usage
# ä½¿ç”¨ç¤ºä¾‹
# instance_info = launch_ec2_instance("g4dn.xlarge", "my-key-pair")
```

#### Step 3: Creating Key Pairs for SSH Access æ­¥éª¤3ï¼šåˆ›å»ºç”¨äºSSHè®¿é—®çš„å¯†é’¥å¯¹

```python
def create_key_pair(key_name="deep-learning-key"):
    """
    Create an EC2 key pair for SSH access
    åˆ›å»ºç”¨äºSSHè®¿é—®çš„EC2å¯†é’¥å¯¹
    """
    ec2 = boto3.client('ec2')
    
    try:
        response = ec2.create_key_pair(KeyName=key_name)
        
        # Save private key to file
        # å°†ç§é’¥ä¿å­˜åˆ°æ–‡ä»¶
        with open(f"{key_name}.pem", 'w') as key_file:
            key_file.write(response['KeyMaterial'])
        
        # Set proper permissions for the key file
        # ä¸ºå¯†é’¥æ–‡ä»¶è®¾ç½®é€‚å½“æƒé™
        import os
        os.chmod(f"{key_name}.pem", 0o400)
        
        print(f"å¯†é’¥å¯¹åˆ›å»ºæˆåŠŸ: {key_name}")
        print(f"ç§é’¥å·²ä¿å­˜åˆ°: {key_name}.pem")
        print("è¯·å®‰å…¨ä¿å­˜æ­¤æ–‡ä»¶ï¼Œå®ƒç”¨äºSSHè¿æ¥åˆ°å®ä¾‹")
        
        return key_name
        
    except Exception as e:
        print(f"åˆ›å»ºå¯†é’¥å¯¹å¤±è´¥: {e}")
        return None

# SSH connection guide
# SSHè¿æ¥æŒ‡å—
def print_ssh_guide(public_ip, key_name):
    """
    Print SSH connection instructions
    æ‰“å°SSHè¿æ¥è¯´æ˜
    """
    print("\n" + "="*60)
    print("SSHè¿æ¥æŒ‡å—:")
    print("="*60)
    print(f"1. ç¡®ä¿å¯†é’¥æ–‡ä»¶æƒé™æ­£ç¡®:")
    print(f"   chmod 400 {key_name}.pem")
    print(f"\n2. è¿æ¥åˆ°å®ä¾‹:")
    print(f"   ssh -i {key_name}.pem ubuntu@{public_ip}")
    print(f"\n3. é¦–æ¬¡è¿æ¥æ—¶ï¼Œè¾“å…¥ 'yes' æ¥å—ä¸»æœºå¯†é’¥")
    print(f"\n4. è¿æ¥æˆåŠŸåï¼Œä½ å°†è¿›å…¥Ubuntuç¯å¢ƒ")
    print("="*60)
```

### 20.3.2 Installing CUDA å®‰è£…CUDA

CUDA (Compute Unified Device Architecture) is NVIDIA's parallel computing platform that enables GPUs to be used for deep learning. Installing CUDA correctly is crucial for GPU-accelerated training.

CUDAï¼ˆè®¡ç®—ç»Ÿä¸€è®¾å¤‡æ¶æ„ï¼‰æ˜¯NVIDIAçš„å¹¶è¡Œè®¡ç®—å¹³å°ï¼Œä½¿GPUèƒ½å¤Ÿç”¨äºæ·±åº¦å­¦ä¹ ã€‚æ­£ç¡®å®‰è£…CUDAå¯¹äºGPUåŠ é€Ÿè®­ç»ƒè‡³å…³é‡è¦ã€‚

Think of CUDA as the bridge that allows your deep learning frameworks to communicate with your GPU hardware.

æŠŠCUDAæƒ³è±¡æˆå…è®¸ä½ çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸GPUç¡¬ä»¶é€šä¿¡çš„æ¡¥æ¢ã€‚

#### CUDA Installation Script CUDAå®‰è£…è„šæœ¬

```bash
#!/bin/bash
# CUDA Installation Script for Ubuntu 20.04
# Ubuntu 20.04çš„CUDAå®‰è£…è„šæœ¬

echo "å¼€å§‹å®‰è£…CUDA..."

# Update system packages
# æ›´æ–°ç³»ç»ŸåŒ…
sudo apt update
sudo apt upgrade -y

# Install required dependencies
# å®‰è£…å¿…éœ€çš„ä¾èµ–é¡¹
sudo apt install -y wget software-properties-common

# Remove any existing NVIDIA drivers (if upgrading)
# åˆ é™¤ä»»ä½•ç°æœ‰çš„NVIDIAé©±åŠ¨ç¨‹åºï¼ˆå¦‚æœå‡çº§ï¼‰
# sudo apt remove --purge nvidia-*
# sudo apt autoremove

# Add NVIDIA package repositories
# æ·»åŠ NVIDIAåŒ…ä»“åº“
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-keyring_1.0-1_all.deb
sudo dpkg -i cuda-keyring_1.0-1_all.deb
sudo apt update

# Install CUDA Toolkit (version 11.8)
# å®‰è£…CUDAå·¥å…·åŒ…ï¼ˆç‰ˆæœ¬11.8ï¼‰
sudo apt install -y cuda-toolkit-11-8

# Install NVIDIA driver
# å®‰è£…NVIDIAé©±åŠ¨ç¨‹åº
sudo apt install -y nvidia-driver-520

# Add CUDA to PATH
# å°†CUDAæ·»åŠ åˆ°PATH
echo 'export PATH=/usr/local/cuda-11.8/bin:$PATH' >> ~/.bashrc
echo 'export LD_LIBRARY_PATH=/usr/local/cuda-11.8/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc

# Reload bash configuration
# é‡æ–°åŠ è½½bashé…ç½®
source ~/.bashrc

echo "CUDAå®‰è£…å®Œæˆï¼è¯·é‡å¯ç³»ç»Ÿä»¥ä½¿NVIDIAé©±åŠ¨ç¨‹åºç”Ÿæ•ˆã€‚"
echo "é‡å¯åï¼Œè¿è¡Œ 'nvidia-smi' éªŒè¯å®‰è£…ã€‚"
```

#### CUDA Verification Script CUDAéªŒè¯è„šæœ¬

```python
# CUDA Verification and Testing
# CUDAéªŒè¯å’Œæµ‹è¯•
import subprocess
import sys

def check_nvidia_driver():
    """
    Check if NVIDIA driver is properly installed
    æ£€æŸ¥NVIDIAé©±åŠ¨ç¨‹åºæ˜¯å¦æ­£ç¡®å®‰è£…
    """
    try:
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
        if result.returncode == 0:
            print("âœ… NVIDIAé©±åŠ¨ç¨‹åºå®‰è£…æ­£ç¡®")
            print(result.stdout)
            return True
        else:
            print("âŒ NVIDIAé©±åŠ¨ç¨‹åºæœªæ­£ç¡®å®‰è£…")
            return False
    except FileNotFoundError:
        print("âŒ nvidia-smiå‘½ä»¤æœªæ‰¾åˆ°ï¼Œè¯·å®‰è£…NVIDIAé©±åŠ¨ç¨‹åº")
        return False

def check_cuda_installation():
    """
    Check CUDA installation and version
    æ£€æŸ¥CUDAå®‰è£…å’Œç‰ˆæœ¬
    """
    try:
        result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)
        if result.returncode == 0:
            print("âœ… CUDAå·¥å…·åŒ…å®‰è£…æ­£ç¡®")
            print(result.stdout)
            return True
        else:
            print("âŒ CUDAå·¥å…·åŒ…æœªæ­£ç¡®å®‰è£…")
            return False
    except FileNotFoundError:
        print("âŒ nvccå‘½ä»¤æœªæ‰¾åˆ°ï¼Œè¯·å®‰è£…CUDAå·¥å…·åŒ…")
        return False

def test_pytorch_cuda():
    """
    Test PyTorch CUDA integration
    æµ‹è¯•PyTorch CUDAé›†æˆ
    """
    try:
        import torch
        print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
        print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
        
        if torch.cuda.is_available():
            print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
            print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
            
            for i in range(torch.cuda.device_count()):
                gpu_name = torch.cuda.get_device_name(i)
                gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
                print(f"GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)")
            
            # Test GPU computation
            # æµ‹è¯•GPUè®¡ç®—
            device = torch.device('cuda')
            x = torch.randn(1000, 1000, device=device)
            y = torch.randn(1000, 1000, device=device)
            z = torch.mm(x, y)
            print("âœ… GPUè®¡ç®—æµ‹è¯•æˆåŠŸ")
            
        else:
            print("âŒ PyTorchæ— æ³•è®¿é—®CUDA")
            
    except ImportError:
        print("âŒ PyTorchæœªå®‰è£…")

def test_tensorflow_gpu():
    """
    Test TensorFlow GPU integration
    æµ‹è¯•TensorFlow GPUé›†æˆ
    """
    try:
        import tensorflow as tf
        print(f"TensorFlowç‰ˆæœ¬: {tf.__version__}")
        
        gpus = tf.config.list_physical_devices('GPU')
        print(f"æ£€æµ‹åˆ°GPUæ•°é‡: {len(gpus)}")
        
        if gpus:
            for i, gpu in enumerate(gpus):
                print(f"GPU {i}: {gpu}")
            
            # Test GPU computation
            # æµ‹è¯•GPUè®¡ç®—
            with tf.device('/GPU:0'):
                a = tf.random.normal([1000, 1000])
                b = tf.random.normal([1000, 1000])
                c = tf.matmul(a, b)
            print("âœ… TensorFlow GPUè®¡ç®—æµ‹è¯•æˆåŠŸ")
        else:
            print("âŒ TensorFlowæ— æ³•æ£€æµ‹åˆ°GPU")
            
    except ImportError:
        print("âŒ TensorFlowæœªå®‰è£…")

# Run all checks
# è¿è¡Œæ‰€æœ‰æ£€æŸ¥
def run_cuda_diagnostics():
    """
    Run comprehensive CUDA diagnostics
    è¿è¡Œå…¨é¢çš„CUDAè¯Šæ–­
    """
    print("CUDAç¯å¢ƒè¯Šæ–­")
    print("="*50)
    
    print("\n1. æ£€æŸ¥NVIDIAé©±åŠ¨ç¨‹åº:")
    driver_ok = check_nvidia_driver()
    
    print("\n2. æ£€æŸ¥CUDAå·¥å…·åŒ…:")
    cuda_ok = check_cuda_installation()
    
    print("\n3. æµ‹è¯•PyTorch CUDA:")
    test_pytorch_cuda()
    
    print("\n4. æµ‹è¯•TensorFlow GPU:")
    test_tensorflow_gpu()
    
    print("\n" + "="*50)
    if driver_ok and cuda_ok:
        print("âœ… CUDAç¯å¢ƒé…ç½®æ­£ç¡®ï¼")
    else:
        print("âŒ CUDAç¯å¢ƒé…ç½®å­˜åœ¨é—®é¢˜ï¼Œè¯·æ£€æŸ¥å®‰è£…æ­¥éª¤")

# Run diagnostics
# è¿è¡Œè¯Šæ–­
# run_cuda_diagnostics()
```

#### Common CUDA Installation Issues å¸¸è§CUDAå®‰è£…é—®é¢˜

```python
# Common CUDA troubleshooting solutions
# å¸¸è§CUDAæ•…éšœæ’é™¤è§£å†³æ–¹æ¡ˆ

def troubleshoot_cuda():
    """
    Provide solutions for common CUDA issues
    ä¸ºå¸¸è§CUDAé—®é¢˜æä¾›è§£å†³æ–¹æ¡ˆ
    """
    print("CUDAæ•…éšœæ’é™¤æŒ‡å—")
    print("="*60)
    
    issues_solutions = {
        "nvidia-smi å‘½ä»¤æœªæ‰¾åˆ°": [
            "sudo apt update",
            "sudo apt install nvidia-driver-520",
            "sudo reboot"
        ],
        
        "CUDAç‰ˆæœ¬ä¸åŒ¹é…": [
            "æ£€æŸ¥PyTorch/TensorFlowæ”¯æŒçš„CUDAç‰ˆæœ¬",
            "å¸è½½å½“å‰CUDA: sudo apt remove cuda-*",
            "å®‰è£…åŒ¹é…ç‰ˆæœ¬çš„CUDA",
            "é‡æ–°å®‰è£…æ·±åº¦å­¦ä¹ æ¡†æ¶"
        ],
        
        "GPUå†…å­˜ä¸è¶³": [
            "å‡å°‘æ‰¹æ¬¡å¤§å° (batch_size)",
            "ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯",
            "å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ",
            "ä½¿ç”¨æ¨¡å‹å¹¶è¡ŒåŒ–"
        ],
        
        "CUDA out of memory": [
            "torch.cuda.empty_cache() # æ¸…ç†GPUç¼“å­˜",
            "å‡å°‘æ¨¡å‹å¤§å°æˆ–è¾“å…¥å¤§å°",
            "ä½¿ç”¨gradient checkpointing",
            "ç›‘æ§GPUå†…å­˜ä½¿ç”¨: nvidia-smi"
        ]
    }
    
    for issue, solutions in issues_solutions.items():
        print(f"\né—®é¢˜: {issue}")
        print("-" * len(issue))
        for i, solution in enumerate(solutions, 1):
            print(f"{i}. {solution}")

# Display troubleshooting guide
# æ˜¾ç¤ºæ•…éšœæ’é™¤æŒ‡å—
# troubleshoot_cuda()
```

### 20.3.3 Installing Libraries for Running the Code å®‰è£…è¿è¡Œä»£ç æ‰€éœ€çš„åº“

After setting up CUDA, you need to install the necessary Python libraries and frameworks for deep learning. This section covers setting up a complete deep learning environment.

è®¾ç½®CUDAåï¼Œä½ éœ€è¦å®‰è£…æ·±åº¦å­¦ä¹ æ‰€éœ€çš„Pythonåº“å’Œæ¡†æ¶ã€‚æœ¬èŠ‚æ¶µç›–è®¾ç½®å®Œæ•´çš„æ·±åº¦å­¦ä¹ ç¯å¢ƒã€‚

#### Python Environment Setup Pythonç¯å¢ƒè®¾ç½®

```bash
#!/bin/bash
# Python Environment Setup for Deep Learning
# æ·±åº¦å­¦ä¹ Pythonç¯å¢ƒè®¾ç½®

echo "è®¾ç½®Pythonæ·±åº¦å­¦ä¹ ç¯å¢ƒ..."

# Install Python package manager and virtual environment tools
# å®‰è£…PythonåŒ…ç®¡ç†å™¨å’Œè™šæ‹Ÿç¯å¢ƒå·¥å…·
sudo apt update
sudo apt install -y python3-pip python3-venv python3-dev

# Install system dependencies
# å®‰è£…ç³»ç»Ÿä¾èµ–é¡¹
sudo apt install -y build-essential cmake git wget curl
sudo apt install -y libopencv-dev libopenblas-dev liblapack-dev
sudo apt install -y libjpeg-dev libpng-dev libtiff-dev
sudo apt install -y libavcodec-dev libavformat-dev libswscale-dev
sudo apt install -y libgtk-3-dev libcanberra-gtk-module libcanberra-gtk3-module

# Create virtual environment
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python3 -m venv ~/deeplearning_env
source ~/deeplearning_env/bin/activate

# Upgrade pip
# å‡çº§pip
pip install --upgrade pip setuptools wheel

echo "Pythonç¯å¢ƒè®¾ç½®å®Œæˆï¼"
```

#### Deep Learning Frameworks Installation æ·±åº¦å­¦ä¹ æ¡†æ¶å®‰è£…

```python
# Deep Learning Libraries Installation Script
# æ·±åº¦å­¦ä¹ åº“å®‰è£…è„šæœ¬
import subprocess
import sys

def install_pytorch():
    """
    Install PyTorch with CUDA support
    å®‰è£…æ”¯æŒCUDAçš„PyTorch
    """
    print("å®‰è£…PyTorch...")
    
    # PyTorch with CUDA 11.8 support
    # æ”¯æŒCUDA 11.8çš„PyTorch
    pytorch_packages = [
        "torch==2.0.0+cu118",
        "torchvision==0.15.0+cu118", 
        "torchaudio==2.0.0+cu118"
    ]
    
    for package in pytorch_packages:
        try:
            subprocess.check_call([
                sys.executable, "-m", "pip", "install", package,
                "--index-url", "https://download.pytorch.org/whl/cu118"
            ])
            print(f"âœ… æˆåŠŸå®‰è£…: {package}")
        except subprocess.CalledProcessError as e:
            print(f"âŒ å®‰è£…å¤±è´¥ {package}: {e}")

def install_tensorflow():
    """
    Install TensorFlow with GPU support
    å®‰è£…æ”¯æŒGPUçš„TensorFlow
    """
    print("å®‰è£…TensorFlow...")
    
    try:
        subprocess.check_call([sys.executable, "-m", "pip", "install", "tensorflow[and-cuda]"])
        print("âœ… æˆåŠŸå®‰è£…TensorFlow GPUç‰ˆæœ¬")
    except subprocess.CalledProcessError as e:
        print(f"âŒ TensorFlowå®‰è£…å¤±è´¥: {e}")

def install_essential_packages():
    """
    Install essential packages for deep learning
    å®‰è£…æ·±åº¦å­¦ä¹ å¿…éœ€åŒ…
    """
    print("å®‰è£…å¿…éœ€çš„æ·±åº¦å­¦ä¹ åŒ…...")
    
    essential_packages = [
        # Data manipulation and analysis
        # æ•°æ®æ“ä½œå’Œåˆ†æ
        "numpy>=1.21.0",
        "pandas>=1.3.0",
        "scipy>=1.7.0",
        
        # Machine learning
        # æœºå™¨å­¦ä¹ 
        "scikit-learn>=1.0.0",
        "xgboost>=1.5.0",
        
        # Visualization
        # å¯è§†åŒ–
        "matplotlib>=3.4.0",
        "seaborn>=0.11.0",
        "plotly>=5.0.0",
        
        # Image processing
        # å›¾åƒå¤„ç†
        "opencv-python>=4.5.0",
        "Pillow>=8.3.0",
        "imageio>=2.9.0",
        
        # Natural Language Processing
        # è‡ªç„¶è¯­è¨€å¤„ç†
        "transformers>=4.10.0",
        "datasets>=1.12.0",
        "tokenizers>=0.10.0",
        "nltk>=3.6.0",
        "spacy>=3.4.0",
        
        # Experiment tracking and monitoring
        # å®éªŒè·Ÿè¸ªå’Œç›‘æ§
        "tensorboard>=2.7.0",
        "wandb>=0.12.0",
        "mlflow>=1.20.0",
        
        # Jupyter and development tools
        # Jupyterå’Œå¼€å‘å·¥å…·
        "jupyter>=1.0.0",
        "jupyterlab>=3.1.0",
        "ipywidgets>=7.6.0",
        
        # Additional utilities
        # é™„åŠ å®ç”¨å·¥å…·
        "tqdm>=4.62.0",
        "requests>=2.26.0",
        "PyYAML>=5.4.0",
        "h5py>=3.4.0"
    ]
    
    for package in essential_packages:
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", package])
            print(f"âœ… æˆåŠŸå®‰è£…: {package}")
        except subprocess.CalledProcessError as e:
            print(f"âŒ å®‰è£…å¤±è´¥ {package}: {e}")

def install_specialized_packages():
    """
    Install specialized deep learning packages
    å®‰è£…ä¸“ä¸šæ·±åº¦å­¦ä¹ åŒ…
    """
    print("å®‰è£…ä¸“ä¸šæ·±åº¦å­¦ä¹ åŒ…...")
    
    specialized_packages = [
        # Computer Vision
        # è®¡ç®—æœºè§†è§‰
        "timm",  # PyTorch Image Models
        "albumentations",  # Image augmentation
        "detectron2 @ git+https://github.com/facebookresearch/detectron2.git",
        
        # Audio processing
        # éŸ³é¢‘å¤„ç†
        "librosa",
        "soundfile",
        
        # Graph Neural Networks
        # å›¾ç¥ç»ç½‘ç»œ
        "torch-geometric",
        "dgl",
        
        # Reinforcement Learning
        # å¼ºåŒ–å­¦ä¹ 
        "gym",
        "stable-baselines3",
        
        # Optimization
        # ä¼˜åŒ–
        "optuna",
        "hyperopt",
        
        # Model deployment
        # æ¨¡å‹éƒ¨ç½²
        "onnx",
        "onnxruntime",
        "torchscript"
    ]
    
    for package in specialized_packages:
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", package])
            print(f"âœ… æˆåŠŸå®‰è£…: {package}")
        except subprocess.CalledProcessError as e:
            print(f"âš ï¸ è·³è¿‡å¯é€‰åŒ… {package}: {e}")

def create_requirements_file():
    """
    Create requirements.txt file for environment reproduction
    åˆ›å»ºrequirements.txtæ–‡ä»¶ä»¥ä¾¿ç¯å¢ƒå¤ç°
    """
    print("åˆ›å»ºrequirements.txtæ–‡ä»¶...")
    
    try:
        result = subprocess.run([sys.executable, "-m", "pip", "freeze"], 
                              capture_output=True, text=True)
        
        with open("requirements.txt", "w") as f:
            f.write(result.stdout)
        
        print("âœ… requirements.txtåˆ›å»ºæˆåŠŸ")
        print("ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤åœ¨æ–°ç¯å¢ƒä¸­å®‰è£…ç›¸åŒçš„åŒ…:")
        print("pip install -r requirements.txt")
        
    except Exception as e:
        print(f"âŒ åˆ›å»ºrequirements.txtå¤±è´¥: {e}")

def setup_jupyter_extensions():
    """
    Setup Jupyter extensions for better development experience
    è®¾ç½®Jupyteræ‰©å±•ä»¥è·å¾—æ›´å¥½çš„å¼€å‘ä½“éªŒ
    """
    print("è®¾ç½®Jupyteræ‰©å±•...")
    
    extensions = [
        "jupyter_contrib_nbextensions",
        "jupyter_nbextensions_configurator",
        "ipywidgets"
    ]
    
    for ext in extensions:
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", ext])
            print(f"âœ… æˆåŠŸå®‰è£…: {ext}")
        except subprocess.CalledProcessError as e:
            print(f"âŒ å®‰è£…å¤±è´¥ {ext}: {e}")
    
    # Enable extensions
    # å¯ç”¨æ‰©å±•
    try:
        subprocess.check_call([sys.executable, "-m", "jupyter", "contrib", "nbextension", "install", "--user"])
        subprocess.check_call([sys.executable, "-m", "jupyter", "nbextensions_configurator", "enable", "--user"])
        print("âœ… Jupyteræ‰©å±•é…ç½®å®Œæˆ")
    except subprocess.CalledProcessError as e:
        print(f"âŒ Jupyteræ‰©å±•é…ç½®å¤±è´¥: {e}")

# Complete installation function
# å®Œæ•´å®‰è£…å‡½æ•°
def complete_environment_setup():
    """
    Set up complete deep learning environment
    è®¾ç½®å®Œæ•´çš„æ·±åº¦å­¦ä¹ ç¯å¢ƒ
    """
    print("å¼€å§‹è®¾ç½®æ·±åº¦å­¦ä¹ ç¯å¢ƒ...")
    print("="*60)
    
    # Install core frameworks
    # å®‰è£…æ ¸å¿ƒæ¡†æ¶
    install_pytorch()
    install_tensorflow()
    
    # Install essential packages
    # å®‰è£…å¿…éœ€åŒ…
    install_essential_packages()
    
    # Install specialized packages
    # å®‰è£…ä¸“ä¸šåŒ…
    install_specialized_packages()
    
    # Setup Jupyter
    # è®¾ç½®Jupyter
    setup_jupyter_extensions()
    
    # Create requirements file
    # åˆ›å»ºrequirementsæ–‡ä»¶
    create_requirements_file()
    
    print("="*60)
    print("âœ… æ·±åº¦å­¦ä¹ ç¯å¢ƒè®¾ç½®å®Œæˆï¼")
    print("\nä¸‹ä¸€æ­¥:")
    print("1. é‡å¯ç»ˆç«¯æˆ–è¿è¡Œ: source ~/.bashrc")
    print("2. æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ: source ~/deeplearning_env/bin/activate")
    print("3. å¯åŠ¨Jupyter: jupyter lab")
    print("4. æµ‹è¯•å®‰è£…: python -c 'import torch; print(torch.cuda.is_available())'")

# Run the complete setup
# è¿è¡Œå®Œæ•´è®¾ç½®
# complete_environment_setup()
```

#### Environment Testing and Validation ç¯å¢ƒæµ‹è¯•å’ŒéªŒè¯

```python
# Comprehensive environment testing
# å…¨é¢ç¯å¢ƒæµ‹è¯•
def test_deep_learning_environment():
    """
    Test the complete deep learning environment setup
    æµ‹è¯•å®Œæ•´çš„æ·±åº¦å­¦ä¹ ç¯å¢ƒè®¾ç½®
    """
    print("æ·±åº¦å­¦ä¹ ç¯å¢ƒæµ‹è¯•")
    print("="*60)
    
    # Test core packages
    # æµ‹è¯•æ ¸å¿ƒåŒ…
    test_results = {}
    
    # Test PyTorch
    # æµ‹è¯•PyTorch
    try:
        import torch
        import torchvision
        import torchaudio
        
        test_results['PyTorch'] = {
            'installed': True,
            'version': torch.__version__,
            'cuda_available': torch.cuda.is_available(),
            'cuda_version': torch.version.cuda if torch.cuda.is_available() else 'N/A',
            'gpu_count': torch.cuda.device_count() if torch.cuda.is_available() else 0
        }
        
        # Test GPU computation
        # æµ‹è¯•GPUè®¡ç®—
        if torch.cuda.is_available():
            device = torch.device('cuda')
            x = torch.randn(100, 100, device=device)
            y = torch.mm(x, x.t())
            test_results['PyTorch']['gpu_test'] = 'Passed'
        else:
            test_results['PyTorch']['gpu_test'] = 'Skipped (No GPU)'
            
    except ImportError as e:
        test_results['PyTorch'] = {'installed': False, 'error': str(e)}
    
    # Test TensorFlow
    # æµ‹è¯•TensorFlow
    try:
        import tensorflow as tf
        
        test_results['TensorFlow'] = {
            'installed': True,
            'version': tf.__version__,
            'gpu_available': len(tf.config.list_physical_devices('GPU')) > 0,
            'gpu_count': len(tf.config.list_physical_devices('GPU'))
        }
        
        # Test GPU computation
        # æµ‹è¯•GPUè®¡ç®—
        if test_results['TensorFlow']['gpu_available']:
            with tf.device('/GPU:0'):
                a = tf.random.normal([100, 100])
                b = tf.matmul(a, a)
            test_results['TensorFlow']['gpu_test'] = 'Passed'
        else:
            test_results['TensorFlow']['gpu_test'] = 'Skipped (No GPU)'
            
    except ImportError as e:
        test_results['TensorFlow'] = {'installed': False, 'error': str(e)}
    
    # Test other essential packages
    # æµ‹è¯•å…¶ä»–å¿…éœ€åŒ…
    essential_packages = [
        'numpy', 'pandas', 'matplotlib', 'scikit-learn',
        'opencv-python', 'transformers', 'datasets'
    ]
    
    for package in essential_packages:
        try:
            module = __import__(package.replace('-', '_'))
            version = getattr(module, '__version__', 'Unknown')
            test_results[package] = {'installed': True, 'version': version}
        except ImportError:
            test_results[package] = {'installed': False}
    
    # Display results
    # æ˜¾ç¤ºç»“æœ
    print("\næµ‹è¯•ç»“æœ:")
    print("-"*60)
    
    for package, result in test_results.items():
        if result['installed']:
            status = "âœ…"
            info = f"ç‰ˆæœ¬: {result.get('version', 'Unknown')}"
            
            if 'cuda_available' in result:
                cuda_status = "âœ…" if result['cuda_available'] else "âŒ"
                info += f", CUDA: {cuda_status}"
                
            if 'gpu_test' in result:
                gpu_status = "âœ…" if result['gpu_test'] == 'Passed' else "âš ï¸"
                info += f", GPUæµ‹è¯•: {gpu_status}"
                
        else:
            status = "âŒ"
            info = f"æœªå®‰è£…: {result.get('error', 'æœªçŸ¥é”™è¯¯')}"
        
        print(f"{status} {package:<15} {info}")
    
    # Summary
    # æ€»ç»“
    installed_count = sum(1 for r in test_results.values() if r['installed'])
    total_count = len(test_results)
    
    print(f"\næ€»ç»“: {installed_count}/{total_count} åŒ…å·²æ­£ç¡®å®‰è£…")
    
    if installed_count == total_count:
        print("ğŸ‰ æ­å–œï¼æ‚¨çš„æ·±åº¦å­¦ä¹ ç¯å¢ƒå·²å®Œå…¨è®¾ç½®å¥½ï¼")
    else:
        print("âš ï¸ æŸäº›åŒ…æœªå®‰è£…ï¼Œè¯·æ£€æŸ¥å®‰è£…æ—¥å¿—")

# Run environment test
# è¿è¡Œç¯å¢ƒæµ‹è¯•
# test_deep_learning_environment()
``` 