# Chapter 20: Appendix: Tools for Deep Learning 
# ç¬¬20ç« ï¼šé™„å½•ï¼šæ·±åº¦å­¦ä¹ å·¥å…·

## Overview æ¦‚è¿°

Deep learning has revolutionized the field of artificial intelligence, but mastering it requires not only understanding the theoretical concepts but also becoming proficient with the practical tools and platforms that make implementation possible. This appendix provides a comprehensive guide to the essential tools, platforms, and environments that every deep learning practitioner should know.

æ·±åº¦å­¦ä¹ å·²ç»å½»åº•æ”¹é©äº†äººå·¥æ™ºèƒ½é¢†åŸŸï¼Œä½†è¦æŒæ¡å®ƒä¸ä»…éœ€è¦ç†è§£ç†è®ºæ¦‚å¿µï¼Œè¿˜éœ€è¦ç†Ÿç»ƒæŒæ¡ä½¿å®ç°æˆä¸ºå¯èƒ½çš„å®ç”¨å·¥å…·å’Œå¹³å°ã€‚è¿™ä¸ªé™„å½•ä¸ºæ¯ä¸ªæ·±åº¦å­¦ä¹ ä»ä¸šè€…åº”è¯¥äº†è§£çš„åŸºæœ¬å·¥å…·ã€å¹³å°å’Œç¯å¢ƒæä¾›äº†å…¨é¢çš„æŒ‡å—ã€‚

In this chapter, we will explore various development environments, from local Jupyter notebooks to cloud-based solutions like Amazon SageMaker, AWS EC2, and Google Colab. We'll also discuss hardware considerations, collaboration tools, and utility functions that can significantly enhance your deep learning workflow.

åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ¢ç´¢å„ç§å¼€å‘ç¯å¢ƒï¼Œä»æœ¬åœ°Jupyterç¬”è®°æœ¬åˆ°åŸºäºäº‘çš„è§£å†³æ–¹æ¡ˆï¼Œå¦‚Amazon SageMakerã€AWS EC2å’ŒGoogle Colabã€‚æˆ‘ä»¬è¿˜å°†è®¨è®ºç¡¬ä»¶è€ƒè™‘å› ç´ ã€åä½œå·¥å…·å’Œå¯ä»¥æ˜¾è‘—å¢å¼ºæ·±åº¦å­¦ä¹ å·¥ä½œæµç¨‹çš„å®ç”¨å‡½æ•°ã€‚

## 20.1 Using Jupyter Notebooks ä½¿ç”¨Jupyterç¬”è®°æœ¬

Jupyter Notebooks have become the de facto standard for interactive data science and machine learning development. They provide an excellent environment for experimenting with code, visualizing data, and documenting your thought process all in one place.

Jupyterç¬”è®°æœ¬å·²æˆä¸ºäº¤äº’å¼æ•°æ®ç§‘å­¦å’Œæœºå™¨å­¦ä¹ å¼€å‘çš„äº‹å®æ ‡å‡†ã€‚å®ƒä»¬ä¸ºåœ¨ä¸€ä¸ªåœ°æ–¹è¯•éªŒä»£ç ã€å¯è§†åŒ–æ•°æ®å’Œè®°å½•æ€ç»´è¿‡ç¨‹æä¾›äº†ä¼˜ç§€çš„ç¯å¢ƒã€‚

### 20.1.1 Editing and Running the Code Locally æœ¬åœ°ç¼–è¾‘å’Œè¿è¡Œä»£ç 

#### Installation and Setup å®‰è£…å’Œè®¾ç½®

To get started with Jupyter notebooks locally, you need to install the necessary software. The easiest way is through Anaconda, which comes with Jupyter pre-installed, or you can install it via pip.

è¦åœ¨æœ¬åœ°å¼€å§‹ä½¿ç”¨Jupyterç¬”è®°æœ¬ï¼Œä½ éœ€è¦å®‰è£…å¿…è¦çš„è½¯ä»¶ã€‚æœ€ç®€å•çš„æ–¹æ³•æ˜¯é€šè¿‡Anacondaï¼Œå®ƒé¢„è£…äº†Jupyterï¼Œæˆ–è€…ä½ å¯ä»¥é€šè¿‡pipå®‰è£…å®ƒã€‚

**Method 1: Using Anaconda (Recommended) æ–¹æ³•1ï¼šä½¿ç”¨Anacondaï¼ˆæ¨èï¼‰**

```bash
# Download and install Anaconda from https://www.anaconda.com/
# ä» https://www.anaconda.com/ ä¸‹è½½å¹¶å®‰è£…Anaconda

# Launch Jupyter Notebook
# å¯åŠ¨Jupyterç¬”è®°æœ¬
jupyter notebook
```

**Method 2: Using pip æ–¹æ³•2ï¼šä½¿ç”¨pip**

```bash
# Install Jupyter
# å®‰è£…Jupyter
pip install jupyter

# Install additional packages for deep learning
# å®‰è£…æ·±åº¦å­¦ä¹ çš„é¢å¤–åŒ…
pip install torch torchvision matplotlib numpy pandas scikit-learn

# Launch Jupyter Notebook
# å¯åŠ¨Jupyterç¬”è®°æœ¬
jupyter notebook
```

#### Creating Your First Notebook åˆ›å»ºä½ çš„ç¬¬ä¸€ä¸ªç¬”è®°æœ¬

When you launch Jupyter, it opens in your web browser. You can create a new notebook by clicking "New" â†’ "Python 3". Let's create a simple deep learning example:

å½“ä½ å¯åŠ¨Jupyteræ—¶ï¼Œå®ƒåœ¨ç½‘é¡µæµè§ˆå™¨ä¸­æ‰“å¼€ã€‚ä½ å¯ä»¥é€šè¿‡ç‚¹å‡»"æ–°å»º"â†’"Python 3"æ¥åˆ›å»ºæ–°ç¬”è®°æœ¬ã€‚è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç®€å•çš„æ·±åº¦å­¦ä¹ ç¤ºä¾‹ï¼š

```python
# Cell 1: Import necessary libraries
# å•å…ƒæ ¼1ï¼šå¯¼å…¥å¿…è¦çš„åº“
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt

print("PyTorch version:", torch.__version__)
print("CUDA available:", torch.cuda.is_available())
```

```python
# Cell 2: Create a simple neural network
# å•å…ƒæ ¼2ï¼šåˆ›å»ºä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œ
class SimpleNet(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(SimpleNet, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

# Create model instance
# åˆ›å»ºæ¨¡å‹å®ä¾‹
model = SimpleNet(input_size=10, hidden_size=20, output_size=1)
print(model)
```

#### Best Practices for Local Development æœ¬åœ°å¼€å‘çš„æœ€ä½³å®è·µ

1. **Organize your notebooks** ç»„ç»‡ä½ çš„ç¬”è®°æœ¬
   - Create separate folders for different projects
   - ä¸ºä¸åŒé¡¹ç›®åˆ›å»ºå•ç‹¬çš„æ–‡ä»¶å¤¹
   - Use descriptive filenames with dates or version numbers
   - ä½¿ç”¨å¸¦æœ‰æ—¥æœŸæˆ–ç‰ˆæœ¬å·çš„æè¿°æ€§æ–‡ä»¶å

2. **Use virtual environments** ä½¿ç”¨è™šæ‹Ÿç¯å¢ƒ
   - Keep dependencies isolated between projects
   - ä¿æŒé¡¹ç›®é—´ä¾èµ–å…³ç³»çš„éš”ç¦»
   ```bash
   # Create virtual environment
   # åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
   conda create -n deeplearning python=3.8
   conda activate deeplearning
   ```

3. **Document your code** è®°å½•ä½ çš„ä»£ç 
   - Use markdown cells to explain your thought process
   - ä½¿ç”¨markdownå•å…ƒæ ¼è§£é‡Šä½ çš„æ€ç»´è¿‡ç¨‹
   - Include comments in code cells
   - åœ¨ä»£ç å•å…ƒæ ¼ä¸­åŒ…å«æ³¨é‡Š

### 20.1.2 Advanced Options é«˜çº§é€‰é¡¹

#### Jupyter Extensions ï¿½jupyteræ‰©å±•

Jupyter extensions can significantly enhance your productivity. Here are some essential ones:

Jupyteræ‰©å±•å¯ä»¥æ˜¾è‘—æé«˜ä½ çš„ç”Ÿäº§åŠ›ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å¿…è¦çš„æ‰©å±•ï¼š

```bash
# Install Jupyter extensions
# å®‰è£…Jupyteræ‰©å±•
pip install jupyter_contrib_nbextensions
jupyter contrib nbextension install --user

# Enable useful extensions
# å¯ç”¨æœ‰ç”¨çš„æ‰©å±•
jupyter nbextension enable --py widgetsnbextension
```

**Popular Extensions çƒ­é—¨æ‰©å±•ï¼š**

1. **Variable Inspector** å˜é‡æ£€æŸ¥å™¨
   - Shows all variables in your namespace
   - æ˜¾ç¤ºå‘½åç©ºé—´ä¸­çš„æ‰€æœ‰å˜é‡
   - Useful for debugging and understanding data structures
   - å¯¹è°ƒè¯•å’Œç†è§£æ•°æ®ç»“æ„å¾ˆæœ‰ç”¨

2. **Table of Contents** ç›®å½•
   - Automatically generates a table of contents from markdown headers
   - ä»markdownæ ‡é¢˜è‡ªåŠ¨ç”Ÿæˆç›®å½•
   - Makes navigation easier in long notebooks
   - ä½¿é•¿ç¬”è®°æœ¬ä¸­çš„å¯¼èˆªæ›´å®¹æ˜“

3. **Code Folding** ä»£ç æŠ˜å 
   - Allows you to collapse code cells
   - å…è®¸ä½ æŠ˜å ä»£ç å•å…ƒæ ¼
   - Helps with organization and focus
   - æœ‰åŠ©äºç»„ç»‡å’Œä¸“æ³¨

#### Magic Commands é­”æ³•å‘½ä»¤

Jupyter notebooks support magic commands that provide powerful functionality:

Jupyterç¬”è®°æœ¬æ”¯æŒæä¾›å¼ºå¤§åŠŸèƒ½çš„é­”æ³•å‘½ä»¤ï¼š

```python
# Time execution of a cell
# è®¡æ—¶å•å…ƒæ ¼çš„æ‰§è¡Œæ—¶é—´
%%time
import time
time.sleep(1)
print("This took about 1 second")

# Profile memory usage
# åˆ†æå†…å­˜ä½¿ç”¨æƒ…å†µ
%load_ext memory_profiler
%memit torch.randn(1000, 1000)

# Load external Python file
# åŠ è½½å¤–éƒ¨Pythonæ–‡ä»¶
# %load external_script.py

# Show matplotlib plots inline
# å†…è”æ˜¾ç¤ºmatplotlibå›¾è¡¨
%matplotlib inline
```

#### GPU Configuration GPUé…ç½®

When working with deep learning models locally, GPU acceleration is crucial:

åœ¨æœ¬åœ°å¤„ç†æ·±åº¦å­¦ä¹ æ¨¡å‹æ—¶ï¼ŒGPUåŠ é€Ÿæ˜¯è‡³å…³é‡è¦çš„ï¼š

```python
# Check GPU availability and configure device
# æ£€æŸ¥GPUå¯ç”¨æ€§å¹¶é…ç½®è®¾å¤‡
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

# Move model to GPU
# å°†æ¨¡å‹ç§»åŠ¨åˆ°GPU
model = model.to(device)
```

### 20.1.3 Summary æ€»ç»“

Jupyter notebooks provide an ideal environment for deep learning experimentation and development. They combine code execution, visualization, and documentation in a single interface, making them perfect for iterative development and research.

Jupyterç¬”è®°æœ¬ä¸ºæ·±åº¦å­¦ä¹ å®éªŒå’Œå¼€å‘æä¾›äº†ç†æƒ³çš„ç¯å¢ƒã€‚å®ƒä»¬åœ¨å•ä¸ªç•Œé¢ä¸­ç»“åˆäº†ä»£ç æ‰§è¡Œã€å¯è§†åŒ–å’Œæ–‡æ¡£ï¼Œä½¿å…¶éå¸¸é€‚åˆè¿­ä»£å¼€å‘å’Œç ”ç©¶ã€‚

Key advantages include:
ä¸»è¦ä¼˜åŠ¿åŒ…æ‹¬ï¼š
- Interactive development and immediate feedback
- äº¤äº’å¼å¼€å‘å’Œå³æ—¶åé¦ˆ
- Rich output formatting (plots, tables, images)
- ä¸°å¯Œçš„è¾“å‡ºæ ¼å¼ï¼ˆå›¾è¡¨ã€è¡¨æ ¼ã€å›¾åƒï¼‰
- Easy sharing and collaboration
- æ˜“äºåˆ†äº«å’Œåä½œ
- Extensive ecosystem of extensions
- æ‰©å±•çš„å¹¿æ³›ç”Ÿæ€ç³»ç»Ÿ

### 20.1.4 Exercises ç»ƒä¹ 

1. **Basic Setup Exercise** åŸºæœ¬è®¾ç½®ç»ƒä¹ 
   - Install Jupyter notebook on your local machine
   - åœ¨æœ¬åœ°æœºå™¨ä¸Šå®‰è£…Jupyterç¬”è®°æœ¬
   - Create a new notebook and verify PyTorch installation
   - åˆ›å»ºæ–°ç¬”è®°æœ¬å¹¶éªŒè¯PyTorchå®‰è£…
   - Implement a simple linear regression model
   - å®ç°ä¸€ä¸ªç®€å•çš„çº¿æ€§å›å½’æ¨¡å‹

2. **Extension Practice** æ‰©å±•ç»ƒä¹ 
   - Install and configure at least 3 Jupyter extensions
   - å®‰è£…å¹¶é…ç½®è‡³å°‘3ä¸ªJupyteræ‰©å±•
   - Use magic commands to profile a deep learning training loop
   - ä½¿ç”¨é­”æ³•å‘½ä»¤åˆ†ææ·±åº¦å­¦ä¹ è®­ç»ƒå¾ªç¯

3. **Documentation Challenge** æ–‡æ¡£æŒ‘æˆ˜
   - Create a well-documented notebook that explains a deep learning concept
   - åˆ›å»ºä¸€ä¸ªè§£é‡Šæ·±åº¦å­¦ä¹ æ¦‚å¿µçš„è¯¦ç»†æ–‡æ¡£ç¬”è®°æœ¬
   - Include mathematical formulas, code examples, and visualizations
   - åŒ…æ‹¬æ•°å­¦å…¬å¼ã€ä»£ç ç¤ºä¾‹å’Œå¯è§†åŒ–

## 20.2 Using Amazon SageMaker ä½¿ç”¨Amazon SageMaker

Amazon SageMaker is a fully managed service that provides every developer and data scientist with the ability to build, train, and deploy machine learning models quickly. It eliminates the heavy lifting from each step of the machine learning process to make it easier to develop high-quality models.

Amazon SageMakeræ˜¯ä¸€é¡¹å®Œå…¨æ‰˜ç®¡çš„æœåŠ¡ï¼Œä¸ºæ¯ä¸ªå¼€å‘è€…å’Œæ•°æ®ç§‘å­¦å®¶æä¾›å¿«é€Ÿæ„å»ºã€è®­ç»ƒå’Œéƒ¨ç½²æœºå™¨å­¦ä¹ æ¨¡å‹çš„èƒ½åŠ›ã€‚å®ƒæ¶ˆé™¤äº†æœºå™¨å­¦ä¹ è¿‡ç¨‹æ¯ä¸ªæ­¥éª¤çš„ç¹é‡å·¥ä½œï¼Œä½¿å¼€å‘é«˜è´¨é‡æ¨¡å‹å˜å¾—æ›´å®¹æ˜“ã€‚

### 20.2.1 Signing Up æ³¨å†Œ

To get started with Amazon SageMaker, you need an AWS account. Here's the step-by-step process:

è¦å¼€å§‹ä½¿ç”¨Amazon SageMakerï¼Œä½ éœ€è¦ä¸€ä¸ªAWSè´¦æˆ·ã€‚ä»¥ä¸‹æ˜¯é€æ­¥è¿‡ç¨‹ï¼š

#### Step 1: Create AWS Account æ­¥éª¤1ï¼šåˆ›å»ºAWSè´¦æˆ·

1. Go to https://aws.amazon.com/
   è®¿é—® https://aws.amazon.com/
2. Click "Create an AWS Account"
   ç‚¹å‡»"åˆ›å»ºAWSè´¦æˆ·"
3. Provide your email, password, and AWS account name
   æä¾›ä½ çš„é‚®ç®±ã€å¯†ç å’ŒAWSè´¦æˆ·å
4. Add payment information (required for verification)
   æ·»åŠ ä»˜æ¬¾ä¿¡æ¯ï¼ˆéªŒè¯æ‰€éœ€ï¼‰
5. Verify your identity via phone
   é€šè¿‡ç”µè¯éªŒè¯èº«ä»½

#### Step 2: Access SageMaker æ­¥éª¤2ï¼šè®¿é—®SageMaker

```bash
# Once logged into AWS Console
# ç™»å½•AWSæ§åˆ¶å°å
# 1. Search for "SageMaker" in the AWS services search bar
# 1. åœ¨AWSæœåŠ¡æœç´¢æ ä¸­æœç´¢"SageMaker"
# 2. Click on "Amazon SageMaker"
# 2. ç‚¹å‡»"Amazon SageMaker"
# 3. You'll be taken to the SageMaker dashboard
# 3. ä½ å°†è¢«å¸¦åˆ°SageMakerä»ªè¡¨æ¿
```

#### Step 3: Understanding Pricing æ­¥éª¤3ï¼šäº†è§£å®šä»·

SageMaker pricing is based on usage:
SageMakerå®šä»·åŸºäºä½¿ç”¨æƒ…å†µï¼š

- **Notebook instances**: Charged per hour of usage
- **ç¬”è®°æœ¬å®ä¾‹**ï¼šæŒ‰ä½¿ç”¨å°æ—¶æ”¶è´¹
- **Training**: Charged per second of training time
- **è®­ç»ƒ**ï¼šæŒ‰è®­ç»ƒæ—¶é—´ç§’è®¡è´¹
- **Inference**: Charged per hour for hosted endpoints
- **æ¨ç†**ï¼šæ‰˜ç®¡ç«¯ç‚¹æŒ‰å°æ—¶æ”¶è´¹

*Important*: Always stop instances when not in use to avoid unnecessary charges.
*é‡è¦æç¤º*ï¼šä¸ä½¿ç”¨æ—¶åŠ¡å¿…åœæ­¢å®ä¾‹ä»¥é¿å…ä¸å¿…è¦çš„è´¹ç”¨ã€‚ 

### 20.2.2 Creating a SageMaker Instance åˆ›å»ºSageMakerå®ä¾‹

Once you have access to SageMaker, creating a notebook instance is straightforward. A notebook instance is a machine learning compute instance running the Jupyter Notebook App, where you can prepare and process data, write code to train models, deploy models, and test or validate your models.

ä¸€æ—¦ä½ å¯ä»¥è®¿é—®SageMakerï¼Œåˆ›å»ºç¬”è®°æœ¬å®ä¾‹å°±å¾ˆç®€å•äº†ã€‚ç¬”è®°æœ¬å®ä¾‹æ˜¯è¿è¡ŒJupyterç¬”è®°æœ¬åº”ç”¨ç¨‹åºçš„æœºå™¨å­¦ä¹ è®¡ç®—å®ä¾‹ï¼Œä½ å¯ä»¥åœ¨å…¶ä¸­å‡†å¤‡å’Œå¤„ç†æ•°æ®ã€ç¼–å†™ä»£ç æ¥è®­ç»ƒæ¨¡å‹ã€éƒ¨ç½²æ¨¡å‹ä»¥åŠæµ‹è¯•æˆ–éªŒè¯æ¨¡å‹ã€‚

#### Step-by-Step Instance Creation é€æ­¥åˆ›å»ºå®ä¾‹

```bash
# Navigate to SageMaker Console
# å¯¼èˆªåˆ°SageMakeræ§åˆ¶å°
# 1. In AWS Console, go to SageMaker service
# 1. åœ¨AWSæ§åˆ¶å°ä¸­ï¼Œè½¬åˆ°SageMakeræœåŠ¡
# 2. Click "Notebook instances" in the left sidebar
# 2. åœ¨å·¦ä¾§è¾¹æ ç‚¹å‡»"ç¬”è®°æœ¬å®ä¾‹"
# 3. Click "Create notebook instance"
# 3. ç‚¹å‡»"åˆ›å»ºç¬”è®°æœ¬å®ä¾‹"
```

#### Instance Configuration å®ä¾‹é…ç½®

When creating a SageMaker notebook instance, you need to configure several important settings:

åˆ›å»ºSageMakerç¬”è®°æœ¬å®ä¾‹æ—¶ï¼Œä½ éœ€è¦é…ç½®å‡ ä¸ªé‡è¦è®¾ç½®ï¼š

**1. Basic Settings åŸºæœ¬è®¾ç½®**

```python
# Instance configuration parameters
# å®ä¾‹é…ç½®å‚æ•°
instance_config = {
    "notebook_instance_name": "my-deep-learning-notebook",  # å®ä¾‹åç§°
    "instance_type": "ml.t3.medium",  # å®ä¾‹ç±»å‹ï¼ˆCPUå®ä¾‹ï¼Œé€‚åˆå¼€å§‹ï¼‰
    "role_arn": "arn:aws:iam::account:role/service-role/AmazonSageMaker-ExecutionRole"
}

# For GPU instances (more expensive but faster for training)
# GPUå®ä¾‹ï¼ˆæ›´æ˜‚è´µä½†è®­ç»ƒæ›´å¿«ï¼‰
gpu_instance_config = {
    "instance_type": "ml.p3.2xlarge",  # GPUå®ä¾‹ç±»å‹
    "volume_size": 20  # å­˜å‚¨å¤§å°ï¼ˆGBï¼‰
}
```

**2. IAM Role Configuration IAMè§’è‰²é…ç½®**

SageMaker needs permissions to access other AWS services. You can either create a new role or use an existing one:

SageMakeréœ€è¦æƒé™æ¥è®¿é—®å…¶ä»–AWSæœåŠ¡ã€‚ä½ å¯ä»¥åˆ›å»ºæ–°è§’è‰²æˆ–ä½¿ç”¨ç°æœ‰è§’è‰²ï¼š

```python
# Creating a SageMaker execution role
# åˆ›å»ºSageMakeræ‰§è¡Œè§’è‰²
import boto3

def create_sagemaker_role():
    """
    Create IAM role for SageMaker with necessary permissions
    ä¸ºSageMakeråˆ›å»ºå…·æœ‰å¿…è¦æƒé™çš„IAMè§’è‰²
    """
    iam = boto3.client('iam')
    
    # Define trust policy
    # å®šä¹‰ä¿¡ä»»ç­–ç•¥
    trust_policy = {
        "Version": "2012-10-17",
        "Statement": [
            {
                "Effect": "Allow",
                "Principal": {"Service": "sagemaker.amazonaws.com"},
                "Action": "sts:AssumeRole"
            }
        ]
    }
    
    # Create role
    # åˆ›å»ºè§’è‰²
    role_name = "SageMakerExecutionRole"
    
    try:
        response = iam.create_role(
            RoleName=role_name,
            AssumeRolePolicyDocument=str(trust_policy),
            Description="Role for SageMaker notebook instances"
        )
        print(f"åˆ›å»ºè§’è‰²æˆåŠŸ: {response['Role']['Arn']}")
        return response['Role']['Arn']
    except Exception as e:
        print(f"åˆ›å»ºè§’è‰²å¤±è´¥: {e}")
        return None
```

#### Instance Types and Pricing å®ä¾‹ç±»å‹å’Œå®šä»·

Understanding different instance types helps you choose the right one for your needs:

äº†è§£ä¸åŒçš„å®ä¾‹ç±»å‹æœ‰åŠ©äºä½ é€‰æ‹©é€‚åˆéœ€æ±‚çš„å®ä¾‹ï¼š

```python
# SageMaker instance types comparison
# SageMakerå®ä¾‹ç±»å‹æ¯”è¾ƒ
instance_types = {
    "ml.t3.medium": {
        "vcpu": 2,
        "memory_gb": 4,
        "gpu": 0,
        "price_per_hour": 0.0464,
        "use_case": "å¼€å‘å’Œæµ‹è¯•"
    },
    "ml.m5.large": {
        "vcpu": 2,
        "memory_gb": 8,
        "gpu": 0,
        "price_per_hour": 0.096,
        "use_case": "ä¸­ç­‰è®¡ç®—éœ€æ±‚"
    },
    "ml.p3.2xlarge": {
        "vcpu": 8,
        "memory_gb": 61,
        "gpu": 1,  # Tesla V100
        "price_per_hour": 3.06,
        "use_case": "GPUåŠ é€Ÿè®­ç»ƒ"
    },
    "ml.p4d.24xlarge": {
        "vcpu": 96,
        "memory_gb": 1152,
        "gpu": 8,  # A100
        "price_per_hour": 32.77,
        "use_case": "å¤§è§„æ¨¡æ·±åº¦å­¦ä¹ "
    }
}

# Display comparison
# æ˜¾ç¤ºæ¯”è¾ƒ
print("SageMakerå®ä¾‹ç±»å‹æ¯”è¾ƒ:")
print("="*80)
print(f"{'ç±»å‹':<15} {'vCPU':<5} {'å†…å­˜(GB)':<8} {'GPU':<3} {'æ¯å°æ—¶ä»·æ ¼($)':<12} {'ç”¨é€”'}")
print("-"*80)

for instance_type, specs in instance_types.items():
    print(f"{instance_type:<15} {specs['vcpu']:<5} {specs['memory_gb']:<8} "
          f"{specs['gpu']:<3} {specs['price_per_hour']:<12} {specs['use_case']}")
```

### 20.2.3 Running and Stopping an Instance è¿è¡Œå’Œåœæ­¢å®ä¾‹

Managing your SageMaker instances properly is crucial for cost control and efficient workflow. Unlike EC2 instances, SageMaker notebook instances are designed to be started and stopped as needed.

æ­£ç¡®ç®¡ç†SageMakerå®ä¾‹å¯¹äºæˆæœ¬æ§åˆ¶å’Œé«˜æ•ˆå·¥ä½œæµç¨‹è‡³å…³é‡è¦ã€‚ä¸EC2å®ä¾‹ä¸åŒï¼ŒSageMakerç¬”è®°æœ¬å®ä¾‹è®¾è®¡ä¸ºæ ¹æ®éœ€è¦å¯åŠ¨å’Œåœæ­¢ã€‚

#### Starting an Instance å¯åŠ¨å®ä¾‹

```python
import boto3
import time

def start_notebook_instance(instance_name):
    """
    Start a SageMaker notebook instance
    å¯åŠ¨SageMakerç¬”è®°æœ¬å®ä¾‹
    """
    sagemaker = boto3.client('sagemaker')
    
    try:
        # Check current status
        # æ£€æŸ¥å½“å‰çŠ¶æ€
        response = sagemaker.describe_notebook_instance(
            NotebookInstanceName=instance_name
        )
        current_status = response['NotebookInstanceStatus']
        
        if current_status == 'InService':
            print(f"å®ä¾‹ {instance_name} å·²ç»åœ¨è¿è¡Œ")
            return response['Url']
        elif current_status == 'Stopped':
            # Start the instance
            # å¯åŠ¨å®ä¾‹
            sagemaker.start_notebook_instance(
                NotebookInstanceName=instance_name
            )
            print(f"æ­£åœ¨å¯åŠ¨å®ä¾‹ {instance_name}...")
            
            # Wait for instance to be ready
            # ç­‰å¾…å®ä¾‹å‡†å¤‡å°±ç»ª
            while True:
                response = sagemaker.describe_notebook_instance(
                    NotebookInstanceName=instance_name
                )
                status = response['NotebookInstanceStatus']
                print(f"å½“å‰çŠ¶æ€: {status}")
                
                if status == 'InService':
                    print("å®ä¾‹å¯åŠ¨æˆåŠŸ!")
                    return response['Url']
                elif status == 'Failed':
                    print("å®ä¾‹å¯åŠ¨å¤±è´¥!")
                    return None
                
                time.sleep(30)  # Wait 30 seconds before checking again
                
    except Exception as e:
        print(f"å¯åŠ¨å®ä¾‹æ—¶å‡ºé”™: {e}")
        return None

# Example usage
# ä½¿ç”¨ç¤ºä¾‹
# notebook_url = start_notebook_instance("my-deep-learning-notebook")
# if notebook_url:
#     print(f"ç¬”è®°æœ¬URL: {notebook_url}")
```

#### Stopping an Instance åœæ­¢å®ä¾‹

**Important**: Always stop your instances when you're not using them to avoid unnecessary charges.

**é‡è¦æç¤º**ï¼šä¸ä½¿ç”¨æ—¶åŠ¡å¿…åœæ­¢å®ä¾‹ï¼Œä»¥é¿å…ä¸å¿…è¦çš„è´¹ç”¨ã€‚

```python
def stop_notebook_instance(instance_name):
    """
    Stop a SageMaker notebook instance
    åœæ­¢SageMakerç¬”è®°æœ¬å®ä¾‹
    """
    sagemaker = boto3.client('sagemaker')
    
    try:
        # Check current status
        # æ£€æŸ¥å½“å‰çŠ¶æ€
        response = sagemaker.describe_notebook_instance(
            NotebookInstanceName=instance_name
        )
        current_status = response['NotebookInstanceStatus']
        
        if current_status == 'Stopped':
            print(f"å®ä¾‹ {instance_name} å·²ç»åœæ­¢")
        elif current_status == 'InService':
            # Stop the instance
            # åœæ­¢å®ä¾‹
            sagemaker.stop_notebook_instance(
                NotebookInstanceName=instance_name
            )
            print(f"æ­£åœ¨åœæ­¢å®ä¾‹ {instance_name}...")
            
            # Wait for instance to stop
            # ç­‰å¾…å®ä¾‹åœæ­¢
            while True:
                response = sagemaker.describe_notebook_instance(
                    NotebookInstanceName=instance_name
                )
                status = response['NotebookInstanceStatus']
                print(f"å½“å‰çŠ¶æ€: {status}")
                
                if status == 'Stopped':
                    print("å®ä¾‹åœæ­¢æˆåŠŸ!")
                    break
                elif status == 'Failed':
                    print("åœæ­¢å®ä¾‹å¤±è´¥!")
                    break
                
                time.sleep(30)
                
    except Exception as e:
        print(f"åœæ­¢å®ä¾‹æ—¶å‡ºé”™: {e}")

# Automated stop scheduler
# è‡ªåŠ¨åœæ­¢è°ƒåº¦å™¨
def schedule_auto_stop(instance_name, hours=2):
    """
    Schedule automatic stop of instance after specified hours
    åœ¨æŒ‡å®šå°æ—¶åå®‰æ’è‡ªåŠ¨åœæ­¢å®ä¾‹
    """
    import threading
    
    def auto_stop():
        time.sleep(hours * 3600)  # Convert hours to seconds
        print(f"è‡ªåŠ¨åœæ­¢å®ä¾‹ {instance_name} (è¿è¡Œäº† {hours} å°æ—¶)")
        stop_notebook_instance(instance_name)
    
    # Start auto-stop timer in background
    # åœ¨åå°å¯åŠ¨è‡ªåŠ¨åœæ­¢è®¡æ—¶å™¨
    timer_thread = threading.Thread(target=auto_stop)
    timer_thread.daemon = True
    timer_thread.start()
    
    print(f"å·²è®¾ç½® {hours} å°æ—¶åè‡ªåŠ¨åœæ­¢å®ä¾‹")
```

#### Instance Lifecycle Management å®ä¾‹ç”Ÿå‘½å‘¨æœŸç®¡ç†

```python
def get_instance_metrics(instance_name, days=7):
    """
    Get usage metrics for cost analysis
    è·å–ç”¨äºæˆæœ¬åˆ†æçš„ä½¿ç”¨æŒ‡æ ‡
    """
    import boto3
    from datetime import datetime, timedelta
    
    cloudwatch = boto3.client('cloudwatch')
    
    # Calculate time range
    # è®¡ç®—æ—¶é—´èŒƒå›´
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(days=days)
    
    # Get CPU utilization metrics
    # è·å–CPUåˆ©ç”¨ç‡æŒ‡æ ‡
    response = cloudwatch.get_metric_statistics(
        Namespace='AWS/SageMaker',
        MetricName='CPUUtilization',
        Dimensions=[
            {
                'Name': 'NotebookInstanceName',
                'Value': instance_name
            }
        ],
        StartTime=start_time,
        EndTime=end_time,
        Period=3600,  # 1 hour intervals
        Statistics=['Average', 'Maximum']
    )
    
    # Calculate estimated costs
    # è®¡ç®—ä¼°ç®—æˆæœ¬
    instance_hours = len(response['Datapoints'])
    estimated_cost = instance_hours * 0.0464  # ml.t3.medium price
    
    print(f"å®ä¾‹ {instance_name} æœ€è¿‘ {days} å¤©çš„ä½¿ç”¨æƒ…å†µ:")
    print(f"è¿è¡Œå°æ—¶æ•°: {instance_hours}")
    print(f"ä¼°ç®—æˆæœ¬: ${estimated_cost:.2f}")
    print(f"å¹³å‡CPUåˆ©ç”¨ç‡: {sum(dp['Average'] for dp in response['Datapoints'])/len(response['Datapoints']):.1f}%" if response['Datapoints'] else "æ— æ•°æ®")

# Cost optimization recommendations
# æˆæœ¬ä¼˜åŒ–å»ºè®®
def analyze_instance_usage(instance_name):
    """
    Analyze instance usage and provide cost optimization recommendations
    åˆ†æå®ä¾‹ä½¿ç”¨æƒ…å†µå¹¶æä¾›æˆæœ¬ä¼˜åŒ–å»ºè®®
    """
    sagemaker = boto3.client('sagemaker')
    
    # Get instance details
    # è·å–å®ä¾‹è¯¦æƒ…
    response = sagemaker.describe_notebook_instance(
        NotebookInstanceName=instance_name
    )
    
    instance_type = response['InstanceType']
    creation_time = response['CreationTime']
    last_modified = response['LastModifiedTime']
    
    # Calculate age
    # è®¡ç®—ä½¿ç”¨æ—¶é•¿
    age = datetime.now(creation_time.tzinfo) - creation_time
    
    print(f"å®ä¾‹åˆ†ææŠ¥å‘Š:")
    print(f"å®ä¾‹ç±»å‹: {instance_type}")
    print(f"åˆ›å»ºæ—¶é—´: {creation_time}")
    print(f"å®ä¾‹å¹´é¾„: {age.days} å¤©")
    
    # Provide recommendations
    # æä¾›å»ºè®®
    if age.days > 30 and instance_type.startswith('ml.p'):
        print("ğŸ’¡ å»ºè®®: GPUå®ä¾‹å·²ä½¿ç”¨è¶…è¿‡30å¤©ï¼Œè€ƒè™‘æ˜¯å¦éœ€è¦é™çº§åˆ°CPUå®ä¾‹ä»¥èŠ‚çœæˆæœ¬")
    elif age.days > 7 and 'large' in instance_type:
        print("ğŸ’¡ å»ºè®®: æ£€æŸ¥æ˜¯å¦å¯ä»¥ä½¿ç”¨æ›´å°çš„å®ä¾‹ç±»å‹")
    
    print("ğŸ’¡ æˆæœ¬ä¼˜åŒ–æç¤º:")
    print("- ä¸ä½¿ç”¨æ—¶ç«‹å³åœæ­¢å®ä¾‹")
    print("- å®šæœŸå¤‡ä»½é‡è¦ç¬”è®°æœ¬åˆ°S3")
    print("- è€ƒè™‘ä½¿ç”¨SageMaker Studioä½œä¸ºç°ä»£æ›¿ä»£æ–¹æ¡ˆ")
```

### 20.2.4 Updating Notebooks æ›´æ–°ç¬”è®°æœ¬

Keeping your SageMaker environment up to date is important for security, performance, and access to the latest features. There are several ways to update and manage your notebooks effectively.

ä¿æŒSageMakerç¯å¢ƒæ›´æ–°å¯¹äºå®‰å…¨æ€§ã€æ€§èƒ½å’Œè®¿é—®æœ€æ–°åŠŸèƒ½å¾ˆé‡è¦ã€‚æœ‰å‡ ç§æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°æ›´æ–°å’Œç®¡ç†ç¬”è®°æœ¬ã€‚

#### Git Integration Gité›†æˆ

SageMaker supports Git integration, allowing you to clone repositories directly into your notebook instance:

SageMakeræ”¯æŒGité›†æˆï¼Œå…è®¸ä½ ç›´æ¥å°†å­˜å‚¨åº“å…‹éš†åˆ°ç¬”è®°æœ¬å®ä¾‹ä¸­ï¼š

```python
# Setting up Git repository in SageMaker
# åœ¨SageMakerä¸­è®¾ç½®Gitå­˜å‚¨åº“
import subprocess
import os

def setup_git_repository(repo_url, local_path="/home/ec2-user/SageMaker"):
    """
    Clone a Git repository to SageMaker notebook instance
    å°†Gitå­˜å‚¨åº“å…‹éš†åˆ°SageMakerç¬”è®°æœ¬å®ä¾‹
    """
    try:
        # Change to SageMaker directory
        # åˆ‡æ¢åˆ°SageMakerç›®å½•
        os.chdir(local_path)
        
        # Clone repository
        # å…‹éš†å­˜å‚¨åº“
        result = subprocess.run(
            ["git", "clone", repo_url],
            capture_output=True,
            text=True
        )
        
        if result.returncode == 0:
            print(f"æˆåŠŸå…‹éš†å­˜å‚¨åº“: {repo_url}")
            print(f"æœ¬åœ°è·¯å¾„: {local_path}")
        else:
            print(f"å…‹éš†å¤±è´¥: {result.stderr}")
            
    except Exception as e:
        print(f"è®¾ç½®Gitå­˜å‚¨åº“æ—¶å‡ºé”™: {e}")

# Example: Clone a deep learning course repository
# ç¤ºä¾‹ï¼šå…‹éš†æ·±åº¦å­¦ä¹ è¯¾ç¨‹å­˜å‚¨åº“
# setup_git_repository("https://github.com/d2l-ai/d2l-en.git")
```

#### Package Management åŒ…ç®¡ç†

Managing Python packages in SageMaker requires understanding the environment structure:

åœ¨SageMakerä¸­ç®¡ç†PythonåŒ…éœ€è¦äº†è§£ç¯å¢ƒç»“æ„ï¼š

```python
# Package installation and management
# åŒ…å®‰è£…å’Œç®¡ç†
import sys
import subprocess

def install_packages(packages):
    """
    Install Python packages in SageMaker environment
    åœ¨SageMakerç¯å¢ƒä¸­å®‰è£…PythonåŒ…
    """
    for package in packages:
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", package])
            print(f"âœ… æˆåŠŸå®‰è£…: {package}")
        except subprocess.CalledProcessError as e:
            print(f"âŒ å®‰è£…å¤±è´¥ {package}: {e}")

# Essential packages for deep learning
# æ·±åº¦å­¦ä¹ å¿…éœ€åŒ…
essential_packages = [
    "torch",
    "torchvision",
    "transformers",
    "datasets",
    "matplotlib",
    "seaborn",
    "scikit-learn",
    "pandas",
    "numpy",
    "tensorboard"
]

# Install packages
# å®‰è£…åŒ…
# install_packages(essential_packages)

# Check installed packages
# æ£€æŸ¥å·²å®‰è£…çš„åŒ…
def list_installed_packages():
    """
    List all installed packages with versions
    åˆ—å‡ºæ‰€æœ‰å·²å®‰è£…åŒ…åŠå…¶ç‰ˆæœ¬
    """
    result = subprocess.run([sys.executable, "-m", "pip", "list"], 
                          capture_output=True, text=True)
    print("å·²å®‰è£…çš„åŒ…:")
    print(result.stdout)

# Create requirements.txt for reproducibility
# åˆ›å»ºrequirements.txtä»¥ç¡®ä¿å¯é‡å¤æ€§
def create_requirements_file():
    """
    Create requirements.txt file for environment reproducibility
    åˆ›å»ºrequirements.txtæ–‡ä»¶ä»¥ç¡®ä¿ç¯å¢ƒå¯é‡å¤æ€§
    """
    result = subprocess.run([sys.executable, "-m", "pip", "freeze"], 
                          capture_output=True, text=True)
    
    with open("requirements.txt", "w") as f:
        f.write(result.stdout)
    
    print("å·²åˆ›å»º requirements.txt æ–‡ä»¶")
    print("ä½¿ç”¨ 'pip install -r requirements.txt' æ¥é‡æ–°åˆ›å»ºç¯å¢ƒ")
```

#### Notebook Lifecycle Configuration ç¬”è®°æœ¬ç”Ÿå‘½å‘¨æœŸé…ç½®

Lifecycle configurations allow you to automate setup tasks when instances start:

ç”Ÿå‘½å‘¨æœŸé…ç½®å…è®¸ä½ åœ¨å®ä¾‹å¯åŠ¨æ—¶è‡ªåŠ¨åŒ–è®¾ç½®ä»»åŠ¡ï¼š

```bash
#!/bin/bash
# Lifecycle configuration script
# ç”Ÿå‘½å‘¨æœŸé…ç½®è„šæœ¬

# This script runs when the notebook instance starts
# æ­¤è„šæœ¬åœ¨ç¬”è®°æœ¬å®ä¾‹å¯åŠ¨æ—¶è¿è¡Œ

set -e

# Update system packages
# æ›´æ–°ç³»ç»ŸåŒ…
sudo yum update -y

# Install additional system dependencies
# å®‰è£…é¢å¤–çš„ç³»ç»Ÿä¾èµ–
sudo yum install -y htop tree

# Install conda packages
# å®‰è£…condaåŒ…
conda install -y -c conda-forge jupyterlab-git

# Install pip packages
# å®‰è£…pipåŒ…
pip install --upgrade pip
pip install torch torchvision torchaudio
pip install transformers datasets
pip install wandb tensorboard

# Set up Jupyter extensions
# è®¾ç½®Jupyteræ‰©å±•
jupyter labextension install @jupyter-widgets/jupyterlab-manager

# Create common directories
# åˆ›å»ºå¸¸ç”¨ç›®å½•
mkdir -p /home/ec2-user/SageMaker/data
mkdir -p /home/ec2-user/SageMaker/models
mkdir -p /home/ec2-user/SageMaker/notebooks

# Set up Git configuration (replace with your details)
# è®¾ç½®Gité…ç½®ï¼ˆæ›¿æ¢ä¸ºä½ çš„è¯¦ç»†ä¿¡æ¯ï¼‰
git config --global user.name "Your Name"
git config --global user.email "your.email@example.com"

echo "Lifecycle configuration completed successfully!"
echo "ç”Ÿå‘½å‘¨æœŸé…ç½®æˆåŠŸå®Œæˆ!"
```

#### Environment Management ç¯å¢ƒç®¡ç†

```python
# Environment setup and management
# ç¯å¢ƒè®¾ç½®å’Œç®¡ç†
import json
import boto3

def create_lifecycle_config(config_name, script_content):
    """
    Create a lifecycle configuration for SageMaker
    ä¸ºSageMakeråˆ›å»ºç”Ÿå‘½å‘¨æœŸé…ç½®
    """
    sagemaker = boto3.client('sagemaker')
    
    try:
        response = sagemaker.create_notebook_instance_lifecycle_config(
            NotebookInstanceLifecycleConfigName=config_name,
            OnStart=[
                {
                    'Content': script_content
                }
            ]
        )
        print(f"æˆåŠŸåˆ›å»ºç”Ÿå‘½å‘¨æœŸé…ç½®: {config_name}")
        return response['NotebookInstanceLifecycleConfigArn']
    except Exception as e:
        print(f"åˆ›å»ºç”Ÿå‘½å‘¨æœŸé…ç½®å¤±è´¥: {e}")
        return None

# Environment monitoring
# ç¯å¢ƒç›‘æ§
def monitor_environment():
    """
    Monitor the current environment status
    ç›‘æ§å½“å‰ç¯å¢ƒçŠ¶æ€
    """
    import psutil
    import GPUtil
    
    # CPU and Memory info
    # CPUå’Œå†…å­˜ä¿¡æ¯
    cpu_percent = psutil.cpu_percent(interval=1)
    memory = psutil.virtual_memory()
    disk = psutil.disk_usage('/')
    
    print("ç¯å¢ƒçŠ¶æ€ç›‘æ§:")
    print("="*50)
    print(f"CPU ä½¿ç”¨ç‡: {cpu_percent:.1f}%")
    print(f"å†…å­˜ä½¿ç”¨ç‡: {memory.percent:.1f}% ({memory.used/1024**3:.1f}GB / {memory.total/1024**3:.1f}GB)")
    print(f"ç£ç›˜ä½¿ç”¨ç‡: {disk.percent:.1f}% ({disk.used/1024**3:.1f}GB / {disk.total/1024**3:.1f}GB)")
    
    # GPU info (if available)
    # GPUä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    try:
        gpus = GPUtil.getGPUs()
        if gpus:
            print(f"GPU æ•°é‡: {len(gpus)}")
            for i, gpu in enumerate(gpus):
                print(f"GPU {i}: {gpu.name}")
                print(f"  æ˜¾å­˜ä½¿ç”¨ç‡: {gpu.memoryUtil*100:.1f}%")
                print(f"  GPUä½¿ç”¨ç‡: {gpu.load*100:.1f}%")
                print(f"  æ¸©åº¦: {gpu.temperature}Â°C")
    except:
        print("æœªæ£€æµ‹åˆ°GPUæˆ–GPUtilä¸å¯ç”¨")

# Run monitoring
# è¿è¡Œç›‘æ§
# monitor_environment()
```

### 20.2.5 Summary æ€»ç»“

Amazon SageMaker provides a powerful, managed environment for deep learning development. Here are the key takeaways:

Amazon SageMakerä¸ºæ·±åº¦å­¦ä¹ å¼€å‘æä¾›äº†å¼ºå¤§çš„æ‰˜ç®¡ç¯å¢ƒã€‚ä»¥ä¸‹æ˜¯å…³é”®è¦ç‚¹ï¼š

**Advantages ä¼˜åŠ¿:**
- Fully managed Jupyter notebook environment å®Œå…¨æ‰˜ç®¡çš„Jupyterç¬”è®°æœ¬ç¯å¢ƒ
- Easy scalability from CPU to GPU instances ä»CPUåˆ°GPUå®ä¾‹çš„è½»æ¾æ‰©å±•
- Integrated with AWS ecosystem AWSç”Ÿæ€ç³»ç»Ÿé›†æˆ
- Built-in security and compliance features å†…ç½®å®‰å…¨å’Œåˆè§„åŠŸèƒ½
- No infrastructure management required æ— éœ€åŸºç¡€è®¾æ–½ç®¡ç†

**Best Practices æœ€ä½³å®è·µ:**
- Always stop instances when not in use ä¸ä½¿ç”¨æ—¶åŠ¡å¿…åœæ­¢å®ä¾‹
- Use lifecycle configurations for consistent environments ä½¿ç”¨ç”Ÿå‘½å‘¨æœŸé…ç½®ç¡®ä¿ç¯å¢ƒä¸€è‡´æ€§
- Implement proper IAM roles and policies å®æ–½é€‚å½“çš„IAMè§’è‰²å’Œç­–ç•¥
- Regular backup of important notebooks å®šæœŸå¤‡ä»½é‡è¦ç¬”è®°æœ¬
- Monitor costs and usage patterns ç›‘æ§æˆæœ¬å’Œä½¿ç”¨æ¨¡å¼

**Cost Optimization æˆæœ¬ä¼˜åŒ–:**
- Start with smaller instances and scale up as needed ä»è¾ƒå°å®ä¾‹å¼€å§‹ï¼Œæ ¹æ®éœ€è¦æ‰©å±•
- Use Spot instances for training jobs ä½¿ç”¨Spotå®ä¾‹è¿›è¡Œè®­ç»ƒä½œä¸š
- Leverage S3 for data storage instead of instance storage åˆ©ç”¨S3è¿›è¡Œæ•°æ®å­˜å‚¨è€Œä¸æ˜¯å®ä¾‹å­˜å‚¨
- Set up billing alerts and usage monitoring è®¾ç½®è´¦å•è­¦æŠ¥å’Œä½¿ç”¨ç›‘æ§

### 20.2.6 Exercises ç»ƒä¹ 

1. **Basic Setup Exercise åŸºæœ¬è®¾ç½®ç»ƒä¹ **
   - Create your first SageMaker notebook instance åˆ›å»ºä½ çš„ç¬¬ä¸€ä¸ªSageMakerç¬”è®°æœ¬å®ä¾‹
   - Install PyTorch and verify GPU availability å®‰è£…PyTorchå¹¶éªŒè¯GPUå¯ç”¨æ€§
   - Run a simple neural network training example è¿è¡Œç®€å•çš„ç¥ç»ç½‘ç»œè®­ç»ƒç¤ºä¾‹

2. **Cost Management Exercise æˆæœ¬ç®¡ç†ç»ƒä¹ **
   - Set up CloudWatch billing alerts è®¾ç½®CloudWatchè´¦å•è­¦æŠ¥
   - Create a script to automatically stop instances after inactivity åˆ›å»ºè„šæœ¬åœ¨ä¸æ´»åŠ¨åè‡ªåŠ¨åœæ­¢å®ä¾‹
   - Compare costs between different instance types æ¯”è¾ƒä¸åŒå®ä¾‹ç±»å‹çš„æˆæœ¬

3. **Environment Configuration Exercise ç¯å¢ƒé…ç½®ç»ƒä¹ **
   - Create a lifecycle configuration script åˆ›å»ºç”Ÿå‘½å‘¨æœŸé…ç½®è„šæœ¬
   - Set up Git integration with your favorite repository è®¾ç½®ä¸ä½ å–œæ¬¢çš„å­˜å‚¨åº“çš„Gité›†æˆ
   - Create a custom conda environment for your project ä¸ºä½ çš„é¡¹ç›®åˆ›å»ºè‡ªå®šä¹‰condaç¯å¢ƒ

## 20.3 Using AWS EC2 Instances ä½¿ç”¨AWS EC2å®ä¾‹

Amazon Elastic Compute Cloud (EC2) provides scalable computing capacity in the cloud. Unlike SageMaker, EC2 gives you complete control over the computing environment, making it ideal when you need custom configurations or want to optimize costs for long-running workloads.

Amazonå¼¹æ€§è®¡ç®—äº‘ï¼ˆEC2ï¼‰åœ¨äº‘ä¸­æä¾›å¯æ‰©å±•çš„è®¡ç®—èƒ½åŠ›ã€‚ä¸SageMakerä¸åŒï¼ŒEC2è®©ä½ å®Œå…¨æ§åˆ¶è®¡ç®—ç¯å¢ƒï¼Œè¿™ä½¿å…¶åœ¨éœ€è¦è‡ªå®šä¹‰é…ç½®æˆ–æƒ³è¦ä¸ºé•¿æ—¶é—´è¿è¡Œçš„å·¥ä½œè´Ÿè½½ä¼˜åŒ–æˆæœ¬æ—¶éå¸¸ç†æƒ³ã€‚

Think of EC2 as renting a virtual computer in the cloud - you get to choose its specifications, install whatever software you need, and configure it exactly how you want.

æŠŠEC2æƒ³è±¡æˆåœ¨äº‘ä¸­ç§Ÿç”¨è™šæ‹Ÿè®¡ç®—æœºâ€”â€”ä½ å¯ä»¥é€‰æ‹©å…¶è§„æ ¼ã€å®‰è£…æ‰€éœ€çš„ä»»ä½•è½¯ä»¶ï¼Œå¹¶å®Œå…¨æŒ‰ç…§ä½ çš„éœ€è¦é…ç½®å®ƒã€‚

### 20.3.1 Creating and Running an EC2 Instance åˆ›å»ºå’Œè¿è¡ŒEC2å®ä¾‹

Creating an EC2 instance for deep learning requires careful consideration of instance types, storage, networking, and security configurations.

ä¸ºæ·±åº¦å­¦ä¹ åˆ›å»ºEC2å®ä¾‹éœ€è¦ä»”ç»†è€ƒè™‘å®ä¾‹ç±»å‹ã€å­˜å‚¨ã€ç½‘ç»œå’Œå®‰å…¨é…ç½®ã€‚

#### Step 1: Choosing the Right Instance Type æ­¥éª¤1ï¼šé€‰æ‹©æ­£ç¡®çš„å®ä¾‹ç±»å‹

```python
# EC2 Instance types for deep learning
# ç”¨äºæ·±åº¦å­¦ä¹ çš„EC2å®ä¾‹ç±»å‹
ec2_instances = {
    # CPU-optimized instances for development
    # ç”¨äºå¼€å‘çš„CPUä¼˜åŒ–å®ä¾‹
    "t3.large": {
        "vcpu": 2,
        "memory_gb": 8,
        "network": "Up to 5 Gbps",
        "price_per_hour": 0.0832,
        "use_case": "å¼€å‘å’Œå°è§„æ¨¡å®éªŒ"
    },
    "m5.xlarge": {
        "vcpu": 4,
        "memory_gb": 16,
        "network": "Up to 10 Gbps",
        "price_per_hour": 0.192,
        "use_case": "ä¸­ç­‰è®¡ç®—éœ€æ±‚"
    },
    
    # GPU instances for training
    # ç”¨äºè®­ç»ƒçš„GPUå®ä¾‹
    "p3.2xlarge": {
        "vcpu": 8,
        "memory_gb": 61,
        "gpu": "1x Tesla V100 (16GB)",
        "gpu_memory": "16 GB",
        "price_per_hour": 3.06,
        "use_case": "å•GPUè®­ç»ƒ"
    },
    "p3.8xlarge": {
        "vcpu": 32,
        "memory_gb": 244,
        "gpu": "4x Tesla V100 (16GB each)",
        "gpu_memory": "64 GB total",
        "price_per_hour": 12.24,
        "use_case": "å¤šGPUè®­ç»ƒ"
    },
    "p4d.24xlarge": {
        "vcpu": 96,
        "memory_gb": 1152,
        "gpu": "8x A100 (40GB each)",
        "gpu_memory": "320 GB total",
        "price_per_hour": 32.77,
        "use_case": "å¤§è§„æ¨¡åˆ†å¸ƒå¼è®­ç»ƒ"
    },
    
    # Cost-effective GPU instances
    # æ€§ä»·æ¯”é«˜çš„GPUå®ä¾‹
    "g4dn.xlarge": {
        "vcpu": 4,
        "memory_gb": 16,
        "gpu": "1x T4 (16GB)",
        "gpu_memory": "16 GB",
        "price_per_hour": 0.526,
        "use_case": "æ¨ç†å’Œè½»é‡çº§è®­ç»ƒ"
    }
}

# Display comparison
# æ˜¾ç¤ºæ¯”è¾ƒ
print("EC2æ·±åº¦å­¦ä¹ å®ä¾‹ç±»å‹æ¯”è¾ƒ:")
print("="*100)
print(f"{'å®ä¾‹ç±»å‹':<12} {'vCPU':<5} {'å†…å­˜(GB)':<8} {'GPU':<25} {'æ¯å°æ—¶($)':<10} {'æ¨èç”¨é€”'}")
print("-"*100)

for instance, specs in ec2_instances.items():
    gpu_info = specs.get('gpu', 'None')
    print(f"{instance:<12} {specs['vcpu']:<5} {specs['memory_gb']:<8} "
          f"{gpu_info:<25} {specs['price_per_hour']:<10} {specs['use_case']}")
```

#### Step 2: Launching an EC2 Instance æ­¥éª¤2ï¼šå¯åŠ¨EC2å®ä¾‹

```python
import boto3
import time

def launch_ec2_instance(instance_type="g4dn.xlarge", key_name="my-key-pair"):
    """
    Launch an EC2 instance optimized for deep learning
    å¯åŠ¨é’ˆå¯¹æ·±åº¦å­¦ä¹ ä¼˜åŒ–çš„EC2å®ä¾‹
    """
    ec2 = boto3.client('ec2')
    
    # Deep Learning AMI (Amazon Machine Image)
    # æ·±åº¦å­¦ä¹ AMIï¼ˆAmazonæœºå™¨æ˜ åƒï¼‰
    # This AMI comes pre-installed with popular ML frameworks
    # æ­¤AMIé¢„è£…äº†æµè¡Œçš„æœºå™¨å­¦ä¹ æ¡†æ¶
    ami_id = "ami-0c94855ba95b798c7"  # Deep Learning AMI (Ubuntu 20.04)
    
    # Security group configuration
    # å®‰å…¨ç»„é…ç½®
    security_group_config = {
        'GroupName': 'deep-learning-sg',
        'Description': 'Security group for deep learning instances',
        'IpPermissions': [
            {
                'IpProtocol': 'tcp',
                'FromPort': 22,
                'ToPort': 22,
                'IpRanges': [{'CidrIp': '0.0.0.0/0', 'Description': 'SSH access'}]
            },
            {
                'IpProtocol': 'tcp',
                'FromPort': 8888,
                'ToPort': 8888,
                'IpRanges': [{'CidrIp': '0.0.0.0/0', 'Description': 'Jupyter Notebook'}]
            },
            {
                'IpProtocol': 'tcp',
                'FromPort': 6006,
                'ToPort': 6006,
                'IpRanges': [{'CidrIp': '0.0.0.0/0', 'Description': 'TensorBoard'}]
            }
        ]
    }
    
    try:
        # Create security group
        # åˆ›å»ºå®‰å…¨ç»„
        try:
            sg_response = ec2.create_security_group(**security_group_config)
            security_group_id = sg_response['GroupId']
            print(f"åˆ›å»ºå®‰å…¨ç»„: {security_group_id}")
        except ec2.exceptions.ClientError as e:
            if 'already exists' in str(e):
                # Get existing security group
                # è·å–ç°æœ‰å®‰å…¨ç»„
                sg_response = ec2.describe_security_groups(
                    GroupNames=[security_group_config['GroupName']]
                )
                security_group_id = sg_response['SecurityGroups'][0]['GroupId']
                print(f"ä½¿ç”¨ç°æœ‰å®‰å…¨ç»„: {security_group_id}")
            else:
                raise e
        
        # Launch instance
        # å¯åŠ¨å®ä¾‹
        response = ec2.run_instances(
            ImageId=ami_id,
            MinCount=1,
            MaxCount=1,
            InstanceType=instance_type,
            KeyName=key_name,
            SecurityGroupIds=[security_group_id],
            BlockDeviceMappings=[
                {
                    'DeviceName': '/dev/sda1',
                    'Ebs': {
                        'VolumeType': 'gp3',
                        'VolumeSize': 100,  # 100 GB storage
                        'DeleteOnTermination': True
                    }
                }
            ],
            TagSpecifications=[
                {
                    'ResourceType': 'instance',
                    'Tags': [
                        {'Key': 'Name', 'Value': 'Deep Learning Instance'},
                        {'Key': 'Purpose', 'Value': 'ML Training'}
                    ]
                }
            ]
        )
        
        instance_id = response['Instances'][0]['InstanceId']
        print(f"å¯åŠ¨å®ä¾‹: {instance_id}")
        
        # Wait for instance to be running
        # ç­‰å¾…å®ä¾‹è¿è¡Œ
        print("ç­‰å¾…å®ä¾‹å¯åŠ¨...")
        waiter = ec2.get_waiter('instance_running')
        waiter.wait(InstanceIds=[instance_id])
        
        # Get instance details
        # è·å–å®ä¾‹è¯¦æƒ…
        instances = ec2.describe_instances(InstanceIds=[instance_id])
        instance = instances['Reservations'][0]['Instances'][0]
        public_ip = instance.get('PublicIpAddress')
        
        print(f"å®ä¾‹å¯åŠ¨æˆåŠŸ!")
        print(f"å®ä¾‹ID: {instance_id}")
        print(f"å…¬ç½‘IP: {public_ip}")
        print(f"SSHè¿æ¥: ssh -i {key_name}.pem ubuntu@{public_ip}")
        
        return {
            'instance_id': instance_id,
            'public_ip': public_ip,
            'instance_type': instance_type
        }
        
    except Exception as e:
        print(f"å¯åŠ¨å®ä¾‹å¤±è´¥: {e}")
        return None

# Example usage
# ä½¿ç”¨ç¤ºä¾‹
# instance_info = launch_ec2_instance("g4dn.xlarge", "my-key-pair")
```

#### Step 3: Creating Key Pairs for SSH Access æ­¥éª¤3ï¼šåˆ›å»ºç”¨äºSSHè®¿é—®çš„å¯†é’¥å¯¹

```python
def create_key_pair(key_name="deep-learning-key"):
    """
    Create an EC2 key pair for SSH access
    åˆ›å»ºç”¨äºSSHè®¿é—®çš„EC2å¯†é’¥å¯¹
    """
    ec2 = boto3.client('ec2')
    
    try:
        response = ec2.create_key_pair(KeyName=key_name)
        
        # Save private key to file
        # å°†ç§é’¥ä¿å­˜åˆ°æ–‡ä»¶
        with open(f"{key_name}.pem", 'w') as key_file:
            key_file.write(response['KeyMaterial'])
        
        # Set proper permissions for the key file
        # ä¸ºå¯†é’¥æ–‡ä»¶è®¾ç½®é€‚å½“æƒé™
        import os
        os.chmod(f"{key_name}.pem", 0o400)
        
        print(f"å¯†é’¥å¯¹åˆ›å»ºæˆåŠŸ: {key_name}")
        print(f"ç§é’¥å·²ä¿å­˜åˆ°: {key_name}.pem")
        print("è¯·å®‰å…¨ä¿å­˜æ­¤æ–‡ä»¶ï¼Œå®ƒç”¨äºSSHè¿æ¥åˆ°å®ä¾‹")
        
        return key_name
        
    except Exception as e:
        print(f"åˆ›å»ºå¯†é’¥å¯¹å¤±è´¥: {e}")
        return None

# SSH connection guide
# SSHè¿æ¥æŒ‡å—
def print_ssh_guide(public_ip, key_name):
    """
    Print SSH connection instructions
    æ‰“å°SSHè¿æ¥è¯´æ˜
    """
    print("\n" + "="*60)
    print("SSHè¿æ¥æŒ‡å—:")
    print("="*60)
    print(f"1. ç¡®ä¿å¯†é’¥æ–‡ä»¶æƒé™æ­£ç¡®:")
    print(f"   chmod 400 {key_name}.pem")
    print(f"\n2. è¿æ¥åˆ°å®ä¾‹:")
    print(f"   ssh -i {key_name}.pem ubuntu@{public_ip}")
    print(f"\n3. é¦–æ¬¡è¿æ¥æ—¶ï¼Œè¾“å…¥ 'yes' æ¥å—ä¸»æœºå¯†é’¥")
    print(f"\n4. è¿æ¥æˆåŠŸåï¼Œä½ å°†è¿›å…¥Ubuntuç¯å¢ƒ")
    print("="*60)
```

### 20.3.2 Installing CUDA å®‰è£…CUDA

CUDA (Compute Unified Device Architecture) is NVIDIA's parallel computing platform that enables GPUs to be used for deep learning. Installing CUDA correctly is crucial for GPU-accelerated training.

CUDAï¼ˆè®¡ç®—ç»Ÿä¸€è®¾å¤‡æ¶æ„ï¼‰æ˜¯NVIDIAçš„å¹¶è¡Œè®¡ç®—å¹³å°ï¼Œä½¿GPUèƒ½å¤Ÿç”¨äºæ·±åº¦å­¦ä¹ ã€‚æ­£ç¡®å®‰è£…CUDAå¯¹äºGPUåŠ é€Ÿè®­ç»ƒè‡³å…³é‡è¦ã€‚

Think of CUDA as the bridge that allows your deep learning frameworks to communicate with your GPU hardware.

æŠŠCUDAæƒ³è±¡æˆå…è®¸ä½ çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸GPUç¡¬ä»¶é€šä¿¡çš„æ¡¥æ¢ã€‚

#### CUDA Installation Script CUDAå®‰è£…è„šæœ¬

```bash
#!/bin/bash
# CUDA Installation Script for Ubuntu 20.04
# Ubuntu 20.04çš„CUDAå®‰è£…è„šæœ¬

echo "å¼€å§‹å®‰è£…CUDA..."

# Update system packages
# æ›´æ–°ç³»ç»ŸåŒ…
sudo apt update
sudo apt upgrade -y

# Install required dependencies
# å®‰è£…å¿…éœ€çš„ä¾èµ–é¡¹
sudo apt install -y wget software-properties-common

# Remove any existing NVIDIA drivers (if upgrading)
# åˆ é™¤ä»»ä½•ç°æœ‰çš„NVIDIAé©±åŠ¨ç¨‹åºï¼ˆå¦‚æœå‡çº§ï¼‰
# sudo apt remove --purge nvidia-*
# sudo apt autoremove

# Add NVIDIA package repositories
# æ·»åŠ NVIDIAåŒ…ä»“åº“
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-keyring_1.0-1_all.deb
sudo dpkg -i cuda-keyring_1.0-1_all.deb
sudo apt update

# Install CUDA Toolkit (version 11.8)
# å®‰è£…CUDAå·¥å…·åŒ…ï¼ˆç‰ˆæœ¬11.8ï¼‰
sudo apt install -y cuda-toolkit-11-8

# Install NVIDIA driver
# å®‰è£…NVIDIAé©±åŠ¨ç¨‹åº
sudo apt install -y nvidia-driver-520

# Add CUDA to PATH
# å°†CUDAæ·»åŠ åˆ°PATH
echo 'export PATH=/usr/local/cuda-11.8/bin:$PATH' >> ~/.bashrc
echo 'export LD_LIBRARY_PATH=/usr/local/cuda-11.8/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc

# Reload bash configuration
# é‡æ–°åŠ è½½bashé…ç½®
source ~/.bashrc

echo "CUDAå®‰è£…å®Œæˆï¼è¯·é‡å¯ç³»ç»Ÿä»¥ä½¿NVIDIAé©±åŠ¨ç¨‹åºç”Ÿæ•ˆã€‚"
echo "é‡å¯åï¼Œè¿è¡Œ 'nvidia-smi' éªŒè¯å®‰è£…ã€‚"
```

#### CUDA Verification Script CUDAéªŒè¯è„šæœ¬

```python
# CUDA Verification and Testing
# CUDAéªŒè¯å’Œæµ‹è¯•
import subprocess
import sys

def check_nvidia_driver():
    """
    Check if NVIDIA driver is properly installed
    æ£€æŸ¥NVIDIAé©±åŠ¨ç¨‹åºæ˜¯å¦æ­£ç¡®å®‰è£…
    """
    try:
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
        if result.returncode == 0:
            print("âœ… NVIDIAé©±åŠ¨ç¨‹åºå®‰è£…æ­£ç¡®")
            print(result.stdout)
            return True
        else:
            print("âŒ NVIDIAé©±åŠ¨ç¨‹åºæœªæ­£ç¡®å®‰è£…")
            return False
    except FileNotFoundError:
        print("âŒ nvidia-smiå‘½ä»¤æœªæ‰¾åˆ°ï¼Œè¯·å®‰è£…NVIDIAé©±åŠ¨ç¨‹åº")
        return False

def check_cuda_installation():
    """
    Check CUDA installation and version
    æ£€æŸ¥CUDAå®‰è£…å’Œç‰ˆæœ¬
    """
    try:
        result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)
        if result.returncode == 0:
            print("âœ… CUDAå·¥å…·åŒ…å®‰è£…æ­£ç¡®")
            print(result.stdout)
            return True
        else:
            print("âŒ CUDAå·¥å…·åŒ…æœªæ­£ç¡®å®‰è£…")
            return False
    except FileNotFoundError:
        print("âŒ nvccå‘½ä»¤æœªæ‰¾åˆ°ï¼Œè¯·å®‰è£…CUDAå·¥å…·åŒ…")
        return False

def test_pytorch_cuda():
    """
    Test PyTorch CUDA integration
    æµ‹è¯•PyTorch CUDAé›†æˆ
    """
    try:
        import torch
        print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
        print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
        
        if torch.cuda.is_available():
            print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
            print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
            
            for i in range(torch.cuda.device_count()):
                gpu_name = torch.cuda.get_device_name(i)
                gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
                print(f"GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)")
            
            # Test GPU computation
            # æµ‹è¯•GPUè®¡ç®—
            device = torch.device('cuda')
            x = torch.randn(1000, 1000, device=device)
            y = torch.randn(1000, 1000, device=device)
            z = torch.mm(x, y)
            print("âœ… GPUè®¡ç®—æµ‹è¯•æˆåŠŸ")
            
        else:
            print("âŒ PyTorchæ— æ³•è®¿é—®CUDA")
            
    except ImportError:
        print("âŒ PyTorchæœªå®‰è£…")

def test_tensorflow_gpu():
    """
    Test TensorFlow GPU integration
    æµ‹è¯•TensorFlow GPUé›†æˆ
    """
    try:
        import tensorflow as tf
        print(f"TensorFlowç‰ˆæœ¬: {tf.__version__}")
        
        gpus = tf.config.list_physical_devices('GPU')
        print(f"æ£€æµ‹åˆ°GPUæ•°é‡: {len(gpus)}")
        
        if gpus:
            for i, gpu in enumerate(gpus):
                print(f"GPU {i}: {gpu}")
            
            # Test GPU computation
            # æµ‹è¯•GPUè®¡ç®—
            with tf.device('/GPU:0'):
                a = tf.random.normal([1000, 1000])
                b = tf.random.normal([1000, 1000])
                c = tf.matmul(a, b)
            print("âœ… TensorFlow GPUè®¡ç®—æµ‹è¯•æˆåŠŸ")
        else:
            print("âŒ TensorFlowæ— æ³•æ£€æµ‹åˆ°GPU")
            
    except ImportError:
        print("âŒ TensorFlowæœªå®‰è£…")

# Run all checks
# è¿è¡Œæ‰€æœ‰æ£€æŸ¥
def run_cuda_diagnostics():
    """
    Run comprehensive CUDA diagnostics
    è¿è¡Œå…¨é¢çš„CUDAè¯Šæ–­
    """
    print("CUDAç¯å¢ƒè¯Šæ–­")
    print("="*50)
    
    print("\n1. æ£€æŸ¥NVIDIAé©±åŠ¨ç¨‹åº:")
    driver_ok = check_nvidia_driver()
    
    print("\n2. æ£€æŸ¥CUDAå·¥å…·åŒ…:")
    cuda_ok = check_cuda_installation()
    
    print("\n3. æµ‹è¯•PyTorch CUDA:")
    test_pytorch_cuda()
    
    print("\n4. æµ‹è¯•TensorFlow GPU:")
    test_tensorflow_gpu()
    
    print("\n" + "="*50)
    if driver_ok and cuda_ok:
        print("âœ… CUDAç¯å¢ƒé…ç½®æ­£ç¡®ï¼")
    else:
        print("âŒ CUDAç¯å¢ƒé…ç½®å­˜åœ¨é—®é¢˜ï¼Œè¯·æ£€æŸ¥å®‰è£…æ­¥éª¤")

# Run diagnostics
# è¿è¡Œè¯Šæ–­
# run_cuda_diagnostics()
```

#### Common CUDA Installation Issues å¸¸è§CUDAå®‰è£…é—®é¢˜

```python
# Common CUDA troubleshooting solutions
# å¸¸è§CUDAæ•…éšœæ’é™¤è§£å†³æ–¹æ¡ˆ

def troubleshoot_cuda():
    """
    Provide solutions for common CUDA issues
    ä¸ºå¸¸è§CUDAé—®é¢˜æä¾›è§£å†³æ–¹æ¡ˆ
    """
    print("CUDAæ•…éšœæ’é™¤æŒ‡å—")
    print("="*60)
    
    issues_solutions = {
        "nvidia-smi å‘½ä»¤æœªæ‰¾åˆ°": [
            "sudo apt update",
            "sudo apt install nvidia-driver-520",
            "sudo reboot"
        ],
        
        "CUDAç‰ˆæœ¬ä¸åŒ¹é…": [
            "æ£€æŸ¥PyTorch/TensorFlowæ”¯æŒçš„CUDAç‰ˆæœ¬",
            "å¸è½½å½“å‰CUDA: sudo apt remove cuda-*",
            "å®‰è£…åŒ¹é…ç‰ˆæœ¬çš„CUDA",
            "é‡æ–°å®‰è£…æ·±åº¦å­¦ä¹ æ¡†æ¶"
        ],
        
        "GPUå†…å­˜ä¸è¶³": [
            "å‡å°‘æ‰¹æ¬¡å¤§å° (batch_size)",
            "ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯",
            "å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ",
            "ä½¿ç”¨æ¨¡å‹å¹¶è¡ŒåŒ–"
        ],
        
        "CUDA out of memory": [
            "torch.cuda.empty_cache() # æ¸…ç†GPUç¼“å­˜",
            "å‡å°‘æ¨¡å‹å¤§å°æˆ–è¾“å…¥å¤§å°",
            "ä½¿ç”¨gradient checkpointing",
            "ç›‘æ§GPUå†…å­˜ä½¿ç”¨: nvidia-smi"
        ]
    }
    
    for issue, solutions in issues_solutions.items():
        print(f"\né—®é¢˜: {issue}")
        print("-" * len(issue))
        for i, solution in enumerate(solutions, 1):
            print(f"{i}. {solution}")

# Display troubleshooting guide
# æ˜¾ç¤ºæ•…éšœæ’é™¤æŒ‡å—
# troubleshoot_cuda()
```

### 20.3.3 Installing Libraries for Running the Code å®‰è£…è¿è¡Œä»£ç æ‰€éœ€çš„åº“

After setting up CUDA, you need to install the necessary Python libraries and frameworks for deep learning. This section covers setting up a complete deep learning environment.

è®¾ç½®CUDAåï¼Œä½ éœ€è¦å®‰è£…æ·±åº¦å­¦ä¹ æ‰€éœ€çš„Pythonåº“å’Œæ¡†æ¶ã€‚æœ¬èŠ‚æ¶µç›–è®¾ç½®å®Œæ•´çš„æ·±åº¦å­¦ä¹ ç¯å¢ƒã€‚

#### Python Environment Setup Pythonç¯å¢ƒè®¾ç½®

```bash
#!/bin/bash
# Python Environment Setup for Deep Learning
# æ·±åº¦å­¦ä¹ Pythonç¯å¢ƒè®¾ç½®

echo "è®¾ç½®Pythonæ·±åº¦å­¦ä¹ ç¯å¢ƒ..."

# Install Python package manager and virtual environment tools
# å®‰è£…PythonåŒ…ç®¡ç†å™¨å’Œè™šæ‹Ÿç¯å¢ƒå·¥å…·
sudo apt update
sudo apt install -y python3-pip python3-venv python3-dev

# Install system dependencies
# å®‰è£…ç³»ç»Ÿä¾èµ–é¡¹
sudo apt install -y build-essential cmake git wget curl
sudo apt install -y libopencv-dev libopenblas-dev liblapack-dev
sudo apt install -y libjpeg-dev libpng-dev libtiff-dev
sudo apt install -y libavcodec-dev libavformat-dev libswscale-dev
sudo apt install -y libgtk-3-dev libcanberra-gtk-module libcanberra-gtk3-module

# Create virtual environment
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python3 -m venv ~/deeplearning_env
source ~/deeplearning_env/bin/activate

# Upgrade pip
# å‡çº§pip
pip install --upgrade pip setuptools wheel

echo "Pythonç¯å¢ƒè®¾ç½®å®Œæˆï¼"
```

#### Deep Learning Frameworks Installation æ·±åº¦å­¦ä¹ æ¡†æ¶å®‰è£…

```python
# Deep Learning Libraries Installation Script
# æ·±åº¦å­¦ä¹ åº“å®‰è£…è„šæœ¬
import subprocess
import sys

def install_pytorch():
    """
    Install PyTorch with CUDA support
    å®‰è£…æ”¯æŒCUDAçš„PyTorch
    """
    print("å®‰è£…PyTorch...")
    
    # PyTorch with CUDA 11.8 support
    # æ”¯æŒCUDA 11.8çš„PyTorch
    pytorch_packages = [
        "torch==2.0.0+cu118",
        "torchvision==0.15.0+cu118", 
        "torchaudio==2.0.0+cu118"
    ]
    
    for package in pytorch_packages:
        try:
            subprocess.check_call([
                sys.executable, "-m", "pip", "install", package,
                "--index-url", "https://download.pytorch.org/whl/cu118"
            ])
            print(f"âœ… æˆåŠŸå®‰è£…: {package}")
        except subprocess.CalledProcessError as e:
            print(f"âŒ å®‰è£…å¤±è´¥ {package}: {e}")

def install_tensorflow():
    """
    Install TensorFlow with GPU support
    å®‰è£…æ”¯æŒGPUçš„TensorFlow
    """
    print("å®‰è£…TensorFlow...")
    
    try:
        subprocess.check_call([sys.executable, "-m", "pip", "install", "tensorflow[and-cuda]"])
        print("âœ… æˆåŠŸå®‰è£…TensorFlow GPUç‰ˆæœ¬")
    except subprocess.CalledProcessError as e:
        print(f"âŒ TensorFlowå®‰è£…å¤±è´¥: {e}")

def install_essential_packages():
    """
    Install essential packages for deep learning
    å®‰è£…æ·±åº¦å­¦ä¹ å¿…éœ€åŒ…
    """
    print("å®‰è£…å¿…éœ€çš„æ·±åº¦å­¦ä¹ åŒ…...")
    
    essential_packages = [
        # Data manipulation and analysis
        # æ•°æ®æ“ä½œå’Œåˆ†æ
        "numpy>=1.21.0",
        "pandas>=1.3.0",
        "scipy>=1.7.0",
        
        # Machine learning
        # æœºå™¨å­¦ä¹ 
        "scikit-learn>=1.0.0",
        "xgboost>=1.5.0",
        
        # Visualization
        # å¯è§†åŒ–
        "matplotlib>=3.4.0",
        "seaborn>=0.11.0",
        "plotly>=5.0.0",
        
        # Image processing
        # å›¾åƒå¤„ç†
        "opencv-python>=4.5.0",
        "Pillow>=8.3.0",
        "imageio>=2.9.0",
        
        # Natural Language Processing
        # è‡ªç„¶è¯­è¨€å¤„ç†
        "transformers>=4.10.0",
        "datasets>=1.12.0",
        "tokenizers>=0.10.0",
        "nltk>=3.6.0",
        "spacy>=3.4.0",
        
        # Experiment tracking and monitoring
        # å®éªŒè·Ÿè¸ªå’Œç›‘æ§
        "tensorboard>=2.7.0",
        "wandb>=0.12.0",
        "mlflow>=1.20.0",
        
        # Jupyter and development tools
        # Jupyterå’Œå¼€å‘å·¥å…·
        "jupyter>=1.0.0",
        "jupyterlab>=3.1.0",
        "ipywidgets>=7.6.0",
        
        # Additional utilities
        # é™„åŠ å®ç”¨å·¥å…·
        "tqdm>=4.62.0",
        "requests>=2.26.0",
        "PyYAML>=5.4.0",
        "h5py>=3.4.0"
    ]
    
    for package in essential_packages:
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", package])
            print(f"âœ… æˆåŠŸå®‰è£…: {package}")
        except subprocess.CalledProcessError as e:
            print(f"âŒ å®‰è£…å¤±è´¥ {package}: {e}")

def install_specialized_packages():
    """
    Install specialized deep learning packages
    å®‰è£…ä¸“ä¸šæ·±åº¦å­¦ä¹ åŒ…
    """
    print("å®‰è£…ä¸“ä¸šæ·±åº¦å­¦ä¹ åŒ…...")
    
    specialized_packages = [
        # Computer Vision
        # è®¡ç®—æœºè§†è§‰
        "timm",  # PyTorch Image Models
        "albumentations",  # Image augmentation
        "detectron2 @ git+https://github.com/facebookresearch/detectron2.git",
        
        # Audio processing
        # éŸ³é¢‘å¤„ç†
        "librosa",
        "soundfile",
        
        # Graph Neural Networks
        # å›¾ç¥ç»ç½‘ç»œ
        "torch-geometric",
        "dgl",
        
        # Reinforcement Learning
        # å¼ºåŒ–å­¦ä¹ 
        "gym",
        "stable-baselines3",
        
        # Optimization
        # ä¼˜åŒ–
        "optuna",
        "hyperopt",
        
        # Model deployment
        # æ¨¡å‹éƒ¨ç½²
        "onnx",
        "onnxruntime",
        "torchscript"
    ]
    
    for package in specialized_packages:
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", package])
            print(f"âœ… æˆåŠŸå®‰è£…: {package}")
        except subprocess.CalledProcessError as e:
            print(f"âš ï¸ è·³è¿‡å¯é€‰åŒ… {package}: {e}")

def create_requirements_file():
    """
    Create requirements.txt file for environment reproduction
    åˆ›å»ºrequirements.txtæ–‡ä»¶ä»¥ä¾¿ç¯å¢ƒå¤ç°
    """
    print("åˆ›å»ºrequirements.txtæ–‡ä»¶...")
    
    try:
        result = subprocess.run([sys.executable, "-m", "pip", "freeze"], 
                              capture_output=True, text=True)
        
        with open("requirements.txt", "w") as f:
            f.write(result.stdout)
        
        print("âœ… requirements.txtåˆ›å»ºæˆåŠŸ")
        print("ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤åœ¨æ–°ç¯å¢ƒä¸­å®‰è£…ç›¸åŒçš„åŒ…:")
        print("pip install -r requirements.txt")
        
    except Exception as e:
        print(f"âŒ åˆ›å»ºrequirements.txtå¤±è´¥: {e}")

def setup_jupyter_extensions():
    """
    Setup Jupyter extensions for better development experience
    è®¾ç½®Jupyteræ‰©å±•ä»¥è·å¾—æ›´å¥½çš„å¼€å‘ä½“éªŒ
    """
    print("è®¾ç½®Jupyteræ‰©å±•...")
    
    extensions = [
        "jupyter_contrib_nbextensions",
        "jupyter_nbextensions_configurator",
        "ipywidgets"
    ]
    
    for ext in extensions:
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", ext])
            print(f"âœ… æˆåŠŸå®‰è£…: {ext}")
        except subprocess.CalledProcessError as e:
            print(f"âŒ å®‰è£…å¤±è´¥ {ext}: {e}")
    
    # Enable extensions
    # å¯ç”¨æ‰©å±•
    try:
        subprocess.check_call([sys.executable, "-m", "jupyter", "contrib", "nbextension", "install", "--user"])
        subprocess.check_call([sys.executable, "-m", "jupyter", "nbextensions_configurator", "enable", "--user"])
        print("âœ… Jupyteræ‰©å±•é…ç½®å®Œæˆ")
    except subprocess.CalledProcessError as e:
        print(f"âŒ Jupyteræ‰©å±•é…ç½®å¤±è´¥: {e}")

# Complete installation function
# å®Œæ•´å®‰è£…å‡½æ•°
def complete_environment_setup():
    """
    Set up complete deep learning environment
    è®¾ç½®å®Œæ•´çš„æ·±åº¦å­¦ä¹ ç¯å¢ƒ
    """
    print("å¼€å§‹è®¾ç½®æ·±åº¦å­¦ä¹ ç¯å¢ƒ...")
    print("="*60)
    
    # Install core frameworks
    # å®‰è£…æ ¸å¿ƒæ¡†æ¶
    install_pytorch()
    install_tensorflow()
    
    # Install essential packages
    # å®‰è£…å¿…éœ€åŒ…
    install_essential_packages()
    
    # Install specialized packages
    # å®‰è£…ä¸“ä¸šåŒ…
    install_specialized_packages()
    
    # Setup Jupyter
    # è®¾ç½®Jupyter
    setup_jupyter_extensions()
    
    # Create requirements file
    # åˆ›å»ºrequirementsæ–‡ä»¶
    create_requirements_file()
    
    print("="*60)
    print("âœ… æ·±åº¦å­¦ä¹ ç¯å¢ƒè®¾ç½®å®Œæˆï¼")
    print("\nä¸‹ä¸€æ­¥:")
    print("1. é‡å¯ç»ˆç«¯æˆ–è¿è¡Œ: source ~/.bashrc")
    print("2. æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ: source ~/deeplearning_env/bin/activate")
    print("3. å¯åŠ¨Jupyter: jupyter lab")
    print("4. æµ‹è¯•å®‰è£…: python -c 'import torch; print(torch.cuda.is_available())'")

# Run the complete setup
# è¿è¡Œå®Œæ•´è®¾ç½®
# complete_environment_setup()
```

#### Environment Testing and Validation ç¯å¢ƒæµ‹è¯•å’ŒéªŒè¯

```python
# Comprehensive environment testing
# å…¨é¢ç¯å¢ƒæµ‹è¯•
def test_deep_learning_environment():
    """
    Test the complete deep learning environment setup
    æµ‹è¯•å®Œæ•´çš„æ·±åº¦å­¦ä¹ ç¯å¢ƒè®¾ç½®
    """
    print("æ·±åº¦å­¦ä¹ ç¯å¢ƒæµ‹è¯•")
    print("="*60)
    
    # Test core packages
    # æµ‹è¯•æ ¸å¿ƒåŒ…
    test_results = {}
    
    # Test PyTorch
    # æµ‹è¯•PyTorch
    try:
        import torch
        import torchvision
        import torchaudio
        
        test_results['PyTorch'] = {
            'installed': True,
            'version': torch.__version__,
            'cuda_available': torch.cuda.is_available(),
            'cuda_version': torch.version.cuda if torch.cuda.is_available() else 'N/A',
            'gpu_count': torch.cuda.device_count() if torch.cuda.is_available() else 0
        }
        
        # Test GPU computation
        # æµ‹è¯•GPUè®¡ç®—
        if torch.cuda.is_available():
            device = torch.device('cuda')
            x = torch.randn(100, 100, device=device)
            y = torch.mm(x, x.t())
            test_results['PyTorch']['gpu_test'] = 'Passed'
        else:
            test_results['PyTorch']['gpu_test'] = 'Skipped (No GPU)'
            
    except ImportError as e:
        test_results['PyTorch'] = {'installed': False, 'error': str(e)}
    
    # Test TensorFlow
    # æµ‹è¯•TensorFlow
    try:
        import tensorflow as tf
        
        test_results['TensorFlow'] = {
            'installed': True,
            'version': tf.__version__,
            'gpu_available': len(tf.config.list_physical_devices('GPU')) > 0,
            'gpu_count': len(tf.config.list_physical_devices('GPU'))
        }
        
        # Test GPU computation
        # æµ‹è¯•GPUè®¡ç®—
        if test_results['TensorFlow']['gpu_available']:
            with tf.device('/GPU:0'):
                a = tf.random.normal([100, 100])
                b = tf.matmul(a, a)
            test_results['TensorFlow']['gpu_test'] = 'Passed'
        else:
            test_results['TensorFlow']['gpu_test'] = 'Skipped (No GPU)'
            
    except ImportError as e:
        test_results['TensorFlow'] = {'installed': False, 'error': str(e)}
    
    # Test other essential packages
    # æµ‹è¯•å…¶ä»–å¿…éœ€åŒ…
    essential_packages = [
        'numpy', 'pandas', 'matplotlib', 'scikit-learn',
        'opencv-python', 'transformers', 'datasets'
    ]
    
    for package in essential_packages:
        try:
            module = __import__(package.replace('-', '_'))
            version = getattr(module, '__version__', 'Unknown')
            test_results[package] = {'installed': True, 'version': version}
        except ImportError:
            test_results[package] = {'installed': False}
    
    # Display results
    # æ˜¾ç¤ºç»“æœ
    print("\næµ‹è¯•ç»“æœ:")
    print("-"*60)
    
    for package, result in test_results.items():
        if result['installed']:
            status = "âœ…"
            info = f"ç‰ˆæœ¬: {result.get('version', 'Unknown')}"
            
            if 'cuda_available' in result:
                cuda_status = "âœ…" if result['cuda_available'] else "âŒ"
                info += f", CUDA: {cuda_status}"
                
            if 'gpu_test' in result:
                gpu_status = "âœ…" if result['gpu_test'] == 'Passed' else "âš ï¸"
                info += f", GPUæµ‹è¯•: {gpu_status}"
                
        else:
            status = "âŒ"
            info = f"æœªå®‰è£…: {result.get('error', 'æœªçŸ¥é”™è¯¯')}"
        
        print(f"{status} {package:<15} {info}")
    
    # Summary
    # æ€»ç»“
    installed_count = sum(1 for r in test_results.values() if r['installed'])
    total_count = len(test_results)
    
    print(f"\næ€»ç»“: {installed_count}/{total_count} åŒ…å·²æ­£ç¡®å®‰è£…")
    
    if installed_count == total_count:
        print("ğŸ‰ æ­å–œï¼æ‚¨çš„æ·±åº¦å­¦ä¹ ç¯å¢ƒå·²å®Œå…¨è®¾ç½®å¥½ï¼")
    else:
        print("âš ï¸ æŸäº›åŒ…æœªå®‰è£…ï¼Œè¯·æ£€æŸ¥å®‰è£…æ—¥å¿—")

# Run environment test
# è¿è¡Œç¯å¢ƒæµ‹è¯•
# test_deep_learning_environment()
``` 

### 20.3.4 Running the Jupyter Notebook Remotely è¿œç¨‹è¿è¡ŒJupyterç¬”è®°æœ¬

Once your EC2 instance is set up with all the necessary software, you'll want to access Jupyter notebooks remotely from your local machine. This allows you to leverage the powerful cloud computing resources while maintaining the familiar interface of Jupyter.

ä¸€æ—¦ä½ çš„EC2å®ä¾‹è®¾ç½®äº†æ‰€æœ‰å¿…è¦çš„è½¯ä»¶ï¼Œä½ å°†å¸Œæœ›ä»æœ¬åœ°æœºå™¨è¿œç¨‹è®¿é—®Jupyterç¬”è®°æœ¬ã€‚è¿™å…è®¸ä½ åˆ©ç”¨å¼ºå¤§çš„äº‘è®¡ç®—èµ„æºï¼ŒåŒæ—¶ä¿æŒç†Ÿæ‚‰çš„Jupyterç•Œé¢ã€‚

Think of this process like connecting to a powerful computer in a data center from your laptop - you get all the computational power without the hardware costs.

æŠŠè¿™ä¸ªè¿‡ç¨‹æƒ³è±¡æˆä»ä½ çš„ç¬”è®°æœ¬ç”µè„‘è¿æ¥åˆ°æ•°æ®ä¸­å¿ƒçš„å¼ºå¤§è®¡ç®—æœºâ€”â€”ä½ è·å¾—äº†æ‰€æœ‰çš„è®¡ç®—èƒ½åŠ›è€Œä¸éœ€è¦ç¡¬ä»¶æˆæœ¬ã€‚

#### Setting Up Jupyter for Remote Access è®¾ç½®Jupyterè¿›è¡Œè¿œç¨‹è®¿é—®

```bash
#!/bin/bash
# Jupyter Remote Setup Script
# Jupyterè¿œç¨‹è®¾ç½®è„šæœ¬

echo "è®¾ç½®Jupyterè¿œç¨‹è®¿é—®..."

# Install Jupyter if not already installed
# å¦‚æœå°šæœªå®‰è£…ï¼Œå®‰è£…Jupyter
pip install jupyter jupyterlab

# Generate Jupyter configuration
# ç”ŸæˆJupyteré…ç½®
jupyter notebook --generate-config

# Create password for Jupyter
# ä¸ºJupyteråˆ›å»ºå¯†ç 
python3 -c "
from jupyter_server.auth import passwd
import getpass
password = getpass.getpass('Enter password for Jupyter: ')
hashed = passwd(password)
print(f'Password hash: {hashed}')
with open('/home/ubuntu/.jupyter/jupyter_notebook_config.py', 'a') as f:
    f.write(f\"\\nc.NotebookApp.password = '{hashed}'\\n\")
    f.write(\"c.NotebookApp.ip = '0.0.0.0'\\n\")
    f.write(\"c.NotebookApp.port = 8888\\n\")
    f.write(\"c.NotebookApp.open_browser = False\\n\")
    f.write(\"c.NotebookApp.allow_remote_access = True\\n\")
"

echo "Jupyteré…ç½®å®Œæˆï¼"
```

#### Secure SSL Configuration SSLå®‰å…¨é…ç½®

```python
# SSL Certificate Setup for Jupyter
# Jupyterçš„SSLè¯ä¹¦è®¾ç½®
import subprocess
import os

def setup_ssl_certificate():
    """
    Create self-signed SSL certificate for secure Jupyter access
    ä¸ºå®‰å…¨Jupyterè®¿é—®åˆ›å»ºè‡ªç­¾åSSLè¯ä¹¦
    """
    print("åˆ›å»ºSSLè¯ä¹¦...")
    
    cert_dir = "/home/ubuntu/.jupyter"
    
    # Create certificate directory if it doesn't exist
    # å¦‚æœè¯ä¹¦ç›®å½•ä¸å­˜åœ¨åˆ™åˆ›å»º
    os.makedirs(cert_dir, exist_ok=True)
    
    # Generate SSL certificate
    # ç”ŸæˆSSLè¯ä¹¦
    ssl_commands = [
        f"openssl req -x509 -nodes -days 365 -newkey rsa:2048 "
        f"-keyout {cert_dir}/mykey.key -out {cert_dir}/mycert.pem "
        f"-subj '/C=US/ST=State/L=City/O=Organization/CN=localhost'"
    ]
    
    for cmd in ssl_commands:
        try:
            subprocess.run(cmd, shell=True, check=True)
            print("âœ… SSLè¯ä¹¦åˆ›å»ºæˆåŠŸ")
        except subprocess.CalledProcessError as e:
            print(f"âŒ SSLè¯ä¹¦åˆ›å»ºå¤±è´¥: {e}")
            return False
    
    # Update Jupyter configuration for SSL
    # æ›´æ–°Jupyteré…ç½®ä»¥ä½¿ç”¨SSL
    config_lines = [
        "c.NotebookApp.certfile = '/home/ubuntu/.jupyter/mycert.pem'",
        "c.NotebookApp.keyfile = '/home/ubuntu/.jupyter/mykey.key'",
        "c.NotebookApp.port = 8888"
    ]
    
    config_file = "/home/ubuntu/.jupyter/jupyter_notebook_config.py"
    
    with open(config_file, 'a') as f:
        f.write("\n# SSL Configuration\n")
        for line in config_lines:
            f.write(f"{line}\n")
    
    print("âœ… SSLé…ç½®å®Œæˆ")
    return True

# Run SSL setup
# è¿è¡ŒSSLè®¾ç½®
# setup_ssl_certificate()
```

#### Starting Jupyter Server JupyteræœåŠ¡å™¨å¯åŠ¨

```python
# Jupyter Server Management
# JupyteræœåŠ¡å™¨ç®¡ç†
import subprocess
import time
import signal
import os

def start_jupyter_server(port=8888, lab=True):
    """
    Start Jupyter server with proper configuration
    ä½¿ç”¨é€‚å½“é…ç½®å¯åŠ¨JupyteræœåŠ¡å™¨
    """
    print(f"åœ¨ç«¯å£ {port} å¯åŠ¨JupyteræœåŠ¡å™¨...")
    
    # Choose between Jupyter Lab and Notebook
    # åœ¨Jupyter Labå’ŒNotebookä¹‹é—´é€‰æ‹©
    command = "jupyter lab" if lab else "jupyter notebook"
    
    # Additional arguments for remote access
    # è¿œç¨‹è®¿é—®çš„é¢å¤–å‚æ•°
    args = [
        f"--port={port}",
        "--no-browser",
        "--allow-root",
        "--ip=0.0.0.0"
    ]
    
    full_command = f"{command} {' '.join(args)}"
    
    print(f"æ‰§è¡Œå‘½ä»¤: {full_command}")
    print("æœåŠ¡å™¨å¯åŠ¨ä¸­... æŒ‰ Ctrl+C åœæ­¢")
    
    try:
        # Start server in background
        # åœ¨åå°å¯åŠ¨æœåŠ¡å™¨
        process = subprocess.Popen(
            full_command,
            shell=True,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True
        )
        
        # Save process ID for later termination
        # ä¿å­˜è¿›ç¨‹IDä»¥ä¾¿ç¨åç»ˆæ­¢
        with open('/tmp/jupyter_pid.txt', 'w') as f:
            f.write(str(process.pid))
        
        print(f"JupyteræœåŠ¡å™¨å·²å¯åŠ¨ï¼Œè¿›ç¨‹ID: {process.pid}")
        print(f"è®¿é—®åœ°å€: https://YOUR_EC2_PUBLIC_IP:{port}")
        
        return process
        
    except Exception as e:
        print(f"å¯åŠ¨JupyteræœåŠ¡å™¨å¤±è´¥: {e}")
        return None

def stop_jupyter_server():
    """
    Stop running Jupyter server
    åœæ­¢è¿è¡Œçš„JupyteræœåŠ¡å™¨
    """
    try:
        with open('/tmp/jupyter_pid.txt', 'r') as f:
            pid = int(f.read().strip())
        
        os.kill(pid, signal.SIGTERM)
        print(f"âœ… JupyteræœåŠ¡å™¨å·²åœæ­¢ (PID: {pid})")
        
        # Remove PID file
        # åˆ é™¤PIDæ–‡ä»¶
        os.remove('/tmp/jupyter_pid.txt')
        
    except FileNotFoundError:
        print("âŒ æœªæ‰¾åˆ°è¿è¡Œä¸­çš„JupyteræœåŠ¡å™¨")
    except Exception as e:
        print(f"âŒ åœæ­¢æœåŠ¡å™¨å¤±è´¥: {e}")

# Automated startup script
# è‡ªåŠ¨å¯åŠ¨è„šæœ¬
def create_startup_script():
    """
    Create a startup script for Jupyter
    ä¸ºJupyteråˆ›å»ºå¯åŠ¨è„šæœ¬
    """
    startup_script = """#!/bin/bash
# Jupyter Auto-start Script
# Jupyterè‡ªåŠ¨å¯åŠ¨è„šæœ¬

# Activate virtual environment
# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source ~/deeplearning_env/bin/activate

# Start Jupyter Lab
# å¯åŠ¨Jupyter Lab
jupyter lab --port=8888 --no-browser --allow-root --ip=0.0.0.0 &

echo "Jupyter Lab started on port 8888"
echo "Access at: https://$(curl -s http://169.254.169.254/latest/meta-data/public-ipv4):8888"
"""
    
    with open('/home/ubuntu/start_jupyter.sh', 'w') as f:
        f.write(startup_script)
    
    # Make script executable
    # ä½¿è„šæœ¬å¯æ‰§è¡Œ
    os.chmod('/home/ubuntu/start_jupyter.sh', 0o755)
    
    print("âœ… Jupyterå¯åŠ¨è„šæœ¬å·²åˆ›å»º: /home/ubuntu/start_jupyter.sh")
    print("ä½¿ç”¨æ–¹æ³•: ./start_jupyter.sh")
```

#### SSH Tunneling for Secure Access å®‰å…¨è®¿é—®çš„SSHéš§é“

```python
# SSH Tunneling Setup
# SSHéš§é“è®¾ç½®
def create_ssh_tunnel_guide(ec2_public_ip, key_file, local_port=8888, remote_port=8888):
    """
    Generate SSH tunneling instructions for secure Jupyter access
    ç”Ÿæˆç”¨äºå®‰å…¨Jupyterè®¿é—®çš„SSHéš§é“è¯´æ˜
    """
    print("SSHéš§é“è®¾ç½®æŒ‡å—")
    print("="*60)
    
    # SSH tunnel command
    # SSHéš§é“å‘½ä»¤
    tunnel_command = (
        f"ssh -i {key_file} -L {local_port}:localhost:{remote_port} "
        f"ubuntu@{ec2_public_ip}"
    )
    
    print("æ–¹æ³•1: SSHéš§é“ (æ¨èï¼Œæ›´å®‰å…¨)")
    print("-"*40)
    print("1. åœ¨æœ¬åœ°ç»ˆç«¯è¿è¡Œä»¥ä¸‹å‘½ä»¤:")
    print(f"   {tunnel_command}")
    print("\n2. ä¿æŒSSHè¿æ¥æ‰“å¼€")
    print("\n3. åœ¨æœ¬åœ°æµè§ˆå™¨è®¿é—®:")
    print(f"   http://localhost:{local_port}")
    print("\n4. è¾“å…¥ä¹‹å‰è®¾ç½®çš„Jupyterå¯†ç ")
    
    print("\næ–¹æ³•2: ç›´æ¥è®¿é—® (éœ€è¦é˜²ç«å¢™é…ç½®)")
    print("-"*40)
    print("1. ç¡®ä¿EC2å®‰å…¨ç»„å…è®¸ç«¯å£8888")
    print("2. åœ¨æµè§ˆå™¨ç›´æ¥è®¿é—®:")
    print(f"   https://{ec2_public_ip}:{remote_port}")
    print("3. æ¥å—è‡ªç­¾åè¯ä¹¦è­¦å‘Š")
    print("4. è¾“å…¥Jupyterå¯†ç ")
    
    # PowerShell version for Windows users
    # Windowsç”¨æˆ·çš„PowerShellç‰ˆæœ¬
    print("\nWindowsç”¨æˆ· (PowerShell):")
    print("-"*40)
    powershell_command = (
        f'ssh -i "{key_file}" -L {local_port}:localhost:{remote_port} '
        f'ubuntu@{ec2_public_ip}'
    )
    print(f"   {powershell_command}")
    
    print("\nå®‰å…¨æç¤º:")
    print("- ä½¿ç”¨SSHéš§é“æ¯”ç›´æ¥æš´éœ²ç«¯å£æ›´å®‰å…¨")
    print("- å®šæœŸæ›´æ”¹Jupyterå¯†ç ")
    print("- ä¸ä½¿ç”¨æ—¶åœæ­¢JupyteræœåŠ¡å™¨")
    print("="*60)

# Example usage
# ä½¿ç”¨ç¤ºä¾‹
# create_ssh_tunnel_guide("54.123.45.67", "my-key.pem")
```

#### Monitoring and Logging ç›‘æ§å’Œæ—¥å¿—

```python
# Jupyter Server Monitoring
# JupyteræœåŠ¡å™¨ç›‘æ§
import psutil
import datetime

def monitor_jupyter_usage():
    """
    Monitor Jupyter server resource usage
    ç›‘æ§JupyteræœåŠ¡å™¨èµ„æºä½¿ç”¨æƒ…å†µ
    """
    print("JupyteræœåŠ¡å™¨ç›‘æ§")
    print("="*50)
    
    # Check if Jupyter is running
    # æ£€æŸ¥Jupyteræ˜¯å¦è¿è¡Œä¸­
    jupyter_processes = []
    for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
        try:
            if 'jupyter' in proc.info['name'].lower():
                jupyter_processes.append(proc)
        except (psutil.NoSuchProcess, psutil.AccessDenied):
            continue
    
    if not jupyter_processes:
        print("âŒ æœªæ£€æµ‹åˆ°è¿è¡Œä¸­çš„JupyteræœåŠ¡å™¨")
        return
    
    print(f"âœ… æ£€æµ‹åˆ° {len(jupyter_processes)} ä¸ªJupyterè¿›ç¨‹")
    
    # Display process information
    # æ˜¾ç¤ºè¿›ç¨‹ä¿¡æ¯
    for proc in jupyter_processes:
        try:
            info = proc.info
            process = psutil.Process(info['pid'])
            
            print(f"\nè¿›ç¨‹ID: {info['pid']}")
            print(f"å‘½ä»¤: {' '.join(info['cmdline'][:3])}...")
            print(f"CPUä½¿ç”¨ç‡: {process.cpu_percent():.1f}%")
            print(f"å†…å­˜ä½¿ç”¨: {process.memory_info().rss / 1024**2:.1f} MB")
            print(f"è¿è¡Œæ—¶é—´: {datetime.datetime.now() - datetime.datetime.fromtimestamp(process.create_time())}")
            
        except (psutil.NoSuchProcess, psutil.AccessDenied):
            continue
    
    # System resources
    # ç³»ç»Ÿèµ„æº
    print(f"\nç³»ç»Ÿèµ„æº:")
    print(f"æ€»CPUä½¿ç”¨ç‡: {psutil.cpu_percent():.1f}%")
    memory = psutil.virtual_memory()
    print(f"å†…å­˜ä½¿ç”¨ç‡: {memory.percent:.1f}% ({memory.used/1024**3:.1f}GB / {memory.total/1024**3:.1f}GB)")
    
    # Check for GPU usage if available
    # å¦‚æœå¯ç”¨ï¼Œæ£€æŸ¥GPUä½¿ç”¨æƒ…å†µ
    try:
        import GPUtil
        gpus = GPUtil.getGPUs()
        if gpus:
            print(f"\nGPUä½¿ç”¨æƒ…å†µ:")
            for i, gpu in enumerate(gpus):
                print(f"GPU {i}: {gpu.load*100:.1f}% ä½¿ç”¨ç‡, {gpu.memoryUtil*100:.1f}% æ˜¾å­˜")
    except ImportError:
        pass

def setup_jupyter_logging():
    """
    Setup logging for Jupyter server
    ä¸ºJupyteræœåŠ¡å™¨è®¾ç½®æ—¥å¿—
    """
    log_dir = "/home/ubuntu/jupyter_logs"
    os.makedirs(log_dir, exist_ok=True)
    
    # Create logging configuration
    # åˆ›å»ºæ—¥å¿—é…ç½®
    log_config = f"""
# Jupyter Logging Configuration
# Jupyteræ—¥å¿—é…ç½®
c.Application.log_level = 'INFO'
c.Application.log_format = '%(asctime)s [%(name)s]%(highlevel)s %(message)s'
c.Application.log_datefmt = '%Y-%m-%d %H:%M:%S'

# Log to file
# è®°å½•åˆ°æ–‡ä»¶
import logging
logging.basicConfig(
    filename='{log_dir}/jupyter.log',
    filemode='a',
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
"""
    
    config_file = "/home/ubuntu/.jupyter/jupyter_notebook_config.py"
    
    with open(config_file, 'a') as f:
        f.write("\n# Logging Configuration\n")
        f.write(log_config)
    
    print(f"âœ… æ—¥å¿—é…ç½®å®Œæˆï¼Œæ—¥å¿—æ–‡ä»¶: {log_dir}/jupyter.log")

# Run monitoring
# è¿è¡Œç›‘æ§
# monitor_jupyter_usage()
# setup_jupyter_logging()
```

### 20.3.5 Closing Unused Instances å…³é—­æœªä½¿ç”¨çš„å®ä¾‹

Properly managing your EC2 instances is crucial for cost control. Unlike traditional computers that you might leave running, cloud instances charge by the hour, so it's essential to shut down instances when they're not in use.

æ­£ç¡®ç®¡ç†EC2å®ä¾‹å¯¹äºæˆæœ¬æ§åˆ¶è‡³å…³é‡è¦ã€‚ä¸ä½ å¯èƒ½è®©å…¶æŒç»­è¿è¡Œçš„ä¼ ç»Ÿè®¡ç®—æœºä¸åŒï¼Œäº‘å®ä¾‹æŒ‰å°æ—¶æ”¶è´¹ï¼Œå› æ­¤åœ¨ä¸ä½¿ç”¨æ—¶å…³é—­å®ä¾‹æ˜¯å¿…è¦çš„ã€‚

Think of it like turning off lights when you leave a room - except these "lights" can cost hundreds of dollars if left on accidentally.

æŠŠå®ƒæƒ³è±¡æˆç¦»å¼€æˆ¿é—´æ—¶å…³ç¯â€”â€”é™¤äº†è¿™äº›"ç¯"å¦‚æœæ„å¤–ç•™ç€å¼€å¯å¯èƒ½èŠ±è´¹æ•°ç™¾ç¾å…ƒã€‚

#### Instance Lifecycle Management å®ä¾‹ç”Ÿå‘½å‘¨æœŸç®¡ç†

```python
import boto3
import datetime
import time

def list_running_instances():
    """
    List all running EC2 instances with their details
    åˆ—å‡ºæ‰€æœ‰è¿è¡Œä¸­çš„EC2å®ä¾‹åŠå…¶è¯¦ç»†ä¿¡æ¯
    """
    ec2 = boto3.client('ec2')
    
    try:
        response = ec2.describe_instances(
            Filters=[
                {'Name': 'instance-state-name', 'Values': ['running']}
            ]
        )
        
        instances = []
        for reservation in response['Reservations']:
            for instance in reservation['Instances']:
                # Calculate running time
                # è®¡ç®—è¿è¡Œæ—¶é—´
                launch_time = instance['LaunchTime']
                running_time = datetime.datetime.now(launch_time.tzinfo) - launch_time
                
                # Get instance name from tags
                # ä»æ ‡ç­¾è·å–å®ä¾‹åç§°
                name = 'Unknown'
                for tag in instance.get('Tags', []):
                    if tag['Key'] == 'Name':
                        name = tag['Value']
                        break
                
                # Calculate estimated cost
                # è®¡ç®—ä¼°ç®—æˆæœ¬
                instance_type = instance['InstanceType']
                estimated_cost = calculate_instance_cost(instance_type, running_time)
                
                instances.append({
                    'InstanceId': instance['InstanceId'],
                    'Name': name,
                    'InstanceType': instance_type,
                    'State': instance['State']['Name'],
                    'LaunchTime': launch_time,
                    'RunningTime': running_time,
                    'PublicIpAddress': instance.get('PublicIpAddress', 'N/A'),
                    'EstimatedCost': estimated_cost
                })
        
        # Display instances
        # æ˜¾ç¤ºå®ä¾‹
        if instances:
            print("è¿è¡Œä¸­çš„EC2å®ä¾‹:")
            print("="*100)
            print(f"{'å®ä¾‹ID':<20} {'åç§°':<15} {'ç±»å‹':<12} {'è¿è¡Œæ—¶é—´':<15} {'å…¬ç½‘IP':<15} {'ä¼°ç®—æˆæœ¬($)'}")
            print("-"*100)
            
            total_cost = 0
            for instance in instances:
                running_hours = instance['RunningTime'].total_seconds() / 3600
                print(f"{instance['InstanceId']:<20} {instance['Name']:<15} "
                      f"{instance['InstanceType']:<12} {running_hours:.1f}h{'':>8} "
                      f"{instance['PublicIpAddress']:<15} {instance['EstimatedCost']:.2f}")
                total_cost += instance['EstimatedCost']
            
            print("-"*100)
            print(f"æ€»ä¼°ç®—æˆæœ¬: ${total_cost:.2f}")
        else:
            print("âœ… æ²¡æœ‰è¿è¡Œä¸­çš„å®ä¾‹")
        
        return instances
        
    except Exception as e:
        print(f"âŒ è·å–å®ä¾‹åˆ—è¡¨å¤±è´¥: {e}")
        return []

def calculate_instance_cost(instance_type, running_time):
    """
    Calculate estimated cost for instance
    è®¡ç®—å®ä¾‹çš„ä¼°ç®—æˆæœ¬
    """
    # Instance pricing per hour (approximate)
    # å®ä¾‹æ¯å°æ—¶å®šä»·ï¼ˆå¤§æ¦‚ï¼‰
    pricing = {
        't3.medium': 0.0416,
        't3.large': 0.0832,
        'm5.large': 0.096,
        'm5.xlarge': 0.192,
        'g4dn.xlarge': 0.526,
        'p3.2xlarge': 3.06,
        'p3.8xlarge': 12.24,
        'p4d.24xlarge': 32.77
    }
    
    hourly_rate = pricing.get(instance_type, 0.1)  # Default rate
    hours = running_time.total_seconds() / 3600
    return hourly_rate * hours

def stop_instance(instance_id, force=False):
    """
    Stop an EC2 instance safely
    å®‰å…¨åœæ­¢EC2å®ä¾‹
    """
    ec2 = boto3.client('ec2')
    
    try:
        # Check instance state first
        # é¦–å…ˆæ£€æŸ¥å®ä¾‹çŠ¶æ€
        response = ec2.describe_instances(InstanceIds=[instance_id])
        instance = response['Reservations'][0]['Instances'][0]
        current_state = instance['State']['Name']
        
        if current_state == 'stopped':
            print(f"å®ä¾‹ {instance_id} å·²ç»åœæ­¢")
            return True
        elif current_state == 'stopping':
            print(f"å®ä¾‹ {instance_id} æ­£åœ¨åœæ­¢ä¸­")
            return True
        elif current_state != 'running':
            print(f"å®ä¾‹ {instance_id} çŠ¶æ€ä¸º {current_state}ï¼Œæ— æ³•åœæ­¢")
            return False
        
        # Confirm before stopping (unless forced)
        # åœæ­¢å‰ç¡®è®¤ï¼ˆé™¤éå¼ºåˆ¶ï¼‰
        if not force:
            instance_name = 'Unknown'
            for tag in instance.get('Tags', []):
                if tag['Key'] == 'Name':
                    instance_name = tag['Value']
                    break
            
            confirm = input(f"ç¡®è®¤åœæ­¢å®ä¾‹ '{instance_name}' ({instance_id})? [y/N]: ")
            if confirm.lower() != 'y':
                print("æ“ä½œå·²å–æ¶ˆ")
                return False
        
        # Stop the instance
        # åœæ­¢å®ä¾‹
        print(f"æ­£åœ¨åœæ­¢å®ä¾‹ {instance_id}...")
        ec2.stop_instances(InstanceIds=[instance_id])
        
        # Wait for instance to stop
        # ç­‰å¾…å®ä¾‹åœæ­¢
        waiter = ec2.get_waiter('instance_stopped')
        waiter.wait(InstanceIds=[instance_id])
        
        print(f"âœ… å®ä¾‹ {instance_id} å·²æˆåŠŸåœæ­¢")
        return True
        
    except Exception as e:
        print(f"âŒ åœæ­¢å®ä¾‹å¤±è´¥: {e}")
        return False

def terminate_instance(instance_id, force=False):
    """
    Terminate an EC2 instance (permanent deletion)
    ç»ˆæ­¢EC2å®ä¾‹ï¼ˆæ°¸ä¹…åˆ é™¤ï¼‰
    """
    ec2 = boto3.client('ec2')
    
    print("âš ï¸ è­¦å‘Š: ç»ˆæ­¢å®ä¾‹å°†æ°¸ä¹…åˆ é™¤æ‰€æœ‰æ•°æ®!")
    print("å¦‚æœåªæ˜¯æƒ³æš‚æ—¶åœæ­¢å®ä¾‹ï¼Œè¯·ä½¿ç”¨stop_instance()å‡½æ•°")
    
    if not force:
        confirm1 = input("ç¡®è®¤è¦ç»ˆæ­¢å®ä¾‹å—? è¾“å…¥ 'TERMINATE' ç¡®è®¤: ")
        if confirm1 != 'TERMINATE':
            print("æ“ä½œå·²å–æ¶ˆ")
            return False
        
        confirm2 = input("æœ€åç¡®è®¤ï¼Œæ­¤æ“ä½œä¸å¯æ’¤é”€! è¾“å…¥ 'YES' ç»§ç»­: ")
        if confirm2 != 'YES':
            print("æ“ä½œå·²å–æ¶ˆ")
            return False
    
    try:
        print(f"æ­£åœ¨ç»ˆæ­¢å®ä¾‹ {instance_id}...")
        ec2.terminate_instances(InstanceIds=[instance_id])
        
        print(f"âœ… å®ä¾‹ {instance_id} ç»ˆæ­¢å‘½ä»¤å·²å‘é€")
        print("å®ä¾‹å°†åœ¨å‡ åˆ†é’Ÿå†…è¢«æ°¸ä¹…åˆ é™¤")
        return True
        
    except Exception as e:
        print(f"âŒ ç»ˆæ­¢å®ä¾‹å¤±è´¥: {e}")
        return False
```

#### Automated Cost Monitoring è‡ªåŠ¨æˆæœ¬ç›‘æ§

```python
# Cost monitoring and alerts
# æˆæœ¬ç›‘æ§å’Œè­¦æŠ¥
import json
from datetime import datetime, timedelta

def setup_cost_alerts():
    """
    Set up cost monitoring and alerts
    è®¾ç½®æˆæœ¬ç›‘æ§å’Œè­¦æŠ¥
    """
    print("è®¾ç½®æˆæœ¬ç›‘æ§...")
    
    # Create cost monitoring configuration
    # åˆ›å»ºæˆæœ¬ç›‘æ§é…ç½®
    config = {
        "daily_budget": 50.0,  # Daily budget in USD
        "weekly_budget": 300.0,  # Weekly budget in USD
        "monthly_budget": 1000.0,  # Monthly budget in USD
        "alert_thresholds": [0.5, 0.8, 0.9],  # Alert at 50%, 80%, 90% of budget
        "notification_email": "your-email@example.com"
    }
    
    # Save configuration
    # ä¿å­˜é…ç½®
    with open('/home/ubuntu/cost_config.json', 'w') as f:
        json.dump(config, f, indent=2)
    
    print("âœ… æˆæœ¬ç›‘æ§é…ç½®å·²ä¿å­˜")
    return config

def check_current_costs():
    """
    Check current month's AWS costs
    æ£€æŸ¥å½“æœˆAWSæˆæœ¬
    """
    import boto3
    from datetime import datetime, timedelta
    
    # Cost Explorer client
    # æˆæœ¬æµè§ˆå™¨å®¢æˆ·ç«¯
    ce = boto3.client('ce')
    
    # Get current month's costs
    # è·å–å½“æœˆæˆæœ¬
    end_date = datetime.now().strftime('%Y-%m-%d')
    start_date = datetime.now().replace(day=1).strftime('%Y-%m-%d')
    
    try:
        response = ce.get_cost_and_usage(
            TimePeriod={
                'Start': start_date,
                'End': end_date
            },
            Granularity='MONTHLY',
            Metrics=['BlendedCost'],
            GroupBy=[
                {
                    'Type': 'DIMENSION',
                    'Key': 'SERVICE'
                }
            ]
        )
        
        print(f"AWSæˆæœ¬æŠ¥å‘Š ({start_date} åˆ° {end_date})")
        print("="*60)
        
        total_cost = 0
        for result in response['ResultsByTime']:
            for group in result['Groups']:
                service = group['Keys'][0]
                cost = float(group['Metrics']['BlendedCost']['Amount'])
                if cost > 0.01:  # Only show costs > $0.01
                    print(f"{service:<30} ${cost:.2f}")
                    total_cost += cost
        
        print("-"*60)
        print(f"{'æ€»è®¡':<30} ${total_cost:.2f}")
        
        # Check against budget
        # å¯¹ç…§é¢„ç®—æ£€æŸ¥
        try:
            with open('/home/ubuntu/cost_config.json', 'r') as f:
                config = json.load(f)
            
            monthly_budget = config['monthly_budget']
            usage_percentage = (total_cost / monthly_budget) * 100
            
            print(f"\né¢„ç®—ä½¿ç”¨æƒ…å†µ:")
            print(f"æœˆåº¦é¢„ç®—: ${monthly_budget:.2f}")
            print(f"å·²ä½¿ç”¨: {usage_percentage:.1f}%")
            
            if usage_percentage > 90:
                print("ğŸš¨ è­¦å‘Š: å·²è¶…è¿‡é¢„ç®—çš„90%!")
            elif usage_percentage > 80:
                print("âš ï¸ æ³¨æ„: å·²è¶…è¿‡é¢„ç®—çš„80%")
            elif usage_percentage > 50:
                print("ğŸ“Š ä¿¡æ¯: å·²ä½¿ç”¨é¢„ç®—çš„ä¸€åŠä»¥ä¸Š")
            
        except FileNotFoundError:
            print("ğŸ’¡ æç¤º: è¿è¡Œ setup_cost_alerts() è®¾ç½®é¢„ç®—ç›‘æ§")
        
        return total_cost
        
    except Exception as e:
        print(f"âŒ è·å–æˆæœ¬ä¿¡æ¯å¤±è´¥: {e}")
        return None

def create_shutdown_scheduler():
    """
    Create automated shutdown scheduler
    åˆ›å»ºè‡ªåŠ¨å…³æœºè°ƒåº¦å™¨
    """
    shutdown_script = """#!/bin/bash
# Automated EC2 Instance Shutdown Script
# è‡ªåŠ¨EC2å®ä¾‹å…³æœºè„šæœ¬

# Check if instance has been idle for more than 2 hours
# æ£€æŸ¥å®ä¾‹æ˜¯å¦å·²ç©ºé—²è¶…è¿‡2å°æ—¶

IDLE_THRESHOLD=7200  # 2 hours in seconds
LAST_ACTIVITY=$(who -u | awk '{print $6}' | head -1)

if [ -z "$LAST_ACTIVITY" ]; then
    echo "No user activity detected"
    IDLE_TIME=999999
else
    CURRENT_TIME=$(date +%s)
    LAST_TIME=$(date -d "$LAST_ACTIVITY" +%s 2>/dev/null || echo $CURRENT_TIME)
    IDLE_TIME=$((CURRENT_TIME - LAST_TIME))
fi

echo "Idle time: $IDLE_TIME seconds"

if [ $IDLE_TIME -gt $IDLE_THRESHOLD ]; then
    echo "Instance has been idle for more than 2 hours"
    echo "Shutting down to save costs..."
    
    # Send notification (optional)
    # å‘é€é€šçŸ¥ï¼ˆå¯é€‰ï¼‰
    wall "Instance will shutdown in 5 minutes due to inactivity"
    
    # Wait 5 minutes then shutdown
    # ç­‰å¾…5åˆ†é’Ÿç„¶åå…³æœº
    sleep 300
    sudo shutdown -h now
else
    echo "Instance is still active"
fi
"""
    
    # Save shutdown script
    # ä¿å­˜å…³æœºè„šæœ¬
    with open('/home/ubuntu/auto_shutdown.sh', 'w') as f:
        f.write(shutdown_script)
    
    # Make executable
    # ä½¿å…¶å¯æ‰§è¡Œ
    os.chmod('/home/ubuntu/auto_shutdown.sh', 0o755)
    
    # Add to crontab (check every hour)
    # æ·»åŠ åˆ°crontabï¼ˆæ¯å°æ—¶æ£€æŸ¥ä¸€æ¬¡ï¼‰
    cron_entry = "0 * * * * /home/ubuntu/auto_shutdown.sh >> /home/ubuntu/shutdown.log 2>&1"
    
    print("è‡ªåŠ¨å…³æœºè„šæœ¬å·²åˆ›å»º:")
    print("æ–‡ä»¶ä½ç½®: /home/ubuntu/auto_shutdown.sh")
    print("\nè¦å¯ç”¨è‡ªåŠ¨å…³æœºï¼Œè¿è¡Œ:")
    print("crontab -e")
    print("ç„¶åæ·»åŠ ä»¥ä¸‹è¡Œ:")
    print(cron_entry)
    print("\nè¿™å°†æ¯å°æ—¶æ£€æŸ¥ä¸€æ¬¡ï¼Œå¦‚æœç©ºé—²è¶…è¿‡2å°æ—¶åˆ™è‡ªåŠ¨å…³æœº")

# Example usage
# ä½¿ç”¨ç¤ºä¾‹
# instances = list_running_instances()
# setup_cost_alerts()
# check_current_costs()
# create_shutdown_scheduler()
```

#### Best Practices for Instance Management å®ä¾‹ç®¡ç†æœ€ä½³å®è·µ

```python
# Instance management best practices
# å®ä¾‹ç®¡ç†æœ€ä½³å®è·µ

def instance_management_checklist():
    """
    Display best practices checklist for EC2 instance management
    æ˜¾ç¤ºEC2å®ä¾‹ç®¡ç†æœ€ä½³å®è·µæ£€æŸ¥åˆ—è¡¨
    """
    checklist = [
        {
            "category": "æˆæœ¬æ§åˆ¶ Cost Control",
            "items": [
                "Stop instances when not in use ä¸ä½¿ç”¨æ—¶åœæ­¢å®ä¾‹",
                "Use appropriate instance types ä½¿ç”¨é€‚å½“çš„å®ä¾‹ç±»å‹",
                "Monitor costs regularly å®šæœŸç›‘æ§æˆæœ¬",
                "Set up billing alerts è®¾ç½®è´¦å•è­¦æŠ¥",
                "Consider Spot instances for training è€ƒè™‘ä½¿ç”¨Spotå®ä¾‹è¿›è¡Œè®­ç»ƒ"
            ]
        },
        {
            "category": "å®‰å…¨ Security",
            "items": [
                "Use key pairs for SSH access ä½¿ç”¨å¯†é’¥å¯¹è¿›è¡ŒSSHè®¿é—®",
                "Configure security groups properly æ­£ç¡®é…ç½®å®‰å…¨ç»„",
                "Keep software updated ä¿æŒè½¯ä»¶æ›´æ–°",
                "Use IAM roles and policies ä½¿ç”¨IAMè§’è‰²å’Œç­–ç•¥",
                "Enable CloudTrail logging å¯ç”¨CloudTrailæ—¥å¿—"
            ]
        },
        {
            "category": "æ•°æ®ç®¡ç† Data Management",
            "items": [
                "Regular backups to S3 å®šæœŸå¤‡ä»½åˆ°S3",
                "Use EBS snapshots ä½¿ç”¨EBSå¿«ç…§",
                "Encrypt sensitive data åŠ å¯†æ•æ„Ÿæ•°æ®",
                "Organize data with proper folder structure ç”¨é€‚å½“çš„æ–‡ä»¶å¤¹ç»“æ„ç»„ç»‡æ•°æ®",
                "Version control your code å¯¹ä»£ç è¿›è¡Œç‰ˆæœ¬æ§åˆ¶"
            ]
        },
        {
            "category": "æ€§èƒ½ä¼˜åŒ– Performance Optimization",
            "items": [
                "Choose GPU instances for training é€‰æ‹©GPUå®ä¾‹è¿›è¡Œè®­ç»ƒ",
                "Use appropriate storage types ä½¿ç”¨é€‚å½“çš„å­˜å‚¨ç±»å‹",
                "Monitor resource utilization ç›‘æ§èµ„æºåˆ©ç”¨ç‡",
                "Optimize batch sizes ä¼˜åŒ–æ‰¹æ¬¡å¤§å°",
                "Use distributed training when appropriate é€‚å½“æ—¶ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒ"
            ]
        }
    ]
    
    print("EC2å®ä¾‹ç®¡ç†æœ€ä½³å®è·µæ£€æŸ¥åˆ—è¡¨")
    print("="*80)
    
    for category in checklist:
        print(f"\n{category['category']}")
        print("-" * len(category['category']))
        for i, item in enumerate(category['items'], 1):
            print(f"{i}. {item}")
    
    print("\n" + "="*80)
    print("ğŸ’¡ æç¤º: å®šæœŸå›é¡¾æ­¤æ£€æŸ¥åˆ—è¡¨ä»¥ç¡®ä¿æœ€ä½³å®è·µ")

def create_management_scripts():
    """
    Create helpful management scripts
    åˆ›å»ºæœ‰ç”¨çš„ç®¡ç†è„šæœ¬
    """
    # Quick status script
    # å¿«é€ŸçŠ¶æ€è„šæœ¬
    status_script = """#!/bin/bash
# Quick EC2 Status Check
# å¿«é€ŸEC2çŠ¶æ€æ£€æŸ¥

echo "=== EC2å®ä¾‹çŠ¶æ€ ==="
aws ec2 describe-instances --query 'Reservations[*].Instances[*].[InstanceId,State.Name,InstanceType,Tags[?Key==\`Name\`].Value|[0]]' --output table

echo -e "\n=== å½“å‰æˆæœ¬ ==="
python3 -c "
import boto3
from datetime import datetime
ce = boto3.client('ce')
end = datetime.now().strftime('%Y-%m-%d')
start = datetime.now().replace(day=1).strftime('%Y-%m-%d')
try:
    response = ce.get_cost_and_usage(
        TimePeriod={'Start': start, 'End': end},
        Granularity='MONTHLY',
        Metrics=['BlendedCost']
    )
    cost = float(response['ResultsByTime'][0]['Total']['BlendedCost']['Amount'])
    print(f'æœ¬æœˆèŠ±è´¹: \${cost:.2f}')
except Exception as e:
    print('æ— æ³•è·å–æˆæœ¬ä¿¡æ¯')
"

echo -e "\n=== ç³»ç»Ÿèµ„æº ==="
free -h
df -h /
nvidia-smi 2>/dev/null || echo "æ— GPUæˆ–nvidia-smiä¸å¯ç”¨"
"""
    
    # Emergency stop script
    # ç´§æ€¥åœæ­¢è„šæœ¬
    emergency_stop = """#!/bin/bash
# Emergency Stop All Instances
# ç´§æ€¥åœæ­¢æ‰€æœ‰å®ä¾‹

echo "ğŸš¨ ç´§æ€¥åœæ­¢æ‰€æœ‰è¿è¡Œä¸­çš„å®ä¾‹"
read -p "ç¡®è®¤åœæ­¢æ‰€æœ‰å®ä¾‹? (è¾“å…¥ YES): " confirm

if [ "$confirm" = "YES" ]; then
    aws ec2 describe-instances --filters "Name=instance-state-name,Values=running" --query 'Reservations[*].Instances[*].InstanceId' --output text | xargs -n1 aws ec2 stop-instances --instance-ids
    echo "âœ… åœæ­¢å‘½ä»¤å·²å‘é€ç»™æ‰€æœ‰è¿è¡Œä¸­çš„å®ä¾‹"
else
    echo "æ“ä½œå·²å–æ¶ˆ"
fi
"""
    
    # Save scripts
    # ä¿å­˜è„šæœ¬
    scripts = {
        'ec2_status.sh': status_script,
        'emergency_stop.sh': emergency_stop
    }
    
    for filename, content in scripts.items():
        with open(f'/home/ubuntu/{filename}', 'w') as f:
            f.write(content)
        os.chmod(f'/home/ubuntu/{filename}', 0o755)
        print(f"âœ… åˆ›å»ºè„šæœ¬: /home/ubuntu/{filename}")

# Run management setup
# è¿è¡Œç®¡ç†è®¾ç½®
# instance_management_checklist()
# create_management_scripts()
```

### 20.3.6 Summary æ€»ç»“

Using AWS EC2 provides maximum flexibility and control for deep learning practitioners. It's like building your own custom deep learning rig in the cloud, giving you the power to choose everything from the operating system to the specific driver versions.

ä½¿ç”¨AWS EC2ä¸ºæ·±åº¦å­¦ä¹ ä»ä¸šè€…æä¾›äº†æœ€å¤§çš„çµæ´»æ€§å’Œæ§åˆ¶åŠ›ã€‚è¿™å°±åƒåœ¨äº‘ä¸­æ„å»ºè‡ªå·±çš„å®šåˆ¶æ·±åº¦å­¦ä¹ è®¾å¤‡ï¼Œè®©ä½ æœ‰æƒé€‰æ‹©ä»æ“ä½œç³»ç»Ÿåˆ°ç‰¹å®šé©±åŠ¨ç¨‹åºç‰ˆæœ¬çš„æ‰€æœ‰å†…å®¹ã€‚

**Advantages ä¼˜åŠ¿:**
- **Complete Control**: You have root access to the instance, allowing for custom software installations, kernel modifications, and specific environment setups.
- **å®Œå…¨æ§åˆ¶**: ä½ æ‹¥æœ‰å®ä¾‹çš„rootè®¿é—®æƒé™ï¼Œå…è®¸è‡ªå®šä¹‰è½¯ä»¶å®‰è£…ã€å†…æ ¸ä¿®æ”¹å’Œç‰¹å®šçš„ç¯å¢ƒè®¾ç½®ã€‚
- **Cost-Effective for Long-Running Jobs**: For workloads that run for extended periods, EC2, especially with Reserved Instances or Spot Instances, can be more cost-effective than managed services.
- **å¯¹é•¿æœŸè¿è¡Œçš„ä½œä¸šå…·æœ‰æˆæœ¬æ•ˆç›Š**: å¯¹äºé•¿æ—¶é—´è¿è¡Œçš„å·¥ä½œè´Ÿè½½ï¼ŒEC2ï¼Œç‰¹åˆ«æ˜¯ä½¿ç”¨é¢„ç•™å®ä¾‹æˆ–Spotå®ä¾‹ï¼Œå¯èƒ½æ¯”æ‰˜ç®¡æœåŠ¡æ›´å…·æˆæœ¬æ•ˆç›Šã€‚
- **Wide Range of Instance Types**: EC2 offers a vast selection of instance types, including the latest GPUs, high-memory, and CPU-optimized options, catering to diverse needs.
- **å¹¿æ³›çš„å®ä¾‹ç±»å‹**: EC2æä¾›å¤§é‡çš„å®ä¾‹ç±»å‹é€‰æ‹©ï¼ŒåŒ…æ‹¬æœ€æ–°çš„GPUã€é«˜å†…å­˜å’ŒCPUä¼˜åŒ–é€‰é¡¹ï¼Œæ»¡è¶³å¤šæ ·åŒ–çš„éœ€æ±‚ã€‚
- **Flexibility**: It's suitable for both training and inference and can be integrated into any custom MLOps pipeline.
- **çµæ´»æ€§**: å®ƒæ—¢é€‚ç”¨äºè®­ç»ƒä¹Ÿé€‚ç”¨äºæ¨ç†ï¼Œå¹¶ä¸”å¯ä»¥é›†æˆåˆ°ä»»ä½•è‡ªå®šä¹‰çš„MLOpsç®¡é“ä¸­ã€‚

**Disadvantages åŠ£åŠ¿:**
- **Management Overhead**: You are responsible for all setup, maintenance, security patching, and troubleshooting.
- **ç®¡ç†å¼€é”€**: ä½ éœ€è¦è´Ÿè´£æ‰€æœ‰çš„è®¾ç½®ã€ç»´æŠ¤ã€å®‰å…¨è¡¥ä¸å’Œæ•…éšœæ’é™¤ã€‚
- **Steeper Learning Curve**: It requires more knowledge of system administration, networking, and security compared to managed services like SageMaker or Colab.
- **æ›´é™¡å³­çš„å­¦ä¹ æ›²çº¿**: ä¸SageMakeræˆ–Colabç­‰æ‰˜ç®¡æœåŠ¡ç›¸æ¯”ï¼Œå®ƒéœ€è¦æ›´å¤šå…³äºç³»ç»Ÿç®¡ç†ã€ç½‘ç»œå’Œå®‰å…¨çš„çŸ¥è¯†ã€‚
- **Cost Risk**: Forgetting to stop an instance can lead to significant, unexpected charges.
- **æˆæœ¬é£é™©**: å¿˜è®°åœæ­¢å®ä¾‹å¯èƒ½å¯¼è‡´é‡å¤§çš„ã€æ„å¤–çš„è´¹ç”¨ã€‚

EC2 is the ideal choice for experienced users who require custom environments, need to run long-term training jobs, or want to build highly customized deep learning pipelines from the ground up.

å¯¹äºéœ€è¦è‡ªå®šä¹‰ç¯å¢ƒã€è¿è¡Œé•¿æœŸè®­ç»ƒä½œä¸šæˆ–å¸Œæœ›ä»å¤´å¼€å§‹æ„å»ºé«˜åº¦å®šåˆ¶åŒ–æ·±åº¦å­¦ä¹ ç®¡é“çš„ç»éªŒä¸°å¯Œçš„ç”¨æˆ·æ¥è¯´ï¼ŒEC2æ˜¯ç†æƒ³çš„é€‰æ‹©ã€‚

### 20.3.7 Exercises ç»ƒä¹ 

1. **Launch a GPU Instance**: Launch a `g4dn.xlarge` EC2 instance using the AWS Deep Learning AMI.
   **å¯åŠ¨GPUå®ä¾‹**: ä½¿ç”¨AWSæ·±åº¦å­¦ä¹ AMIå¯åŠ¨ä¸€ä¸ª`g4dn.xlarge` EC2å®ä¾‹ã€‚
2. **Connect and Verify**: Connect to your instance via SSH. Run `nvidia-smi` and `python -c "import torch; print(torch.cuda.is_available())"` to verify that the GPU and PyTorch are configured correctly.
   **è¿æ¥å’ŒéªŒè¯**: é€šè¿‡SSHè¿æ¥åˆ°ä½ çš„å®ä¾‹ã€‚è¿è¡Œ`nvidia-smi`å’Œ`python -c "import torch; print(torch.cuda.is_available())"`æ¥éªŒè¯GPUå’ŒPyTorchæ˜¯å¦é…ç½®æ­£ç¡®ã€‚
3. **Remote Jupyter Setup**: Set up a Jupyter Lab server on the instance that is accessible remotely. Try connecting both via direct access (opening the port in the security group) and via an SSH tunnel. Which method do you prefer and why?
   **è¿œç¨‹Jupyterè®¾ç½®**: åœ¨å®ä¾‹ä¸Šè®¾ç½®ä¸€ä¸ªå¯ä»¥è¿œç¨‹è®¿é—®çš„Jupyter LabæœåŠ¡å™¨ã€‚å°è¯•é€šè¿‡ç›´æ¥è®¿é—®ï¼ˆåœ¨å®‰å…¨ç»„ä¸­å¼€æ”¾ç«¯å£ï¼‰å’ŒSSHéš§é“ä¸¤ç§æ–¹å¼è¿æ¥ã€‚ä½ æ›´å–œæ¬¢å“ªç§æ–¹æ³•ï¼Œä¸ºä»€ä¹ˆï¼Ÿ
4. **Automation Script**: Write a shell script that automates the process of checking for running instances and prompts the user to stop them to save costs.
   **è‡ªåŠ¨åŒ–è„šæœ¬**: ç¼–å†™ä¸€ä¸ªshellè„šæœ¬ï¼Œè‡ªåŠ¨åŒ–æ£€æŸ¥æ­£åœ¨è¿è¡Œçš„å®ä¾‹çš„è¿‡ç¨‹ï¼Œå¹¶æç¤ºç”¨æˆ·åœæ­¢å®ƒä»¬ä»¥èŠ‚çœæˆæœ¬ã€‚
5. **Cost Calculation**: Assume you train a model for 8 hours on a `p3.2xlarge` instance and 40 hours on a `g4dn.xlarge` instance. Using the prices mentioned in this chapter, which training run was more expensive?
   **æˆæœ¬è®¡ç®—**: å‡è®¾ä½ åœ¨ä¸€ä¸ª`p3.2xlarge`å®ä¾‹ä¸Šè®­ç»ƒæ¨¡å‹8å°æ—¶ï¼Œåœ¨ä¸€ä¸ª`g4dn.xlarge`å®ä¾‹ä¸Šè®­ç»ƒ40å°æ—¶ã€‚ä½¿ç”¨æœ¬ç« æåˆ°çš„ä»·æ ¼ï¼Œå“ªä¸ªè®­ç»ƒè¿è¡Œæ›´æ˜‚è´µï¼Ÿ

## 20.4 Using Google Colab ä½¿ç”¨Google Colab

Google Colaboratory, or "Colab" for short, is a free, cloud-based Jupyter notebook environment provided by Google. It is an excellent tool for students, researchers, and developers to learn and experiment with deep learning without worrying about hardware setup or costs.

Google Colaboratoryï¼Œç®€ç§°"Colab"ï¼Œæ˜¯è°·æ­Œæä¾›çš„ä¸€ä¸ªå…è´¹çš„ã€åŸºäºäº‘çš„Jupyterç¬”è®°æœ¬ç¯å¢ƒã€‚å¯¹äºå­¦ç”Ÿã€ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…æ¥è¯´ï¼Œå®ƒæ˜¯ä¸€ä¸ªå­¦ä¹ å’Œå®éªŒæ·±åº¦å­¦ä¹ çš„ç»ä½³å·¥å…·ï¼Œæ— éœ€æ‹…å¿ƒç¡¬ä»¶è®¾ç½®æˆ–æˆæœ¬ã€‚

Think of Colab as a "free-to-use public library" for deep learning. You get access to powerful computers (with GPUs!) for a limited time, perfect for learning, prototyping, and running small to medium-sized experiments.

æŠŠColabæƒ³è±¡æˆä¸€ä¸ªæ·±åº¦å­¦ä¹ çš„"å…è´¹å…¬å…±å›¾ä¹¦é¦†"ã€‚ä½ å¯ä»¥åœ¨æœ‰é™çš„æ—¶é—´å†…ä½¿ç”¨åŠŸèƒ½å¼ºå¤§çš„è®¡ç®—æœºï¼ˆå¸¦GPUï¼ï¼‰ï¼Œéå¸¸é€‚åˆå­¦ä¹ ã€åŸå‹è®¾è®¡å’Œè¿è¡Œä¸­å°å‹å®éªŒã€‚

#### Getting Started with Colab å¼€å§‹ä½¿ç”¨Colab

1. **Access Colab**: Go to https://colab.research.google.com/. All you need is a Google account.
   **è®¿é—®Colab**: å‰å¾€ https://colab.research.google.com/ã€‚ä½ åªéœ€è¦ä¸€ä¸ªè°·æ­Œè´¦æˆ·ã€‚
2. **Create a Notebook**: Click on "File" -> "New notebook" to create a new notebook. The interface is very similar to a standard Jupyter Notebook.
   **åˆ›å»ºç¬”è®°æœ¬**: ç‚¹å‡»"æ–‡ä»¶"->"æ–°å»ºç¬”è®°æœ¬"æ¥åˆ›å»ºä¸€ä¸ªæ–°çš„ç¬”è®°æœ¬ã€‚å…¶ç•Œé¢ä¸æ ‡å‡†çš„Jupyter Notebookéå¸¸ç›¸ä¼¼ã€‚

#### Enabling GPU and TPU å¯ç”¨GPUå’ŒTPU

One of Colab's most significant advantages is free access to hardware accelerators.
Colabæœ€æ˜¾è‘—çš„ä¼˜åŠ¿ä¹‹ä¸€æ˜¯å…è´¹ä½¿ç”¨ç¡¬ä»¶åŠ é€Ÿå™¨ã€‚

```python
# To enable GPU or TPU
# å¯ç”¨GPUæˆ–TPU
# 1. Click on "Runtime" -> "Change runtime type"
# 1. ç‚¹å‡»"è¿è¡Œæ—¶"->"æ›´æ”¹è¿è¡Œæ—¶ç±»å‹"
# 2. Under "Hardware accelerator", select "GPU" or "TPU"
# 2. åœ¨"ç¡¬ä»¶åŠ é€Ÿå™¨"ä¸‹ï¼Œé€‰æ‹©"GPU"æˆ–"TPU"
# 3. Click "Save"
# 3. ç‚¹å‡»"ä¿å­˜"

# You can verify the assigned GPU
#ä½ å¯ä»¥éªŒè¯åˆ†é…åˆ°çš„GPU
!nvidia-smi

# Check PyTorch's access to the GPU
# æ£€æŸ¥PyTorchå¯¹GPUçš„è®¿é—®
import torch
print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
```

#### Working with Files and Data å¤„ç†æ–‡ä»¶å’Œæ•°æ®

Colab provides a temporary file system that is reset after each session. For persistent storage, you should integrate with Google Drive.
Colabæä¾›ä¸€ä¸ªåœ¨æ¯æ¬¡ä¼šè¯åéƒ½ä¼šé‡ç½®çš„ä¸´æ—¶æ–‡ä»¶ç³»ç»Ÿã€‚è¦å®ç°æŒä¹…åŒ–å­˜å‚¨ï¼Œä½ åº”è¯¥ä¸Google Driveé›†æˆã€‚

```python
# Mount Google Drive
# æŒ‚è½½Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Now you can access your Google Drive files at /content/drive/MyDrive/
# ç°åœ¨ä½ å¯ä»¥åœ¨ /content/drive/MyDrive/ è®¿é—®ä½ çš„Google Driveæ–‡ä»¶äº†
!ls /content/drive/MyDrive/

# Uploading files directly
# ç›´æ¥ä¸Šä¼ æ–‡ä»¶
from google.colab import files
uploaded = files.upload()

# Cloning a Git repository
# å…‹éš†ä¸€ä¸ªGitä»“åº“
!git clone https://github.com/d2l-ai/d2l-en.git
```

#### Installing Packages å®‰è£…åŒ…

You can install any Python package using `pip`, just like in a regular environment.
ä½ å¯ä»¥åƒåœ¨å¸¸è§„ç¯å¢ƒä¸­ä½¿ç”¨`pip`ä¸€æ ·å®‰è£…ä»»ä½•PythonåŒ…ã€‚

```python
# Install a package
# å®‰è£…ä¸€ä¸ªåŒ…
!pip install transformers

# Install a specific version
# å®‰è£…ç‰¹å®šç‰ˆæœ¬
!pip install pandas==1.3.5
```

#### Colab Pro and Pro+

For users who need more resources, Google offers paid versions, Colab Pro and Pro+.
å¯¹äºéœ€è¦æ›´å¤šèµ„æºçš„ç”¨æˆ·ï¼Œè°·æ­Œæä¾›äº†ä»˜è´¹ç‰ˆæœ¬ï¼ŒColab Proå’ŒPro+ã€‚

- **Longer runtimes**: Notebooks can run for up to 24 hours.
- **æ›´é•¿çš„è¿è¡Œæ—¶**: ç¬”è®°æœ¬å¯ä»¥è¿è¡Œé•¿è¾¾24å°æ—¶ã€‚
- **Better GPUs**: Priority access to faster GPUs like P100s and V100s.
- **æ›´å¥½çš„GPU**: ä¼˜å…ˆä½¿ç”¨æ›´å¿«çš„GPUï¼Œå¦‚P100å’ŒV100ã€‚
- **More memory**: Access to high-memory VM instances.
- **æ›´å¤šå†…å­˜**: è®¿é—®é«˜å†…å­˜çš„è™šæ‹Ÿæœºå®ä¾‹ã€‚

### 20.4.1 Summary æ€»ç»“

Google Colab is an invaluable tool for the deep learning community, especially for those just starting.
Google Colabæ˜¯æ·±åº¦å­¦ä¹ ç¤¾åŒºä¸€ä¸ªæ— ä»·çš„å·¥å…·ï¼Œç‰¹åˆ«æ˜¯å¯¹äºåˆå­¦è€…ã€‚

**Advantages ä¼˜åŠ¿:**
- **Free Access to GPUs**: The biggest advantage is free access to powerful hardware accelerators.
- **å…è´¹ä½¿ç”¨GPU**: æœ€å¤§çš„ä¼˜åŠ¿æ˜¯å…è´¹ä½¿ç”¨å¼ºå¤§çš„ç¡¬ä»¶åŠ é€Ÿå™¨ã€‚
- **Zero Configuration**: No setup is required. You can start coding in your browser immediately.
- **é›¶é…ç½®**: æ— éœ€ä»»ä½•è®¾ç½®ã€‚ä½ å¯ä»¥ç«‹å³åœ¨æµè§ˆå™¨ä¸­å¼€å§‹ç¼–ç ã€‚
- **Easy Collaboration**: Notebooks can be shared and edited collaboratively, just like Google Docs.
- **è½»æ¾åä½œ**: ç¬”è®°æœ¬å¯ä»¥åƒè°·æ­Œæ–‡æ¡£ä¸€æ ·å…±äº«å’ŒååŒç¼–è¾‘ã€‚
- **Integration with Google Drive**: Seamless integration for persistent data storage.
- **ä¸Google Driveé›†æˆ**: ä¸ºæŒä¹…åŒ–æ•°æ®å­˜å‚¨æä¾›æ— ç¼é›†æˆã€‚

**Limitations é™åˆ¶:**
- **Session Timeouts**: Sessions are disconnected after a period of inactivity (around 90 minutes) and have a maximum lifetime (around 12 hours for the free version).
- **ä¼šè¯è¶…æ—¶**: ä¸€æ®µæ—¶é—´ä¸æ´»åŠ¨åï¼ˆçº¦90åˆ†é’Ÿï¼‰ï¼Œä¼šè¯ä¼šæ–­å¼€ï¼Œå¹¶ä¸”æœ‰æœ€å¤§ç”Ÿå‘½å‘¨æœŸï¼ˆå…è´¹ç‰ˆçº¦12å°æ—¶ï¼‰ã€‚
- **Resource Limits**: CPU, RAM, and disk space are limited and not guaranteed.
- **èµ„æºé™åˆ¶**: CPUã€RAMå’Œç£ç›˜ç©ºé—´æ˜¯æœ‰é™ä¸”ä¸ä¿è¯çš„ã€‚
- **Not for Production**: Not suitable for critical, long-running training jobs or production deployments.
- **ä¸é€‚ç”¨äºç”Ÿäº§ç¯å¢ƒ**: ä¸é€‚åˆå…³é”®çš„ã€é•¿æ—¶é—´è¿è¡Œçš„è®­ç»ƒä½œä¸šæˆ–ç”Ÿäº§éƒ¨ç½²ã€‚

### 20.4.2 Exercises ç»ƒä¹ 

1. **Create and Share**: Create a new Colab notebook, add a text cell explaining a simple deep learning concept, and share it with a friend.
   **åˆ›å»ºä¸åˆ†äº«**: åˆ›å»ºä¸€ä¸ªæ–°çš„Colabç¬”è®°æœ¬ï¼Œæ·»åŠ ä¸€ä¸ªæ–‡æœ¬å•å…ƒæ ¼è§£é‡Šä¸€ä¸ªç®€å•çš„æ·±åº¦å­¦ä¹ æ¦‚å¿µï¼Œå¹¶ä¸æœ‹å‹åˆ†äº«ã€‚
2. **GPU vs. CPU**: Create a notebook and run a matrix multiplication task (e.g., multiplying two 5000x5000 tensors). Time the execution on a CPU runtime, then switch to a GPU runtime and compare the speeds.
   **GPU vs. CPU**: åˆ›å»ºä¸€ä¸ªç¬”è®°æœ¬å¹¶è¿è¡Œä¸€ä¸ªçŸ©é˜µä¹˜æ³•ä»»åŠ¡ï¼ˆä¾‹å¦‚ï¼Œå°†ä¸¤ä¸ª5000x5000çš„å¼ é‡ç›¸ä¹˜ï¼‰ã€‚åœ¨CPUè¿è¡Œæ—¶ä¸‹è®¡æ—¶ï¼Œç„¶ååˆ‡æ¢åˆ°GPUè¿è¡Œæ—¶å¹¶æ¯”è¾ƒé€Ÿåº¦ã€‚
3. **Google Drive Integration**: Mount your Google Drive in a Colab notebook. Create a new file in your Drive from the notebook, and then read it back to verify.
   **Google Driveé›†æˆ**: åœ¨Colabç¬”è®°æœ¬ä¸­æŒ‚è½½ä½ çš„Google Driveã€‚ä»ç¬”è®°æœ¬ä¸­åœ¨ä½ çš„Driveé‡Œåˆ›å»ºä¸€ä¸ªæ–°æ–‡ä»¶ï¼Œç„¶åè¯»å›å®ƒè¿›è¡ŒéªŒè¯ã€‚
4. **Custom Package**: Find a Python package on PyPI that is not pre-installed in Colab. Install it and use one of its functions.
   **è‡ªå®šä¹‰åŒ…**: åœ¨PyPIä¸Šæ‰¾åˆ°ä¸€ä¸ªæ²¡æœ‰é¢„è£…åœ¨Colabä¸­çš„PythonåŒ…ã€‚å®‰è£…å®ƒå¹¶ä½¿ç”¨å®ƒçš„ä¸€ä¸ªå‡½æ•°ã€‚

## 20.5 Selecting Servers and GPUs é€‰æ‹©æœåŠ¡å™¨å’ŒGPU

Choosing the right hardware is a critical decision that impacts your productivity, research velocity, and budget. Whether you build your own machine, use a university cluster, or rent from the cloud, understanding the components is key.

é€‰æ‹©åˆé€‚çš„ç¡¬ä»¶æ˜¯ä¸€ä¸ªå…³é”®å†³ç­–ï¼Œå®ƒä¼šå½±å“ä½ çš„ç”Ÿäº§åŠ›ã€ç ”ç©¶é€Ÿåº¦å’Œé¢„ç®—ã€‚æ— è®ºä½ æ˜¯è‡ªå·±ç»„è£…æœºå™¨ã€ä½¿ç”¨å­¦æ ¡çš„é›†ç¾¤è¿˜æ˜¯ä»äº‘ç«¯ç§Ÿç”¨ï¼Œäº†è§£è¿™äº›ç»„ä»¶éƒ½æ˜¯å…³é”®ã€‚

### 20.5.1 Selecting Servers é€‰æ‹©æœåŠ¡å™¨

A deep learning "server" is essentially a powerful computer. When choosing one, consider the following components:
æ·±åº¦å­¦ä¹ "æœåŠ¡å™¨"æœ¬è´¨ä¸Šæ˜¯ä¸€å°åŠŸèƒ½å¼ºå¤§çš„è®¡ç®—æœºã€‚é€‰æ‹©æ—¶ï¼Œè¯·è€ƒè™‘ä»¥ä¸‹ç»„ä»¶ï¼š

1.  **CPU (Central Processing Unit)**: While the GPU does the heavy lifting for model training, the CPU is crucial for data preprocessing, data loading, and overall system responsiveness. A modern CPU with multiple cores (e.g., 8 or more) is recommended.
    **CPUï¼ˆä¸­å¤®å¤„ç†å™¨ï¼‰**: è™½ç„¶GPUè´Ÿè´£æ¨¡å‹è®­ç»ƒçš„ç¹é‡å·¥ä½œï¼Œä½†CPUå¯¹äºæ•°æ®é¢„å¤„ç†ã€æ•°æ®åŠ è½½å’Œæ•´ä¸ªç³»ç»Ÿçš„å“åº”èƒ½åŠ›è‡³å…³é‡è¦ã€‚æ¨èä½¿ç”¨å…·æœ‰å¤šæ ¸ï¼ˆä¾‹å¦‚8æ ¸æˆ–æ›´å¤šï¼‰çš„ç°ä»£CPUã€‚
2.  **RAM (Random Access Memory)**: Your entire dataset, or at least a large mini-batch, needs to fit into RAM during data loading. For large datasets (e.g., high-resolution images or long videos), 64GB of RAM is a good starting point, with 128GB or more being ideal for serious work.
    **RAMï¼ˆéšæœºå­˜å–å­˜å‚¨å™¨ï¼‰**: åœ¨æ•°æ®åŠ è½½æœŸé—´ï¼Œä½ çš„æ•´ä¸ªæ•°æ®é›†ï¼Œæˆ–è€…è‡³å°‘ä¸€ä¸ªå¤§çš„å°æ‰¹é‡æ•°æ®ï¼Œéœ€è¦èƒ½è£…å…¥RAMã€‚å¯¹äºå¤§å‹æ•°æ®é›†ï¼ˆä¾‹å¦‚ï¼Œé«˜åˆ†è¾¨ç‡å›¾åƒæˆ–é•¿è§†é¢‘ï¼‰ï¼Œ64GBçš„RAMæ˜¯ä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹ï¼Œè€Œ128GBæˆ–æ›´å¤šæ˜¯è¿›è¡Œä¸¥è‚ƒå·¥ä½œçš„ç†æƒ³é€‰æ‹©ã€‚
3.  **Storage**: A fast storage drive is essential to prevent data loading from becoming a bottleneck. An NVMe SSD (Non-Volatile Memory Express Solid State Drive) is highly recommended for the operating system and active datasets due to its superior read/write speeds. A larger, more affordable traditional HDD (Hard Disk Drive) can be used for archiving old datasets.
    **å­˜å‚¨**: å¿«é€Ÿçš„å­˜å‚¨é©±åŠ¨å™¨å¯¹äºé˜²æ­¢æ•°æ®åŠ è½½æˆä¸ºç“¶é¢ˆè‡³å…³é‡è¦ã€‚ç”±äºå…¶å“è¶Šçš„è¯»å†™é€Ÿåº¦ï¼Œå¼ºçƒˆæ¨èä½¿ç”¨NVMe SSDï¼ˆéæ˜“å¤±æ€§å†…å­˜å›ºæ€ç¡¬ç›˜ï¼‰æ¥å®‰è£…æ“ä½œç³»ç»Ÿå’Œå­˜æ”¾æ´»åŠ¨æ•°æ®é›†ã€‚æ›´å¤§ã€æ›´å®æƒ çš„ä¼ ç»ŸHDDï¼ˆç¡¬ç›˜é©±åŠ¨å™¨ï¼‰å¯ç”¨äºå½’æ¡£æ—§æ•°æ®é›†ã€‚
4.  **Motherboard and Power Supply**: Ensure the motherboard has enough PCIe slots for your GPUs and that the Power Supply Unit (PSU) can provide sufficient wattage for all components, especially the power-hungry GPUs, with some headroom.
    **ä¸»æ¿å’Œç”µæº**: ç¡®ä¿ä¸»æ¿æœ‰è¶³å¤Ÿçš„PCIeæ’æ§½ä¾›ä½ çš„GPUä½¿ç”¨ï¼Œå¹¶ä¸”ç”µæºå•å…ƒï¼ˆPSUï¼‰èƒ½ä¸ºæ‰€æœ‰ç»„ä»¶ï¼Œç‰¹åˆ«æ˜¯è€—ç”µçš„GPUï¼Œæä¾›è¶³å¤Ÿçš„ç“¦æ•°ï¼Œå¹¶ç•™æœ‰ä¸€äº›ä½™é‡ã€‚

### 20.5.2 Selecting GPUs é€‰æ‹©GPU

The GPU is the heart of a deep learning machine. Here are the key factors to consider:
GPUæ˜¯æ·±åº¦å­¦ä¹ æœºå™¨çš„å¿ƒè„ã€‚ä»¥ä¸‹æ˜¯éœ€è¦è€ƒè™‘çš„å…³é”®å› ç´ ï¼š

1.  **VRAM (Video RAM)**: This is the most critical factor. VRAM determines the maximum size of the models and the batch size you can use. Larger models (like Transformers) and high-resolution data require more VRAM. A minimum of 10-12GB is recommended to start, but 24GB or more (like the RTX 3090/4090) is much better for serious research.
    **VRAMï¼ˆæ˜¾å­˜ï¼‰**: è¿™æ˜¯æœ€å…³é”®çš„å› ç´ ã€‚VRAMå†³å®šäº†ä½ èƒ½ä½¿ç”¨çš„æ¨¡å‹çš„æœ€å¤§å°ºå¯¸å’Œæ‰¹é‡å¤§å°ã€‚æ›´å¤§çš„æ¨¡å‹ï¼ˆå¦‚Transformerï¼‰å’Œé«˜åˆ†è¾¨ç‡æ•°æ®éœ€è¦æ›´å¤šçš„VRAMã€‚å»ºè®®è‡³å°‘ä»10-12GBå¼€å§‹ï¼Œä½†24GBæˆ–æ›´å¤šï¼ˆå¦‚RTX 3090/4090ï¼‰å¯¹äºä¸¥è‚ƒçš„ç ”ç©¶æ¥è¯´è¦å¥½å¾—å¤šã€‚
2.  **CUDA Cores and Tensor Cores**: More CUDA cores generally mean faster parallel processing. Tensor Cores, available on newer NVIDIA GPUs (Volta architecture and later), provide massive speedups for mixed-precision training (using FP16 and FP32).
    **CUDAæ ¸å¿ƒå’Œå¼ é‡æ ¸å¿ƒ**: æ›´å¤šçš„CUDAæ ¸å¿ƒé€šå¸¸æ„å‘³ç€æ›´å¿«çš„å¹¶è¡Œå¤„ç†ã€‚è¾ƒæ–°çš„NVIDIA GPUï¼ˆVoltaæ¶æ„åŠä»¥åï¼‰ä¸Šå¯ç”¨çš„å¼ é‡æ ¸å¿ƒä¸ºæ··åˆç²¾åº¦è®­ç»ƒï¼ˆä½¿ç”¨FP16å’ŒFP32ï¼‰æä¾›äº†å·¨å¤§çš„åŠ é€Ÿã€‚
3.  **Consumer vs. Datacenter GPUs**:
    **æ¶ˆè´¹çº§ vs. æ•°æ®ä¸­å¿ƒçº§GPU**:
    - **Consumer GPUs (e.g., NVIDIA GeForce RTX series)**: Offer the best performance-per-dollar. They are excellent for individuals and small research labs. The main drawback is their blower-style coolers are not ideal for stacking multiple GPUs close together in a server chassis.
    - **æ¶ˆè´¹çº§GPUï¼ˆä¾‹å¦‚ï¼ŒNVIDIA GeForce RTXç³»åˆ—ï¼‰**: æä¾›æœ€ä½³çš„æ€§ä»·æ¯”ã€‚å®ƒä»¬éå¸¸é€‚åˆä¸ªäººå’Œå°å‹ç ”ç©¶å®éªŒå®¤ã€‚ä¸»è¦ç¼ºç‚¹æ˜¯å®ƒä»¬çš„æ¶¡è½®å¼æ•£çƒ­å™¨ä¸é€‚åˆåœ¨æœåŠ¡å™¨æœºç®±ä¸­å°†å¤šä¸ªGPUç´§å¯†å †å åœ¨ä¸€èµ·ã€‚
    - **Datacenter GPUs (e.g., NVIDIA A100, H100)**: Are designed for 24/7 operation in servers. They have better multi-GPU support (e.g., NVLink), more VRAM, and are built for reliability. However, they are significantly more expensive.
    - **æ•°æ®ä¸­å¿ƒçº§GPUï¼ˆä¾‹å¦‚ï¼ŒNVIDIA A100, H100ï¼‰**: ä¸“ä¸ºåœ¨æœåŠ¡å™¨ä¸­å…¨å¤©å€™è¿è¡Œè€Œè®¾è®¡ã€‚å®ƒä»¬å…·æœ‰æ›´å¥½çš„å¤šGPUæ”¯æŒï¼ˆä¾‹å¦‚NVLinkï¼‰ã€æ›´å¤šçš„VRAMï¼Œå¹¶ä¸”ä¸ºå¯é æ€§è€Œæ„å»ºã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„ä»·æ ¼è¦æ˜‚è´µå¾—å¤šã€‚
4.  **Cloud GPUs**: Renting GPUs from the cloud (AWS, GCP, Azure) is an excellent option. It gives you access to the most powerful datacenter GPUs without the upfront cost and maintenance overhead. This is ideal for short-term projects or when you need to scale up for a large experiment.
    **äº‘GPU**: ä»äº‘ç«¯ï¼ˆAWSã€GCPã€Azureï¼‰ç§Ÿç”¨GPUæ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é€‰æ‹©ã€‚å®ƒè®©ä½ èƒ½å¤Ÿä½¿ç”¨æœ€å¼ºå¤§çš„æ•°æ®ä¸­å¿ƒGPUï¼Œè€Œæ²¡æœ‰å‰æœŸæˆæœ¬å’Œç»´æŠ¤å¼€é”€ã€‚è¿™å¯¹äºçŸ­æœŸé¡¹ç›®æˆ–å½“ä½ éœ€è¦ä¸ºå¤§å‹å®éªŒè¿›è¡Œæ‰©å±•æ—¶æ˜¯ç†æƒ³çš„ã€‚

### 20.5.3 Summary æ€»ç»“

Building or choosing a server requires balancing performance, cost, and your specific needs.
æ„å»ºæˆ–é€‰æ‹©æœåŠ¡å™¨éœ€è¦åœ¨æ€§èƒ½ã€æˆæœ¬å’Œä½ çš„ç‰¹å®šéœ€æ±‚ä¹‹é—´å–å¾—å¹³è¡¡ã€‚

- **For Beginners/Students**: Start with Google Colab or a cloud provider's free tier. If buying, a consumer GPU like an RTX 3060 (12GB) is a good entry point.
- **å¯¹äºåˆå­¦è€…/å­¦ç”Ÿ**: ä»Google Colabæˆ–äº‘æä¾›å•†çš„å…è´¹å¥—é¤å¼€å§‹ã€‚å¦‚æœè´­ä¹°ï¼ŒåƒRTX 3060ï¼ˆ12GBï¼‰è¿™æ ·çš„æ¶ˆè´¹çº§GPUæ˜¯ä¸€ä¸ªä¸é”™çš„å…¥é—¨é€‰æ‹©ã€‚
- **For Researchers/Enthusiasts**: A machine with an RTX 3090/4090 (24GB) offers great performance and enough VRAM for most modern models.
- **å¯¹äºç ”ç©¶äººå‘˜/çˆ±å¥½è€…**: é…å¤‡RTX 3090/4090ï¼ˆ24GBï¼‰çš„æœºå™¨å¯æä¾›å‡ºè‰²çš„æ€§èƒ½å’Œè¶³å¤Ÿçš„VRAMæ¥è¿è¡Œå¤§å¤šæ•°ç°ä»£æ¨¡å‹ã€‚
- **For Professionals/Companies**: Use cloud GPUs like the A100 for maximum performance and scalability, or build dedicated servers with multiple datacenter-grade GPUs for continuous workloads.
- **å¯¹äºä¸“ä¸šäººå£«/å…¬å¸**: ä½¿ç”¨åƒA100è¿™æ ·çš„äº‘GPUä»¥è·å¾—æœ€å¤§çš„æ€§èƒ½å’Œå¯æ‰©å±•æ€§ï¼Œæˆ–ä¸ºæŒç»­çš„å·¥ä½œè´Ÿè½½æ„å»ºé…å¤‡å¤šä¸ªæ•°æ®ä¸­å¿ƒçº§GPUçš„ä¸“ç”¨æœåŠ¡å™¨ã€‚

Always remember that the field evolves quickly. The "best" hardware today might be superseded tomorrow. Renting from the cloud offers the flexibility to always have access to the latest and greatest hardware.
æ°¸è¿œè®°ä½ï¼Œè¿™ä¸ªé¢†åŸŸå‘å±•å¾ˆå¿«ã€‚ä»Šå¤©"æœ€å¥½"çš„ç¡¬ä»¶æ˜å¤©å¯èƒ½å°±ä¼šè¢«å–ä»£ã€‚ä»äº‘ç«¯ç§Ÿç”¨æä¾›äº†å§‹ç»ˆå¯ä»¥è®¿é—®æœ€æ–°ã€æœ€å¼ºå¤§ç¡¬ä»¶çš„çµæ´»æ€§ã€‚

## 20.6 Contributing to This Book ä¸ºæœ¬ä¹¦åšè´¡çŒ®

This book is an open-source project, and we welcome contributions from the community. Whether it's fixing a typo, clarifying an explanation, or proposing a new section, your help is valuable.
è¿™æœ¬ä¹¦æ˜¯ä¸€ä¸ªå¼€æºé¡¹ç›®ï¼Œæˆ‘ä»¬æ¬¢è¿ç¤¾åŒºçš„è´¡çŒ®ã€‚æ— è®ºæ˜¯ä¿®æ­£ä¸€ä¸ªæ‹¼å†™é”™è¯¯ã€æ¾„æ¸…ä¸€ä¸ªè§£é‡Šï¼Œè¿˜æ˜¯æè®®ä¸€ä¸ªæ–°çš„ç« èŠ‚ï¼Œä½ çš„å¸®åŠ©éƒ½æ˜¯å®è´µçš„ã€‚

### 20.6.1 Submitting Minor Changes æäº¤å¾®å°æ›´æ”¹

For minor changes like fixing typos, correcting grammatical errors, or improving a sentence for clarity, the easiest way to contribute is through the GitHub interface.
å¯¹äºåƒä¿®å¤æ‹¼å†™é”™è¯¯ã€çº æ­£è¯­æ³•é”™è¯¯æˆ–ä¸ºæ›´æ¸…æ™°è€Œæ”¹è¿›å¥å­è¿™æ ·çš„å¾®å°æ›´æ”¹ï¼Œæœ€ç®€å•çš„è´¡çŒ®æ–¹å¼æ˜¯é€šè¿‡GitHubç•Œé¢ã€‚

1.  **Find the File**: Navigate to the corresponding file in the project's GitHub repository.
    **æ‰¾åˆ°æ–‡ä»¶**: åœ¨é¡¹ç›®çš„GitHubä»“åº“ä¸­å¯¼èˆªåˆ°ç›¸åº”çš„æ–‡ä»¶ã€‚
2.  **Edit File**: Click the "Edit this file" (pencil) icon.
    **ç¼–è¾‘æ–‡ä»¶**: ç‚¹å‡»"ç¼–è¾‘æ­¤æ–‡ä»¶"ï¼ˆé“…ç¬”ï¼‰å›¾æ ‡ã€‚
3.  **Make Changes**: Make your changes directly in the browser editor.
    **è¿›è¡Œæ›´æ”¹**: ç›´æ¥åœ¨æµè§ˆå™¨ç¼–è¾‘å™¨ä¸­è¿›è¡Œæ›´æ”¹ã€‚
4.  **Propose Changes**: Scroll down and describe your change, then click "Propose changes". This will create a pull request for the maintainers to review.
    **æè®®æ›´æ”¹**: å‘ä¸‹æ»šåŠ¨å¹¶æè¿°ä½ çš„æ›´æ”¹ï¼Œç„¶åç‚¹å‡»"æè®®æ›´æ”¹"ã€‚è¿™å°†åˆ›å»ºä¸€ä¸ªä¾›ç»´æŠ¤è€…å®¡æŸ¥çš„æ‹‰å–è¯·æ±‚ã€‚

### 20.6.2 Proposing Major Changes æè®®é‡å¤§æ›´æ”¹

For major changes, such as adding a new chapter, significantly restructuring a section, or changing code examples across the book, it is best to start a discussion first.
å¯¹äºé‡å¤§æ›´æ”¹ï¼Œä¾‹å¦‚æ·»åŠ æ–°ç« èŠ‚ã€å¤§å¹…é‡ç»„æŸä¸ªéƒ¨åˆ†æˆ–æ›´æ”¹å…¨ä¹¦çš„ä»£ç ç¤ºä¾‹ï¼Œæœ€å¥½å…ˆå‘èµ·è®¨è®ºã€‚

1.  **Create an Issue**: Go to the "Issues" tab in the GitHub repository.
    **åˆ›å»ºè®®é¢˜**: å‰å¾€GitHubä»“åº“çš„"Issues"é€‰é¡¹å¡ã€‚
2.  **Start a Discussion**: Create a new issue to describe your proposed change. Explain why the change is needed and what your approach would be. This allows the community and maintainers to provide feedback before you invest significant time in making the changes.
    **å‘èµ·è®¨è®º**: åˆ›å»ºä¸€ä¸ªæ–°çš„è®®é¢˜æ¥æè¿°ä½ æè®®çš„æ›´æ”¹ã€‚è§£é‡Šä¸ºä»€ä¹ˆéœ€è¦è¿™ä¸ªæ›´æ”¹ä»¥åŠä½ çš„æ–¹æ³•ä¼šæ˜¯ä»€ä¹ˆã€‚è¿™ä½¿å¾—ç¤¾åŒºå’Œç»´æŠ¤è€…å¯ä»¥åœ¨ä½ æŠ•å…¥å¤§é‡æ—¶é—´è¿›è¡Œæ›´æ”¹ä¹‹å‰æä¾›åé¦ˆã€‚

### 20.6.3 Submitting Major Changes æäº¤é‡å¤§æ›´æ”¹

Once your proposal has been discussed, you can submit the change via a pull request.
ä¸€æ—¦ä½ çš„æè®®è¢«è®¨è®ºè¿‡ï¼Œä½ å°±å¯ä»¥é€šè¿‡ä¸€ä¸ªæ‹‰å–è¯·æ±‚æ¥æäº¤æ›´æ”¹ã€‚

1.  **Fork the Repository**: Create a copy of the repository under your own GitHub account.
    **å¤åˆ»ä»“åº“**: åœ¨ä½ è‡ªå·±çš„GitHubè´¦æˆ·ä¸‹åˆ›å»ºè¯¥ä»“åº“çš„ä¸€ä¸ªå‰¯æœ¬ã€‚
2.  **Clone Your Fork**: Clone your forked repository to your local machine. `git clone https://github.com/YOUR_USERNAME/REPOSITORY_NAME.git`
    **å…‹éš†ä½ çš„å¤åˆ»**: å°†ä½ å¤åˆ»çš„ä»“åº“å…‹éš†åˆ°ä½ çš„æœ¬åœ°æœºå™¨ä¸Šã€‚`git clone https://github.com/YOUR_USERNAME/REPOSITORY_NAME.git`
3.  **Create a New Branch**: Create a new branch for your changes. `git checkout -b my-major-change`
    **åˆ›å»ºæ–°åˆ†æ”¯**: ä¸ºä½ çš„æ›´æ”¹åˆ›å»ºä¸€ä¸ªæ–°åˆ†æ”¯ã€‚`git checkout -b my-major-change`
4.  **Make Your Changes**: Edit the files locally, add new files, and commit your changes.
    **è¿›è¡Œæ›´æ”¹**: åœ¨æœ¬åœ°ç¼–è¾‘æ–‡ä»¶ï¼Œæ·»åŠ æ–°æ–‡ä»¶ï¼Œå¹¶æäº¤ä½ çš„æ›´æ”¹ã€‚
5.  **Push to Your Fork**: Push your branch to your forked repository on GitHub. `git push origin my-major-change`
    **æ¨é€åˆ°ä½ çš„å¤åˆ»**: å°†ä½ çš„åˆ†æ”¯æ¨é€åˆ°ä½ åœ¨GitHubä¸Šçš„å¤åˆ»ä»“åº“ã€‚`git push origin my-major-change`
6.  **Create a Pull Request**: Go to the original repository on GitHub. You will see a prompt to create a pull request from your new branch. Fill in the details, reference the issue you created, and submit it for review.
    **åˆ›å»ºæ‹‰å–è¯·æ±‚**: å‰å¾€GitHubä¸Šçš„åŸå§‹ä»“åº“ã€‚ä½ ä¼šçœ‹åˆ°ä¸€ä¸ªä»ä½ çš„æ–°åˆ†æ”¯åˆ›å»ºæ‹‰å–è¯·æ±‚çš„æç¤ºã€‚å¡«å†™è¯¦ç»†ä¿¡æ¯ï¼Œå¼•ç”¨ä½ åˆ›å»ºçš„è®®é¢˜ï¼Œå¹¶æäº¤ä»¥ä¾›å®¡æŸ¥ã€‚

### 20.6.4 Summary æ€»ç»“

Contributing to an open-source project is a great way to give back to the community, improve your skills, and build your profile.
ä¸ºå¼€æºé¡¹ç›®åšè´¡çŒ®æ˜¯å›é¦ˆç¤¾åŒºã€æå‡æŠ€èƒ½å’Œå»ºç«‹ä¸ªäººå½¢è±¡çš„å¥½æ–¹æ³•ã€‚

- **Start Small**: Fixing typos is a great way to get started.
- **ä»å°å¤„ç€æ‰‹**: ä¿®å¤æ‹¼å†™é”™è¯¯æ˜¯å¼€å§‹çš„å¥½æ–¹æ³•ã€‚
- **Discuss First**: For major changes, always discuss them in an issue before starting work.
- **å…ˆè®¨è®º**: å¯¹äºé‡å¤§æ›´æ”¹ï¼Œåœ¨å¼€å§‹å·¥ä½œå‰ä¸€å®šè¦åœ¨è®®é¢˜ä¸­è®¨è®ºå®ƒä»¬ã€‚
- **Follow Guidelines**: Adhere to the project's coding style and contribution guidelines.
- **éµå®ˆæŒ‡å—**: éµå®ˆé¡¹ç›®çš„ç¼–ç é£æ ¼å’Œè´¡çŒ®æŒ‡å—ã€‚

### 20.6.5 Exercises ç»ƒä¹ 

1.  **Find a Typo**: Browse through the book's source files and find a typo or a sentence that could be clearer. Submit a minor change using the GitHub UI.
    **æ‰¾ä¸ªé”™å­—**: æµè§ˆæœ¬ä¹¦çš„æºæ–‡ä»¶ï¼Œæ‰¾ä¸€ä¸ªæ‹¼å†™é”™è¯¯æˆ–ä¸€ä¸ªå¯ä»¥æ›´æ¸…æ™°çš„å¥å­ã€‚ä½¿ç”¨GitHubç•Œé¢æäº¤ä¸€ä¸ªå¾®å°æ›´æ”¹ã€‚
2.  **Propose an Idea**: Think of a new example or a small topic that could be added to one of the chapters. Open an issue to propose your idea.
    **æä¸ªæƒ³æ³•**: æƒ³ä¸€ä¸ªå¯ä»¥æ·»åŠ åˆ°æŸä¸ªç« èŠ‚çš„æ–°ä¾‹å­æˆ–å°ä¸»é¢˜ã€‚å¼€ä¸€ä¸ªè®®é¢˜æ¥æè®®ä½ çš„æƒ³æ³•ã€‚
3.  **Local Setup**: Fork and clone the repository. Create a new branch. You don't have to make any changes, but practicing the setup workflow is valuable.
    **æœ¬åœ°è®¾ç½®**: å¤åˆ»å¹¶å…‹éš†ä»“åº“ã€‚åˆ›å»ºä¸€ä¸ªæ–°åˆ†æ”¯ã€‚ä½ ä¸éœ€è¦åšä»»ä½•æ›´æ”¹ï¼Œä½†ç»ƒä¹ è®¾ç½®å·¥ä½œæµç¨‹æ˜¯å¾ˆæœ‰ä»·å€¼çš„ã€‚

## 20.7 Utility Functions and Classes å®ç”¨å‡½æ•°å’Œç±»

Throughout this book, we use a set of utility functions and classes to simplify the code, making it more focused on the core deep learning concepts. These utilities are collected in the `d2l` library.
åœ¨æœ¬ä¹¦ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸€å¥—å®ç”¨å‡½æ•°å’Œç±»æ¥ç®€åŒ–ä»£ç ï¼Œä½¿å…¶æ›´ä¸“æ³¨äºæ ¸å¿ƒçš„æ·±åº¦å­¦ä¹ æ¦‚å¿µã€‚è¿™äº›å®ç”¨å·¥å…·è¢«æ”¶é›†åœ¨`d2l`åº“ä¸­ã€‚

The purpose of the `d2l` library is to abstract away repetitive boilerplate code, such as:
`d2l`åº“çš„ç›®çš„æ˜¯æŠ½è±¡æ‰é‡å¤çš„æ ·æ¿ä»£ç ï¼Œä¾‹å¦‚ï¼š

- **Data Loading**: Functions for loading standard datasets like MNIST, Fashion-MNIST, and CIFAR-10.
- **æ•°æ®åŠ è½½**: åŠ è½½åƒMNISTã€Fashion-MNISTå’ŒCIFAR-10è¿™æ ·çš„æ ‡å‡†æ•°æ®é›†çš„å‡½æ•°ã€‚
- **Visualization**: Functions to plot images, loss curves, and confusion matrices.
- **å¯è§†åŒ–**: ç»˜åˆ¶å›¾åƒã€æŸå¤±æ›²çº¿å’Œæ··æ·†çŸ©é˜µçš„å‡½æ•°ã€‚
- **Training Loops**: A standardized trainer class that handles the training and evaluation loop.
- **è®­ç»ƒå¾ªç¯**: ä¸€ä¸ªæ ‡å‡†åŒ–çš„è®­ç»ƒå™¨ç±»ï¼Œå¤„ç†è®­ç»ƒå’Œè¯„ä¼°å¾ªç¯ã€‚
- **Timers and Animators**: Classes to measure execution time and create animations of the training process.
- **è®¡æ—¶å™¨å’ŒåŠ¨ç”»å™¨**: ç”¨æ¥æµ‹é‡æ‰§è¡Œæ—¶é—´å’Œåˆ›å»ºè®­ç»ƒè¿‡ç¨‹åŠ¨ç”»çš„ç±»ã€‚

By using these utilities, we can write code that is both concise and readable. For example, instead of writing a full training loop from scratch in every chapter, we can just use `d2l.train_ch3`.
é€šè¿‡ä½¿ç”¨è¿™äº›å®ç”¨å·¥å…·ï¼Œæˆ‘ä»¬å¯ä»¥ç¼–å†™æ—¢ç®€æ´åˆå¯è¯»çš„ä»£ç ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬ä¸å¿…åœ¨æ¯ä¸€ç« éƒ½ä»å¤´å¼€å§‹ç¼–å†™ä¸€ä¸ªå®Œæ•´çš„è®­ç»ƒå¾ªç¯ï¼Œè€Œåªéœ€ä½¿ç”¨`d2l.train_ch3`ã€‚

You are encouraged to inspect the source code of the `d2l` library to understand how these functions are implemented. It is a great way to learn about the practical aspects of building deep learning pipelines.
æˆ‘ä»¬é¼“åŠ±ä½ æŸ¥çœ‹`d2l`åº“çš„æºä»£ç ï¼Œä»¥äº†è§£è¿™äº›å‡½æ•°æ˜¯å¦‚ä½•å®ç°çš„ã€‚è¿™æ˜¯å­¦ä¹ æ„å»ºæ·±åº¦å­¦ä¹ ç®¡é“å®è·µæ–¹é¢çš„å¥½æ–¹æ³•ã€‚

## 20.8 The d2l API Document d2l APIæ–‡æ¡£

To see the full list of available utility functions and classes, you can refer to the official API documentation for the `d2l` package. The documentation provides a detailed description of each function and class, including its parameters and usage examples.
è¦æŸ¥çœ‹æ‰€æœ‰å¯ç”¨çš„å®ç”¨å‡½æ•°å’Œç±»çš„å®Œæ•´åˆ—è¡¨ï¼Œä½ å¯ä»¥å‚è€ƒ`d2l`åŒ…çš„å®˜æ–¹APIæ–‡æ¡£ã€‚è¯¥æ–‡æ¡£ä¸ºæ¯ä¸ªå‡½æ•°å’Œç±»æä¾›äº†è¯¦ç»†çš„æè¿°ï¼ŒåŒ…æ‹¬å…¶å‚æ•°å’Œä½¿ç”¨ç¤ºä¾‹ã€‚

### 20.8.1 Classes ç±»

The API documentation lists all major classes, such as:
APIæ–‡æ¡£åˆ—å‡ºäº†æ‰€æœ‰ä¸»è¦çš„ç±»ï¼Œä¾‹å¦‚ï¼š

- `d2l.Timer`: A class for measuring execution time.
- `d2l.Timer`: ä¸€ä¸ªç”¨äºæµ‹é‡æ‰§è¡Œæ—¶é—´çš„ç±»ã€‚
- `d2l.Accumulator`: A class for accumulating sums over multiple variables.
- `d2l.Accumulator`: ä¸€ä¸ªç”¨äºåœ¨å¤šä¸ªå˜é‡ä¸Šç´¯åŠ æ€»å’Œçš„ç±»ã€‚
- `d2l.Animator`: A class for drawing data in an animation.
- `d2l.Animator`: ä¸€ä¸ªç”¨äºåœ¨åŠ¨ç”»ä¸­ç»˜åˆ¶æ•°æ®çš„ç±»ã€‚
- `d2l.Trainer`: A class for training models.
- `d2l.Trainer`: ä¸€ä¸ªç”¨äºè®­ç»ƒæ¨¡å‹çš„ç±»ã€‚

### 20.8.2 Functions å‡½æ•°

It also documents all functions, categorized by module, such as:
å®ƒè¿˜è®°å½•äº†æ‰€æœ‰æŒ‰æ¨¡å—åˆ†ç±»çš„å‡½æ•°ï¼Œä¾‹å¦‚ï¼š

- **Data Loading**: `d2l.load_data_fashion_mnist`, `d2l.load_data_cifar10`.
- **æ•°æ®åŠ è½½**: `d2l.load_data_fashion_mnist`, `d2l.load_data_cifar10`ã€‚
- **Training Utilities**: `d2l.train_epoch_ch3`, `d2l.evaluate_accuracy_gpu`.
- **è®­ç»ƒå·¥å…·**: `d2l.train_epoch_ch3`, `d2l.evaluate_accuracy_gpu`ã€‚
- **Visualization**: `d2l.show_images`, `d2l.plot`.
- **å¯è§†åŒ–**: `d2l.show_images`, `d2l.plot`ã€‚

Exploring the API is a good exercise to become familiar with the tools at your disposal and to learn how to write more efficient deep learning code.
æ¢ç´¢APIæ˜¯ç†Ÿæ‚‰å¯ç”¨å·¥å…·å¹¶å­¦ä¹ å¦‚ä½•ç¼–å†™æ›´é«˜æ•ˆæ·±åº¦å­¦ä¹ ä»£ç çš„å¥½ç»ƒä¹ ã€‚