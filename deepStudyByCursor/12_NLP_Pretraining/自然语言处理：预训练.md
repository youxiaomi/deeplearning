<!--  --># Natural Language Processing: Pretraining 自然语言处理：预训练

## Chapter Overview 章节概述

Natural Language Processing (NLP) pretraining has become the cornerstone of modern language understanding systems. This chapter explores the fundamental concepts and techniques that enable machines to understand and process human language effectively.

自然语言处理（NLP）预训练已成为现代语言理解系统的基石。本章探讨使机器能够有效理解和处理人类语言的基本概念和技术。

---

## 12.1 Word Embedding (word2vec) 词嵌入

### 12.1.1 One-Hot Vectors Are a Bad Choice 独热向量是糟糕的选择

**Why Traditional Approaches Fall Short 为什么传统方法不足**

One-hot encoding represents each word as a binary vector where only one element is 1 and all others are 0. For a vocabulary of size V, each word becomes a V-dimensional sparse vector.

独热编码将每个词表示为一个二进制向量，其中只有一个元素为1，其他都为0。对于大小为V的词汇表，每个词都变成一个V维稀疏向量。

**Problems with One-Hot Encoding 独热编码的问题：**

1. **Curse of Dimensionality 维度诅咒**: With vocabularies containing hundreds of thousands of words, vectors become extremely high-dimensional and sparse.
   
   随着包含数十万词汇的词汇表，向量变得极其高维和稀疏。

2. **No Semantic Relationships 无语义关系**: One-hot vectors treat all words as equally different. The dot product between any two different words is always 0, providing no information about semantic similarity.
   
   独热向量将所有词视为同等不同。任意两个不同词之间的点积总是0，无法提供语义相似性信息。

3. **Computational Inefficiency 计算效率低**: Operations on high-dimensional sparse vectors are computationally expensive and memory-intensive.
   
   高维稀疏向量的操作计算昂贵且内存密集。

**Real-World Example 现实世界的例子：**

Consider the words "king", "queen", "man", and "woman". In one-hot encoding:
- king = [1, 0, 0, 0, ...]
- queen = [0, 1, 0, 0, ...]
- man = [0, 0, 1, 0, ...]
- woman = [0, 0, 0, 1, ...]

考虑词汇"king"、"queen"、"man"和"woman"。在独热编码中，我们无法表达它们之间的语义关系，比如"king"和"queen"都表示君主，"man"和"woman"都表示人类性别。

### 12.1.2 Self-Supervised word2vec 自监督word2vec

**The Revolutionary Approach 革命性方法**

Word2vec introduces a self-supervised learning paradigm where the model learns word representations by predicting context words from target words (or vice versa) using the word's surrounding context in large text corpora.

Word2vec引入了一种自监督学习范式，模型通过使用大型文本语料库中词汇的周围上下文，从目标词预测上下文词（或反之）来学习词表示。

**Key Innovation 关键创新：**

Instead of relying on manually labeled data, word2vec leverages the natural structure of language itself. The idea is that words appearing in similar contexts tend to have similar meanings.

word2vec不依赖手动标记的数据，而是利用语言本身的自然结构。其理念是出现在相似上下文中的词往往具有相似的含义。

**Mathematical Foundation 数学基础：**

The objective is to learn word representations that maximize the probability of observing the actual context words given the target word:

目标是学习能够最大化给定目标词观察到实际上下文词概率的词表示：

```
maximize ∏(w,c)∈D P(c|w)
```

Where D is the training dataset of (word, context) pairs.
其中D是（词，上下文）对的训练数据集。

### 12.1.3 The Skip-Gram Model 跳字模型

**Model Architecture 模型架构**

The Skip-gram model predicts context words given a center word. For a center word w_c, it aims to predict surrounding words w_{c-m}, ..., w_{c-1}, w_{c+1}, ..., w_{c+m} within a window of size m.

跳字模型根据中心词预测上下文词。对于中心词w_c，它旨在预测窗口大小为m内的周围词w_{c-m}, ..., w_{c-1}, w_{c+1}, ..., w_{c+m}。

**Mathematical Formulation 数学公式：**

The probability of observing context word w_o given center word w_c is computed using softmax:

给定中心词w_c观察到上下文词w_o的概率使用softmax计算：

```
P(w_o|w_c) = exp(u_o^T v_c) / Σ_{i∈V} exp(u_i^T v_c)
```

Where:
- v_c: center word embedding vector 中心词嵌入向量
- u_o: context word embedding vector 上下文词嵌入向量
- V: vocabulary set 词汇集合

**Training Process 训练过程：**

1. **Sliding Window 滑动窗口**: Move a fixed-size window across the text corpus
   在文本语料库中移动固定大小的窗口

2. **Extract Pairs 提取词对**: For each center word, create training pairs with all context words in the window
   对于每个中心词，与窗口中的所有上下文词创建训练对

3. **Optimize Objective 优化目标**: Maximize the log-likelihood of all (center, context) pairs
   最大化所有（中心，上下文）对的对数似然

**Example Training Data 训练数据示例：**

Text: "The quick brown fox jumps over the lazy dog"
Window size = 2

Training pairs for "brown":
- (brown, The), (brown, quick), (brown, fox), (brown, jumps)

文本："The quick brown fox jumps over the lazy dog"
为"brown"生成的训练对展示了如何从上下文中学习词的含义。

### 12.1.4 The Continuous Bag of Words (CBOW) Model 连续词袋模型

**Model Architecture 模型架构**

CBOW takes the opposite approach to Skip-gram: it predicts the center word given its surrounding context words. This model aggregates context information to predict the target word.

CBOW采用与跳字模型相反的方法：它根据周围的上下文词预测中心词。该模型聚合上下文信息来预测目标词。

**Mathematical Formulation 数学公式：**

Given context words w_{c-m}, ..., w_{c-1}, w_{c+1}, ..., w_{c+m}, the probability of the center word w_c is:

给定上下文词w_{c-m}, ..., w_{c-1}, w_{c+1}, ..., w_{c+m}，中心词w_c的概率为：

```
P(w_c|Context) = exp(u_c^T v̄) / Σ_{i∈V} exp(u_i^T v̄)
```

Where v̄ is the average of context word embeddings:
其中v̄是上下文词嵌入的平均值：

```
v̄ = (1/2m) Σ_{j=-m,j≠0}^m v_{c+j}
```

**Skip-gram vs CBOW Comparison 跳字模型与CBOW比较：**

| Aspect 方面 | Skip-gram 跳字模型 | CBOW |
|-------------|-------------------|------|
| Prediction 预测 | Context from center word 从中心词预测上下文 | Center word from context 从上下文预测中心词 |
| Training Speed 训练速度 | Slower 较慢 | Faster 较快 |
| Performance on Rare Words 罕见词性能 | Better 更好 | Worse 较差 |
| Memory Usage 内存使用 | Higher 较高 | Lower 较低 |

**Practical Example 实际例子：**

Text: "The cat sits on the mat"
Window size = 2

CBOW training example:
- Context: [The, cat, on, the] → Target: sits
- Context: [cat, sits, the, mat] → Target: on

CBOW通过聚合多个上下文词的信息来预测中心词，这种方法在处理常见词时通常更有效。

### 12.1.5 Summary 总结

Word embeddings revolutionized NLP by providing dense, low-dimensional representations that capture semantic relationships between words. The key insights are:

词嵌入通过提供捕获词汇间语义关系的密集、低维表示彻底改变了NLP。关键洞察包括：

1. **Distributed Representations 分布式表示**: Words are represented as dense vectors where similar words have similar vector representations
   词汇表示为密集向量，相似词具有相似的向量表示

2. **Self-Supervision 自监督**: No manual labeling required; the model learns from the natural structure of language
   无需手动标记；模型从语言的自然结构中学习

3. **Semantic Relationships 语义关系**: Mathematical operations on embeddings can capture semantic analogies (e.g., king - man + woman ≈ queen)
   嵌入上的数学运算可以捕获语义类比

---

## 12.2 Approximate Training 近似训练

### 12.2.1 Negative Sampling 负采样

**The Computational Challenge 计算挑战**

The standard softmax computation in word2vec requires summing over the entire vocabulary, which becomes computationally prohibitive for large vocabularies (often 100K+ words).

word2vec中的标准softmax计算需要对整个词汇表求和，这对于大型词汇表（通常10万+词汇）来说在计算上是令人望而却步的。

**Negative Sampling Solution 负采样解决方案**

Instead of computing probabilities over the entire vocabulary, negative sampling transforms the multi-class classification problem into a binary classification problem for each (center, context) word pair.

负采样不是计算整个词汇表上的概率，而是将多类分类问题转换为每个（中心，上下文）词对的二元分类问题。

**Mathematical Formulation 数学公式：**

For a positive (center, context) pair (w_c, w_o), the objective becomes:

对于正（中心，上下文）对(w_c, w_o)，目标变为：

```
log σ(u_o^T v_c) + Σ_{i=1}^K E_{w_i~P_n(w)} [log σ(-u_i^T v_c)]
```

Where:
- σ(x) = 1/(1+exp(-x)): sigmoid function sigmoid函数
- K: number of negative samples 负样本数量
- P_n(w): negative sampling distribution 负采样分布

**Negative Sampling Distribution 负采样分布：**

The probability of sampling word w_i as a negative example is:

将词w_i采样为负样本的概率是：

```
P_n(w_i) = f(w_i)^{3/4} / Σ_j f(w_j)^{3/4}
```

Where f(w_i) is the frequency of word w_i. The 3/4 power reduces the probability of very frequent words and increases that of rare words.

其中f(w_i)是词w_i的频率。3/4次幂降低了高频词的概率，增加了罕见词的概率。

**Algorithm Steps 算法步骤：**

1. **Positive Pair 正对**: For each training pair (center, context), this is the positive example
   对于每个训练对（中心，上下文），这是正例

2. **Sample Negatives 采样负例**: Randomly sample K words from the vocabulary according to P_n(w)
   根据P_n(w)从词汇表中随机采样K个词

3. **Binary Classification 二元分类**: Train the model to distinguish positive pairs from negative pairs
   训练模型区分正对和负对

**Practical Benefits 实际优势：**

- **Computational Efficiency 计算效率**: Reduces complexity from O(|V|) to O(K) where K << |V|
  将复杂度从O(|V|)降低到O(K)，其中K << |V|

- **Better Rare Word Handling 更好的罕见词处理**: The 3/4 power in sampling gives rare words more training opportunities
  采样中的3/4次幂为罕见词提供更多训练机会

### 12.2.2 Hierarchical Softmax 分层softmax

**The Tree-Based Approach 基于树的方法**

Hierarchical softmax organizes the vocabulary into a binary tree structure, where each leaf represents a word and each internal node represents a binary classifier.

分层softmax将词汇表组织成二叉树结构，其中每个叶子代表一个词，每个内部节点代表一个二元分类器。

**Mathematical Foundation 数学基础：**

The probability of reaching word w from the root is the product of probabilities along the path:

从根到达词w的概率是沿路径概率的乘积：

```
P(w|w_c) = ∏_{j=1}^{L(w)} σ([[n(w,j+1) = ch(n(w,j))]] · u_{n(w,j)}^T v_c)
```

Where:
- L(w): length of path to word w 到词w的路径长度
- n(w,j): j-th node on path to w 到w的路径上第j个节点  
- ch(n): left child of node n 节点n的左孩子
- [[ ·]]: indicator function taking values -1 or +1 取值为-1或+1的指示函数

**Tree Construction Strategies 树构造策略：**

1. **Huffman Tree 霍夫曼树**: More frequent words get shorter paths, reducing average computation time
   更频繁的词获得更短的路径，减少平均计算时间

2. **Balanced Tree 平衡树**: Ensures O(log|V|) depth for all words
   确保所有词的深度为O(log|V|)

**Advantages and Disadvantages 优缺点：**

**Advantages 优势:**
- Guaranteed O(log|V|) complexity 保证O(log|V|)复杂度
- No need for negative sampling 无需负采样
- Exact probability computation 精确概率计算

**Disadvantages 劣势:**
- Tree structure is fixed during training 训练期间树结构固定
- Less flexible than negative sampling 比负采样灵活性差
- Path dependencies can create biases 路径依赖可能产生偏差

---

## 12.3 The Dataset for Pretraining Word Embeddings 词嵌入预训练数据集

### 12.3.1 Reading the Dataset 读取数据集

**Data Preprocessing Pipeline 数据预处理流水线**

Processing raw text for word embedding training involves several crucial steps to ensure high-quality representations.

为词嵌入训练处理原始文本涉及几个关键步骤，以确保高质量的表示。

**Text Cleaning Steps 文本清理步骤：**

1. **Tokenization 分词**: Split text into individual tokens
   将文本分割为单个标记

2. **Lowercasing 小写化**: Convert all text to lowercase for consistency
   将所有文本转换为小写以保持一致性

3. **Special Character Handling 特殊字符处理**: Remove or normalize punctuation and special characters
   删除或标准化标点符号和特殊字符

4. **Frequency Filtering 频率过滤**: Remove extremely rare or common words
   删除极其罕见或常见的词

**Example Code Structure 示例代码结构：**

```python
import torch
import collections
import random

def read_ptb():
    """Read the Penn Treebank dataset"""
    # 读取Penn Treebank数据集
    with open('ptb.train.txt', 'r') as f:
        lines = f.readlines()
    
    # Tokenize and clean
    # 分词和清理
    tokens = []
    for line in lines:
        tokens.extend(line.strip().lower().split())
    
    return tokens

def build_vocab(tokens, min_freq=10):
    """Build vocabulary with frequency filtering"""
    # 构建带频率过滤的词汇表
    counter = collections.Counter(tokens)
    vocab = {word: idx for idx, (word, freq) in 
             enumerate(counter.most_common()) if freq >= min_freq}
    
    return vocab, counter
```

### 12.3.2 Subsampling 子采样

**The Frequency Imbalance Problem 频率不平衡问题**

High-frequency words like "the", "and", "is" appear much more often than content words, leading to biased training where function words dominate the learning process.

像"the"、"and"、"is"这样的高频词比内容词出现得更频繁，导致偏向性训练，其中功能词主导学习过程。

**Subsampling Strategy 子采样策略**

Subsampling randomly discards frequent words during training. The probability of keeping word w_i is:

子采样在训练期间随机丢弃频繁词。保留词w_i的概率是：

```
P(keep w_i) = min(1, sqrt(t/f(w_i)) + t/f(w_i))
```

Where:
- f(w_i): frequency of word w_i 词w_i的频率
- t: threshold parameter (typically 10^-4 to 10^-5) 阈值参数（通常为10^-4到10^-5）

**Implementation Example 实现示例：**

```python
def subsample_tokens(tokens, vocab, counter, t=1e-4):
    """Subsample frequent tokens"""
    # 对频繁标记进行子采样
    total_count = sum(counter.values())
    
    def keep_token(token):
        freq = counter[token] / total_count
        return random.random() < min(1.0, (t/freq)**0.5 + t/freq)
    
    return [token for token in tokens if keep_token(token)]
```

**Benefits of Subsampling 子采样的好处：**

1. **Balanced Learning 平衡学习**: Reduces dominance of frequent words
   减少频繁词的主导地位

2. **Better Rare Word Representations 更好的罕见词表示**: Gives more training opportunities to content words
   为内容词提供更多训练机会

3. **Improved Semantic Quality 改善语义质量**: Results in more meaningful word relationships
   产生更有意义的词汇关系

### 12.3.3 Extracting Center Words and Context Words 提取中心词和上下文词

**Window-Based Context Extraction 基于窗口的上下文提取**

The quality of word embeddings heavily depends on how we define and extract the context for each word.

词嵌入的质量很大程度上取决于我们如何定义和提取每个词的上下文。

**Dynamic Window Size 动态窗口大小**

Instead of using a fixed window, we can randomly sample the window size for each center word to add variety to the training data:

我们可以为每个中心词随机采样窗口大小，而不是使用固定窗口，以增加训练数据的多样性：

```python
def extract_center_context_pairs(tokens, max_window_size=5):
    """Extract (center, context) pairs with dynamic window"""
    # 用动态窗口提取（中心，上下文）对
    pairs = []
    
    for i in range(len(tokens)):
        # Random window size for this center word
        # 为此中心词随机选择窗口大小
        window_size = random.randint(1, max_window_size)
        
        center = tokens[i]
        context = []
        
        # Extract context words within window
        # 在窗口内提取上下文词
        for j in range(max(0, i - window_size), 
                      min(len(tokens), i + window_size + 1)):
            if i != j:  # Skip center word
                context.append(tokens[j])
        
        # Create pairs
        # 创建词对
        for ctx_word in context:
            pairs.append((center, ctx_word))
    
    return pairs
```

**Context Weighting 上下文加权**

Words closer to the center word are often more semantically related. We can implement distance-based weighting:

离中心词更近的词通常在语义上更相关。我们可以实现基于距离的加权：

```python
def weighted_context_sampling(tokens, center_idx, max_window_size=5):
    """Sample context with distance-based weights"""
    # 基于距离权重的上下文采样
    weights = []
    indices = []
    
    for j in range(max(0, center_idx - max_window_size),
                  min(len(tokens), center_idx + max_window_size + 1)):
        if j != center_idx:
            distance = abs(j - center_idx)
            weight = 1.0 / distance  # Closer words get higher weight
            weights.append(weight)
            indices.append(j)
    
    # Sample based on weights
    # 基于权重采样
    return random.choices(indices, weights=weights, k=min(len(indices), 10))
```

### 12.3.4 Negative Sampling 负采样

**Efficient Negative Sample Generation 高效负样本生成**

The choice of negative samples significantly impacts training quality and speed.

负样本的选择显著影响训练质量和速度。

**Unigram Distribution 一元分布**

The standard approach uses the 3/4 power of word frequencies:

标准方法使用词频的3/4次幂：

```python
class NegativeSampler:
    def __init__(self, counter, power=0.75):
        # 初始化负采样器
        words = list(counter.keys())
        freqs = [counter[word] ** power for word in words]
        
        # Create probability distribution
        # 创建概率分布
        total = sum(freqs)
        self.probs = [freq / total for freq in freqs]
        self.words = words
    
    def sample(self, num_samples):
        """Sample negative words"""
        # 采样负词
        return random.choices(self.words, weights=self.probs, 
                            k=num_samples)
```

**Advanced Sampling Strategies 高级采样策略：**

1. **Alias Method 别名方法**: O(1) sampling after O(n) preprocessing
   O(n)预处理后O(1)采样

2. **Hierarchical Sampling 分层采样**: Use frequency bins for efficient sampling
   使用频率箱进行高效采样

3. **Context-Aware Sampling 上下文感知采样**: Avoid sampling words that appear in the current context
   避免采样当前上下文中出现的词

### 12.3.5 Loading Training Examples in Minibatches 以小批量加载训练样本

**Efficient Batch Processing 高效批处理**

For large-scale training, we need efficient batching strategies that maximize GPU utilization while maintaining training quality.

对于大规模训练，我们需要在保持训练质量的同时最大化GPU利用率的高效批处理策略。

**Batch Construction Algorithm 批构造算法：**

```python
class Word2VecDataLoader:
    def __init__(self, pairs, negative_sampler, batch_size=512, 
                 num_negatives=5):
        self.pairs = pairs
        self.negative_sampler = negative_sampler
        self.batch_size = batch_size
        self.num_negatives = num_negatives
    
    def __iter__(self):
        # Shuffle training pairs
        # 打乱训练对
        random.shuffle(self.pairs)
        
        for i in range(0, len(self.pairs), self.batch_size):
            batch_pairs = self.pairs[i:i+self.batch_size]
            
            centers = []
            contexts = []
            labels = []
            
            for center, context in batch_pairs:
                # Positive example
                # 正例
                centers.append(center)
                contexts.append(context)
                labels.append(1)
                
                # Negative examples
                # 负例
                neg_contexts = self.negative_sampler.sample(
                    self.num_negatives)
                for neg_ctx in neg_contexts:
                    centers.append(center)
                    contexts.append(neg_ctx)
                    labels.append(0)
            
            yield torch.tensor(centers), torch.tensor(contexts), \
                  torch.tensor(labels, dtype=torch.float)
```

**Memory Optimization 内存优化：**

1. **Streaming Processing 流式处理**: Process data on-the-fly to reduce memory usage
   即时处理数据以减少内存使用

2. **Vocabulary Indexing 词汇索引**: Convert words to indices early to save memory
   早期将词转换为索引以节省内存

3. **Efficient Data Structures 高效数据结构**: Use appropriate data types for different components
   为不同组件使用适当的数据类型

---

## 12.4 Pretraining word2vec 预训练word2vec

### 12.4.1 The Skip-Gram Model 跳字模型

**PyTorch Implementation PyTorch实现**

Here's a complete implementation of the Skip-gram model with negative sampling:

以下是带负采样的跳字模型的完整实现：

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class SkipGramModel(nn.Module):
    def __init__(self, vocab_size, embed_size):
        super(SkipGramModel, self).__init__()
        
        # Center word embeddings (input embeddings)
        # 中心词嵌入（输入嵌入）
        self.center_embeddings = nn.Embedding(vocab_size, embed_size)
        
        # Context word embeddings (output embeddings)  
        # 上下文词嵌入（输出嵌入）
        self.context_embeddings = nn.Embedding(vocab_size, embed_size)
        
        # Initialize embeddings
        # 初始化嵌入
        self.init_embeddings()
    
    def init_embeddings(self):
        """Initialize embeddings with small random values"""
        # 用小随机值初始化嵌入
        initrange = 0.5 / self.center_embeddings.embedding_dim
        self.center_embeddings.weight.data.uniform_(-initrange, initrange)
        self.context_embeddings.weight.data.uniform_(-initrange, initrange)
    
    def forward(self, center_words, context_words, labels):
        """Forward pass for Skip-gram with negative sampling"""
        # 带负采样的跳字模型前向传播
        
        # Get embeddings
        # 获取嵌入
        center_embeds = self.center_embeddings(center_words)  # [batch_size, embed_size]
        context_embeds = self.context_embeddings(context_words)  # [batch_size, embed_size]
        
        # Compute dot product
        # 计算点积
        scores = torch.sum(center_embeds * context_embeds, dim=1)  # [batch_size]
        
        # Apply sigmoid and compute loss
        # 应用sigmoid并计算损失
        predictions = torch.sigmoid(scores)
        loss = F.binary_cross_entropy(predictions, labels.float())
        
        return loss
    
    def get_word_embedding(self, word_idx):
        """Get the final embedding for a word"""
        # 获取词的最终嵌入
        return self.center_embeddings(torch.tensor([word_idx]))
```

**Training Loop 训练循环：**

```python
def train_word2vec(model, dataloader, epochs=5, lr=0.025):
    """Train the word2vec model"""
    # 训练word2vec模型
    optimizer = torch.optim.SGD(model.parameters(), lr=lr)
    
    for epoch in range(epochs):
        total_loss = 0
        num_batches = 0
        
        for centers, contexts, labels in dataloader:
            optimizer.zero_grad()
            
            # Forward pass
            # 前向传播
            loss = model(centers, contexts, labels)
            
            # Backward pass
            # 反向传播
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            num_batches += 1
            
            # Learning rate decay
            # 学习率衰减
            if num_batches % 10000 == 0:
                for param_group in optimizer.param_groups:
                    param_group['lr'] *= 0.98
        
        avg_loss = total_loss / num_batches
        print(f'Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss:.4f}')
```

### 12.4.2 Training 训练

**Hyperparameter Selection 超参数选择**

The choice of hyperparameters significantly affects the quality of learned embeddings:

超参数的选择显著影响学习嵌入的质量：

**Key Hyperparameters 关键超参数：**

1. **Embedding Dimension 嵌入维度**: Typically 100-300 for most applications
   大多数应用通常为100-300

2. **Window Size 窗口大小**: 5-10 words, affects the type of relationships captured
   5-10个词，影响捕获的关系类型

3. **Negative Sampling 负采样**: 5-20 negative samples per positive example
   每个正例5-20个负样本

4. **Learning Rate 学习率**: Start with 0.025, decay during training
   从0.025开始，训练期间衰减

5. **Subsampling Threshold 子采样阈值**: 10^-4 to 10^-5 for frequency filtering
   频率过滤的阈值为10^-4到10^-5

**Training Strategies 训练策略：**

```python
def advanced_training_loop(model, dataloader, epochs=5):
    """Advanced training with learning rate scheduling and monitoring"""
    # 带学习率调度和监控的高级训练
    
    optimizer = torch.optim.SGD(model.parameters(), lr=0.025)
    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)
    
    # Training monitoring
    # 训练监控
    losses = []
    
    for epoch in range(epochs):
        epoch_loss = 0
        num_batches = 0
        
        # Progress tracking
        # 进度跟踪
        if epoch % 1 == 0:
            print(f"Starting epoch {epoch+1}/{epochs}")
        
        for batch_idx, (centers, contexts, labels) in enumerate(dataloader):
            optimizer.zero_grad()
            
            loss = model(centers, contexts, labels)
            loss.backward()
            
            # Gradient clipping
            # 梯度裁剪
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            epoch_loss += loss.item()
            num_batches += 1
            
            # Periodic logging
            # 定期记录
            if batch_idx % 1000 == 0:
                current_lr = optimizer.param_groups[0]['lr']
                print(f"  Batch {batch_idx}, Loss: {loss.item():.4f}, LR: {current_lr:.6f}")
        
        # End of epoch processing
        # epoch结束处理
        avg_loss = epoch_loss / num_batches
        losses.append(avg_loss)
        scheduler.step()
        
        print(f"Epoch {epoch+1} completed. Average loss: {avg_loss:.4f}")
    
    return losses
```

### 12.4.3 Applying Word Embeddings 应用词嵌入

**Embedding Analysis 嵌入分析**

Once trained, word embeddings can be analyzed and applied in various ways:

训练完成后，词嵌入可以通过多种方式进行分析和应用：

**Similarity Analysis 相似性分析：**

```python
def find_similar_words(model, vocab, word, top_k=10):
    """Find words most similar to the given word"""
    # 找到与给定词最相似的词
    if word not in vocab:
        print(f"Word '{word}' not in vocabulary")
        return []
    
    word_idx = vocab[word]
    word_embedding = model.get_word_embedding(word_idx)
    
    # Compute similarities with all words
    # 计算与所有词的相似性
    similarities = []
    for other_word, other_idx in vocab.items():
        if other_word != word:
            other_embedding = model.get_word_embedding(other_idx)
            similarity = F.cosine_similarity(word_embedding, other_embedding)
            similarities.append((other_word, similarity.item()))
    
    # Sort by similarity
    # 按相似性排序
    similarities.sort(key=lambda x: x[1], reverse=True)
    
    return similarities[:top_k]

# Example usage
# 使用示例
similar_to_king = find_similar_words(model, vocab, "king", top_k=5)
print("Words similar to 'king':", similar_to_king)
```

**Analogy Solving 类比求解：**

```python
def solve_analogy(model, vocab, word_a, word_b, word_c):
    """Solve analogy: word_a is to word_b as word_c is to ?"""
    # 解决类比：word_a之于word_b如同word_c之于？
    
    # Get embeddings
    # 获取嵌入
    embed_a = model.get_word_embedding(vocab[word_a])
    embed_b = model.get_word_embedding(vocab[word_b])  
    embed_c = model.get_word_embedding(vocab[word_c])
    
    # Compute target embedding: embed_b - embed_a + embed_c
    # 计算目标嵌入：embed_b - embed_a + embed_c
    target_embedding = embed_b - embed_a + embed_c
    
    # Find closest word
    # 找到最近的词
    best_similarity = -1
    best_word = None
    
    for word, idx in vocab.items():
        if word not in [word_a, word_b, word_c]:
            word_embedding = model.get_word_embedding(idx)
            similarity = F.cosine_similarity(target_embedding, word_embedding)
            
            if similarity > best_similarity:
                best_similarity = similarity
                best_word = word
    
    return best_word, best_similarity.item()

# Example: king - man + woman = ?
# 示例：king - man + woman = ?
result, score = solve_analogy(model, vocab, "king", "man", "woman")
print(f"king - man + woman = {result} (score: {score:.3f})")
```

**Visualization 可视化：**

```python
def visualize_embeddings(model, vocab, words, method='tsne'):
    """Visualize word embeddings in 2D"""
    # 在2D中可视化词嵌入
    import matplotlib.pyplot as plt
    from sklearn.manifold import TSNE
    from sklearn.decomposition import PCA
    
    # Get embeddings for selected words
    # 获取选定词的嵌入
    embeddings = []
    labels = []
    
    for word in words:
        if word in vocab:
            embed = model.get_word_embedding(vocab[word]).detach().numpy()
            embeddings.append(embed.squeeze())
            labels.append(word)
    
    embeddings = np.array(embeddings)
    
    # Dimensionality reduction
    # 降维
    if method == 'tsne':
        reducer = TSNE(n_components=2, random_state=42)
    else:
        reducer = PCA(n_components=2)
    
    reduced_embeddings = reducer.fit_transform(embeddings)
    
    # Plot
    # 绘图
    plt.figure(figsize=(12, 8))
    plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1])
    
    for i, label in enumerate(labels):
        plt.annotate(label, (reduced_embeddings[i, 0], reduced_embeddings[i, 1]))
    
    plt.title(f'Word Embeddings Visualization ({method.upper()})')
    plt.xlabel('Dimension 1')
    plt.ylabel('Dimension 2')
    plt.grid(True)
    plt.show()
```

---

## 12.5 Word Embedding with Global Vectors (GloVe) 全局向量词嵌入

### 12.5.1 Skip-Gram with Global Corpus Statistics 带全局语料库统计的跳字模型

**Limitations of word2vec word2vec的局限性**

While word2vec successfully captures semantic relationships, it has several limitations:

虽然word2vec成功捕获了语义关系，但它有几个局限性：

1. **Local Context Only 仅局部上下文**: Only considers immediate context windows, ignoring global statistics
   仅考虑直接上下文窗口，忽略全局统计

2. **Inefficient Training 训练效率低**: Requires multiple passes over the corpus
   需要多次遍历语料库

3. **Suboptimal Use of Statistics 次优统计使用**: Doesn't directly leverage word co-occurrence statistics
   不直接利用词共现统计

**Global Co-occurrence Matrix 全局共现矩阵**

GloVe addresses these issues by first constructing a global word-word co-occurrence matrix X, where X_{ij} represents how often word i appears in the context of word j across the entire corpus.

GloVe通过首先构建全局词-词共现矩阵X来解决这些问题，其中X_{ij}表示词i在整个语料库中出现在词j上下文中的频率。

**Mathematical Foundation 数学基础：**

The key insight is that ratios of co-occurrence probabilities encode semantic relationships better than raw probabilities.

关键洞察是共现概率的比率比原始概率更好地编码语义关系。

Consider three words: ice, steam, and solid. The ratio P(solid|ice)/P(solid|steam) should be large since "solid" relates more to "ice" than to "steam".

考虑三个词：ice、steam和solid。比率P(solid|ice)/P(solid|steam)应该很大，因为"solid"与"ice"的关系比与"steam"的关系更密切。

### 12.5.2 The GloVe Model GloVe模型

**Model Formulation 模型公式化**

GloVe aims to learn embeddings such that the dot product of two word vectors equals the logarithm of their co-occurrence probability:

GloVe旨在学习嵌入，使得两个词向量的点积等于它们共现概率的对数：

```
w_i^T w_j + b_i + b_j = log(X_{ij})
```

Where:
- w_i, w_j: word embeddings for words i and j 词i和j的词嵌入
- b_i, b_j: bias terms for words i and j 词i和j的偏置项
- X_{ij}: co-occurrence count 共现计数

**Loss Function 损失函数：**

The GloVe objective function incorporates a weighting function to handle the varying reliability of co-occurrence statistics:

GloVe目标函数包含一个权重函数来处理共现统计的不同可靠性：

```
J = Σ_{i,j=1}^V f(X_{ij})(w_i^T w_j + b_i + b_j - log(X_{ij}))^2
```

Where f(X_{ij}) is a weighting function:

其中f(X_{ij})是权重函数：

```
f(x) = {
    (x/x_max)^α  if x < x_max
    1            if x ≥ x_max
}
```

Typically, x_max = 100 and α = 0.75.

**Implementation Example 实现示例：**

```python
import torch
import torch.nn as nn
import numpy as np
from collections import defaultdict
import scipy.sparse as sp

class GloVeModel(nn.Module):
    def __init__(self, vocab_size, embed_size):
        super(GloVeModel, self).__init__()
        
        # Word embeddings (main vectors)
        # 词嵌入（主向量）
        self.word_embeddings = nn.Embedding(vocab_size, embed_size)
        
        # Context embeddings  
        # 上下文嵌入
        self.context_embeddings = nn.Embedding(vocab_size, embed_size)
        
        # Bias terms
        # 偏置项
        self.word_biases = nn.Embedding(vocab_size, 1)
        self.context_biases = nn.Embedding(vocab_size, 1)
        
        # Initialize parameters
        # 初始化参数
        self.init_embeddings()
    
    def init_embeddings(self):
        """Initialize embeddings with small random values"""
        # 用小随机值初始化嵌入
        initrange = 0.5 / self.word_embeddings.embedding_dim
        
        self.word_embeddings.weight.data.uniform_(-initrange, initrange)
        self.context_embeddings.weight.data.uniform_(-initrange, initrange)
        self.word_biases.weight.data.uniform_(-initrange, initrange)
        self.context_biases.weight.data.uniform_(-initrange, initrange)
    
    def forward(self, word_indices, context_indices, cooccurrence_values):
        """Forward pass for GloVe"""
        # GloVe前向传播
        
        # Get embeddings and biases
        # 获取嵌入和偏置
        word_embeds = self.word_embeddings(word_indices)
        context_embeds = self.context_embeddings(context_indices)
        word_bias = self.word_biases(word_indices).squeeze()
        context_bias = self.context_biases(context_indices).squeeze()
        
        # Compute dot products
        # 计算点积
        dot_products = torch.sum(word_embeds * context_embeds, dim=1)
        
        # Compute predictions
        # 计算预测
        predictions = dot_products + word_bias + context_bias
        
        # Compute loss with weighting
        # 计算带权重的损失
        log_cooccurrence = torch.log(cooccurrence_values)
        weights = self.compute_weights(cooccurrence_values)
        
        loss = torch.sum(weights * (predictions - log_cooccurrence) ** 2)
        
        return loss / len(word_indices)
    
    def compute_weights(self, cooccurrence_values, x_max=100, alpha=0.75):
        """Compute weighting function f(x)"""
        # 计算权重函数f(x)
        weights = torch.pow(cooccurrence_values / x_max, alpha)
        weights = torch.clamp(weights, max=1.0)
        return weights
```

**Co-occurrence Matrix Construction 共现矩阵构建：**

```python
def build_cooccurrence_matrix(tokens, vocab, window_size=5):
    """Build global co-occurrence matrix"""
    # 构建全局共现矩阵
    cooccurrence = defaultdict(lambda: defaultdict(float))
    
    for i, center_word in enumerate(tokens):
        if center_word not in vocab:
            continue
            
        center_idx = vocab[center_word]
        
        # Define context window
        # 定义上下文窗口
        start_idx = max(0, i - window_size)
        end_idx = min(len(tokens), i + window_size + 1)
        
        for j in range(start_idx, end_idx):
            if i != j and tokens[j] in vocab:
                context_idx = vocab[tokens[j]]
                distance = abs(i - j)
                
                # Weight by inverse distance
                # 按逆距离加权
                weight = 1.0 / distance
                cooccurrence[center_idx][context_idx] += weight
    
    return cooccurrence

def cooccurrence_to_sparse(cooccurrence_dict, vocab_size):
    """Convert co-occurrence dict to sparse matrix"""
    # 将共现字典转换为稀疏矩阵
    row_indices = []
    col_indices = []
    values = []
    
    for i, context_dict in cooccurrence_dict.items():
        for j, value in context_dict.items():
            if value > 0:
                row_indices.append(i)
                col_indices.append(j)
                values.append(value)
    
    cooccurrence_matrix = sp.coo_matrix(
        (values, (row_indices, col_indices)),
        shape=(vocab_size, vocab_size)
    )
    
    return cooccurrence_matrix
```

### 12.5.3 Interpreting GloVe from the Ratio of Co-occurrence Probabilities 从共现概率比率解释GloVe

**Theoretical Foundation 理论基础**

GloVe's design is motivated by the observation that ratios of co-occurrence probabilities can encode semantic relationships more effectively than absolute probabilities.

GloVe的设计基于这样的观察：共现概率的比率比绝对概率更有效地编码语义关系。

**Mathematical Derivation 数学推导：**

Consider the ratio of probabilities:

考虑概率比率：

```
P(k|i) / P(k|j) = (X_{ik} / X_i) / (X_{jk} / X_j) = (X_{ik} * X_j) / (X_{jk} * X_i)
```

Where X_i = Σ_k X_{ik} is the total count for word i.

其中X_i = Σ_k X_{ik}是词i的总计数。

**Key Insights 关键洞察：**

1. **Discriminative Ratios 判别性比率**: When word k is related to word i but not j, the ratio P(k|i)/P(k|j) >> 1
   当词k与词i相关但与词j无关时，比率P(k|i)/P(k|j) >> 1

2. **Neutral Ratios 中性比率**: When word k is equally related (or unrelated) to both words, the ratio ≈ 1
   当词k与两个词equally相关（或无关）时，比率≈1

3. **Inverse Relationships 逆关系**: When word k is related to word j but not i, the ratio P(k|i)/P(k|j) << 1
   当词k与词j相关但与词i无关时，比率P(k|i)/P(k|j) << 1

**Example Analysis 示例分析：**

| Probability | ice | steam | Ratio |
|-------------|-----|-------|-------|
| P(solid\|·) | 1.9×10⁻⁴ | 2.2×10⁻⁵ | 8.9 |
| P(gas\|·) | 6.6×10⁻⁵ | 7.8×10⁻⁴ | 0.085 |
| P(water\|·) | 3.0×10⁻³ | 2.2×10⁻³ | 1.36 |
| P(fashion\|·) | 1.7×10⁻⁵ | 1.8×10⁻⁵ | 0.96 |

This table shows how probability ratios effectively capture semantic relationships:
- "solid" is strongly associated with "ice" (ratio = 8.9)
- "gas" is strongly associated with "steam" (ratio = 0.085)
- "water" and "fashion" are neutral (ratios ≈ 1)

这个表格显示了概率比率如何有效捕获语义关系。

---

## 12.6 Subword Embedding 子词嵌入

### 12.6.1 The fastText Model fastText模型

**Motivation for Subword Embeddings 子词嵌入的动机**

Traditional word-level embeddings face several challenges:

传统词级嵌入面临几个挑战：

1. **Out-of-Vocabulary (OOV) Words 词汇表外词汇**: Cannot handle words not seen during training
   无法处理训练期间未见过的词

2. **Morphological Richness 形态丰富性**: Ignores internal word structure and morphological relationships
   忽略词内部结构和形态关系

3. **Rare Word Problem 罕见词问题**: Insufficient training data for rare words leads to poor representations
   罕见词训练数据不足导致表示质量差

**fastText Innovation fastText创新**

fastText extends the Skip-gram model by representing each word as a bag of character n-grams. This allows the model to:

fastText通过将每个词表示为字符n-gram的集合来扩展跳字模型。这允许模型：

- Generate embeddings for unseen words 为未见过的词生成嵌入
- Capture morphological similarities 捕获形态相似性  
- Share information between related words 在相关词之间共享信息

**Mathematical Formulation 数学公式化：**

For a word w, fastText represents it as the sum of embeddings of its character n-grams:

对于词w，fastText将其表示为字符n-gram嵌入的和：

```
v_w = Σ_{g∈G_w} z_g
```

Where:
- G_w: set of character n-grams for word w 词w的字符n-gram集合
- z_g: embedding vector for n-gram g n-gram g的嵌入向量

**Implementation Example 实现示例：**

```python
class FastTextModel(nn.Module):
    def __init__(self, vocab_size, embed_size, min_n=3, max_n=6):
        super(FastTextModel, self).__init__()
        
        self.embed_size = embed_size
        self.min_n = min_n
        self.max_n = max_n
        
        # Word embeddings
        # 词嵌入
        self.word_embeddings = nn.Embedding(vocab_size, embed_size)
        
        # Character n-gram embeddings
        # 字符n-gram嵌入
        self.ngram_embeddings = nn.ModuleDict()
        
        # Initialize with reasonable hash table size
        # 用合理的哈希表大小初始化
        for n in range(min_n, max_n + 1):
            # Use hash table to map n-grams to indices
            # 使用哈希表将n-gram映射到索引
            hash_size = 2000000  # 2M buckets
            self.ngram_embeddings[str(n)] = nn.Embedding(hash_size, embed_size)
    
    def get_ngrams(self, word):
        """Extract character n-grams from word"""
        # 从词中提取字符n-gram
        word = f"<{word}>"  # Add boundary markers
        ngrams = []
        
        for n in range(self.min_n, self.max_n + 1):
            for i in range(len(word) - n + 1):
                ngram = word[i:i+n]
                ngrams.append((ngram, n))
        
        return ngrams
    
    def hash_ngram(self, ngram, hash_size=2000000):
        """Hash n-gram to index"""
        # 将n-gram哈希到索引
        return hash(ngram) % hash_size
    
    def get_word_embedding(self, word, word_idx=None):
        """Get embedding for word using subword information"""
        # 使用子词信息获取词嵌入
        
        # Start with word embedding if available
        # 如果可用，从词嵌入开始
        if word_idx is not None:
            word_embed = self.word_embeddings(torch.tensor([word_idx]))
        else:
            word_embed = torch.zeros(1, self.embed_size)
        
        # Add n-gram embeddings
        # 添加n-gram嵌入
        ngrams = self.get_ngrams(word)
        
        for ngram, n in ngrams:
            ngram_idx = self.hash_ngram(ngram)
            ngram_embed = self.ngram_embeddings[str(n)](torch.tensor([ngram_idx]))
            word_embed += ngram_embed
        
        # Average the embeddings
        # 平均嵌入
        word_embed /= (len(ngrams) + 1)  # +1 for word embedding
        
        return word_embed
    
    def forward(self, center_words, context_words, labels):
        """Forward pass for fastText"""
        # fastText前向传播
        batch_size = center_words.size(0)
        
        # Get center word embeddings using subword info
        # 使用子词信息获取中心词嵌入
        center_embeds = []
        for i in range(batch_size):
            center_word_idx = center_words[i].item()
            # Here you'd need to map index back to word
            # 这里需要将索引映射回词
            center_embed = self.get_word_embedding("example", center_word_idx)
            center_embeds.append(center_embed)
        
        center_embeds = torch.cat(center_embeds, dim=0)
        
        # Get context embeddings (simplified)
        # 获取上下文嵌入（简化）
        context_embeds = self.word_embeddings(context_words)
        
        # Compute scores and loss
        # 计算分数和损失
        scores = torch.sum(center_embeds * context_embeds, dim=1)
        predictions = torch.sigmoid(scores)
        loss = F.binary_cross_entropy(predictions, labels.float())
        
        return loss
```

**Character N-gram Extraction 字符N-gram提取：**

```python
def extract_character_ngrams(word, min_n=3, max_n=6):
    """Extract all character n-grams from a word"""
    # 从词中提取所有字符n-gram
    
    # Add boundary markers
    # 添加边界标记
    word = f"<{word}>"
    
    ngrams = set()
    
    for n in range(min_n, max_n + 1):
        for i in range(len(word) - n + 1):
            ngram = word[i:i+n]
            ngrams.add(ngram)
    
    return list(ngrams)

# Example usage
# 使用示例
word = "hello"
ngrams = extract_character_ngrams(word)
print(f"N-grams for '{word}': {ngrams}")
# Output: ['<he', 'hel', 'ell', 'llo', 'lo>', '<hel', 'hell', 'ello', 'llo>', '<hell', 'hello', 'ello>', '<hello>']
```

**Benefits of fastText fastText的优势：**

1. **OOV Handling OOV处理**: Can generate reasonable embeddings for unseen words
   可以为未见过的词生成合理的嵌入

2. **Morphological Awareness 形态感知**: Captures similarities between morphologically related words
   捕获形态相关词之间的相似性

3. **Better Rare Word Representations 更好的罕见词表示**: Subword information helps with sparse data
   子词信息有助于处理稀疏数据

### 12.6.2 Byte Pair Encoding (BPE) 字节对编码

**The Segmentation Problem 分割问题**

How do we optimally segment words into subwords? BPE provides a data-driven approach to learn subword vocabularies.

我们如何最优地将词分割为子词？BPE提供了一种数据驱动的方法来学习子词词汇表。

**BPE Algorithm BPE算法**

BPE iteratively merges the most frequent pair of consecutive symbols:

BPE迭代地合并最频繁的连续符号对：

1. **Initialize 初始化**: Start with characters as basic units
   从字符作为基本单位开始

2. **Count Pairs 计数对**: Count frequency of all adjacent symbol pairs
   计算所有相邻符号对的频率

3. **Merge Most Frequent 合并最频繁**: Merge the most frequent pair
   合并最频繁的对

4. **Repeat 重复**: Continue until desired vocabulary size is reached
   继续直到达到所需的词汇表大小

**Implementation Example 实现示例：**

```python
from collections import defaultdict, Counter
import re

class BPETokenizer:
    def __init__(self, vocab_size=10000):
        self.vocab_size = vocab_size
        self.word_freqs = {}
        self.splits = {}
        self.merges = {}
    
    def pre_tokenize(self, text):
        """Basic pre-tokenization (split on whitespace and punctuation)"""
        # 基本预分词（按空格和标点分割）
        words = re.findall(r'\w+|[^\w\s]', text.lower())
        return words
    
    def get_word_freqs(self, corpus):
        """Get word frequencies from corpus"""
        # 从语料库获取词频
        word_freqs = Counter()
        for text in corpus:
            words = self.pre_tokenize(text)
            word_freqs.update(words)
        return word_freqs
    
    def get_splits(self, word_freqs):
        """Initialize splits with character-level tokenization"""
        # 用字符级分词初始化分割
        splits = {}
        for word, freq in word_freqs.items():
            splits[word] = [c for c in word]
        return splits
    
    def compute_pair_freqs(self, splits, word_freqs):
        """Compute frequencies of all adjacent pairs"""
        # 计算所有相邻对的频率
        pair_freqs = defaultdict(int)
        
        for word, freq in word_freqs.items():
            split = splits[word]
            if len(split) == 1:
                continue
                
            for i in range(len(split) - 1):
                pair = (split[i], split[i + 1])
                pair_freqs[pair] += freq
        
        return pair_freqs
    
    def merge_pair(self, a, b, splits):
        """Merge all occurrences of pair (a, b)"""
        # 合并所有出现的对(a, b)
        new_splits = {}
        
        for word, split in splits.items():
            new_split = []
            i = 0
            
            while i < len(split):
                if (i < len(split) - 1 and 
                    split[i] == a and split[i + 1] == b):
                    new_split.append(a + b)
                    i += 2
                else:
                    new_split.append(split[i])
                    i += 1
            
            new_splits[word] = new_split
        
        return new_splits
    
    def train(self, corpus):
        """Train BPE on corpus"""
        # 在语料库上训练BPE
        
        # Get word frequencies
        # 获取词频
        self.word_freqs = self.get_word_freqs(corpus)
        
        # Initialize splits
        # 初始化分割
        self.splits = self.get_splits(self.word_freqs)
        
        # Get initial vocabulary
        # 获取初始词汇表
        vocab = set()
        for word in self.word_freqs:
            vocab.update(word)
        
        # Perform merges
        # 执行合并
        num_merges = self.vocab_size - len(vocab)
        
        for i in range(num_merges):
            pair_freqs = self.compute_pair_freqs(self.splits, self.word_freqs)
            
            if not pair_freqs:
                break
            
            # Find most frequent pair
            # 找到最频繁的对
            best_pair = max(pair_freqs, key=pair_freqs.get)
            
            # Merge the pair
            # 合并对
            self.splits = self.merge_pair(best_pair[0], best_pair[1], self.splits)
            
            # Record the merge
            # 记录合并
            self.merges[best_pair] = best_pair[0] + best_pair[1]
            
            print(f"Merge {i+1}: {best_pair} -> {best_pair[0] + best_pair[1]}")
    
    def tokenize(self, word):
        """Tokenize a word using learned BPE"""
        # 使用学习的BPE对词进行分词
        
        # Start with character split
        # 从字符分割开始
        split = [c for c in word]
        
        # Apply merges in order
        # 按顺序应用合并
        for pair, merged in self.merges.items():
            new_split = []
            i = 0
            
            while i < len(split):
                if (i < len(split) - 1 and 
                    split[i] == pair[0] and split[i + 1] == pair[1]):
                    new_split.append(merged)
                    i += 2
                else:
                    new_split.append(split[i])
                    i += 1
            
            split = new_split
        
        return split

# Example usage
# 使用示例
corpus = [
    "low lower newest widest",
    "low lower newest widest", 
    "low lower newest widest"
]

bpe = BPETokenizer(vocab_size=20)
bpe.train(corpus)

# Tokenize new words
# 对新词进行分词
print("Tokenization results:")
test_words = ["lower", "lowest", "newer", "wider"]
for word in test_words:
    tokens = bpe.tokenize(word)
    print(f"{word} -> {tokens}")
```

**Advantages of BPE BPE的优势：**

1. **Data-Driven 数据驱动**: Learns optimal segmentation from data
   从数据中学习最优分割

2. **Flexible Vocabulary Size 灵活的词汇表大小**: Can control vocabulary size precisely
   可以精确控制词汇表大小

3. **Language Agnostic 语言无关**: Works across different languages and scripts
   适用于不同语言和文字

4. **Handles Rare Words 处理罕见词**: Better coverage of rare and compound words
   更好地覆盖罕见词和复合词

---

## 12.7 Word Similarity and Analogy 词相似性与类比

### 12.7.1 Loading Pretrained Word Vectors 加载预训练词向量

Pretrained word vectors provide a starting point for many NLP tasks, offering rich semantic representations learned from large text corpora.

预训练词向量为许多NLP任务提供了起点，提供了从大型文本语料库中学习到的丰富语义表示。

**Popular Pretrained Word Vectors 流行的预训练词向量：**

1. **Word2Vec**: Trained on Google News dataset (3 billion words)
   在Google News数据集上训练（30亿词）

2. **GloVe**: Trained on Common Crawl, Wikipedia, and Twitter data
   在Common Crawl、Wikipedia和Twitter数据上训练

3. **FastText**: Supports subword information, covers many languages
   支持子词信息，覆盖多种语言

**Loading Word Vectors Example 加载词向量示例：**

```python
import torch
import numpy as np
from collections import defaultdict
import requests
import zipfile
import os

class PretrainedWordVectors:
    def __init__(self, embedding_file):
        self.word_to_idx = {}
        self.idx_to_word = {}
        self.embeddings = []
        self.load_embeddings(embedding_file)
    
    def load_embeddings(self, embedding_file):
        """Load pretrained word vectors from file"""
        # 从文件加载预训练词向量
        print(f"Loading embeddings from {embedding_file}...")
        
        with open(embedding_file, 'r', encoding='utf-8') as f:
            for idx, line in enumerate(f):
                parts = line.strip().split()
                word = parts[0]
                vector = np.array([float(x) for x in parts[1:]])
                
                self.word_to_idx[word] = idx
                self.idx_to_word[idx] = word
                self.embeddings.append(vector)
        
        self.embeddings = np.array(self.embeddings)
        self.embedding_dim = self.embeddings.shape[1]
        print(f"Loaded {len(self.embeddings)} word vectors with dimension {self.embedding_dim}")
    
    def get_vector(self, word):
        """Get vector for a specific word"""
        # 获取特定词的向量
        if word in self.word_to_idx:
            idx = self.word_to_idx[word]
            return self.embeddings[idx]
        else:
            return None
    
    def download_glove(self, url="http://nlp.stanford.edu/data/glove.6B.zip"):
        """Download GloVe vectors"""
        # 下载GloVe向量
        if not os.path.exists("glove.6B.50d.txt"):
            print("Downloading GloVe vectors...")
            response = requests.get(url)
            with open("glove.6B.zip", "wb") as f:
                f.write(response.content)
            
            with zipfile.ZipFile("glove.6B.zip", 'r') as zip_ref:
                zip_ref.extractall()
            
            print("GloVe vectors downloaded and extracted.")

# Usage example 使用示例
# word_vectors = PretrainedWordVectors("glove.6B.50d.txt")
```

**Real-World Example with Simplified Data 简化数据的实际例子：**

```python
# Simulate loading word vectors 模拟加载词向量
class SimpleWordVectors:
    def __init__(self):
        # Simplified word vectors for demonstration
        # 简化的词向量用于演示
        self.word_vectors = {
            'king': np.array([0.1, 0.3, -0.5, 0.7, 0.2]),
            'queen': np.array([0.2, 0.4, -0.4, 0.6, 0.3]),
            'man': np.array([0.0, 0.2, -0.3, 0.5, 0.1]),
            'woman': np.array([0.1, 0.3, -0.2, 0.4, 0.2]),
            'prince': np.array([0.05, 0.25, -0.45, 0.65, 0.15]),
            'princess': np.array([0.15, 0.35, -0.35, 0.55, 0.25]),
            'dog': np.array([0.8, 0.1, 0.2, -0.3, 0.9]),
            'cat': np.array([0.7, 0.2, 0.3, -0.2, 0.8]),
            'puppy': np.array([0.75, 0.15, 0.25, -0.25, 0.85]),
            'kitten': np.array([0.65, 0.25, 0.35, -0.15, 0.75])
        }
        self.embedding_dim = 5
    
    def get_vector(self, word):
        return self.word_vectors.get(word, None)
    
    def get_vocabulary(self):
        return list(self.word_vectors.keys())

# Initialize simple word vectors 初始化简单词向量
simple_vectors = SimpleWordVectors()
```

### 12.7.2 Applying Pretrained Word Vectors 应用预训练词向量

Once loaded, pretrained word vectors can be used for various tasks including similarity computation, analogy solving, and as initialization for downstream models.

加载后，预训练词向量可用于各种任务，包括相似性计算、类比求解以及作为下游模型的初始化。

**Word Similarity Computation 词相似性计算：**

```python
import numpy as np
from scipy.spatial.distance import cosine

class WordSimilarity:
    def __init__(self, word_vectors):
        self.word_vectors = word_vectors
    
    def cosine_similarity(self, word1, word2):
        """Compute cosine similarity between two words"""
        # 计算两个词之间的余弦相似性
        vec1 = self.word_vectors.get_vector(word1)
        vec2 = self.word_vectors.get_vector(word2)
        
        if vec1 is None or vec2 is None:
            return None
        
        # Cosine similarity = 1 - cosine distance
        # 余弦相似性 = 1 - 余弦距离
        similarity = 1 - cosine(vec1, vec2)
        return similarity
    
    def find_most_similar(self, target_word, n=5, exclude_self=True):
        """Find the most similar words to target word"""
        # 找到与目标词最相似的词
        target_vec = self.word_vectors.get_vector(target_word)
        if target_vec is None:
            return []
        
        similarities = []
        for word in self.word_vectors.get_vocabulary():
            if exclude_self and word == target_word:
                continue
            
            similarity = self.cosine_similarity(target_word, word)
            if similarity is not None:
                similarities.append((word, similarity))
        
        # Sort by similarity
        # 按相似性排序
        similarities.sort(key=lambda x: x[1], reverse=True)
        
        return similarities[:n]
    
    def word_analogy(self, word_a, word_b, word_c, n=1):
        """Solve analogy: word_a is to word_b as word_c is to ?"""
        # 解决类比：word_a之于word_b如同word_c之于？
        
        # Get embeddings
        # 获取嵌入
        embed_a = self.word_vectors.get_vector(vocab[word_a])
        embed_b = self.word_vectors.get_vector(vocab[word_b])  
        embed_c = self.word_vectors.get_vector(vocab[word_c])
        
        # Compute target embedding: embed_b - embed_a + embed_c
        # 计算目标嵌入：embed_b - embed_a + embed_c
        target_embedding = embed_b - embed_a + embed_c
        
        # Find closest word
        # 找到最近的词
        best_similarity = -1
        best_word = None
        
        for word, idx in vocab.items():
            if word not in [word_a, word_b, word_c]:
                word_embedding = self.word_vectors.get_vector(idx)
                similarity = F.cosine_similarity(target_embedding, word_embedding)
            
                if similarity > best_similarity:
                    best_similarity = similarity
                    best_word = word
        
        return best_word, best_similarity.item()

# Example: king - man + woman = ?
# 示例：king - man + woman = ?
result, score = solve_analogy(model, vocab, "king", "man", "woman")
print(f"king - man + woman = {result} (score: {score:.3f})")
```

**Visualization 可视化：**

```python
def visualize_embeddings(model, vocab, words, method='tsne'):
    """Visualize word embeddings in 2D"""
    # 在2D中可视化词嵌入
    import matplotlib.pyplot as plt
    from sklearn.manifold import TSNE
    from sklearn.decomposition import PCA
    
    # Get embeddings for selected words
    # 获取选定词的嵌入
    embeddings = []
    labels = []
    
    for word in words:
        if word in vocab:
            embed = model.get_word_embedding(vocab[word]).detach().numpy()
            embeddings.append(embed.squeeze())
            labels.append(word)
    
    embeddings = np.array(embeddings)
    
    # Dimensionality reduction
    # 降维
    if method == 'tsne':
        reducer = TSNE(n_components=2, random_state=42)
    else:
        reducer = PCA(n_components=2)
    
    reduced_embeddings = reducer.fit_transform(embeddings)
    
    # Plot
    # 绘图
    plt.figure(figsize=(12, 8))
    plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1])
    
    for i, label in enumerate(labels):
        plt.annotate(label, (reduced_embeddings[i, 0], reduced_embeddings[i, 1]))
    
    plt.title(f'Word Embeddings Visualization ({method.upper()})')
    plt.xlabel('Dimension 1')
    plt.ylabel('Dimension 2')
    plt.grid(True)
    plt.show()
```

---

## 12.8 Bidirectional Encoder Representations from Transformers (BERT) 双向编码器表示

### 12.8.1 From Context-Independent to Context-Sensitive 从上下文无关到上下文敏感

Traditional word embeddings like Word2Vec and GloVe provide fixed representations for words, regardless of their context. This creates limitations when dealing with polysemous words (words with multiple meanings).

传统的词嵌入如Word2Vec和GloVe为词提供固定的表示，无论其上下文如何。这在处理多义词（具有多重含义的词）时产生了限制。

**Problems with Context-Independent Embeddings 上下文无关嵌入的问题：**

1. **Polysemy 多义性**: Words like "bank" can mean financial institution or river bank
   像"bank"这样的词可以表示金融机构或河岸

2. **Fixed Representations 固定表示**: Same word always has the same vector
   同一个词总是有相同的向量

3. **Limited Contextual Understanding 有限的上下文理解**: Cannot capture meaning variations
   无法捕获含义变化

**Example of Polysemy 多义性示例：**

```python
# Traditional embeddings give same vector for "bank" in both contexts
# 传统嵌入在两种上下文中为"bank"提供相同的向量

sentence1 = "I went to the bank to withdraw money"  # Financial institution 金融机构
sentence2 = "We sat by the river bank"  # River side 河岸

# Word2Vec/GloVe: bank → [0.1, 0.3, -0.2, ...] (same vector in both cases)
# Word2Vec/GloVe: bank → [0.1, 0.3, -0.2, ...] (两种情况下相同的向量)
```

**Context-Sensitive Solution 上下文敏感解决方案：**

Context-sensitive embeddings generate different representations for the same word based on its surrounding context.

上下文敏感嵌入根据周围上下文为同一个词生成不同的表示。

```python
# Context-sensitive embeddings (like BERT) provide different vectors
# 上下文敏感嵌入（如BERT）提供不同的向量

# "bank" in financial context → [0.1, 0.3, -0.2, 0.8, ...]
# "bank" in river context → [0.4, -0.1, 0.6, -0.3, ...]
```

### 12.8.2 From Task-Specific to Task-Agnostic 从任务特定到任务无关

Early NLP models were designed for specific tasks, requiring separate architectures and training for each application. This approach was inefficient and limited knowledge transfer.

早期的NLP模型是为特定任务设计的，每个应用都需要单独的架构和训练。这种方法效率低下且限制知识转移。

**Evolution of NLP Models NLP模型的演变：**

1. **Task-Specific Models 任务特定模型** (Pre-2018):
   - Separate models for each task (每个任务都有单独的模型)
   - Limited knowledge sharing (有限的知识共享)
   - High development cost (高开发成本)

2. **Pre-trained Language Models 预训练语言模型** (2018+):
   - Single model for multiple tasks (多任务的单一模型)
   - Transfer learning approach (迁移学习方法)
   - Efficient knowledge reuse (高效的知识重用)

**Task-Agnostic Benefits 任务无关的优势：**

- **Efficiency 效率**: Train once, use for many tasks (训练一次，用于多个任务)
- **Performance 性能**: Better results with less task-specific data (用更少的任务特定数据获得更好的结果)
- **Consistency 一致性**: Uniform approach across different NLP applications (不同NLP应用中的统一方法)

### 12.8.3 BERT: Combining the Best of Both Worlds BERT：结合两个世界的优势

BERT (Bidirectional Encoder Representations from Transformers) revolutionized NLP by combining context-sensitive representations with task-agnostic pretraining.

BERT（双向编码器表示）通过结合上下文敏感表示和任务无关预训练彻底改变了NLP。

**Key Innovations 关键创新：**

1. **Bidirectional Context 双向上下文**: Considers both left and right context simultaneously
   同时考虑左右上下文

2. **Masked Language Modeling 掩码语言建模**: Predicts masked words using bidirectional context
   使用双向上下文预测掩码词

3. **Transfer Learning 迁移学习**: Pretrain on large corpus, fine-tune for specific tasks
   在大型语料库上预训练，为特定任务微调

**BERT Architecture Overview BERT架构概述：**

```python
import torch
import torch.nn as nn
import math

class BERTConfig:
    def __init__(self):
        self.vocab_size = 30522  # WordPiece vocabulary size WordPiece词汇表大小
        self.hidden_size = 768   # Hidden dimension 隐藏维度
        self.num_attention_heads = 12  # Number of attention heads 注意力头数
        self.num_hidden_layers = 12    # Number of transformer layers Transformer层数
        self.intermediate_size = 3072  # Feed-forward network size 前馈网络大小
        self.max_position_embeddings = 512  # Maximum sequence length 最大序列长度
        self.type_vocab_size = 2  # Number of token types (for segment embeddings) 标记类型数（用于分段嵌入）
        self.dropout_prob = 0.1   # Dropout probability Dropout概率

class MultiHeadAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.num_attention_heads = config.num_attention_heads
        self.attention_head_size = config.hidden_size // config.num_attention_heads
        self.all_head_size = self.num_attention_heads * self.attention_head_size
        
        self.query = nn.Linear(config.hidden_size, self.all_head_size)
        self.key = nn.Linear(config.hidden_size, self.all_head_size)
        self.value = nn.Linear(config.hidden_size, self.all_head_size)
        
        self.dropout = nn.Dropout(config.dropout_prob)
        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
    
    def transpose_for_scores(self, x):
        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
        x = x.view(*new_x_shape)
        return x.permute(0, 2, 1, 3)
    
    def forward(self, hidden_states, attention_mask=None):
        mixed_query_layer = self.query(hidden_states)
        mixed_key_layer = self.key(hidden_states)
        mixed_value_layer = self.value(hidden_states)
        
        query_layer = self.transpose_for_scores(mixed_query_layer)
        key_layer = self.transpose_for_scores(mixed_key_layer)
        value_layer = self.transpose_for_scores(mixed_value_layer)
        
        # Compute attention scores 计算注意力分数
        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
        attention_scores = attention_scores / math.sqrt(self.attention_head_size)
        
        if attention_mask is not None:
            attention_scores = attention_scores + attention_mask
        
        attention_probs = nn.Softmax(dim=-1)(