# Handwritten Digit Recognition Project

æ‰‹å†™æ•°å­—è¯†åˆ«é¡¹ç›®

## Project Overview

é¡¹ç›®æ¦‚è¿°

This project demonstrates handwritten digit recognition using Multi-Layer Perceptron (MLP) with backpropagation algorithm. We'll build a neural network from scratch and then compare it with PyTorch implementation.

æœ¬é¡¹ç›®å±•ç¤ºäº†ä½¿ç”¨å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰å’Œåå‘ä¼ æ’­ç®—æ³•è¿›è¡Œæ‰‹å†™æ•°å­—è¯†åˆ«ã€‚æˆ‘ä»¬å°†ä»é›¶å¼€å§‹æ„å»ºç¥ç»ç½‘ç»œï¼Œç„¶åä¸PyTorchå®ç°è¿›è¡Œæ¯”è¾ƒã€‚

## Dataset

æ•°æ®é›†

We use the MNIST dataset which contains:
- **Training set**: 60,000 images (è®­ç»ƒé›†ï¼š60,000å¼ å›¾ç‰‡)
- **Validation set**: 10,000 images (éªŒè¯é›†ï¼š10,000å¼ å›¾ç‰‡) 
- **Test set**: 10,000 images (æµ‹è¯•é›†ï¼š10,000å¼ å›¾ç‰‡)
- **Image size**: 28Ã—28 pixels (å›¾ç‰‡å°ºå¯¸ï¼š28Ã—28åƒç´ )
- **Classes**: 10 digits (0-9) (ç±»åˆ«ï¼š10ä¸ªæ•°å­—0-9)

## Project Structure

é¡¹ç›®ç»“æ„

```
æ‰‹å†™æ•°å­—è¯†åˆ«é¡¹ç›®/
â”œâ”€â”€ README.md                    # Project documentation (é¡¹ç›®æ–‡æ¡£)
â”œâ”€â”€ data/                        # Data directory (æ•°æ®ç›®å½•)
â”‚   â”œâ”€â”€ raw/                     # Raw MNIST data (åŸå§‹MNISTæ•°æ®)
â”‚   â””â”€â”€ processed/               # Processed data (å¤„ç†åæ•°æ®)
â”œâ”€â”€ src/                         # Source code (æºä»£ç )
â”‚   â”œâ”€â”€ data_loader.py          # Data loading utilities (æ•°æ®åŠ è½½å·¥å…·)
â”‚   â”œâ”€â”€ mlp_scratch.py          # MLP from scratch (ä»é›¶å®ç°MLP)
â”‚   â”œâ”€â”€ mlp_pytorch.py          # MLP using PyTorch (ä½¿ç”¨PyTorchçš„MLP)
â”‚   â”œâ”€â”€ train.py                # Training script (è®­ç»ƒè„šæœ¬)
â”‚   â”œâ”€â”€ evaluate.py             # Evaluation script (è¯„ä¼°è„šæœ¬)
â”‚   â””â”€â”€ utils.py                # Utility functions (å·¥å…·å‡½æ•°)
â”œâ”€â”€ notebooks/                   # Jupyter notebooks (Jupyterç¬”è®°æœ¬)
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb      # Data exploration (æ•°æ®æ¢ç´¢)
â”‚   â”œâ”€â”€ 02_mlp_from_scratch.ipynb      # MLP implementation (MLPå®ç°)
â”‚   â””â”€â”€ 03_pytorch_comparison.ipynb    # PyTorch comparison (PyTorchå¯¹æ¯”)
â”œâ”€â”€ models/                      # Saved models (ä¿å­˜çš„æ¨¡å‹)
â”œâ”€â”€ results/                     # Training results (è®­ç»ƒç»“æœ)
â”‚   â”œâ”€â”€ plots/                   # Training plots (è®­ç»ƒå›¾è¡¨)
â”‚   â””â”€â”€ logs/                    # Training logs (è®­ç»ƒæ—¥å¿—)
â””â”€â”€ requirements.txt             # Dependencies (ä¾èµ–é¡¹)
```

## Learning Objectives

å­¦ä¹ ç›®æ ‡

1. **Mathematical Foundation** (æ•°å­¦åŸºç¡€)
   - Understand forward propagation (ç†è§£å‰å‘ä¼ æ’­)
   - Master backpropagation algorithm (æŒæ¡åå‘ä¼ æ’­ç®—æ³•)
   - Learn gradient descent optimization (å­¦ä¹ æ¢¯åº¦ä¸‹é™ä¼˜åŒ–)

2. **Implementation Skills** (å®ç°æŠ€èƒ½)
   - Build neural network from scratch (ä»é›¶æ„å»ºç¥ç»ç½‘ç»œ)
   - Use matrix operations efficiently (é«˜æ•ˆä½¿ç”¨çŸ©é˜µè¿ç®—)
   - Handle data preprocessing (å¤„ç†æ•°æ®é¢„å¤„ç†)

3. **Practical Experience** (å®è·µç»éªŒ)
   - Train and validate models (è®­ç»ƒå’ŒéªŒè¯æ¨¡å‹)
   - Analyze training dynamics (åˆ†æè®­ç»ƒåŠ¨æ€)
   - Compare different implementations (æ¯”è¾ƒä¸åŒå®ç°)

## Getting Started

å¼€å§‹ä½¿ç”¨

1. **Install Dependencies** (å®‰è£…ä¾èµ–)
   ```bash
   pip install -r requirements.txt
   ```

2. **Download Data** (ä¸‹è½½æ•°æ®)
   ```bash
   python src/data_loader.py
   ```

3. **Train Model** (è®­ç»ƒæ¨¡å‹)
   ```bash
   # Train from scratch implementation
   python src/train.py --model scratch
   
   # Train PyTorch implementation
   python src/train.py --model pytorch
   ```

4. **Evaluate Model** (è¯„ä¼°æ¨¡å‹)
   ```bash
   python src/evaluate.py --model_path models/best_model.pkl
   ```

## Key Concepts Covered

æ¶µç›–çš„å…³é”®æ¦‚å¿µ

### 1. Neural Network Architecture

ç¥ç»ç½‘ç»œæ¶æ„

- **Input Layer**: 784 neurons (28Ã—28 flattened) (è¾“å…¥å±‚ï¼š784ä¸ªç¥ç»å…ƒ)
- **Hidden Layer 1**: 128 neurons with ReLU activation (éšè—å±‚1ï¼š128ä¸ªç¥ç»å…ƒï¼ŒReLUæ¿€æ´»)
- **Hidden Layer 2**: 64 neurons with ReLU activation (éšè—å±‚2ï¼š64ä¸ªç¥ç»å…ƒï¼ŒReLUæ¿€æ´»)
- **Output Layer**: 10 neurons with Softmax activation (è¾“å‡ºå±‚ï¼š10ä¸ªç¥ç»å…ƒSoftmaxæ¿€æ´»)

### 2. Mathematical Components

æ•°å­¦ç»„ä»¶

- **Activation Functions**: ReLU, Sigmoid, Softmax (æ¿€æ´»å‡½æ•°)
- **Loss Function**: Cross-entropy loss (æŸå¤±å‡½æ•°ï¼šäº¤å‰ç†µæŸå¤±)
- **Optimization**: Stochastic Gradient Descent (ä¼˜åŒ–ï¼šéšæœºæ¢¯åº¦ä¸‹é™)
- **Regularization**: L2 regularization, Dropout (æ­£åˆ™åŒ–ï¼šL2æ­£åˆ™åŒ–ï¼ŒDropout)

### 3. Training Process

è®­ç»ƒè¿‡ç¨‹

- **Forward Pass**: Compute predictions (å‰å‘ä¼ æ’­ï¼šè®¡ç®—é¢„æµ‹)
- **Loss Calculation**: Measure prediction error (æŸå¤±è®¡ç®—ï¼šæµ‹é‡é¢„æµ‹è¯¯å·®)
- **Backward Pass**: Compute gradients (åå‘ä¼ æ’­ï¼šè®¡ç®—æ¢¯åº¦)
- **Parameter Update**: Apply gradient descent (å‚æ•°æ›´æ–°ï¼šåº”ç”¨æ¢¯åº¦ä¸‹é™)

## Expected Results

é¢„æœŸç»“æœ

- **Training Accuracy**: ~98% (è®­ç»ƒå‡†ç¡®ç‡ï¼šçº¦98%)
- **Validation Accuracy**: ~97% (éªŒè¯å‡†ç¡®ç‡ï¼šçº¦97%)
- **Test Accuracy**: ~96% (æµ‹è¯•å‡†ç¡®ç‡ï¼šçº¦96%)
- **Training Time**: ~10 minutes on CPU (è®­ç»ƒæ—¶é—´ï¼šCPUçº¦10åˆ†é’Ÿ)

## Next Steps

ä¸‹ä¸€æ­¥

After completing this project, you can:

å®Œæˆæ­¤é¡¹ç›®åï¼Œæ‚¨å¯ä»¥ï¼š

1. **Experiment with architectures** (å®éªŒä¸åŒæ¶æ„)
   - Try different layer sizes (å°è¯•ä¸åŒå±‚å¤§å°)
   - Add more hidden layers (æ·»åŠ æ›´å¤šéšè—å±‚)
   - Use different activation functions (ä½¿ç”¨ä¸åŒæ¿€æ´»å‡½æ•°)

2. **Improve performance** (æå‡æ€§èƒ½)
   - Implement batch normalization (å®ç°æ‰¹é‡å½’ä¸€åŒ–)
   - Add learning rate scheduling (æ·»åŠ å­¦ä¹ ç‡è°ƒåº¦)
   - Use advanced optimizers (ä½¿ç”¨é«˜çº§ä¼˜åŒ–å™¨)

3. **Extend to other datasets** (æ‰©å±•åˆ°å…¶ä»–æ•°æ®é›†)
   - Fashion-MNIST
   - CIFAR-10 (requires CNN architecture)
   - Custom digit datasets

## Mathematical Notation Reference

æ•°å­¦ç¬¦å·å‚è€ƒ

For detailed mathematical notation used in this project, refer to:
`../01_Perceptron/æ•°å­¦ç¬¦å·è¯¦è§£ä¸è¯»éŸ³.md`

æœ‰å…³æ­¤é¡¹ç›®ä¸­ä½¿ç”¨çš„è¯¦ç»†æ•°å­¦ç¬¦å·ï¼Œè¯·å‚è€ƒï¼š
`../01_Perceptron/æ•°å­¦ç¬¦å·è¯¦è§£ä¸è¯»éŸ³.md`

## Support

æ”¯æŒ

If you encounter any issues or have questions:

å¦‚æœæ‚¨é‡åˆ°ä»»ä½•é—®é¢˜æˆ–æœ‰ç–‘é—®ï¼š

1. Check the mathematical foundations in `../å¤šå±‚æ„ŸçŸ¥æœºä¸åå‘ä¼ æ’­.md`
2. Review the quiz questions in `../quiz.md`
3. Study the notation guide in `../01_Perceptron/æ•°å­¦ç¬¦å·è¯¦è§£ä¸è¯»éŸ³.md`

---

**Happy Learning! å¿«ä¹å­¦ä¹ ï¼** ğŸš€ 