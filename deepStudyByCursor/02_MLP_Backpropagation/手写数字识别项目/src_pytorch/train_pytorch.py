import os
import sys
import argparse
import time
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import seaborn as sns
import numpy as np

# Import our PyTorch specific modules
# å¯¼å…¥æˆ‘ä»¬PyTorchç‰¹æœ‰çš„æ¨¡å—
from data_loader_pytorch import MNISTDataLoaderPyTorch
from model_pytorch import MLPClassifierPyTorch

def create_results_directory():
    """
    Create results directory structure
    åˆ›å»ºç»“æœç›®å½•ç»“æ„
    """
    results_dir = '../results_pytorch' # New results directory for PyTorch
    subdirs = ['models', 'plots', 'logs']
    
    for subdir in [results_dir] + [os.path.join(results_dir, sub) for sub in subdirs]:
        os.makedirs(subdir, exist_ok=True)
    
    return results_dir

def train_mnist_classifier_pytorch(config):
    """
    Train MLP classifier on MNIST dataset using PyTorch and GPU (if available).
    åœ¨MNISTæ•°æ®é›†ä¸Šä½¿ç”¨PyTorchå’ŒGPUï¼ˆå¦‚æœå¯ç”¨ï¼‰è®­ç»ƒMLPåˆ†ç±»å™¨ã€‚
    
    Args:
        config: Configuration dictionary with training parameters
    """
    print("ğŸš€ Starting MNIST Handwritten Digit Recognition Training (PyTorch)")
    print("ğŸš€ å¼€å§‹MNISTæ‰‹å†™æ•°å­—è¯†åˆ«è®­ç»ƒ (PyTorch)")
    print("=" * 70)
    
    # Set device (è®¾ç½®è®¾å¤‡)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device} (æ­£åœ¨ä½¿ç”¨è®¾å¤‡: {device})")
    print("-" * 50)
    
    # Create results directory (åˆ›å»ºç»“æœç›®å½•)
    results_dir = create_results_directory()
    
    # Initialize data loader (åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨)
    print("ğŸ“ Loading MNIST dataset... (æ­£åœ¨åŠ è½½MNISTæ•°æ®é›†...)")
    data_loader = MNISTDataLoaderPyTorch(
        data_dir='../data',
        batch_size=config['batch_size'],
        validation_split=config['validation_split']
    )
    
    train_loader = data_loader.get_train_loader()
    val_loader = data_loader.get_val_loader()
    test_loader = data_loader.get_test_loader()
    
    print("âœ… Data loaded successfully! (æ•°æ®åŠ è½½æˆåŠŸï¼)")
    print(f"Number of training batches: {len(train_loader)} (è®­ç»ƒæ‰¹æ¬¡æ•°é‡: {len(train_loader)})")
    print(f"Number of validation batches: {len(val_loader)} (éªŒè¯æ‰¹æ¬¡æ•°é‡: {len(val_loader)})")
    print(f"Number of test batches: {len(test_loader)} (æµ‹è¯•æ‰¹æ¬¡æ•°é‡: {len(test_loader)})")
    print(f"Input features: {data_loader.get_num_features()} (è¾“å…¥ç‰¹å¾: {data_loader.get_num_features()})")
    print(f"Number of classes: {data_loader.get_num_classes()} (ç±»åˆ«æ•°: {data_loader.get_num_classes()})")
    print("-" * 50)
    
    # Create model (åˆ›å»ºæ¨¡å‹)
    print("ğŸ§  Creating MLP model... (æ­£åœ¨åˆ›å»ºMLPæ¨¡å‹...)")
    model = MLPClassifierPyTorch(
        layer_sizes=config['layer_sizes'],
        activations=config['activations']
    ).to(device) # Move model to device (å°†æ¨¡å‹ç§»åŠ¨åˆ°è®¾å¤‡)
    
    print(f"Model architecture: {model}")
    print(f"æ¨¡å‹æ¶æ„: {model}")
    print("-" * 50)
    
    # Define loss function and optimizer (å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨)
    criterion = nn.CrossEntropyLoss() # Suitable for multi-class classification (é€‚ç”¨äºå¤šåˆ†ç±»)
    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])
    
    # Store training history (å­˜å‚¨è®­ç»ƒå†å²)
    history = {
        'train_loss': [], 'val_loss': [],
        'train_accuracy': [], 'val_accuracy': []
    }
    
    # Train model (è®­ç»ƒæ¨¡å‹)
    print("ğŸ‹ï¸ Starting training... (å¼€å§‹è®­ç»ƒ...)")
    start_time = time.time()
    
    for epoch in range(1, config['epochs'] + 1):
        # Training phase (è®­ç»ƒé˜¶æ®µ)
        model.train() # Set model to training mode (è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼)
        running_loss = 0.0
        correct_train = 0
        total_train = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device) # Move data to device (å°†æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡)
            
            optimizer.zero_grad() # Zero the gradients (æ¢¯åº¦æ¸…é›¶)
            output = model(data) # Forward pass (å‰å‘ä¼ æ’­)
            loss = criterion(output, target) # Calculate loss (è®¡ç®—æŸå¤±)
            loss.backward() # Backward pass (åå‘ä¼ æ’­)
            optimizer.step() # Update weights (æ›´æ–°æƒé‡)
            
            running_loss += loss.item()
            _, predicted = torch.max(output.data, 1)
            total_train += target.size(0)
            correct_train += (predicted == target).sum().item()

        avg_train_loss = running_loss / len(train_loader)
        train_accuracy = correct_train / total_train
        history['train_loss'].append(avg_train_loss)
        history['train_accuracy'].append(train_accuracy)
        
        # Validation phase (éªŒè¯é˜¶æ®µ)
        model.eval() # Set model to evaluation mode (è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼)
        val_loss = 0.0
        correct_val = 0
        total_val = 0
        
        with torch.no_grad(): # Disable gradient calculation (ç¦ç”¨æ¢¯åº¦è®¡ç®—)
            for data, target in val_loader:
                data, target = data.to(device), target.to(device)
                output = model(data)
                loss = criterion(output, target)
                
                val_loss += loss.item()
                _, predicted = torch.max(output.data, 1)
                total_val += target.size(0)
                correct_val += (predicted == target).sum().item()
        
        avg_val_loss = val_loss / len(val_loader)
        val_accuracy = correct_val / total_val
        history['val_loss'].append(avg_val_loss)
        history['val_accuracy'].append(val_accuracy)
        
        print(f"Epoch {epoch}/{config['epochs']} - "
              f"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f} | "
              f"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}")
    
    training_time = time.time() - start_time
    print(f"â±ï¸ Training completed in {training_time:.2f} seconds")
    print(f"â±ï¸ è®­ç»ƒåœ¨ {training_time:.2f} ç§’å†…å®Œæˆ")
    print("-" * 50)
    
    # Evaluate model on test set (åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹)
    print("ğŸ“Š Evaluating model on test set... (æ­£åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹...)")
    model.eval()
    test_loss = 0.0
    correct_test = 0
    total_test = 0
    all_predictions = []
    all_true_labels = []

    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            loss = criterion(output, target)
            
            test_loss += loss.item()
            _, predicted = torch.max(output.data, 1)
            total_test += target.size(0)
            correct_test += (predicted == target).sum().item()
            
            all_predictions.extend(predicted.cpu().numpy())
            all_true_labels.extend(target.cpu().numpy())

    avg_test_loss = test_loss / len(test_loader)
    test_accuracy = correct_test / total_test
    
    print("ğŸ“ˆ Final Results (æœ€ç»ˆç»“æœ):")
    print(f"  Test       - Loss: {avg_test_loss:.4f}, Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)")
    print("-" * 50)
    
    # Save model (ä¿å­˜æ¨¡å‹)
    model_path = os.path.join(results_dir, 'models', 'mnist_mlp_pytorch.pth')
    torch.save(model.state_dict(), model_path) # Save only the model's state dictionary
    print(f"âœ… Model saved to: {model_path}")
    print(f"âœ… æ¨¡å‹ä¿å­˜åˆ°: {model_path}")
    
    # Plot and save training history (ç»˜åˆ¶å¹¶ä¿å­˜è®­ç»ƒå†å²)
    plot_path = os.path.join(results_dir, 'plots', 'training_history_pytorch.png')
    plot_training_history(history, plot_path)
    
    # Generate detailed analysis (ç”Ÿæˆè¯¦ç»†åˆ†æ)
    generate_analysis_report_pytorch(model, data_loader, results_dir, config, training_time, 
                                     history, all_predictions, all_true_labels, device)
    
    # Visualize predictions (å¯è§†åŒ–é¢„æµ‹)
    visualize_predictions_pytorch(model, test_loader, results_dir, device)
    
    return model, history

def plot_training_history(history, save_path):
    """
    Plot and save training history (loss and accuracy).
    ç»˜åˆ¶å¹¶ä¿å­˜è®­ç»ƒå†å²ï¼ˆæŸå¤±å’Œå‡†ç¡®ç‡ï¼‰ã€‚
    """
    plt.figure(figsize=(12, 5))

    # Plot Loss (ç»˜åˆ¶æŸå¤±)
    plt.subplot(1, 2, 1)
    plt.plot(history['train_loss'], label='Training Loss')
    plt.plot(history['val_loss'], label='Validation Loss')
    plt.title('Loss Over Epochs (æ¯ä¸ªEpochçš„æŸå¤±)')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)

    # Plot Accuracy (ç»˜åˆ¶å‡†ç¡®ç‡)
    plt.subplot(1, 2, 2)
    plt.plot(history['train_accuracy'], label='Training Accuracy')
    plt.plot(history['val_accuracy'], label='Validation Accuracy')
    plt.title('Accuracy Over Epochs (æ¯ä¸ªEpochçš„å‡†ç¡®ç‡)')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.savefig(save_path)
    plt.show()
    print(f"âœ… Training history plot saved to: {save_path}")
    print(f"âœ… è®­ç»ƒå†å²å›¾ä¿å­˜åˆ°: {save_path}")

def generate_analysis_report_pytorch(model, data_loader, results_dir, config, training_time, 
                                     history, all_predictions, all_true_labels, device):
    """
    Generate detailed analysis report for PyTorch model.
    ä¸ºPyTorchæ¨¡å‹ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Šã€‚
    """
    print("ğŸ“ Generating analysis report... (æ­£åœ¨ç”Ÿæˆåˆ†ææŠ¥å‘Š...)")
    
    test_loader = data_loader.get_test_loader()
    train_loader = data_loader.get_train_loader()
    val_loader = data_loader.get_val_loader()

    # Calculate accuracies and losses (è®¡ç®—å‡†ç¡®ç‡å’ŒæŸå¤±)
    model.eval() # Set model to evaluation mode
    
    def calculate_metrics(loader, criterion, device):
        total_loss = 0.0
        correct = 0
        total = 0
        with torch.no_grad():
            for data, target in loader:
                data, target = data.to(device), target.to(device)
                output = model(data)
                loss = criterion(output, target)
                total_loss += loss.item()
                _, predicted = torch.max(output.data, 1)
                total += target.size(0)
                correct += (predicted == target).sum().item()
        return total_loss / len(loader), correct / total

    criterion = nn.CrossEntropyLoss()
    train_loss, train_accuracy = calculate_metrics(train_loader, criterion, device)
    val_loss, val_accuracy = calculate_metrics(val_loader, criterion, device)
    test_loss, test_accuracy = calculate_metrics(test_loader, criterion, device)

    # Per-class accuracy (æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡)
    cm = confusion_matrix(all_true_labels, all_predictions)
    class_accuracies = {}
    for i in range(10):
        true_positives = cm[i, i]
        total_for_class = np.sum(cm[i, :])
        if total_for_class > 0:
            class_accuracies[i] = true_positives / total_for_class
        else:
            class_accuracies[i] = 0.0
    
    # Generate report text (ç”ŸæˆæŠ¥å‘Šæ–‡æœ¬)
    report_path = os.path.join(results_dir, 'logs', 'training_report_pytorch.txt')
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("MNIST Handwritten Digit Recognition - PyTorch Training Report\n")
        f.write("MNISTæ‰‹å†™æ•°å­—è¯†åˆ« - PyTorchè®­ç»ƒæŠ¥å‘Š\n")
        f.write("=" * 60 + "\n\n")
        
        # Configuration (é…ç½®)
        f.write("Configuration (é…ç½®):\n")
        f.write("-" * 30 + "\n")
        f.write(f"Model Architecture: {config['layer_sizes']}\n")
        f.write(f"Activations: {config['activations']}\n")
        f.write(f"Epochs: {config['epochs']}\n")
        f.write(f"Batch Size: {config['batch_size']}\n")
        f.write(f"Learning Rate: {config['learning_rate']}\n")
        f.write(f"Validation Split: {config['validation_split']}\n")
        f.write(f"Training Time: {training_time:.2f} seconds\n")
        f.write(f"Device Used: {device}\n\n")
        
        # Dataset Information (æ•°æ®é›†ä¿¡æ¯)
        f.write("Dataset Information (æ•°æ®é›†ä¿¡æ¯):\n")
        f.write("-" * 30 + "\n")
        f.write(f"Training samples: {len(data_loader.train_dataset)}\n")
        f.write(f"Validation samples: {len(data_loader.val_dataset)}\n")
        f.write(f"Test samples: {len(data_loader.test_dataset)}\n")
        f.write(f"Input features: {data_loader.get_num_features()} (28x28 pixels)\n")
        f.write(f"Number of classes: {data_loader.get_num_classes()} (digits 0-9)\n\n")
        
        # Final Performance (æœ€ç»ˆæ€§èƒ½)
        f.write("Final Performance (æœ€ç»ˆæ€§èƒ½):\n")
        f.write("-" * 30 + "\n")
        f.write(f"Training Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\n")
        f.write(f"Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f} ({val_accuracy*100:.2f}%)\n")
        f.write(f"Test Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\n\n")
        
        # Per-class accuracy (æ¯ä¸ªç±»åˆ«å‡†ç¡®ç‡)
        f.write("Per-Class Accuracy (æ¯ä¸ªç±»åˆ«å‡†ç¡®ç‡):\n")
        f.write("-" * 30 + "\n")
        for class_idx, accuracy in class_accuracies.items():
            f.write(f"Digit {class_idx}: {accuracy:.4f} ({accuracy*100:.2f}%)\n")
        f.write("\n")
        
        # Training Progress (è®­ç»ƒè¿›åº¦)
        f.write("Training Progress (è®­ç»ƒè¿›åº¦):\n")
        f.write("-" * 30 + "\n")
        f.write(f"Best Training Accuracy: {max(history['train_accuracy']):.4f}\n")
        f.write(f"Best Validation Accuracy: {max(history['val_accuracy']):.4f}\n")
        f.write(f"Final Training Loss: {history['train_loss'][-1]:.4f}\n")
        f.write(f"Final Validation Loss: {history['val_loss'][-1]:.4f}\n")
    
    print(f"âœ… Analysis report saved to: {report_path}")
    print(f"âœ… åˆ†ææŠ¥å‘Šä¿å­˜åˆ°: {report_path}")

def visualize_predictions_pytorch(model, test_loader, results_dir, device):
    """
    Visualize model predictions on test samples for PyTorch model.
    å¯è§†åŒ–PyTorchæ¨¡å‹åœ¨æµ‹è¯•æ ·æœ¬ä¸Šçš„é¢„æµ‹ã€‚
    """
    print("ğŸ¨ Creating prediction visualizations... (æ­£åœ¨åˆ›å»ºé¢„æµ‹å¯è§†åŒ–...)")
    
    model.eval() # Set model to evaluation mode
    
    # Get a batch of test data (è·å–ä¸€ä¸ªæ‰¹æ¬¡çš„æµ‹è¯•æ•°æ®)
    dataiter = iter(test_loader)
    images, labels = next(dataiter)
    
    # Move images and labels to the same device as model
    # å°†å›¾ç‰‡å’Œæ ‡ç­¾ç§»åŠ¨åˆ°ä¸æ¨¡å‹ç›¸åŒçš„è®¾å¤‡ä¸Š
    images, labels = images.to(device), labels.to(device)

    # Make predictions (è¿›è¡Œé¢„æµ‹)
    with torch.no_grad():
        outputs = model(images)
        probabilities = F.softmax(outputs, dim=1) # Apply softmax to get probabilities
        _, predictions = torch.max(outputs, 1)

    # Select random samples for visualization (é€‰æ‹©éšæœºæ ·æœ¬è¿›è¡Œå¯è§†åŒ–)
    n_samples = 20
    if images.size(0) < n_samples:
        n_samples = images.size(0) # Adjust if batch size is smaller
    
    indices = np.random.choice(images.size(0), n_samples, replace=False)
    
    # Move data back to CPU for plotting (å°†æ•°æ®ç§»å›CPUè¿›è¡Œç»˜å›¾)
    images_cpu = images.cpu().numpy()
    labels_cpu = labels.cpu().numpy()
    predictions_cpu = predictions.cpu().numpy()
    probabilities_cpu = probabilities.cpu().numpy()
    
    # Create visualization (åˆ›å»ºå¯è§†åŒ–)
    fig, axes = plt.subplots(4, 5, figsize=(15, 12))
    fig.suptitle('Model Predictions on Test Set (PyTorch) (æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„é¢„æµ‹)', fontsize=16, y=0.98)
    
    for i, idx in enumerate(indices):
        row, col = i // 5, i % 5
        
        # Reshape image (é‡å¡‘å›¾ç‰‡)
        image = images_cpu[idx].reshape(28, 28)
        true_label = labels_cpu[idx]
        pred_label = predictions_cpu[idx]
        confidence = probabilities_cpu[idx][pred_label]
        
        # Plot image (ç»˜åˆ¶å›¾ç‰‡)
        axes[row, col].imshow(image, cmap='gray')
        
        # Set title with prediction info (è®¾ç½®å¸¦é¢„æµ‹ä¿¡æ¯çš„æ ‡é¢˜)
        color = 'green' if pred_label == true_label else 'red'
        title = f'True: {true_label}, Pred: {pred_label}\nConf: {confidence:.3f}'
        axes[row, col].set_title(title, color=color, fontsize=10)
        axes[row, col].axis('off')
    
    plt.tight_layout()
    
    # Save visualization (ä¿å­˜å¯è§†åŒ–)
    pred_plot_path = os.path.join(results_dir, 'plots', 'predictions_visualization_pytorch.png')
    plt.savefig(pred_plot_path, dpi=300, bbox_inches='tight')
    plt.show()
    
    # Create confusion matrix visualization (åˆ›å»ºæ··æ·†çŸ©é˜µå¯è§†åŒ–)
    create_confusion_matrix_pytorch(all_true_labels, all_predictions, results_dir)
    
    print(f"âœ… Prediction visualizations saved to: {pred_plot_path}")
    print(f"âœ… é¢„æµ‹å¯è§†åŒ–ä¿å­˜åˆ°: {pred_plot_path}")

def create_confusion_matrix_pytorch(true_labels, predictions, results_dir):
    """
    Create and save confusion matrix for PyTorch model.
    ä¸ºPyTorchæ¨¡å‹åˆ›å»ºå¹¶ä¿å­˜æ··æ·†çŸ©é˜µã€‚
    """
    import seaborn as sns
    
    # Create confusion matrix (åˆ›å»ºæ··æ·†çŸ©é˜µ)
    cm = confusion_matrix(true_labels, predictions)
    
    # Normalize confusion matrix (æ ‡å‡†åŒ–æ··æ·†çŸ©é˜µ)
    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    
    # Plot confusion matrix (ç»˜åˆ¶æ··æ·†çŸ©é˜µ)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm_normalized, annot=True, fmt='.3f', cmap='Blues',
                xticklabels=range(10), yticklabels=range(10))
    plt.title('Normalized Confusion Matrix (PyTorch) (æ ‡å‡†åŒ–æ··æ·†çŸ©é˜µ)')
    plt.xlabel('Predicted Label (é¢„æµ‹æ ‡ç­¾)')
    plt.ylabel('True Label (çœŸå®æ ‡ç­¾)')
    
    # Save confusion matrix (ä¿å­˜æ··æ·†çŸ©é˜µ)
    cm_path = os.path.join(results_dir, 'plots', 'confusion_matrix_pytorch.png')
    plt.savefig(cm_path, dpi=300, bbox_inches='tight')
    plt.show()
    
    print(f"âœ… Confusion matrix saved to: {cm_path}")
    print(f"âœ… æ··æ·†çŸ©é˜µä¿å­˜åˆ°: {cm_path}")

def main():
    """
    Main training function for PyTorch MLP.
    PyTorch MLPçš„ä¸»è®­ç»ƒå‡½æ•°ã€‚
    """
    parser = argparse.ArgumentParser(description='Train MLP for MNIST digit recognition using PyTorch')
    parser.add_argument('--epochs', type=int, default=10, help='Number of training epochs') # Reduced epochs for faster demo
    parser.add_argument('--batch_size', type=int, default=128, help='Batch size')
    parser.add_argument('--learning_rate', type=float, default=0.001, help='Learning rate')
    parser.add_argument('--hidden_sizes', nargs='+', type=int, default=[256, 128], 
                        help='Hidden layer sizes')
    parser.add_argument('--validation_split', type=float, default=0.1, 
                        help='Validation split ratio')
    
    args = parser.parse_args()
    
    # Configuration (é…ç½®)
    config = {
        'layer_sizes': [784] + args.hidden_sizes + [10],  # Input: 784, Output: 10
        'activations': ['relu'] * len(args.hidden_sizes), # All hidden layers use ReLU
        'epochs': args.epochs,
        'batch_size': args.batch_size,
        'learning_rate': args.learning_rate,
        'validation_split': args.validation_split
    }
    
    # Set random seed for reproducibility (è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡ç°æ€§)
    torch.manual_seed(42)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(42)
    
    try:
        # Train model (è®­ç»ƒæ¨¡å‹)
        model, history = train_mnist_classifier_pytorch(config)
        
        print("\nğŸ‰ Training completed successfully! (è®­ç»ƒæˆåŠŸå®Œæˆï¼)")
        print("ğŸ‰ Check the results_pytorch directory for saved models and visualizations.")
        print("ğŸ‰ æŸ¥çœ‹results_pytorchç›®å½•ä¸­ä¿å­˜çš„æ¨¡å‹å’Œå¯è§†åŒ–ç»“æœã€‚")
        
    except Exception as e:
        print(f"âŒ Error during training: {e}")
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        raise

if __name__ == "__main__":
    main() 