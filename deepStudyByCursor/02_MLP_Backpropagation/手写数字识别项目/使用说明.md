# Handwritten Digit Recognition Project - Usage Guide

手写数字识别项目 - 使用指南

## Quick Start

快速开始

### 1. Install Dependencies

安装依赖

```bash
# Navigate to project directory
# 导航到项目目录
cd deepStudyByCursor/02_MLP_Backpropagation/手写数字识别项目

# Install required packages
# 安装所需包
pip install -r requirements.txt
```

### 2. Download and Prepare Data

下载并准备数据

```bash
# Navigate to src directory
# 导航到src目录
cd src

# Download MNIST data and create preprocessed files
# 下载MNIST数据并创建预处理文件
python data_loader.py
```

### 3. Train the Model

训练模型

```bash
# Train with default settings
# 使用默认设置训练
python train.py

# Train with custom parameters
# 使用自定义参数训练
python train.py --epochs 100 --batch_size 32 --learning_rate 0.005 --hidden_sizes 256 128
```

### 4. Check Results

查看结果

训练完成后，检查以下目录的结果：

- `results/models/` - Saved model files (保存的模型文件)
- `results/plots/` - Training plots and visualizations (训练图表和可视化)
- `results/logs/` - Training logs and reports (训练日志和报告)

## Detailed Usage

详细使用说明

### Training Parameters

训练参数

You can customize the training process with these parameters:

您可以使用这些参数自定义训练过程：

| Parameter | Default | Description |
|-----------|---------|-------------|
| `--epochs` | 50 | Number of training epochs (训练轮数) |
| `--batch_size` | 64 | Batch size for training (训练批大小) |
| `--learning_rate` | 0.01 | Learning rate (学习率) |
| `--hidden_sizes` | [128, 64] | Hidden layer sizes (隐藏层大小) |
| `--validation_split` | 0.1 | Validation split ratio (验证集分割比例) |

### Examples

示例

```bash
# Example 1: Quick training with small model
# 示例1：使用小模型快速训练
python train.py --epochs 20 --hidden_sizes 64 32

# Example 2: Deep model with more epochs
# 示例2：使用更多轮次的深度模型
python train.py --epochs 100 --hidden_sizes 256 128 64 --learning_rate 0.005

# Example 3: Large batch training
# 示例3：大批量训练
python train.py --batch_size 128 --epochs 50
```

## Understanding the Results

理解结果

### Training Plots

训练图表

After training, you'll find these visualizations in `results/plots/`:

训练后，您将在 `results/plots/` 中找到这些可视化：

1. **training_history.png** - Loss and accuracy curves (损失和准确率曲线)
2. **predictions_visualization.png** - Sample predictions (样本预测)
3. **confusion_matrix.png** - Confusion matrix (混淆矩阵)
4. **sample_images.png** - Sample MNIST images (MNIST样本图片)

### Performance Metrics

性能指标

Check the training report in `results/logs/training_report.txt` for:

在 `results/logs/training_report.txt` 中查看训练报告：

- **Training Accuracy** (训练准确率)
- **Validation Accuracy** (验证准确率)
- **Test Accuracy** (测试准确率)
- **Per-class accuracy** (每类准确率)
- **Training time** (训练时间)

### Expected Performance

预期性能

With default settings, you should expect:

使用默认设置，您应该期望：

- **Training Accuracy**: ~98-99% (训练准确率：约98-99%)
- **Validation Accuracy**: ~96-97% (验证准确率：约96-97%)
- **Test Accuracy**: ~95-96% (测试准确率：约95-96%)
- **Training Time**: 2-5 minutes on CPU (训练时间：CPU上2-5分钟)

## Project Structure

项目结构

```
手写数字识别项目/
├── README.md                    # Project overview (项目概述)
├── 使用说明.md                   # This usage guide (本使用指南)
├── requirements.txt             # Dependencies (依赖项)
├── src/                         # Source code (源代码)
│   ├── data_loader.py          # Data loading (数据加载)
│   ├── mlp_scratch.py          # MLP implementation (MLP实现)
│   ├── train.py                # Training script (训练脚本)
│   └── utils.py                # Utility functions (工具函数)
├── data/                        # Data directory (数据目录)
│   ├── raw/                     # Raw MNIST files (原始MNIST文件)
│   └── processed/               # Processed data (处理后数据)
├── results/                     # Results (结果)
│   ├── models/                  # Saved models (保存的模型)
│   ├── plots/                   # Visualizations (可视化)
│   └── logs/                    # Training logs (训练日志)
└── notebooks/                   # Jupyter notebooks (Jupyter笔记本)
    └── 01_data_exploration.ipynb
```

## Mathematical Foundation

数学基础

This project implements the complete mathematical foundation of MLPs:

本项目实现了MLP的完整数学基础：

### Forward Propagation

前向传播

```
z^(l) = W^(l) · a^(l-1) + b^(l)
a^(l) = f(z^(l))
```

Where:
- `z^(l)` = Pre-activation at layer l (第l层的预激活)
- `W^(l)` = Weight matrix at layer l (第l层的权重矩阵)
- `a^(l)` = Activation at layer l (第l层的激活值)
- `f()` = Activation function (激活函数)

### Backpropagation

反向传播

```
δ^(L) = ∇_a C ⊙ f'(z^(L))  (Output layer)
δ^(l) = ((W^(l+1))^T δ^(l+1)) ⊙ f'(z^(l))  (Hidden layers)
```

### Weight Updates

权重更新

```
W^(l) := W^(l) - η · ∂C/∂W^(l)
b^(l) := b^(l) - η · ∂C/∂b^(l)
```

Where `η` is the learning rate (学习率).

## Troubleshooting

故障排除

### Common Issues

常见问题

1. **Memory Error (内存错误)**
   - Reduce batch size: `--batch_size 32`
   - Use smaller model: `--hidden_sizes 64 32`

2. **Slow Training (训练缓慢)**
   - Increase batch size: `--batch_size 128`
   - Reduce epochs: `--epochs 30`

3. **Poor Accuracy (准确率低)**
   - Increase learning rate: `--learning_rate 0.02`
   - Add more hidden units: `--hidden_sizes 256 128 64`
   - Train longer: `--epochs 100`

4. **Data Not Found (数据未找到)**
   ```bash
   cd src
   python data_loader.py
   ```

### Getting Help

获取帮助

If you encounter issues:

如果遇到问题：

1. Check the training report in `results/logs/` (检查results/logs/中的训练报告)
2. Review the mathematical foundations in `../多层感知机与反向传播.md`
3. Study the notation guide in `../01_Perceptron/数学符号详解与读音.md`

## Next Steps

下一步

After successfully training your model:

成功训练模型后：

1. **Experiment with architectures** (实验不同架构)
   - Try different layer sizes (尝试不同层大小)
   - Add more layers (添加更多层)
   - Use different activation functions (使用不同激活函数)

2. **Improve performance** (提升性能)
   - Implement learning rate scheduling (实现学习率调度)
   - Add regularization techniques (添加正则化技术)
   - Try advanced optimizers (尝试高级优化器)

3. **Compare implementations** (比较实现)
   - Implement PyTorch version (实现PyTorch版本)
   - Compare performance and speed (比较性能和速度)
   - Analyze differences (分析差异)

---

**Happy Learning! 快乐学习！** 🚀

For more advanced topics, continue to the next sections in the deep learning curriculum.

要了解更高级的主题，请继续深度学习课程的下一节。 