
# 多层感知机与反向传播：神经网络的核心学习机制

**English Translation:** Multilayer Perceptron and Backpropagation: The Core Learning Mechanism of Neural Networks

## 1. 为什么需要多层感知机？

**English Translation:** Why Do We Need Multilayer Perceptrons?

### 1.1 回顾感知机的局限性

**English Translation:** Reviewing the Limitations of Perceptrons

在第一章中，我们了解了单层感知机只能解决**线性可分**的问题。最经典的例子就是**异或 (XOR) 问题**：

**English Translation:** In Chapter 1, we learned that single-layer perceptrons can only solve **linearly separable** problems. The most classic example is the **XOR (Exclusive OR) problem**:

**异或真值表：**
- 输入 (0, 0) → 输出 0
- 输入 (0, 1) → 输出 1  
- 输入 (1, 0) → 输出 1
- 输入 (1, 1) → 输出 0

**English Translation:** **XOR Truth Table:**
- Input (0, 0) → Output 0
- Input (0, 1) → Output 1  
- Input (1, 0) → Output 1
- Input (1, 1) → Output 0

如果我们试图用单层感知机来解决这个问题，设模型为：

**English Translation:** If we try to solve this problem with a single-layer perceptron, let the model be:

$$y = \text{step}(w_1x_1 + w_2x_2 + b)$$

其中 step 函数为：$\text{step}(z) = \begin{cases} 1 & \text{if } z > 0 \\ 0 & \text{if } z \leq 0 \end{cases}$

**English Translation:** where the step function is: $\text{step}(z) = \begin{cases} 1 & \text{if } z > 0 \\ 0 & \text{if } z \leq 0 \end{cases}$

**数学证明单层感知机无法解决XOR：**

**English Translation:** **Mathematical Proof that Single-Layer Perceptrons Cannot Solve XOR:**

假设存在权重 $w_1, w_2$ 和偏置 $b$ 能够解决XOR问题，那么必须同时满足：

**English Translation:** Assume there exist weights $w_1, w_2$ and bias $b$ that can solve the XOR problem, then the following conditions must be satisfied simultaneously:

1. $(0,0)$: $w_1 \cdot 0 + w_2 \cdot 0 + b \leq 0 \Rightarrow b \leq 0$
2. $(0,1)$: $w_1 \cdot 0 + w_2 \cdot 1 + b > 0 \Rightarrow w_2 + b > 0$
3. $(1,0)$: $w_1 \cdot 1 + w_2 \cdot 0 + b > 0 \Rightarrow w_1 + b > 0$
4. $(1,1)$: $w_1 \cdot 1 + w_2 \cdot 1 + b \leq 0 \Rightarrow w_1 + w_2 + b \leq 0$

从条件1得到 $b \leq 0$，从条件2得到 $w_2 > -b \geq 0$，从条件3得到 $w_1 > -b \geq 0$。

**English Translation:** From condition 1, we get $b \leq 0$. From condition 2, we get $w_2 > -b \geq 0$. From condition 3, we get $w_1 > -b \geq 0$.

因此 $w_1 > 0$ 且 $w_2 > 0$，这与条件4 ($w_1 + w_2 + b \leq 0$) 矛盾，因为 $w_1 + w_2 > 0$ 而 $b \leq 0$。

**English Translation:** Therefore, $w_1 > 0$ and $w_2 > 0$, which contradicts condition 4 ($w_1 + w_2 + b \leq 0$), because $w_1 + w_2 > 0$ while $b \leq 0$.

**结论：** 单层感知机无法解决非线性可分问题。

**English Translation:** **Conclusion:** Single-layer perceptrons cannot solve non-linearly separable problems.

### 1.2 引入隐藏层：多层感知机的诞生

**English Translation:** Introducing Hidden Layers: The Birth of Multilayer Perceptrons

**多层感知机 (Multilayer Perceptron, MLP)** 通过引入一个或多个**隐藏层**，赋予模型表达非线性关系的能力。

**English Translation:** **Multilayer Perceptrons (MLPs)** give models the ability to express non-linear relationships by introducing one or more **hidden layers**.

**类比：多层感知机就像一个专家团队**
- **输入层**：收集原始信息的"信息员"
- **隐藏层**：处理和分析信息的"专家"
- **输出层**：做出最终决策的"决策者"

**English Translation:** **Analogy: Multilayer perceptrons are like a team of experts**
- **Input layer**: "Information collectors" that gather raw information
- **Hidden layers**: "Experts" that process and analyze information
- **Output layer**: "Decision makers" that make final decisions

每一层的专家都会对信息进行某种"加工"，然后传递给下一层。

**English Translation:** The experts at each layer perform some kind of "processing" on the information and then pass it to the next layer.

## 2. 多层感知机的结构

**English Translation:** Structure of Multilayer Perceptrons

### 2.1 基本架构

**English Translation:** Basic Architecture

一个典型的三层MLP包括：
- **输入层**：$n$ 个神经元，对应 $n$ 个输入特征
- **隐藏层**：$h$ 个神经元
- **输出层**：$m$ 个神经元，对应 $m$ 个输出类别或值

**English Translation:** A typical three-layer MLP includes:
- **Input layer**: $n$ neurons, corresponding to $n$ input features
- **Hidden layer**: $h$ neurons
- **Output layer**: $m$ neurons, corresponding to $m$ output categories or values

### 2.2 数学表示

**English Translation:** Mathematical Representation

**符号定义：**
- $\mathbf{x} = [x_1, x_2, ..., x_n]^T$：输入向量
- $\mathbf{W}^{(1)}$：输入层到隐藏层的权重矩阵 ($h \times n$)
- $\mathbf{b}^{(1)}$：隐藏层的偏置向量 ($h \times 1$)
- $\mathbf{h}$：隐藏层的输出向量 ($h \times 1$)
- $\mathbf{W}^{(2)}$：隐藏层到输出层的权重矩阵 ($m \times h$)
- $\mathbf{b}^{(2)}$：输出层的偏置向量 ($m \times 1$)
- $\mathbf{y}$：最终输出向量 ($m \times 1$)

**English Translation:** **Symbol Definitions:**
- $\mathbf{x} = [x_1, x_2, ..., x_n]^T$: Input vector
- $\mathbf{W}^{(1)}$: Weight matrix from input layer to hidden layer ($h \times n$)
- $\mathbf{b}^{(1)}$: Bias vector of hidden layer ($h \times 1$)
- $\mathbf{h}$: Output vector of hidden layer ($h \times 1$)
- $\mathbf{W}^{(2)}$: Weight matrix from hidden layer to output layer ($m \times h$)
- $\mathbf{b}^{(2)}$: Bias vector of output layer ($m \times 1$)
- $\mathbf{y}$: Final output vector ($m \times 1$)

**前向传播过程：**

**English Translation:** **Forward Propagation Process:**

1. **隐藏层计算：**
   $$\mathbf{z}^{(1)} = \mathbf{W}^{(1)} \mathbf{x} + \mathbf{b}^{(1)}$$
   $$\mathbf{h} = \sigma(\mathbf{z}^{(1)})$$

**English Translation:** 1. **Hidden Layer Computation:**

2. **输出层计算：**
   $$\mathbf{z}^{(2)} = \mathbf{W}^{(2)} \mathbf{h} + \mathbf{b}^{(2)}$$
   $$\mathbf{y} = \sigma(\mathbf{z}^{(2)})$$

**English Translation:** 2. **Output Layer Computation:**

其中 $\sigma$ 是激活函数（如Sigmoid或ReLU）。

**English Translation:** where $\sigma$ is the activation function (such as Sigmoid or ReLU).

## 3. 核心算法：反向传播 (Backpropagation)

**English Translation:** Core Algorithm: Backpropagation

### 3.1 反向传播的直观理解

**English Translation:** Intuitive Understanding of Backpropagation

**类比：工厂流水线逆向追溯问题**

**English Translation:** **Analogy: Reverse Tracing Problems in Factory Assembly Lines**

想象一个制造手机的工厂：
- **原材料** → **零件加工** → **组装** → **质检** → **成品**

**English Translation:** Imagine a factory manufacturing mobile phones:
- **Raw Materials** → **Parts Processing** → **Assembly** → **Quality Control** → **Finished Product**

如果最终产品有质量问题，我们需要逆向追溯：
1. 首先检查质检环节
2. 然后检查组装环节  
3. 接着检查零件加工环节
4. 最后检查原材料

**English Translation:** If the final product has quality issues, we need to trace back in reverse:
1. First check the quality control stage
2. Then check the assembly stage
3. Next check the parts processing stage
4. Finally check the raw materials

反向传播就是这样一个"逆向追溯"的过程，从输出层的误差开始，逐层向前传播，计算每一层参数对最终误差的"贡献"。

**English Translation:** Backpropagation is such a "reverse tracing" process, starting from the error at the output layer, propagating layer by layer forward, calculating the "contribution" of each layer's parameters to the final error.

### 3.2 数学推导：详细计算实例

**English Translation:** Mathematical Derivation: Detailed Calculation Example

让我们用一个具体的例子来演示反向传播的完整计算过程。

**English Translation:** Let's use a concrete example to demonstrate the complete calculation process of backpropagation.

**问题设定：** 用MLP解决XOR问题

**English Translation:** **Problem Setup:** Using MLP to solve the XOR problem

**网络结构：**
- 输入层：2个神经元 ($x_1, x_2$)
- 隐藏层：2个神经元 ($h_1, h_2$)
- 输出层：1个神经元 ($y$)
- 激活函数：Sigmoid $\sigma(z) = \frac{1}{1+e^{-z}}$

**English Translation:** **Network Structure:**
- Input layer: 2 neurons ($x_1, x_2$)
- Hidden layer: 2 neurons ($h_1, h_2$)
- Output layer: 1 neuron ($y$)
- Activation function: Sigmoid $\sigma(z) = \frac{1}{1+e^{-z}}$

**初始权重和偏置：**

**English Translation:** **Initial Weights and Biases:**

$$\mathbf{W}^{(1)} = \begin{bmatrix} 0.5 & 0.5 \\ -0.5 & -0.5 \end{bmatrix}, \quad \mathbf{b}^{(1)} = \begin{bmatrix} -0.2 \\ 0.7 \end{bmatrix}$$

$$\mathbf{W}^{(2)} = \begin{bmatrix} 1.0 & 1.0 \end{bmatrix}, \quad b^{(2)} = -0.5$$

**训练样本：** $(x_1, x_2) = (1, 1)$，目标输出 $t = 0$

**English Translation:** **Training Sample:** $(x_1, x_2) = (1, 1)$, target output $t = 0$

#### 步骤1：前向传播

**English Translation:** #### Step 1: Forward Propagation

**隐藏层计算：**

**English Translation:** **Hidden Layer Computation:**

$$z_1^{(1)} = w_{11}^{(1)}x_1 + w_{12}^{(1)}x_2 + b_1^{(1)} = 0.5 \times 1 + 0.5 \times 1 + (-0.2) = 0.8$$

$$z_2^{(1)} = w_{21}^{(1)}x_1 + w_{22}^{(1)}x_2 + b_2^{(1)} = (-0.5) \times 1 + (-0.5) \times 1 + 0.7 = -0.3$$

$$h_1 = \sigma(0.8) = \frac{1}{1+e^{-0.8}} = \frac{1}{1+0.449} = 0.690$$

$$h_2 = \sigma(-0.3) = \frac{1}{1+e^{0.3}} = \frac{1}{1+1.350} = 0.426$$

**输出层计算：**

**English Translation:** **Output Layer Computation:**

$$z^{(2)} = w_1^{(2)}h_1 + w_2^{(2)}h_2 + b^{(2)} = 1.0 \times 0.690 + 1.0 \times 0.426 + (-0.5) = 0.616$$

$$y = \sigma(0.616) = \frac{1}{1+e^{-0.616}} = \frac{1}{1+0.540} = 0.649$$

**损失计算：**
使用均方误差损失：

**English Translation:** **Loss Calculation:**
Using mean squared error loss:

$$L = \frac{1}{2}(t - y)^2 = \frac{1}{2}(0 - 0.649)^2 = \frac{1}{2} \times 0.421 = 0.211$$

#### 步骤2：反向传播

**English Translation:** #### Step 2: Backpropagation

**Sigmoid函数的导数：**

**English Translation:** **Derivative of Sigmoid Function:**

$$\sigma'(z) = \sigma(z)(1-\sigma(z))$$

**输出层误差计算：**

**English Translation:** **Output Layer Error Calculation:**

$$\delta^{(2)} = \frac{\partial L}{\partial z^{(2)}} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial z^{(2)}}$$

$$\frac{\partial L}{\partial y} = -(t - y) = -(0 - 0.649) = 0.649$$

$$\frac{\partial y}{\partial z^{(2)}} = \sigma'(z^{(2)}) = y(1-y) = 0.649 \times (1-0.649) = 0.649 \times 0.351 = 0.228$$

$$\delta^{(2)} = 0.649 \times 0.228 = 0.148$$

**隐藏层误差计算：**

**English Translation:** **Hidden Layer Error Calculation:**

$$\delta_1^{(1)} = \frac{\partial L}{\partial z_1^{(1)}} = \frac{\partial L}{\partial z^{(2)}} \cdot \frac{\partial z^{(2)}}{\partial h_1} \cdot \frac{\partial h_1}{\partial z_1^{(1)}}$$

$$\frac{\partial z^{(2)}}{\partial h_1} = w_1^{(2)} = 1.0$$

$$\frac{\partial h_1}{\partial z_1^{(1)}} = \sigma'(z_1^{(1)}) = h_1(1-h_1) = 0.690 \times (1-0.690) = 0.690 \times 0.310 = 0.214$$

$$\delta_1^{(1)} = \delta^{(2)} \times w_1^{(2)} \times \sigma'(z_1^{(1)}) = 0.148 \times 1.0 \times 0.214 = 0.032$$

类似地：

**English Translation:** Similarly:

$$\delta_2^{(1)} = \delta^{(2)} \times w_2^{(2)} \times \sigma'(z_2^{(1)}) = 0.148 \times 1.0 \times h_2(1-h_2)$$

$$= 0.148 \times 1.0 \times 0.426 \times (1-0.426) = 0.148 \times 0.426 \times 0.574 = 0.036$$

#### 步骤3：权重和偏置更新

**English Translation:** #### Step 3: Weight and Bias Updates

设学习率 $\eta = 0.5$

**English Translation:** Let learning rate $\eta = 0.5$

**输出层权重更新：**

**English Translation:** **Output Layer Weight Updates:**

$$\frac{\partial L}{\partial w_1^{(2)}} = \delta^{(2)} \times h_1 = 0.148 \times 0.690 = 0.102$$

$$\frac{\partial L}{\partial w_2^{(2)}} = \delta^{(2)} \times h_2 = 0.148 \times 0.426 = 0.063$$

$$w_1^{(2),new} = w_1^{(2)} - \eta \frac{\partial L}{\partial w_1^{(2)}} = 1.0 - 0.5 \times 0.102 = 1.0 - 0.051 = 0.949$$

$$w_2^{(2),new} = w_2^{(2)} - \eta \frac{\partial L}{\partial w_2^{(2)}} = 1.0 - 0.5 \times 0.063 = 1.0 - 0.032 = 0.968$$

**输出层偏置更新：**

**English Translation:** **Output Layer Bias Updates:**

$$\frac{\partial L}{\partial b^{(2)}} = \delta^{(2)} = 0.148$$

$$b^{(2),new} = b^{(2)} - \eta \frac{\partial L}{\partial b^{(2)}} = -0.5 - 0.5 \times 0.148 = -0.5 - 0.074 = -0.574$$

**隐藏层权重更新：**

**English Translation:** **Hidden Layer Weight Updates:**

$$\frac{\partial L}{\partial w_{11}^{(1)}} = \delta_1^{(1)} \times x_1 = 0.032 \times 1 = 0.032$$

$$\frac{\partial L}{\partial w_{12}^{(1)}} = \delta_1^{(1)} \times x_2 = 0.032 \times 1 = 0.032$$

$$w_{11}^{(1),new} = 0.5 - 0.5 \times 0.032 = 0.5 - 0.016 = 0.484$$

$$w_{12}^{(1),new} = 0.5 - 0.5 \times 0.032 = 0.5 - 0.016 = 0.484$$

类似地计算其他权重：

**English Translation:** Similarly, calculate other weights:

$$w_{21}^{(1),new} = -0.5 - 0.5 \times 0.036 = -0.518$$
$$w_{22}^{(1),new} = -0.5 - 0.5 \times 0.036 = -0.518$$

**隐藏层偏置更新：**

**English Translation:** **Hidden Layer Bias Updates:**

$$b_1^{(1),new} = -0.2 - 0.5 \times 0.032 = -0.216$$
$$b_2^{(1),new} = 0.7 - 0.5 \times 0.036 = 0.682$$

### 3.3 梯度下降的几何直观

**English Translation:** Geometric Intuition of Gradient Descent

**梯度下降就像一个盲人下山：**

**English Translation:** **Gradient descent is like a blind person going downhill:**

想象你是一个蒙着眼睛的人，站在山坡上，目标是找到山脚（损失函数的最小值）。

**English Translation:** Imagine you are a blindfolded person standing on a hillside, with the goal of finding the foot of the mountain (the minimum value of the loss function).

1. **感受坡度：** 用脚感受当前位置的坡度（计算梯度）
2. **选择方向：** 向最陡的下坡方向走（负梯度方向）
3. **迈出步子：** 根据坡度走一小步（学习率控制步长）
4. **重复过程：** 到达新位置后，重复上述过程

**English Translation:** 
1. **Feel the slope:** Use your feet to sense the slope at the current position (calculate gradient)
2. **Choose direction:** Walk in the steepest downhill direction (negative gradient direction)
3. **Take a step:** Take a small step based on the slope (learning rate controls step size)
4. **Repeat process:** After reaching a new position, repeat the above process

**数学表示：**

**English Translation:** **Mathematical Representation:**

$$\theta_{new} = \theta_{old} - \eta \nabla_\theta L(\theta)$$

其中：
- $\theta$ 代表所有参数（权重和偏置）
- $\eta$ 是学习率（步长）
- $\nabla_\theta L(\theta)$ 是损失函数对参数的梯度

**English Translation:** where:
- $\theta$ represents all parameters (weights and biases)
- $\eta$ is the learning rate (step size)
- $\nabla_\theta L(\theta)$ is the gradient of the loss function with respect to parameters

### 3.4 链式法则 (Chain Rule)

**English Translation:** Chain Rule

反向传播的核心数学原理是**链式法则**。

**English Translation:** The core mathematical principle of backpropagation is the **chain rule**.

**链式法则的基本形式：**
如果 $y = f(u)$ 且 $u = g(x)$，那么：

**English Translation:** **Basic Form of Chain Rule:**
If $y = f(u)$ and $u = g(x)$, then:

$$\frac{dy}{dx} = \frac{dy}{du} \cdot \frac{du}{dx}$$

**在神经网络中的应用：**
对于一个三层网络，权重 $w_{ij}^{(1)}$ 对损失函数的影响需要通过以下路径：

**English Translation:** **Application in Neural Networks:**
For a three-layer network, the influence of weight $w_{ij}^{(1)}$ on the loss function needs to go through the following path:

$$w_{ij}^{(1)} \to z_j^{(1)} \to h_j \to z^{(2)} \to y \to L$$

因此：

**English Translation:** Therefore:

$$\frac{\partial L}{\partial w_{ij}^{(1)}} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial z^{(2)}} \cdot \frac{\partial z^{(2)}}{\partial h_j} \cdot \frac{\partial h_j}{\partial z_j^{(1)}} \cdot \frac{\partial z_j^{(1)}}{\partial w_{ij}^{(1)}}$$

**实际计算示例：**
对于前面例子中的 $w_{11}^{(1)}$：

**English Translation:** **Actual Calculation Example:**
For $w_{11}^{(1)}$ in the previous example:

$$\frac{\partial z_1^{(1)}}{\partial w_{11}^{(1)}} = x_1 = 1$$

$$\frac{\partial h_1}{\partial z_1^{(1)}} = \sigma'(z_1^{(1)}) = 0.214$$

$$\frac{\partial z^{(2)}}{\partial h_1} = w_1^{(2)} = 1.0$$

$$\frac{\partial y}{\partial z^{(2)}} = \sigma'(z^{(2)}) = 0.228$$

$$\frac{\partial L}{\partial y} = 0.649$$

$$\frac{\partial L}{\partial w_{11}^{(1)}} = 0.649 \times 0.228 \times 1.0 \times 0.214 \times 1 = 0.032$$

这正是我们前面计算的结果！

**English Translation:** This is exactly the result we calculated earlier!

## 4. 损失函数与优化器

**English Translation:** Loss Functions and Optimizers

### 4.1 分类问题的损失函数：交叉熵损失

**English Translation:** Loss Functions for Classification Problems: Cross-Entropy Loss

**二分类交叉熵损失：**

**English Translation:** **Binary Cross-Entropy Loss:**

$$L = -[t \log(y) + (1-t) \log(1-y)]$$

**多分类交叉熵损失：**

**English Translation:** **Multi-class Cross-Entropy Loss:**

$$L = -\sum_{i=1}^{C} t_i \log(y_i)$$

其中 $C$ 是类别数，$t_i$ 是第 $i$ 类的真实标签（one-hot编码），$y_i$ 是第 $i$ 类的预测概率。

**English Translation:** where $C$ is the number of classes, $t_i$ is the true label for class $i$ (one-hot encoded), and $y_i$ is the predicted probability for class $i$.

**数值计算例子：**

**English Translation:** **Numerical Calculation Example:**

假设有3个类别的分类问题：
- 真实标签：$\mathbf{t} = [0, 1, 0]$（第2类）
- 预测概率：$\mathbf{y} = [0.2, 0.7, 0.1]$

**English Translation:** Assume a 3-class classification problem:
- True label: $\mathbf{t} = [0, 1, 0]$ (class 2)
- Predicted probabilities: $\mathbf{y} = [0.2, 0.7, 0.1]$

交叉熵损失：

**English Translation:** Cross-entropy loss:

$$L = -(0 \times \log(0.2) + 1 \times \log(0.7) + 0 \times \log(0.1))$$
$$= -\log(0.7) = -(-0.357) = 0.357$$

**损失函数的梯度：**

**English Translation:** **Gradient of Loss Function:**

$$\frac{\partial L}{\partial y_i} = -\frac{t_i}{y_i}$$

对于Softmax输出的特殊情况：

**English Translation:** For the special case of Softmax output:

$$\frac{\partial L}{\partial z_i} = y_i - t_i$$

这个简洁的结果使得反向传播计算变得非常高效。

**English Translation:** This concise result makes backpropagation calculations very efficient.

### 4.2 优化器：随机梯度下降 (SGD)

**English Translation:** Optimizer: Stochastic Gradient Descent (SGD)

**批量梯度下降 vs 随机梯度下降：**

**English Translation:** **Batch Gradient Descent vs Stochastic Gradient Descent:**

**批量梯度下降 (Batch GD)：**

**English Translation:** **Batch Gradient Descent (Batch GD):**

$$\theta = \theta - \eta \frac{1}{N} \sum_{i=1}^{N} \nabla_\theta L_i(\theta)$$

使用所有样本计算梯度，更新稳定但计算量大。

**English Translation:** Uses all samples to calculate gradients, updates are stable but computationally expensive.

**随机梯度下降 (SGD)：**

**English Translation:** **Stochastic Gradient Descent (SGD):**

$$\theta = \theta - \eta \nabla_\theta L_i(\theta)$$

每次只使用一个样本更新，计算快但有噪声。

**English Translation:** Uses only one sample for each update, fast computation but noisy.

**小批量梯度下降 (Mini-batch GD)：**

**English Translation:** **Mini-batch Gradient Descent (Mini-batch GD):**

$$\theta = \theta - \eta \frac{1}{B} \sum_{i=1}^{B} \nabla_\theta L_i(\theta)$$

使用小批量样本，平衡了效率和稳定性。

**English Translation:** Uses mini-batches of samples, balancing efficiency and stability.

### 4.3 学习率调度策略

**English Translation:** Learning Rate Scheduling Strategies

**固定学习率的问题：**
- 学习率太大：可能错过最优解，导致震荡
- 学习率太小：收敛过慢

**English Translation:** **Problems with Fixed Learning Rate:**
- Learning rate too large: may miss optimal solution, causing oscillations
- Learning rate too small: convergence too slow

**常见的学习率调度：**

**English Translation:** **Common Learning Rate Schedules:**

1. **指数衰减：**

**English Translation:** 1. **Exponential Decay:**

   $$\eta_t = \eta_0 \times \gamma^{t/T}$$
   
   其中 $\gamma < 1$，$T$ 是衰减周期。

**English Translation:** where $\gamma < 1$ and $T$ is the decay period.

2. **余弦退火：**

**English Translation:** 2. **Cosine Annealing:**

   $$\eta_t = \eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min})(1 + \cos(\frac{t\pi}{T}))$$

**数值例子：**
初始学习率 $\eta_0 = 0.1$，衰减因子 $\gamma = 0.95$，每10个epoch衰减一次：

**English Translation:** **Numerical Example:**
Initial learning rate $\eta_0 = 0.1$, decay factor $\gamma = 0.95$, decay every 10 epochs:

- Epoch 0: $\eta = 0.1$
- Epoch 10: $\eta = 0.1 \times 0.95^1 = 0.095$
- Epoch 20: $\eta = 0.1 \times 0.95^2 = 0.090$
- Epoch 30: $\eta = 0.1 \times 0.95^3 = 0.086$

## 5. 实际应用场景

**English Translation:** Practical Application Scenarios

### 5.1 手写数字识别 (MNIST)

**English Translation:** Handwritten Digit Recognition (MNIST)

**问题描述：** 识别28×28像素的手写数字图像（0-9）

**English Translation:** **Problem Description:** Recognize 28×28 pixel handwritten digit images (0-9)

**网络架构：**
- 输入层：784个神经元（28×28像素展平）
- 隐藏层1：128个神经元，ReLU激活
- 隐藏层2：64个神经元，ReLU激活  
- 输出层：10个神经元，Softmax激活

**English Translation:** **Network Architecture:**
- Input layer: 784 neurons (28×28 pixels flattened)
- Hidden layer 1: 128 neurons, ReLU activation
- Hidden layer 2: 64 neurons, ReLU activation
- Output layer: 10 neurons, Softmax activation

**数学建模：**

**English Translation:** **Mathematical Modeling:**

**输入预处理：**

**English Translation:** **Input Preprocessing:**

$$x_{norm} = \frac{x - 127.5}{127.5}$$

将像素值从[0,255]归一化到[-1,1]。

**English Translation:** Normalize pixel values from [0,255] to [-1,1].

**前向传播：**

**English Translation:** **Forward Propagation:**

$$\mathbf{h}^{(1)} = \text{ReLU}(\mathbf{W}^{(1)}\mathbf{x} + \mathbf{b}^{(1)})$$
$$\mathbf{h}^{(2)} = \text{ReLU}(\mathbf{W}^{(2)}\mathbf{h}^{(1)} + \mathbf{b}^{(2)})$$
$$\mathbf{y} = \text{Softmax}(\mathbf{W}^{(3)}\mathbf{h}^{(2)} + \mathbf{b}^{(3)})$$

**参数数量计算：**

**English Translation:** **Parameter Count Calculation:**

- $\mathbf{W}^{(1)}$: $128 \times 784 = 100,352$
- $\mathbf{b}^{(1)}$: $128$
- $\mathbf{W}^{(2)}$: $64 \times 128 = 8,192$
- $\mathbf{b}^{(2)}$: $64$
- $\mathbf{W}^{(3)}$: $10 \times 64 = 640$
- $\mathbf{b}^{(3)}$: $10$

总参数：$100,352 + 128 + 8,192 + 64 + 640 + 10 = 109,386$

**English Translation:** Total parameters: $100,352 + 128 + 8,192 + 64 + 640 + 10 = 109,386$

### 5.2 简单文本分类

**English Translation:** Simple Text Classification

**问题描述：** 对电影评论进行情感分析（正面/负面）

**English Translation:** **Problem Description:** Perform sentiment analysis on movie reviews (positive/negative)

**文本向量化：** 使用词袋模型或TF-IDF

**English Translation:** **Text Vectorization:** Using bag-of-words model or TF-IDF

**TF-IDF计算示例：**

**English Translation:** **TF-IDF Calculation Example:**

给定词汇表：["good", "bad", "movie", "great", "terrible"]

**English Translation:** Given vocabulary: ["good", "bad", "movie", "great", "terrible"]

文档："This movie is really good and great"

**English Translation:** Document: "This movie is really good and great"

1. **词频 (TF)：**
   - good: 1, great: 1, movie: 1, 其他: 0
   - TF向量：[1, 0, 1, 1, 0]

**English Translation:** 1. **Term Frequency (TF):**
   - good: 1, great: 1, movie: 1, others: 0
   - TF vector: [1, 0, 1, 1, 0]

2. **逆文档频率 (IDF)：**
   假设语料库中各词的文档频率为：
   - good: 出现在50%文档中，IDF = log(1/0.5) = 0.693
   - movie: 出现在80%文档中，IDF = log(1/0.8) = 0.223
   - great: 出现在30%文档中，IDF = log(1/0.3) = 1.204

**English Translation:** 2. **Inverse Document Frequency (IDF):**
   Assume document frequencies in the corpus are:
   - good: appears in 50% of documents, IDF = log(1/0.5) = 0.693
   - movie: appears in 80% of documents, IDF = log(1/0.8) = 0.223
   - great: appears in 30% of documents, IDF = log(1/0.3) = 1.204

3. **TF-IDF：**
   - TF-IDF向量：[1×0.693, 0, 1×0.223, 1×1.204, 0] = [0.693, 0, 0.223, 1.204, 0]

**English Translation:** 3. **TF-IDF:**
   - TF-IDF vector: [1×0.693, 0, 1×0.223, 1×1.204, 0] = [0.693, 0, 0.223, 1.204, 0]

**网络架构：**
- 输入层：词汇表大小（如5000）
- 隐藏层：100个神经元
- 输出层：1个神经元（Sigmoid激活，输出情感概率）

**English Translation:** **Network Architecture:**
- Input layer: vocabulary size (e.g., 5000)
- Hidden layer: 100 neurons
- Output layer: 1 neuron (Sigmoid activation, outputs sentiment probability)

通过这些详细的数学计算例子，我们可以看到多层感知机如何通过反向传播算法，从简单的数学运算中"学会"复杂的模式识别能力。这正是深度学习的魅力所在：用简单的数学构建块，组合出强大的智能系统。

**English Translation:** Through these detailed mathematical calculation examples, we can see how multilayer perceptrons learn complex pattern recognition capabilities from simple mathematical operations through the backpropagation algorithm. This is the charm of deep learning: using simple mathematical building blocks to construct powerful intelligent systems. 