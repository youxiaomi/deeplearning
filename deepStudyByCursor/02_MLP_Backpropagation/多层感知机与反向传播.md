# Multilayer Perceptron and Backpropagation: The Core Learning Mechanism of Neural Networks

多层感知机与反向传播：神经网络的核心学习机制

## 1. Why Do We Need Multilayer Perceptrons?

为什么需要多层感知机？

### 1.1 Reviewing the Limitations of Perceptrons

回顾感知机的局限性

In Chapter 1, we learned that single-layer perceptrons can only solve **linearly separable** problems. The most classic example is the **XOR (Exclusive OR) problem**:

在第一章中，我们了解了单层感知机只能解决**线性可分**的问题。最经典的例子就是**异或 (XOR) 问题**：

**XOR Truth Table:**

- Input (0, 0) → Output 0
- Input (0, 1) → Output 1  
- Input (1, 0) → Output 1
- Input (1, 1) → Output 0

异或真值表：
- 输入 (0, 0) → 输出 0
- 输入 (0, 1) → 输出 1  
- 输入 (1, 0) → 输出 1
- 输入 (1, 1) → 输出 0

If we try to solve this problem with a single-layer perceptron, let the model be:

如果我们试图用单层感知机来解决这个问题，设模型为：

$$y = \text{step}(w_1x_1 + w_2x_2 + b)$$

where the step function is: $\text{step}(z) = \begin{cases} 1 & \text{if } z > 0 \\ 0 & \text{if } z \leq 0 \end{cases}$

其中 step 函数为：$\text{step}(z) = \begin{cases} 1 & \text{if } z > 0 \\ 0 & \text{if } z \leq 0 \end{cases}$

**Mathematical Proof that Single-Layer Perceptrons Cannot Solve XOR:**

数学证明单层感知机无法解决XOR：

Assume there exist weights $w_1, w_2$ and bias $b$ that can solve the XOR problem, then the following conditions must be satisfied simultaneously:

假设存在权重 $w_1, w_2$ 和偏置 $b$ 能够解决XOR问题，那么必须同时满足：

1. $(0,0)$: $w_1 \cdot 0 + w_2 \cdot 0 + b \leq 0 \Rightarrow b \leq 0$
2. $(0,1)$: $w_1 \cdot 0 + w_2 \cdot 1 + b > 0 \Rightarrow w_2 + b > 0$
3. $(1,0)$: $w_1 \cdot 1 + w_2 \cdot 0 + b > 0 \Rightarrow w_1 + b > 0$
4. $(1,1)$: $w_1 \cdot 1 + w_2 \cdot 1 + b \leq 0 \Rightarrow w_1 + w_2 + b \leq 0$

From condition 1, we get $b \leq 0$. From condition 2, we get $w_2 > -b \geq 0$. From condition 3, we get $w_1 > -b \geq 0$.

从条件1得到 $b \leq 0$，从条件2得到 $w_2 > -b \geq 0$，从条件3得到 $w_1 > -b \geq 0$。

Therefore, $w_1 > 0$ and $w_2 > 0$, which contradicts condition 4 ($w_1 + w_2 + b \leq 0$), because $w_1 + w_2 > 0$ while $b \leq 0$.

因此 $w_1 > 0$ 且 $w_2 > 0$，这与条件4 ($w_1 + w_2 + b \leq 0$) 矛盾，因为 $w_1 + w_2 > 0$ 而 $b \leq 0$。

**Conclusion:** Single-layer perceptrons cannot solve non-linearly separable problems.

结论：单层感知机无法解决非线性可分问题。

### 1.2 Introducing Hidden Layers: The Birth of Multilayer Perceptrons

引入隐藏层：多层感知机的诞生

**Multilayer Perceptrons (MLPs)** give models the ability to express non-linear relationships by introducing one or more **hidden layers**.

多层感知机 (Multilayer Perceptron, MLP) 通过引入一个或多个**隐藏层**，赋予模型表达非线性关系的能力。

**Analogy: Multilayer perceptrons are like a team of experts**
- **Input layer**: "Information collectors" that gather raw information
- **Hidden layers**: "Experts" that process and analyze information
- **Output layer**: "Decision makers" that make final decisions

类比：多层感知机就像一个专家团队
- **输入层**：收集原始信息的"信息员"
- **隐藏层**：处理和分析信息的"专家"
- **输出层**：做出最终决策的"决策者"

The experts at each layer perform some kind of "processing" on the information and then pass it to the next layer.

每一层的专家都会对信息进行某种"加工"，然后传递给下一层。

## 2. Structure of Multilayer Perceptrons

多层感知机的结构

### 2.1 Basic Architecture

基本架构

A typical three-layer MLP includes:
- **Input layer**: $n$ neurons, corresponding to $n$ input features
- **Hidden layer**: $h$ neurons
- **Output layer**: $m$ neurons, corresponding to $m$ output categories or values

一个典型的三层MLP包括：
- **输入层**：$n$ 个神经元，对应 $n$ 个输入特征
- **隐藏层**：$h$ 个神经元
- **输出层**：$m$ 个神经元，对应 $m$ 个输出类别或值

### 2.2 Mathematical Representation

数学表示

**Symbol Definitions:**
- $\mathbf{x} = [x_1, x_2, ..., x_n]^T$: Input vector
- $\mathbf{W}^{(1)}$: Weight matrix from input layer to hidden layer ($h \times n$)
- $\mathbf{b}^{(1)}$: Bias vector of hidden layer ($h \times 1$)
- $\mathbf{h}$: Output vector of hidden layer ($h \times 1$)
- $\mathbf{W}^{(2)}$: Weight matrix from hidden layer to output layer ($m \times h$)
- $\mathbf{b}^{(2)}$: Bias vector of output layer ($m \times 1$)
- $\mathbf{y}$: Final output vector ($m \times 1$)

符号定义：
- $\mathbf{x} = [x_1, x_2, ..., x_n]^T$：输入向量
- $\mathbf{W}^{(1)}$：输入层到隐藏层的权重矩阵 ($h \times n$)
- $\mathbf{b}^{(1)}$：隐藏层的偏置向量 ($h \times 1$)
- $\mathbf{h}$：隐藏层的输出向量 ($h \times 1$)
- $\mathbf{W}^{(2)}$：隐藏层到输出层的权重矩阵 ($m \times h$)
- $\mathbf{b}^{(2)}$：输出层的偏置向量 ($m \times 1$)
- $\mathbf{y}$：最终输出向量 ($m \times 1$)

**Forward Propagation Process:**

前向传播过程：

1. **Hidden Layer Computation:**
   $$\mathbf{z}^{(1)} = \mathbf{W}^{(1)} \mathbf{x} + \mathbf{b}^{(1)}$$
   $$\mathbf{h} = \sigma(\mathbf{z}^{(1)})$$

1. 隐藏层计算：

2. **Output Layer Computation:**
   $$\mathbf{z}^{(2)} = \mathbf{W}^{(2)} \mathbf{h} + \mathbf{b}^{(2)}$$
   $$\mathbf{y} = \sigma(\mathbf{z}^{(2)})$$

2. 输出层计算：

where $\sigma$ is the activation function (such as Sigmoid or ReLU).

其中 $\sigma$ 是激活函数（如Sigmoid或ReLU）。

## 3. Core Algorithm: Backpropagation

核心算法：反向传播 (Backpropagation)

### 3.1 Intuitive Understanding of Backpropagation

反向传播的直观理解

**Analogy: Reverse Tracing Problems in Factory Assembly Lines**

类比：工厂流水线逆向追溯问题

Imagine a factory manufacturing mobile phones:
- **Raw Materials** → **Parts Processing** → **Assembly** → **Quality Control** → **Finished Product**

想象一个制造手机的工厂：
- **原材料** → **零件加工** → **组装** → **质检** → **成品**

If the final product has quality issues, we need to trace back in reverse:
1. First check the quality control stage
2. Then check the assembly stage
3. Next check the parts processing stage
4. Finally check the raw materials

如果最终产品有质量问题，我们需要逆向追溯：
1. 首先检查质检环节
2. 然后检查组装环节  
3. 接着检查零件加工环节
4. 最后检查原材料

Backpropagation is such a "reverse tracing" process, starting from the error at the output layer, propagating layer by layer forward, calculating the "contribution" of each layer's parameters to the final error.

反向传播就是这样一个"逆向追溯"的过程，从输出层的误差开始，逐层向前传播，计算每一层参数

对最终误差的"贡献"。

### 3.2 Mathematical Derivation: Detailed Calculation Example

数学推导：详细计算实例

Let's use a concrete example to demonstrate the complete calculation process of backpropagation.

让我们用一个具体的例子来演示反向传播的完整计算过程。

**Problem Setup:** Using MLP to solve the XOR problem

问题设定：用MLP解决XOR问题

**Network Structure:**
- Input layer: 2 neurons ($x_1, x_2$)
- Hidden layer: 2 neurons ($h_1, h_2$)
- Output layer: 1 neuron ($y$)
- Activation function: Sigmoid $\sigma(z) = \frac{1}{1+e^{-z}}$

网络结构：
- 输入层：2个神经元 ($x_1, x_2$)
- 隐藏层：2个神经元 ($h_1, h_2$)
- 输出层：1个神经元 ($y$)
- 激活函数：Sigmoid $\sigma(z) = \frac{1}{1+e^{-z}}$

**Initial Weights and Biases:**

初始权重和偏置：

$$\mathbf{W}^{(1)} = \begin{bmatrix} 0.5 & 0.5 \\ -0.5 & -0.5 \end{bmatrix}, \quad \mathbf{b}^{(1)} = \begin{bmatrix} -0.2 \\ 0.7 \end{bmatrix}$$

$$\mathbf{W}^{(2)} = \begin{bmatrix} 1.0 & 1.0 \end{bmatrix}, \quad b^{(2)} = -0.5$$

**Training Sample:** $(x_1, x_2) = (1, 1)$, target output $t = 0$

训练样本：$(x_1, x_2) = (1, 1)$，目标输出 $t = 0$

#### Step 1: Forward Propagation

步骤1：前向传播

**Hidden Layer Computation:**

隐藏层计算：

$$z_1^{(1)} = w_{11}^{(1)}x_1 + w_{12}^{(1)}x_2 + b_1^{(1)} = 0.5 \times 1 + 0.5 \times 1 + (-0.2) = 0.8$$

$$z_2^{(1)} = w_{21}^{(1)}x_1 + w_{22}^{(1)}x_2 + b_2^{(1)} = (-0.5) \times 1 + (-0.5) \times 1 + 0.7 = -0.3$$

$$h_1 = \sigma(0.8) = \frac{1}{1+e^{-0.8}} = \frac{1}{1+0.449} = 0.690$$

$$h_2 = \sigma(-0.3) = \frac{1}{1+e^{0.3}} = \frac{1}{1+1.350} = 0.426$$

**Output Layer Computation:**

输出层计算：

$$z^{(2)} = w_1^{(2)}h_1 + w_2^{(2)}h_2 + b^{(2)} = 1.0 \times 0.690 + 1.0 \times 0.426 + (-0.5) = 0.616$$

$$y = \sigma(0.616) = \frac{1}{1+e^{-0.616}} = \frac{1}{1+0.540} = 0.649$$

**Loss Calculation:**
Using mean squared error loss:

损失计算：
使用均方误差损失：

$$L = \frac{1}{2}(t - y)^2 = \frac{1}{2}(0 - 0.649)^2 = \frac{1}{2} \times 0.421 = 0.211$$

#### Step 2: Backpropagation

步骤2：反向传播

**Derivative of Sigmoid Function:**

Sigmoid函数的导数：

$$\sigma'(z) = \sigma(z)(1-\sigma(z))$$

**Detailed Derivation Process:**

详细求导过程：

The Sigmoid function is defined as:

Sigmoid函数定义为：

$$\sigma(z) = \frac{1}{1+e^{-z}}$$

To find its derivative, we'll use the **quotient rule** and **chain rule**.

为了求它的导数，我们将使用**商法则**和**链式法则**。

**Step 1: Rewrite the function**

步骤1：重写函数

We can rewrite the Sigmoid function as:

我们可以将Sigmoid函数重写为：

$$\sigma(z) = (1+e^{-z})^{-1}$$

**Step 2: Apply the chain rule**

步骤2：应用链式法则

Using the chain rule: $\frac{d}{dz}[f(g(z))] = f'(g(z)) \cdot g'(z)$

使用链式法则：$\frac{d}{dz}[f(g(z))] = f'(g(z)) \cdot g'(z)$

Let $u = 1+e^{-z}$, then $\sigma(z) = u^{-1}$

设 $u = 1+e^{-z}$，那么 $\sigma(z) = u^{-1}$

$$\frac{d\sigma}{dz} = \frac{d}{du}(u^{-1}) \cdot \frac{du}{dz}$$

**Step 3: Calculate each derivative**

步骤3：计算各个导数

$$\frac{d}{du}(u^{-1}) = -u^{-2} = -\frac{1}{u^2} = -\frac{1}{(1+e^{-z})^2}$$

$$\frac{du}{dz} = \frac{d}{dz}(1+e^{-z}) = 0 + e^{-z} \cdot (-1) = -e^{-z}$$

**Step 4: Combine the results**

步骤4：合并结果

$$\frac{d\sigma}{dz} = -\frac{1}{(1+e^{-z})^2} \cdot (-e^{-z}) = \frac{e^{-z}}{(1+e^{-z})^2}$$

**Step 5: Simplify to elegant form**

步骤5：简化为优雅形式

Now we'll manipulate this expression to get the elegant form:

现在我们将操作这个表达式得到优雅的形式：

$$\frac{e^{-z}}{(1+e^{-z})^2} = \frac{1}{1+e^{-z}} \cdot \frac{e^{-z}}{1+e^{-z}}$$

Notice that $\frac{1}{1+e^{-z}} = \sigma(z)$

注意到 $\frac{1}{1+e^{-z}} = \sigma(z)$

For the second fraction, we can write:

对于第二个分数，我们可以写成：

$$\frac{e^{-z}}{1+e^{-z}} = \frac{1+e^{-z}-1}{1+e^{-z}} = \frac{1+e^{-z}}{1+e^{-z}} - \frac{1}{1+e^{-z}} = 1 - \sigma(z)$$

Therefore:

因此：

$$\sigma'(z) = \sigma(z) \cdot (1-\sigma(z))$$

**Verification with Numerical Example:**

数值验证例子：

Let's verify this formula with $z = 0$:

让我们用 $z = 0$ 验证这个公式：

$$\sigma(0) = \frac{1}{1+e^{0}} = \frac{1}{1+1} = 0.5$$

Using our derived formula:

使用我们推导的公式：

$$\sigma'(0) = \sigma(0)(1-\sigma(0)) = 0.5 \times (1-0.5) = 0.5 \times 0.5 = 0.25$$

Using the original derivative formula:

使用原始导数公式：

$$\sigma'(0) = \frac{e^{0}}{(1+e^{0})^2} = \frac{1}{(1+1)^2} = \frac{1}{4} = 0.25$$

✓ The results match!

✓ 结果匹配！

**Why This Form is Beautiful:**

为什么这个形式很优美：

1. **Computational efficiency**: We only need to compute $\sigma(z)$ once, then use it to get $\sigma'(z)$
2. **Numerical stability**: Avoids computing exponentials twice
3. **Elegant mathematics**: Shows the inherent symmetry of the Sigmoid function

1. **计算效率**：我们只需要计算一次 $\sigma(z)$，然后用它得到 $\sigma'(z)$
2. **数值稳定性**：避免了两次计算指数函数
3. **优雅数学**：展现了Sigmoid函数的内在对称性

**Geometric Interpretation:**

几何解释：

The derivative $\sigma'(z) = \sigma(z)(1-\sigma(z))$ reaches its maximum when $\sigma(z) = 0.5$ (i.e., when $z = 0$), giving $\sigma'(0) = 0.25$. This means the Sigmoid function has the steepest slope at its center point.

导数 $\sigma'(z) = \sigma(z)(1-\sigma(z))$ 在 $\sigma(z) = 0.5$（即 $z = 0$）时达到最大值，给出 $\sigma'(0) = 0.25$。这意味着Sigmoid函数在其中心点处有最陡的斜率。

**Output Layer Error Calculation:**

输出层误差计算：

$$\delta^{(2)} = \frac{\partial L}{\partial z^{(2)}} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial z^{(2)}}$$

$$\frac{\partial L}{\partial y} = -(t - y) = -(0 - 0.649) = 0.649$$

$$\frac{\partial y}{\partial z^{(2)}} = \sigma'(z^{(2)}) = y(1-y) = 0.649 \times (1-0.649) = 0.649 \times 0.351 = 0.228$$

$$\delta^{(2)} = 0.649 \times 0.228 = 0.148$$

**Hidden Layer Error Calculation:**

隐藏层误差计算：

$$\delta_1^{(1)} = \frac{\partial L}{\partial z_1^{(1)}} = \frac{\partial L}{\partial z^{(2)}} \cdot \frac{\partial z^{(2)}}{\partial h_1} \cdot \frac{\partial h_1}{\partial z_1^{(1)}}$$

$$\frac{\partial z^{(2)}}{\partial h_1} = w_1^{(2)} = 1.0$$

$$\frac{\partial h_1}{\partial z_1^{(1)}} = \sigma'(z_1^{(1)}) = h_1(1-h_1) = 0.690 \times (1-0.690) = 0.690 \times 0.310 = 0.214$$

$$\delta_1^{(1)} = \delta^{(2)} \times w_1^{(2)} \times \sigma'(z_1^{(1)}) = 0.148 \times 1.0 \times 0.214 = 0.032$$

Similarly:

类似地：

$$\delta_2^{(1)} = \delta^{(2)} \times w_2^{(2)} \times \sigma'(z_2^{(1)}) = 0.148 \times 1.0 \times h_2(1-h_2)$$

$$= 0.148 \times 1.0 \times 0.426 \times (1-0.426) = 0.148 \times 0.426 \times 0.574 = 0.036$$

#### Step 3: Weight and Bias Updates

步骤3：权重和偏置更新

Let learning rate $\eta = 0.5$

设学习率 $\eta = 0.5$

**Output Layer Weight Updates:**

输出层权重更新：

$$\frac{\partial L}{\partial w_1^{(2)}} = \delta^{(2)} \times h_1 = 0.148 \times 0.690 = 0.102$$

$$\frac{\partial L}{\partial w_2^{(2)}} = \delta^{(2)} \times h_2 = 0.148 \times 0.426 = 0.063$$

$$w_1^{(2),new} = w_1^{(2)} - \eta \frac{\partial L}{\partial w_1^{(2)}} = 1.0 - 0.5 \times 0.102 = 1.0 - 0.051 = 0.949$$

$$w_2^{(2),new} = w_2^{(2)} - \eta \frac{\partial L}{\partial w_2^{(2)}} = 1.0 - 0.5 \times 0.063 = 1.0 - 0.032 = 0.968$$

**Output Layer Bias Updates:**

输出层偏置更新：

$$\frac{\partial L}{\partial b^{(2)}} = \delta^{(2)} = 0.148$$

$$b^{(2),new} = b^{(2)} - \eta \frac{\partial L}{\partial b^{(2)}} = -0.5 - 0.5 \times 0.148 = -0.5 - 0.074 = -0.574$$

**Hidden Layer Weight Updates:**

隐藏层权重更新：

$$\frac{\partial L}{\partial w_{11}^{(1)}} = \delta_1^{(1)} \times x_1 = 0.032 \times 1 = 0.032$$

$$\frac{\partial L}{\partial w_{12}^{(1)}} = \delta_1^{(1)} \times x_2 = 0.032 \times 1 = 0.032$$

$$w_{11}^{(1),new} = 0.5 - 0.5 \times 0.032 = 0.5 - 0.016 = 0.484$$

$$w_{12}^{(1),new} = 0.5 - 0.5 \times 0.032 = 0.5 - 0.016 = 0.484$$

Similarly, calculate other weights:

类似地计算其他权重：

$$w_{21}^{(1),new} = -0.5 - 0.5 \times 0.036 = -0.518$$
$$w_{22}^{(1),new} = -0.5 - 0.5 \times 0.036 = -0.518$$

**Hidden Layer Bias Updates:**

隐藏层偏置更新：

$$b_1^{(1),new} = -0.2 - 0.5 \times 0.032 = -0.216$$
$$b_2^{(1),new} = 0.7 - 0.5 \times 0.036 = 0.682$$

### 3.3 Geometric Intuition of Gradient Descent

梯度下降的几何直观

**Gradient descent is like a blind person going downhill:**

梯度下降就像一个盲人下山：

Imagine you are a blindfolded person standing on a hillside, with the goal of finding the foot of the mountain (the minimum value of the loss function).

想象你是一个蒙着眼睛的人，站在山坡上，目标是找到山脚（损失函数的最小值）。

1. **Feel the slope:** Use your feet to sense the slope at the current position (calculate gradient)
2. **Choose direction:** Walk in the steepest downhill direction (negative gradient direction)
3. **Take a step:** Take a small step based on the slope (learning rate controls step size)
4. **Repeat process:** After reaching a new position, repeat the above process

1. 感受坡度：用脚感受当前位置的坡度（计算梯度）
2. 选择方向：向最陡的下坡方向走（负梯度方向）
3. 迈出步子：根据坡度走一小步（学习率控制步长）
4. 重复过程：到达新位置后，重复上述过程

**Mathematical Representation:**

数学表示：

$$\theta_{new} = \theta_{old} - \eta \nabla_\theta L(\theta)$$

where:
- $\theta$ represents all parameters (weights and biases)
- $\eta$ is the learning rate (step size)
- $\nabla_\theta L(\theta)$ is the gradient of the loss function with respect to parameters

其中：
- $\theta$ 代表所有参数（权重和偏置）
- $\eta$ 是学习率（步长）
- $\nabla_\theta L(\theta)$ 是损失函数对参数的梯度

### 3.4 Chain Rule

链式法则 (Chain Rule)

The core mathematical principle of backpropagation is the **chain rule**.

反向传播的核心数学原理是**链式法则**。

**Basic Form of Chain Rule:**
If $y = f(u)$ and $u = g(x)$, then:

链式法则的基本形式：
如果 $y = f(u)$ 且 $u = g(x)$，那么：

$$\frac{dy}{dx} = \frac{dy}{du} \cdot \frac{du}{dx}$$

**Application in Neural Networks:**
For a three-layer network, the influence of weight $w_{ij}^{(1)}$ on the loss function needs to go through the following path:

在神经网络中的应用：
对于一个三层网络，权重 $w_{ij}^{(1)}$ 对损失函数的影响需要通过以下路径：

$$w_{ij}^{(1)} \to z_j^{(1)} \to h_j \to z^{(2)} \to y \to L$$

Therefore:

因此：

$$\frac{\partial L}{\partial w_{ij}^{(1)}} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial z^{(2)}} \cdot \frac{\partial z^{(2)}}{\partial h_j} \cdot \frac{\partial h_j}{\partial z_j^{(1)}} \cdot \frac{\partial z_j^{(1)}}{\partial w_{ij}^{(1)}}$$

**Actual Calculation Example:**
For $w_{11}^{(1)}$ in the previous example:

实际计算示例：
对于前面例子中的 $w_{11}^{(1)}$：

$$\frac{\partial z_1^{(1)}}{\partial w_{11}^{(1)}} = x_1 = 1$$

$$\frac{\partial h_1}{\partial z_1^{(1)}} = \sigma'(z_1^{(1)}) = 0.214$$

$$\frac{\partial z^{(2)}}{\partial h_1} = w_1^{(2)} = 1.0$$

$$\frac{\partial y}{\partial z^{(2)}} = \sigma'(z^{(2)}) = 0.228$$

$$\frac{\partial L}{\partial y} = 0.649$$

$$\frac{\partial L}{\partial w_{11}^{(1)}} = 0.649 \times 0.228 \times 1.0 \times 0.214 \times 1 = 0.032$$

This is exactly the result we calculated earlier!

这正是我们前面计算的结果！

## 4. Loss Functions and Optimizers

损失函数与优化器

### 4.1 Loss Functions for Classification Problems: Cross-Entropy Loss

分类问题的损失函数：交叉熵损失

**Binary Cross-Entropy Loss:**

二分类交叉熵损失：

$$L = -[t \log(y) + (1-t) \log(1-y)]$$

**Multi-class Cross-Entropy Loss:**

多分类交叉熵损失：

$$L = -\sum_{i=1}^{C} t_i \log(y_i)$$

where $C$ is the number of classes, $t_i$ is the true label for class $i$ (one-hot encoded), and $y_i$ is the predicted probability for class $i$.

其中 $C$ 是类别数，$t_i$ 是第 $i$ 类的真实标签（one-hot编码），$y_i$ 是第 $i$ 类的预测概率。

**Numerical Calculation Example:**

数值计算例子：

Assume a 3-class classification problem:
- True label: $\mathbf{t} = [0, 1, 0]$ (class 2)
- Predicted probabilities: $\mathbf{y} = [0.2, 0.7, 0.1]$

假设有3个类别的分类问题：
- 真实标签：$\mathbf{t} = [0, 1, 0]$（第2类）
- 预测概率：$\mathbf{y} = [0.2, 0.7, 0.1]$

Cross-entropy loss:

交叉熵损失：

$$L = -(0 \times \log(0.2) + 1 \times \log(0.7) + 0 \times \log(0.1))$$
$$= -\log(0.7) = -(-0.357) = 0.357$$

**Gradient of Loss Function:**

损失函数的梯度：

$$\frac{\partial L}{\partial y_i} = -\frac{t_i}{y_i}$$

For the special case of Softmax output:

对于Softmax输出的特殊情况：

$$\frac{\partial L}{\partial z_i} = y_i - t_i$$

**Detailed Derivation of Cross-Entropy Loss Gradients:**

交叉熵损失梯度的详细推导：

Let's derive these two important gradient formulas step by step.

让我们逐步推导这两个重要的梯度公式。

**Part 1: Gradient with respect to output probabilities**

第一部分：对输出概率的梯度

The multi-class cross-entropy loss is defined as:

多分类交叉熵损失定义为：

$$L = -\sum_{i=1}^{C} t_i \log(y_i)$$

where $C$ is the number of classes, $t_i$ is the true label (one-hot encoded), and $y_i$ is the predicted probability.

其中 $C$ 是类别数，$t_i$ 是真实标签（one-hot编码），$y_i$ 是预测概率。

To find $\frac{\partial L}{\partial y_j}$, we take the partial derivative with respect to $y_j$:

为了求 $\frac{\partial L}{\partial y_j}$，我们对 $y_j$ 求偏导：

$$\frac{\partial L}{\partial y_j} = \frac{\partial}{\partial y_j}\left(-\sum_{i=1}^{C} t_i \log(y_i)\right)$$

$$= -\frac{\partial}{\partial y_j}\left(\sum_{i=1}^{C} t_i \log(y_i)\right)$$

Since only the $j$-th term in the sum depends on $y_j$:

由于求和中只有第 $j$ 项依赖于 $y_j$：

$$\frac{\partial L}{\partial y_j} = -\frac{\partial}{\partial y_j}(t_j \log(y_j)) = -t_j \frac{\partial}{\partial y_j}(\log(y_j))$$

**Important Note about Logarithm Base:**

关于对数底数的重要说明：

In machine learning and cross-entropy loss, we typically use the **natural logarithm** $\ln(x)$ (base $e$), not the common logarithm $\log_{10}(x)$ (base 10). The notation $\log(x)$ in machine learning contexts usually refers to $\ln(x)$.

在机器学习和交叉熵损失中，我们通常使用**自然对数** $\ln(x)$（以$e$为底），而不是常用对数 $\log_{10}(x)$（以10为底）。在机器学习语境中，$\log(x)$ 记号通常指的是 $\ln(x)$。

**Derivative formulas for different logarithm bases:**

不同底数对数的导数公式：

- Natural logarithm: $\frac{d}{dx}(\ln(x)) = \frac{1}{x}$
- Common logarithm: $\frac{d}{dx}(\log_{10}(x)) = \frac{1}{x \ln(10)}$
- General base: $\frac{d}{dx}(\log_a(x)) = \frac{1}{x \ln(a)}$

- 自然对数：$\frac{d}{dx}(\ln(x)) = \frac{1}{x}$
- 常用对数：$\frac{d}{dx}(\log_{10}(x)) = \frac{1}{x \ln(10)}$
- 一般底数：$\frac{d}{dx}(\log_a(x)) = \frac{1}{x \ln(a)}$

Since we're using natural logarithm in cross-entropy loss: $L = -\sum_{i=1}^{C} t_i \ln(y_i)$

由于我们在交叉熵损失中使用自然对数：$L = -\sum_{i=1}^{C} t_i \ln(y_i)$

Using the derivative $\frac{d}{dx}(\ln(x)) = \frac{1}{x}$:

使用导数 $\frac{d}{dx}(\ln(x)) = \frac{1}{x}$：

$$\frac{\partial L}{\partial y_j} = -t_j \cdot \frac{1}{y_j} = -\frac{t_j}{y_j}$$

**Part 2: Gradient with respect to logits (Softmax case)**

第二部分：对logits的梯度（Softmax情况）

For the Softmax activation, we have:

对于Softmax激活函数，我们有：

$$y_i = \frac{e^{z_i}}{\sum_{k=1}^{C} e^{z_k}}$$

where $z_i$ are the logits (pre-activation values).

其中 $z_i$ 是logits（激活前的值）。

To find $\frac{\partial L}{\partial z_j}$, we use the chain rule:

为了求 $\frac{\partial L}{\partial z_j}$，我们使用链式法则：

$$\frac{\partial L}{\partial z_j} = \sum_{i=1}^{C} \frac{\partial L}{\partial y_i} \frac{\partial y_i}{\partial z_j}$$

We already know that $\frac{\partial L}{\partial y_i} = -\frac{t_i}{y_i}$, so:

我们已经知道 $\frac{\partial L}{\partial y_i} = -\frac{t_i}{y_i}$，所以：

$$\frac{\partial L}{\partial z_j} = \sum_{i=1}^{C} \left(-\frac{t_i}{y_i}\right) \frac{\partial y_i}{\partial z_j}$$

**Step 2.1: Calculate $\frac{\partial y_i}{\partial z_j}$ for Softmax**

步骤2.1：计算Softmax的 $\frac{\partial y_i}{\partial z_j}$

For Softmax, we need to consider two cases:

对于Softmax，我们需要考虑两种情况：

**Case 1: $i = j$**

情况1：$i = j$

$$\frac{\partial y_i}{\partial z_i} = \frac{\partial}{\partial z_i}\left(\frac{e^{z_i}}{\sum_{k=1}^{C} e^{z_k}}\right)$$

Using the quotient rule: $\frac{d}{dx}\left(\frac{f(x)}{g(x)}\right) = \frac{f'(x)g(x) - f(x)g'(x)}{[g(x)]^2}$

使用 商法则：$\frac{d}{dx}\left(\frac{f(x)}{g(x)}\right) = \frac{f'(x)g(x) - f(x)g'(x)}{[g(x)]^2}$

$$\frac{\partial y_i}{\partial z_i} = \frac{e^{z_i} \cdot \sum_{k=1}^{C} e^{z_k} - e^{z_i} \cdot e^{z_i}}{(\sum_{k=1}^{C} e^{z_k})^2}$$

$$= \frac{e^{z_i}(\sum_{k=1}^{C} e^{z_k} - e^{z_i})}{(\sum_{k=1}^{C} e^{z_k})^2}$$

$$= \frac{e^{z_i}}{\sum_{k=1}^{C} e^{z_k}} \cdot \frac{\sum_{k=1}^{C} e^{z_k} - e^{z_i}}{\sum_{k=1}^{C} e^{z_k}}$$

$$= y_i \cdot (1 - y_i)$$

**Case 2: $i \neq j$**

情况2：$i \neq j$

$$\frac{\partial y_i}{\partial z_j} = \frac{\partial}{\partial z_j}\left(\frac{e^{z_i}}{\sum_{k=1}^{C} e^{z_k}}\right)$$

$$= \frac{0 \cdot \sum_{k=1}^{C} e^{z_k} - e^{z_i} \cdot e^{z_j}}{(\sum_{k=1}^{C} e^{z_k})^2}$$

$$= -\frac{e^{z_i} \cdot e^{z_j}}{(\sum_{k=1}^{C} e^{z_k})^2} = -y_i y_j$$

**Step 2.2: Substitute back into the gradient formula**

步骤2.2：代入梯度公式

**Why we need to consider both cases:**

为什么需要考虑两种情况：

The chain rule formula is: $\frac{\partial L}{\partial z_j} = \sum_{i=1}^{C} \frac{\partial L}{\partial y_i} \frac{\partial y_i}{\partial z_j}$

链式法则公式是：$\frac{\partial L}{\partial z_j} = \sum_{i=1}^{C} \frac{\partial L}{\partial y_i} \frac{\partial y_i}{\partial z_j}$

This sum includes **all** $i$ from 1 to $C$, so we must consider:
- When $i = j$: one term where $\frac{\partial y_i}{\partial z_j} = y_i(1-y_i)$
- When $i \neq j$: $(C-1)$ terms where $\frac{\partial y_i}{\partial z_j} = -y_i y_j$

这个求和包括**所有**从1到$C$的$i$，所以我们必须考虑：
- 当 $i = j$ 时：一项，其中 $\frac{\partial y_i}{\partial z_j} = y_i(1-y_i)$
- 当 $i \neq j$ 时：$(C-1)$ 项，其中 $\frac{\partial y_i}{\partial z_j} = -y_i y_j$

**Detailed substitution:**

详细代入过程：

$$\frac{\partial L}{\partial z_j} = \sum_{i=1}^{C} \frac{\partial L}{\partial y_i} \frac{\partial y_i}{\partial z_j}$$

We already know that $\frac{\partial L}{\partial y_i} = -\frac{t_i}{y_i}$, so:

我们已经知道 $\frac{\partial L}{\partial y_i} = -\frac{t_i}{y_i}$，所以：

$$\frac{\partial L}{\partial z_j} = \sum_{i=1}^{C} \left(-\frac{t_i}{y_i}\right) \frac{\partial y_i}{\partial z_j}$$

**Split the sum into two parts:**

将求和分为两部分：

$$\frac{\partial L}{\partial z_j} = \left(-\frac{t_j}{y_j}\right) \frac{\partial y_j}{\partial z_j} + \sum_{i \neq j} \left(-\frac{t_i}{y_i}\right) \frac{\partial y_i}{\partial z_j}$$

**Substitute the derivatives we calculated:**

代入我们计算的导数：

For $i = j$: $\frac{\partial y_j}{\partial z_j} = y_j(1-y_j)$

对于 $i = j$：$\frac{\partial y_j}{\partial z_j} = y_j(1-y_j)$

For $i \neq j$: $\frac{\partial y_i}{\partial z_j} = -y_i y_j$

对于 $i \neq j$：$\frac{\partial y_i}{\partial z_j} = -y_i y_j$

$$\frac{\partial L}{\partial z_j} = \left(-\frac{t_j}{y_j}\right) \cdot y_j(1-y_j) + \sum_{i \neq j} \left(-\frac{t_i}{y_i}\right) \cdot (-y_i y_j)$$

$$= -\frac{t_j}{y_j} \cdot y_j(1-y_j) - \sum_{i \neq j} \frac{t_i}{y_i} \cdot (-y_i y_j)$$

$$= -t_j(1-y_j) + \sum_{i \neq j} t_i y_j$$

**Final algebraic simplification:**

最终代数化简：

$$\frac{\partial L}{\partial z_j} = -t_j(1-y_j) + \sum_{i \neq j} t_i y_j$$

$$= -t_j + t_j y_j + y_j \sum_{i \neq j} t_i$$

Since $\sum_{i=1}^{C} t_i = 1$ (one-hot encoding), we have $\sum_{i \neq j} t_i = 1 - t_j$:

由于 $\sum_{i=1}^{C} t_i = 1$（one-hot编码），我们有 $\sum_{i \neq j} t_i = 1 - t_j$：

$$\frac{\partial L}{\partial z_j} = -t_j + t_j y_j + y_j(1 - t_j)$$

$$= -t_j + t_j y_j + y_j - t_j y_j$$

$$= -t_j + y_j$$

$$= y_j - t_j$$

**Verification with our numerical example:**

用我们的数值例子验证：

**Step-by-step calculation for $j = 2$:**

对 $j = 2$ 的逐步计算：

Let's use a 3-class example: $C = 3$, $j = 2$
- $\mathbf{t} = [0, 1, 0]$ (true labels)
- $\mathbf{y} = [0.2, 0.7, 0.1]$ (predicted probabilities)

让我们用一个3类的例子：$C = 3$, $j = 2$
- $\mathbf{t} = [0, 1, 0]$（真实标签）
- $\mathbf{y} = [0.2, 0.7, 0.1]$（预测概率）

**Method 1: Direct term-by-term calculation**

方法1：逐项直接计算

$$\frac{\partial L}{\partial z_2} = \sum_{i=1}^{3} \left(-\frac{t_i}{y_i}\right) \frac{\partial y_i}{\partial z_2}$$

**For $i = 1$ (where $i \neq j$):**
$$\left(-\frac{t_1}{y_1}\right) \frac{\partial y_1}{\partial z_2} = \left(-\frac{0}{0.2}\right) \times (-y_1 y_2) = 0 \times (-0.2 \times 0.7) = 0$$

**For $i = 2$ (where $i = j$):**
$$\left(-\frac{t_2}{y_2}\right) \frac{\partial y_2}{\partial z_2} = \left(-\frac{1}{0.7}\right) \times y_2(1-y_2) = -1.429 \times 0.7 \times 0.3 = -0.300$$

**For $i = 3$ (where $i \neq j$):**
$$\left(-\frac{t_3}{y_3}\right) \frac{\partial y_3}{\partial z_2} = \left(-\frac{0}{0.1}\right) \times (-y_3 y_2) = 0 \times (-0.1 \times 0.7) = 0$$

**Sum all terms:**
$$\frac{\partial L}{\partial z_2} = 0 + (-0.300) + 0 = -0.300$$

**Method 2: Using the intermediate algebraic form**

方法2：使用中间代数形式

$$\frac{\partial L}{\partial z_2} = -t_2(1-y_2) + \sum_{i \neq 2} t_i y_2$$
$$= -1 \times (1-0.7) + (0 + 0) \times 0.7$$
$$= -1 \times 0.3 + 0 = -0.3$$

**Method 3: Using the final elegant formula**

方法3：使用最终优雅公式

$$\frac{\partial L}{\partial z_2} = y_2 - t_2 = 0.7 - 1 = -0.3$$ ✓

All three methods give the same result! This confirms our derivation is correct.

三种方法都给出相同的结果！这证实了我们的推导是正确的。

**Why the Softmax gradient is so elegant:**

为什么Softmax梯度如此优雅：

The formula $\frac{\partial L}{\partial z_i} = y_i - t_i$ is remarkably simple and intuitive:
- If $y_i > t_i$ (overconfident), gradient is positive → decrease $z_i$
- If $y_i < t_i$ (underconfident), gradient is negative → increase $z_i$
- The magnitude of adjustment is proportional to the prediction error

公式 $\frac{\partial L}{\partial z_i} = y_i - t_i$ 非常简单直观：
- 如果 $y_i > t_i$（过度自信），梯度为正 → 减少 $z_i$
- 如果 $y_i < t_i$（信心不足），梯度为负 → 增加 $z_i$
- 调整幅度与预测误差成正比

This concise result makes backpropagation calculations very efficient.