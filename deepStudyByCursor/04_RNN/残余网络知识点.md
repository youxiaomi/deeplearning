# Residual Networks (ResNet): The "Highway" Revolution in Deep Learning
# æ®‹ä½™ç½‘ç»œï¼šæ·±åº¦å­¦ä¹ ä¸­çš„"é«˜é€Ÿå…¬è·¯"é©å‘½

## æ ¸å¿ƒæ¦‚å¿µæ€»è§ˆ (Core Concepts Overview)

### What is ResNet?
### ä»€ä¹ˆæ˜¯ResNetï¼Ÿ

**ResNet (Residual Network)** is a revolutionary deep learning architecture that solved the **degradation problem** in very deep neural networks. Before ResNet, deeper networks often performed worse than shallower ones, even on training data. ResNet introduced **skip connections** (also called shortcut connections) that allow information to "jump over" layers, creating "highways" for gradient flow.

**ResNetï¼ˆæ®‹ä½™ç½‘ç»œï¼‰**æ˜¯ä¸€ç§é©å‘½æ€§çš„æ·±åº¦å­¦ä¹ æ¶æ„ï¼Œè§£å†³äº†è¶…æ·±ç¥ç»ç½‘ç»œä¸­çš„**é€€åŒ–é—®é¢˜**ã€‚åœ¨ResNetä¹‹å‰ï¼Œæ›´æ·±çš„ç½‘ç»œå¾€å¾€æ¯”è¾ƒæµ…çš„ç½‘ç»œè¡¨ç°æ›´å·®ï¼Œå³ä½¿åœ¨è®­ç»ƒæ•°æ®ä¸Šä¹Ÿæ˜¯å¦‚æ­¤ã€‚ResNetå¼•å…¥äº†**è·³è·ƒè¿æ¥**ï¼ˆä¹Ÿç§°ä¸ºå¿«æ·è¿æ¥ï¼‰ï¼Œå…è®¸ä¿¡æ¯"è·³è¿‡"å±‚ï¼Œä¸ºæ¢¯åº¦æµåˆ›å»º"é«˜é€Ÿå…¬è·¯"ã€‚

**Analogy:** Think of a multi-story building where you need to get from the ground floor to the top. Traditional deep networks are like taking stairs step by step, where each step becomes harder as you go higher. ResNet is like having both stairs AND elevators - you can take the stairs for some floors, but the elevator (skip connection) helps you reach the top more efficiently!
**ç±»æ¯”ï¼š** æƒ³è±¡ä¸€åº§å¤šå±‚å»ºç­‘ï¼Œä½ éœ€è¦ä»ä¸€æ¥¼åˆ°é¡¶å±‚ã€‚ä¼ ç»Ÿçš„æ·±åº¦ç½‘ç»œå°±åƒä¸€æ­¥ä¸€æ­¥çˆ¬æ¥¼æ¢¯ï¼Œæ¯ä¸€æ­¥éƒ½éšç€æ¥¼å±‚å¢é«˜è€Œå˜å¾—æ›´å›°éš¾ã€‚ResNetå°±åƒæ—¢æœ‰æ¥¼æ¢¯åˆæœ‰ç”µæ¢¯â€”â€”ä½ å¯ä»¥åœ¨æŸäº›æ¥¼å±‚èµ°æ¥¼æ¢¯ï¼Œä½†ç”µæ¢¯ï¼ˆè·³è·ƒè¿æ¥ï¼‰å¸®åŠ©ä½ æ›´é«˜æ•ˆåœ°åˆ°è¾¾é¡¶å±‚ï¼

## 1. The Problem ResNet Solved
## 1. ResNetè§£å†³çš„é—®é¢˜

### 1.1 The Degradation Problem
### 1.1 é€€åŒ–é—®é¢˜

**Key Insight:** Adding more layers to deep networks was making them perform WORSE, not better.
**å…³é”®æ´å¯Ÿï¼š** åœ¨æ·±åº¦ç½‘ç»œä¸­æ·»åŠ æ›´å¤šå±‚ä½¿å®ƒä»¬è¡¨ç°æ›´å·®ï¼Œè€Œä¸æ˜¯æ›´å¥½ã€‚

```python
import torch
import torch.nn as nn
import matplotlib.pyplot as plt

def illustrate_degradation_problem():
    """
    æ¼”ç¤ºç½‘ç»œé€€åŒ–é—®é¢˜
    Illustrate the degradation problem
    """
    # æ¨¡æ‹Ÿä¸åŒæ·±åº¦ç½‘ç»œçš„è®­ç»ƒè¯¯å·®
    # Simulate training errors for different depth networks
    
    depths = [10, 20, 30, 40, 50, 60, 70]
    
    # ä¼ ç»Ÿæ·±åº¦ç½‘ç»œï¼šéšæ·±åº¦å¢åŠ ï¼Œè®­ç»ƒè¯¯å·®ä¸Šå‡
    # Traditional deep networks: training error increases with depth
    traditional_errors = [3.5, 4.2, 5.8, 7.1, 9.3, 12.1, 15.7]
    
    # ResNetï¼šéšæ·±åº¦å¢åŠ ï¼Œè®­ç»ƒè¯¯å·®ä¸‹é™
    # ResNet: training error decreases with depth
    resnet_errors = [3.5, 3.1, 2.8, 2.5, 2.3, 2.1, 2.0]
    
    plt.figure(figsize=(10, 6))
    plt.plot(depths, traditional_errors, 'r-o', label='Traditional Deep Networks', linewidth=2, markersize=8)
    plt.plot(depths, resnet_errors, 'b-s', label='ResNet', linewidth=2, markersize=8)
    
    plt.xlabel('Network Depth (Number of Layers)', fontsize=12)
    plt.ylabel('Training Error (%)', fontsize=12)
    plt.title('The Degradation Problem: Why Deeper â‰  Better (Before ResNet)', fontsize=14)
    plt.legend(fontsize=11)
    plt.grid(True, alpha=0.3)
    plt.xticks(depths)
    
    # æ·»åŠ è¯´æ˜æ–‡æœ¬
    plt.text(45, 10, 'Problem: Deeper networks\nperform worse!', 
             bbox=dict(boxstyle="round,pad=0.3", facecolor="lightcoral", alpha=0.7),
             fontsize=10, ha='center')
    
    plt.text(60, 4, 'Solution: ResNet allows\ndeeper networks to excel!', 
             bbox=dict(boxstyle="round,pad=0.3", facecolor="lightblue", alpha=0.7),
             fontsize=10, ha='center')
    
    plt.tight_layout()
    plt.show()

# è¿è¡Œæ¼”ç¤º
illustrate_degradation_problem()
```

### 1.2 Why Did This Happen?
### 1.2 ä¸ºä»€ä¹ˆä¼šå‘ç”Ÿè¿™ç§æƒ…å†µï¼Ÿ

**Gradient Vanishing/Exploding:** In very deep networks, gradients become extremely small (vanishing) or extremely large (exploding) as they propagate backwards through many layers.
**æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸ï¼š** åœ¨éå¸¸æ·±çš„ç½‘ç»œä¸­ï¼Œæ¢¯åº¦åœ¨é€šè¿‡è®¸å¤šå±‚å‘åä¼ æ’­æ—¶å˜å¾—æå°ï¼ˆæ¶ˆå¤±ï¼‰æˆ–æå¤§ï¼ˆçˆ†ç‚¸ï¼‰ã€‚

**Mathematical Explanation:**
**æ•°å­¦è§£é‡Šï¼š**

In a chain of multiplications (like backpropagation), if each factor is:
åœ¨ä¹˜æ³•é“¾ä¸­ï¼ˆå¦‚åå‘ä¼ æ’­ï¼‰ï¼Œå¦‚æœæ¯ä¸ªå› å­æ˜¯ï¼š
- Slightly less than 1 â†’ Product approaches 0 (vanishing)
- ç¨å¾®å°äº1 â†’ ä¹˜ç§¯è¶‹è¿‘äº0ï¼ˆæ¶ˆå¤±ï¼‰
- Slightly greater than 1 â†’ Product explodes (exploding)
- ç¨å¾®å¤§äº1 â†’ ä¹˜ç§¯çˆ†ç‚¸ï¼ˆçˆ†ç‚¸ï¼‰

```python
import numpy as np

def demonstrate_gradient_vanishing():
    """
    æ¼”ç¤ºæ¢¯åº¦æ¶ˆå¤±ç°è±¡
    Demonstrate gradient vanishing phenomenon
    """
    print("æ¢¯åº¦æ¶ˆå¤±æ¼”ç¤º (Gradient Vanishing Demonstration)")
    print("=" * 50)
    
    # æ¨¡æ‹Ÿé€šè¿‡å¤šå±‚çš„æ¢¯åº¦ä¼ æ’­
    # Simulate gradient propagation through multiple layers
    initial_gradient = 1.0
    layer_gradient_factor = 0.8  # æ¯å±‚æ¢¯åº¦è¡°å‡å› å­
    
    gradients = [initial_gradient]
    
    for layer in range(1, 21):  # 20å±‚ç½‘ç»œ
        current_gradient = gradients[-1] * layer_gradient_factor
        gradients.append(current_gradient)
        
        if layer % 5 == 0:
            print(f"Layer {layer:2d}: Gradient = {current_gradient:.6f}")
    
    print(f"\nåŸå§‹æ¢¯åº¦: {initial_gradient}")
    print(f"20å±‚åæ¢¯åº¦: {gradients[-1]:.10f}")
    print(f"æ¢¯åº¦è¡°å‡å€æ•°: {gradients[-1]/initial_gradient:.2e}")

demonstrate_gradient_vanishing()
```

## 2. ResNet's Brilliant Solution: Skip Connections
## 2. ResNetçš„ç»å¦™è§£å†³æ–¹æ¡ˆï¼šè·³è·ƒè¿æ¥

### 2.1 The Core Idea: Residual Learning
### 2.1 æ ¸å¿ƒæ€æƒ³ï¼šæ®‹å·®å­¦ä¹ 

Instead of learning the direct mapping H(x), ResNet learns the **residual mapping** F(x) = H(x) - x, and then adds back the input: H(x) = F(x) + x.
ResNetä¸å­¦ä¹ ç›´æ¥æ˜ å°„H(x)ï¼Œè€Œæ˜¯å­¦ä¹ **æ®‹å·®æ˜ å°„**F(x) = H(x) - xï¼Œç„¶ååŠ å›è¾“å…¥ï¼šH(x) = F(x) + xã€‚

**Analogy:** Instead of learning how to paint a complete picture from scratch, you learn how to make small improvements to an existing picture!
**ç±»æ¯”ï¼š** ä¸æ˜¯å­¦ä¹ å¦‚ä½•ä»å¤´ç”»ä¸€å¹…å®Œæ•´çš„ç”»ï¼Œè€Œæ˜¯å­¦ä¹ å¦‚ä½•å¯¹ç°æœ‰çš„ç”»è¿›è¡Œå°æ”¹è¿›ï¼

### 2.2 Mathematical Foundation
### 2.2 æ•°å­¦åŸºç¡€

**Traditional Deep Network Layer:**
**ä¼ ç»Ÿæ·±åº¦ç½‘ç»œå±‚ï¼š**
$$H(x) = \sigma(W_2 \sigma(W_1 x + b_1) + b_2)$$

**ResNet Block (Residual Block):**
**ResNetå—ï¼ˆæ®‹å·®å—ï¼‰ï¼š**
$$H(x) = \sigma(W_2 \sigma(W_1 x + b_1) + b_2) + x$$
$$H(x) = F(x) + x$$

Where F(x) is the residual function that the network learns to approximate.
å…¶ä¸­F(x)æ˜¯ç½‘ç»œå­¦ä¹ è¿‘ä¼¼çš„æ®‹å·®å‡½æ•°ã€‚

### 2.3 Why Skip Connections Work
### 2.3 ä¸ºä»€ä¹ˆè·³è·ƒè¿æ¥æœ‰æ•ˆ

**1. Gradient Highway:** Skip connections create direct paths for gradients to flow backward
**1. æ¢¯åº¦é«˜é€Ÿå…¬è·¯ï¼š** è·³è·ƒè¿æ¥ä¸ºæ¢¯åº¦å‘åæµåŠ¨åˆ›å»ºç›´æ¥è·¯å¾„

**2. Identity Mapping:** If the optimal function is close to identity, it's easier to learn F(x) â‰ˆ 0 than H(x) = x
**2. æ’ç­‰æ˜ å°„ï¼š** å¦‚æœæœ€ä¼˜å‡½æ•°æ¥è¿‘æ’ç­‰æ˜ å°„ï¼Œå­¦ä¹ F(x) â‰ˆ 0æ¯”H(x) = xæ›´å®¹æ˜“

**3. Feature Reuse:** Lower-level features can be directly used by higher-level layers
**3. ç‰¹å¾é‡ç”¨ï¼š** ä½çº§ç‰¹å¾å¯ä»¥è¢«é«˜çº§å±‚ç›´æ¥ä½¿ç”¨

## 3. ResNet Architecture Building Blocks
## 3. ResNetæ¶æ„æ„å»ºå—

### 3.1 Basic Residual Block
### 3.1 åŸºæœ¬æ®‹å·®å—

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class BasicResidualBlock(nn.Module):
    """
    åŸºæœ¬æ®‹å·®å— - ç”¨äºResNet-18å’ŒResNet-34
    Basic Residual Block - Used in ResNet-18 and ResNet-34
    
    Structure: Conv -> BatchNorm -> ReLU -> Conv -> BatchNorm -> Add -> ReLU
    ç»“æ„: å·ç§¯ -> æ‰¹å½’ä¸€åŒ– -> ReLU -> å·ç§¯ -> æ‰¹å½’ä¸€åŒ– -> ç›¸åŠ  -> ReLU
    """
    def __init__(self, in_channels, out_channels, stride=1):
        super(BasicResidualBlock, self).__init__()
        
        # ç¬¬ä¸€ä¸ªå·ç§¯å±‚
        # First convolutional layer
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, 
                              stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        
        # ç¬¬äºŒä¸ªå·ç§¯å±‚
        # Second convolutional layer
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, 
                              stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        
        # è·³è·ƒè¿æ¥çš„æŠ•å½±å±‚ï¼ˆå½“ç»´åº¦ä¸åŒ¹é…æ—¶ï¼‰
        # Projection layer for skip connection (when dimensions don't match)
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, 
                         stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )
    
    def forward(self, x):
        # ä¿å­˜è¾“å…¥ç”¨äºè·³è·ƒè¿æ¥
        # Save input for skip connection
        identity = x
        
        # ä¸»è·¯å¾„è®¡ç®—
        # Main path computation
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        
        # è·³è·ƒè¿æ¥
        # Skip connection
        out += self.shortcut(identity)
        out = F.relu(out)
        
        return out

# æ¼”ç¤ºåŸºæœ¬æ®‹å·®å—
def demo_basic_block():
    """
    æ¼”ç¤ºåŸºæœ¬æ®‹å·®å—çš„ä½¿ç”¨
    Demonstrate basic residual block usage
    """
    print("åŸºæœ¬æ®‹å·®å—æ¼”ç¤º (Basic Residual Block Demo)")
    print("=" * 50)
    
    # åˆ›å»ºæ®‹å·®å—
    block = BasicResidualBlock(in_channels=64, out_channels=64)
    
    # åˆ›å»ºç¤ºä¾‹è¾“å…¥
    x = torch.randn(1, 64, 32, 32)  # (batch_size, channels, height, width)
    
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    
    # å‰å‘ä¼ æ’­
    output = block(x)
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    
    # æ˜¾ç¤ºå‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in block.parameters() if p.requires_grad)
    print(f"å¯è®­ç»ƒå‚æ•°æ•°é‡: {total_params:,}")
    
    return block, output

demo_basic_block()
```

### 3.2 Bottleneck Residual Block
### 3.2 ç“¶é¢ˆæ®‹å·®å—

```python
class BottleneckResidualBlock(nn.Module):
    """
    ç“¶é¢ˆæ®‹å·®å— - ç”¨äºResNet-50, ResNet-101, ResNet-152
    Bottleneck Residual Block - Used in ResNet-50, ResNet-101, ResNet-152
    
    ç»“æ„: 1x1 Conv -> 3x3 Conv -> 1x1 Conv (å¸¦è·³è·ƒè¿æ¥)
    Structure: 1x1 Conv -> 3x3 Conv -> 1x1 Conv (with skip connection)
    
    ä¼˜åŠ¿: å‡å°‘å‚æ•°æ•°é‡ï¼Œæé«˜è®¡ç®—æ•ˆç‡
    Advantage: Reduces parameters, improves computational efficiency
    """
    expansion = 4  # è¾“å‡ºé€šé“æ˜¯è¾“å…¥é€šé“çš„4å€
    
    def __init__(self, in_channels, out_channels, stride=1):
        super(BottleneckResidualBlock, self).__init__()
        
        # 1x1 å·ç§¯é™ç»´
        # 1x1 convolution for dimension reduction
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        
        # 3x3 å·ç§¯è¿›è¡Œç‰¹å¾æå–
        # 3x3 convolution for feature extraction
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, 
                              stride=stride, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        
        # 1x1 å·ç§¯å‡ç»´
        # 1x1 convolution for dimension expansion
        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, 
                              kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)
        
        # è·³è·ƒè¿æ¥æŠ•å½±
        # Skip connection projection
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels * self.expansion:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels * self.expansion, 
                         kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels * self.expansion)
            )
    
    def forward(self, x):
        identity = x
        
        # ç“¶é¢ˆè·¯å¾„: é™ç»´ -> ç‰¹å¾æå– -> å‡ç»´
        # Bottleneck path: reduce -> extract -> expand
        out = F.relu(self.bn1(self.conv1(x)))
        out = F.relu(self.bn2(self.conv2(out)))
        out = self.bn3(self.conv3(out))
        
        # æ·»åŠ è·³è·ƒè¿æ¥
        # Add skip connection
        out += self.shortcut(identity)
        out = F.relu(out)
        
        return out

def compare_block_efficiency():
    """
    æ¯”è¾ƒåŸºæœ¬å—å’Œç“¶é¢ˆå—çš„æ•ˆç‡
    Compare efficiency of basic and bottleneck blocks
    """
    print("æ®‹å·®å—æ•ˆç‡æ¯”è¾ƒ (Residual Block Efficiency Comparison)")
    print("=" * 60)
    
    # ç›¸åŒè¾“å‡ºé€šé“æ•°çš„æƒ…å†µä¸‹æ¯”è¾ƒ
    in_channels, out_channels = 64, 64
    
    # åŸºæœ¬å—
    basic_block = BasicResidualBlock(in_channels, out_channels)
    basic_params = sum(p.numel() for p in basic_block.parameters())
    
    # ç“¶é¢ˆå—
    bottleneck_block = BottleneckResidualBlock(in_channels, out_channels)
    bottleneck_params = sum(p.numel() for p in bottleneck_block.parameters())
    
    print(f"åŸºæœ¬å—å‚æ•°æ•°é‡: {basic_params:,}")
    print(f"ç“¶é¢ˆå—å‚æ•°æ•°é‡: {bottleneck_params:,}")
    print(f"å‚æ•°å‡å°‘æ¯”ä¾‹: {(1 - bottleneck_params/basic_params)*100:.1f}%")
    
    # è®¡ç®—é€Ÿåº¦æµ‹è¯•
    x = torch.randn(1, in_channels, 32, 32)
    
    import time
    
    # åŸºæœ¬å—é€Ÿåº¦æµ‹è¯•
    start_time = time.time()
    for _ in range(100):
        _ = basic_block(x)
    basic_time = time.time() - start_time
    
    # ç“¶é¢ˆå—é€Ÿåº¦æµ‹è¯•
    start_time = time.time()
    for _ in range(100):
        _ = bottleneck_block(x)
    bottleneck_time = time.time() - start_time
    
    print(f"åŸºæœ¬å—æ‰§è¡Œæ—¶é—´: {basic_time:.4f}ç§’")
    print(f"ç“¶é¢ˆå—æ‰§è¡Œæ—¶é—´: {bottleneck_time:.4f}ç§’")
    print(f"é€Ÿåº¦æå‡: {(basic_time/bottleneck_time):.2f}x")

compare_block_efficiency()
```

## 4. Complete ResNet Architecture
## 4. å®Œæ•´çš„ResNetæ¶æ„

### 4.1 ResNet Family
### 4.1 ResNetç³»åˆ—

```python
class ResNet(nn.Module):
    """
    å®Œæ•´çš„ResNetæ¶æ„
    Complete ResNet Architecture
    
    æ”¯æŒResNet-18, 34, 50, 101, 152
    Supports ResNet-18, 34, 50, 101, 152
    """
    def __init__(self, block, layers, num_classes=1000):
        super(ResNet, self).__init__()
        self.in_channels = 64
        
        # åˆå§‹å·ç§¯å±‚
        # Initial convolutional layer
        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        
        # æ®‹å·®å±‚
        # Residual layers
        self.layer1 = self._make_layer(block, 64, layers[0])
        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)
        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)
        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)
        
        # å…¨å±€å¹³å‡æ± åŒ–å’Œåˆ†ç±»å™¨
        # Global average pooling and classifier
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512 * block.expansion, num_classes)
        
    def _make_layer(self, block, out_channels, blocks, stride=1):
        """
        åˆ›å»ºæ®‹å·®å±‚
        Create residual layer
        """
        layers = []
        layers.append(block(self.in_channels, out_channels, stride))
        self.in_channels = out_channels * block.expansion
        
        for _ in range(1, blocks):
            layers.append(block(self.in_channels, out_channels))
            
        return nn.Sequential(*layers)
    
    def forward(self, x):
        # åˆå§‹ç‰¹å¾æå–
        # Initial feature extraction
        x = F.relu(self.bn1(self.conv1(x)))
        x = self.maxpool(x)
        
        # é€šè¿‡æ®‹å·®å±‚
        # Through residual layers
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        
        # åˆ†ç±»
        # Classification
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.fc(x)
        
        return x

def create_resnet_variants():
    """
    åˆ›å»ºä¸åŒçš„ResNetå˜ä½“
    Create different ResNet variants
    """
    
    # ResNeté…ç½® (æ¯å±‚çš„å—æ•°)
    # ResNet configurations (number of blocks in each layer)
    configs = {
        'ResNet-18': [2, 2, 2, 2],
        'ResNet-34': [3, 4, 6, 3],
        'ResNet-50': [3, 4, 6, 3],
        'ResNet-101': [3, 4, 23, 3],
        'ResNet-152': [3, 8, 36, 3]
    }
    
    models = {}
    
    # åˆ›å»ºResNet-18å’ŒResNet-34 (ä½¿ç”¨åŸºæœ¬å—)
    # Create ResNet-18 and ResNet-34 (using basic blocks)
    for name in ['ResNet-18', 'ResNet-34']:
        models[name] = ResNet(BasicResidualBlock, configs[name], num_classes=1000)
    
    # åˆ›å»ºResNet-50, 101, 152 (ä½¿ç”¨ç“¶é¢ˆå—)
    # Create ResNet-50, 101, 152 (using bottleneck blocks)
    for name in ['ResNet-50', 'ResNet-101', 'ResNet-152']:
        models[name] = ResNet(BottleneckResidualBlock, configs[name], num_classes=1000)
    
    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    # Print model information
    print("ResNetæ¨¡å‹ç³»åˆ—å‚æ•°å¯¹æ¯” (ResNet Model Family Parameter Comparison)")
    print("=" * 70)
    print(f"{'Model':<12} {'Parameters':<15} {'Layers':<10} {'Block Type':<15}")
    print("-" * 70)
    
    block_types = {
        'ResNet-18': 'Basic',
        'ResNet-34': 'Basic', 
        'ResNet-50': 'Bottleneck',
        'ResNet-101': 'Bottleneck',
        'ResNet-152': 'Bottleneck'
    }
    
    for name, model in models.items():
        param_count = sum(p.numel() for p in model.parameters() if p.requires_grad)
        layer_count = sum(configs[name]) * 2 + 2  # æ®‹å·®å—æ•° * 2 + é¦–å°¾å±‚
        print(f"{name:<12} {param_count:>10,}    {layer_count:<10} {block_types[name]:<15}")
    
    return models

# åˆ›å»ºResNetæ¨¡å‹ç³»åˆ—
resnet_models = create_resnet_variants()
```

## 5. Training and Implementation Tips
## 5. è®­ç»ƒå’Œå®ç°æŠ€å·§

### 5.1 Training Best Practices
### 5.1 è®­ç»ƒæœ€ä½³å®è·µ

```python
import torch.optim as optim
from torch.optim.lr_scheduler import StepLR

def train_resnet_tips():
    """
    ResNetè®­ç»ƒæŠ€å·§å’Œæœ€ä½³å®è·µ
    ResNet training tips and best practices
    """
    print("ResNetè®­ç»ƒæœ€ä½³å®è·µ (ResNet Training Best Practices)")
    print("=" * 55)
    
    # 1. å­¦ä¹ ç‡è°ƒåº¦
    print("1. å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥ (Learning Rate Scheduling):")
    print("   - åˆå§‹å­¦ä¹ ç‡: 0.1")
    print("   - æ¯30ä¸ªepochè¡°å‡10å€")
    print("   - ä½¿ç”¨ä½™å¼¦é€€ç«æˆ–æ­¥é•¿è°ƒåº¦")
    
    # 2. æ•°æ®å¢å¼º
    print("\n2. æ•°æ®å¢å¼º (Data Augmentation):")
    print("   - éšæœºè£å‰ªå’Œç¿»è½¬")
    print("   - é¢œè‰²æŠ–åŠ¨")
    print("   - å½’ä¸€åŒ–åˆ°ImageNetç»Ÿè®¡")
    
    # 3. æ‰¹å½’ä¸€åŒ–
    print("\n3. æ‰¹å½’ä¸€åŒ– (Batch Normalization):")
    print("   - æ¯ä¸ªå·ç§¯å±‚åä½¿ç”¨BatchNorm")
    print("   - æœ‰åŠ©äºæ¢¯åº¦æµåŠ¨å’Œè®­ç»ƒç¨³å®šæ€§")
    
    # 4. æƒé‡åˆå§‹åŒ–
    print("\n4. æƒé‡åˆå§‹åŒ– (Weight Initialization):")
    print("   - ä½¿ç”¨Kaimingåˆå§‹åŒ–")
    print("   - é’ˆå¯¹ReLUæ¿€æ´»å‡½æ•°ä¼˜åŒ–")

def implement_training_loop():
    """
    å®ç°ResNetè®­ç»ƒå¾ªç¯
    Implement ResNet training loop
    """
    # åˆ›å»ºResNet-18æ¨¡å‹
    model = ResNet(BasicResidualBlock, [2, 2, 2, 2], num_classes=10)  # CIFAR-10
    
    # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)
    scheduler = StepLR(optimizer, step_size=30, gamma=0.1)
    criterion = nn.CrossEntropyLoss()
    
    # è®­ç»ƒå‡½æ•°ç¤ºä¾‹
    def train_epoch(model, train_loader, optimizer, criterion, device):
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)
            
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
            
            running_loss += loss.item()
            _, predicted = output.max(1)
            total += target.size(0)
            correct += predicted.eq(target).sum().item()
            
            if batch_idx % 100 == 0:
                print(f'Batch {batch_idx}, Loss: {loss.item():.4f}, '
                      f'Acc: {100.*correct/total:.2f}%')
        
        return running_loss / len(train_loader), 100. * correct / total
    
    print("è®­ç»ƒå¾ªç¯ç¤ºä¾‹å·²å®ç° (Training loop example implemented)")
    return model, optimizer, scheduler, criterion

model, optimizer, scheduler, criterion = implement_training_loop()
```

### 5.2 Common Implementation Mistakes
### 5.2 å¸¸è§å®ç°é”™è¯¯

```python
def common_resnet_mistakes():
    """
    ResNetå®ç°ä¸­çš„å¸¸è§é”™è¯¯
    Common mistakes in ResNet implementation
    """
    print("ResNetå®ç°å¸¸è§é”™è¯¯ (Common ResNet Implementation Mistakes)")
    print("=" * 60)
    
    mistakes = [
        {
            "é”™è¯¯": "å¿˜è®°åœ¨è·³è·ƒè¿æ¥ä¸­å¤„ç†ç»´åº¦ä¸åŒ¹é…",
            "è¯´æ˜": "å½“stride!=1æˆ–é€šé“æ•°æ”¹å˜æ—¶éœ€è¦æŠ•å½±å±‚",
            "è§£å†³": "ä½¿ç”¨1x1å·ç§¯è°ƒæ•´ç»´åº¦"
        },
        {
            "é”™è¯¯": "åœ¨è·³è·ƒè¿æ¥åå¿˜è®°åº”ç”¨ReLU",
            "è¯´æ˜": "æ®‹å·®è¿æ¥åéœ€è¦æ¿€æ´»å‡½æ•°",
            "è§£å†³": "ç¡®ä¿æœ€ç»ˆè¾“å‡ºç»è¿‡ReLUæ¿€æ´»"
        },
        {
            "é”™è¯¯": "æ‰¹å½’ä¸€åŒ–ä½ç½®ä¸æ­£ç¡®",
            "è¯´æ˜": "BatchNormåº”è¯¥åœ¨å·ç§¯åã€æ¿€æ´»å‰",
            "è§£å†³": "éµå¾ªConv->BN->ReLUçš„é¡ºåº"
        },
        {
            "é”™è¯¯": "å­¦ä¹ ç‡è®¾ç½®è¿‡é«˜",
            "è¯´æ˜": "ResNetå¯¹å­¦ä¹ ç‡æ•æ„Ÿ",
            "è§£å†³": "ä»0.1å¼€å§‹ï¼Œä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦"
        }
    ]
    
    for i, mistake in enumerate(mistakes, 1):
        print(f"{i}. {mistake['é”™è¯¯']}")
        print(f"   è¯´æ˜: {mistake['è¯´æ˜']}")
        print(f"   è§£å†³: {mistake['è§£å†³']}\n")

common_resnet_mistakes()
```

## 6. Applications and Impact
## 6. åº”ç”¨å’Œå½±å“

### 6.1 Revolutionary Impact
### 6.1 é©å‘½æ€§å½±å“

**ImageNet Competition Results:**
**ImageNetç«èµ›ç»“æœï¼š**
- 2015: ResNet-152 achieved 3.57% top-5 error (human-level performance: ~5%)
- 2015å¹´: ResNet-152è¾¾åˆ°3.57%çš„top-5é”™è¯¯ç‡ï¼ˆäººç±»æ°´å¹³è¡¨ç°ï¼šçº¦5%ï¼‰
- First time a deep learning model surpassed human performance on ImageNet
- æ·±åº¦å­¦ä¹ æ¨¡å‹é¦–æ¬¡åœ¨ImageNetä¸Šè¶…è¶Šäººç±»è¡¨ç°

### 6.2 Real-World Applications
### 6.2 ç°å®ä¸–ç•Œåº”ç”¨

```python
def resnet_applications():
    """
    ResNetçš„å®é™…åº”ç”¨é¢†åŸŸ
    Real-world applications of ResNet
    """
    applications = {
        "åŒ»å­¦å½±åƒ": {
            "åº”ç”¨": "Xå…‰ç‰‡åˆ†æã€CTæ‰«æã€MRIå›¾åƒåˆ†æ",
            "ä¼˜åŠ¿": "æ·±åº¦ç½‘ç»œèƒ½æ•è·ç»†å¾®çš„åŒ»å­¦ç‰¹å¾",
            "æ¡ˆä¾‹": "è‚ºç‚æ£€æµ‹ã€ç™Œç—‡ç­›æŸ¥"
        },
        "è‡ªåŠ¨é©¾é©¶": {
            "åº”ç”¨": "é“è·¯æ ‡å¿—è¯†åˆ«ã€è¡Œäººæ£€æµ‹ã€è½¦è¾†è¯†åˆ«",
            "ä¼˜åŠ¿": "å®æ—¶æ€§å¼ºï¼Œå‡†ç¡®ç‡é«˜",
            "æ¡ˆä¾‹": "Teslaã€Waymoçš„è§†è§‰ç³»ç»Ÿ"
        },
        "äººè„¸è¯†åˆ«": {
            "åº”ç”¨": "å®‰é˜²ç›‘æ§ã€èº«ä»½éªŒè¯ã€ç¤¾äº¤åª’ä½“",
            "ä¼˜åŠ¿": "å¯¹å…‰ç…§ã€è§’åº¦å˜åŒ–é²æ£’",
            "æ¡ˆä¾‹": "Face IDã€æ”¯ä»˜å®äººè„¸æ”¯ä»˜"
        },
        "å·¥ä¸šæ£€æµ‹": {
            "åº”ç”¨": "äº§å“ç¼ºé™·æ£€æµ‹ã€è´¨é‡æ§åˆ¶",
            "ä¼˜åŠ¿": "æ¯”äººå·¥æ£€æµ‹æ›´ç¨³å®šã€å‡†ç¡®",
            "æ¡ˆä¾‹": "åŠå¯¼ä½“åˆ¶é€ ã€çººç»‡å“æ£€æµ‹"
        }
    }
    
    print("ResNetå®é™…åº”ç”¨æ¡ˆä¾‹ (ResNet Real-World Applications)")
    print("=" * 50)
    
    for field, details in applications.items():
        print(f"\nğŸ”¹ {field}")
        print(f"   åº”ç”¨åœºæ™¯: {details['åº”ç”¨']}")
        print(f"   æŠ€æœ¯ä¼˜åŠ¿: {details['ä¼˜åŠ¿']}")
        print(f"   å…¸å‹æ¡ˆä¾‹: {details['æ¡ˆä¾‹']}")

resnet_applications()
```

## 7. Key Takeaways and Summary
## 7. å…³é”®è¦ç‚¹å’Œæ€»ç»“

### 7.1 Core Innovations
### 7.1 æ ¸å¿ƒåˆ›æ–°

```python
def resnet_key_innovations():
    """
    ResNetçš„å…³é”®åˆ›æ–°ç‚¹
    Key innovations of ResNet
    """
    innovations = {
        "è·³è·ƒè¿æ¥": {
            "ä½œç”¨": "è§£å†³æ¢¯åº¦æ¶ˆå¤±é—®é¢˜",
            "æœºåˆ¶": "åˆ›å»ºæ¢¯åº¦æµåŠ¨çš„é«˜é€Ÿå…¬è·¯",
            "å…¬å¼": "H(x) = F(x) + x"
        },
        "æ®‹å·®å­¦ä¹ ": {
            "ä½œç”¨": "ç®€åŒ–å­¦ä¹ ä»»åŠ¡",
            "æœºåˆ¶": "å­¦ä¹ æ®‹å·®è€Œéå®Œæ•´æ˜ å°„",
            "ä¼˜åŠ¿": "æ›´å®¹æ˜“ä¼˜åŒ–"
        },
        "æ·±åº¦çªç ´": {
            "ä½œç”¨": "ä½¿è¶…æ·±ç½‘ç»œæˆä¸ºå¯èƒ½",
            "æˆå°±": "152å±‚ç½‘ç»œæˆåŠŸè®­ç»ƒ",
            "å½±å“": "å¼€å¯äº†æ·±åº¦å­¦ä¹ æ–°æ—¶ä»£"
        },
        "é€šç”¨æ¶æ„": {
            "ä½œç”¨": "å¹¿æ³›é€‚ç”¨çš„è®¾è®¡æ¨¡å¼",
            "åº”ç”¨": "å½±å“äº†åç»­æ‰€æœ‰æ·±åº¦æ¶æ„",
            "ä¾‹å­": "DenseNetã€Highway Networks"
        }
    }
    
    print("ResNetå…³é”®åˆ›æ–°æ€»ç»“ (ResNet Key Innovations Summary)")
    print("=" * 55)
    
    for innovation, details in innovations.items():
        print(f"\nâœ¨ {innovation}")
        for key, value in details.items():
            print(f"   {key}: {value}")

resnet_key_innovations()
```

### 7.2 Why ResNet Changed Everything
### 7.2 ä¸ºä»€ä¹ˆResNetæ”¹å˜äº†ä¸€åˆ‡

**Before ResNet:** Deep networks were hard to train and often performed worse than shallow ones.
**ResNetä¹‹å‰ï¼š** æ·±åº¦ç½‘ç»œéš¾ä»¥è®­ç»ƒï¼Œé€šå¸¸æ¯”æµ…å±‚ç½‘ç»œè¡¨ç°æ›´å·®ã€‚

**After ResNet:** The deeper, the better! Networks with hundreds of layers became feasible.
**ResNetä¹‹åï¼š** è¶Šæ·±è¶Šå¥½ï¼å…·æœ‰æ•°ç™¾å±‚çš„ç½‘ç»œå˜å¾—å¯è¡Œã€‚

**Legacy:** Every modern deep architecture (Transformers, EfficientNets, etc.) uses some form of skip connections inspired by ResNet.
**é—äº§ï¼š** æ¯ä¸ªç°ä»£æ·±åº¦æ¶æ„ï¼ˆTransformersã€EfficientNetsç­‰ï¼‰éƒ½ä½¿ç”¨æŸç§å½¢å¼çš„å—ResNetå¯å‘çš„è·³è·ƒè¿æ¥ã€‚

## 8. Practice Exercises
## 8. ç»ƒä¹ é¢˜

### Exercise 1: Implement a Mini ResNet
### ç»ƒä¹ 1ï¼šå®ç°ä¸€ä¸ªè¿·ä½ ResNet

```python
def exercise_mini_resnet():
    """
    ç»ƒä¹ ï¼šå®ç°ä¸€ä¸ªç”¨äºCIFAR-10çš„è¿·ä½ ResNet
    Exercise: Implement a mini ResNet for CIFAR-10
    """
    print("ç»ƒä¹ ï¼šå®ç°è¿·ä½ ResNet (Exercise: Implement Mini ResNet)")
    print("=" * 50)
    
    print("ä»»åŠ¡è¦æ±‚:")
    print("1. è®¾è®¡ä¸€ä¸ª10å±‚çš„å°å‹ResNet")
    print("2. é€‚ç”¨äºCIFAR-10æ•°æ®é›†(32x32å›¾åƒ)")
    print("3. åŒ…å«2ä¸ªæ®‹å·®å—")
    print("4. æœ€ç»ˆåˆ†ç±»10ä¸ªç±»åˆ«")
    
    # å­¦ç”Ÿéœ€è¦å®Œæˆçš„ä»£ç æ¡†æ¶
    class MiniResNet(nn.Module):
        def __init__(self, num_classes=10):
            super(MiniResNet, self).__init__()
            # TODO: å®ç°ç½‘ç»œç»“æ„
            pass
        
        def forward(self, x):
            # TODO: å®ç°å‰å‘ä¼ æ’­
            pass
    
    print("\næç¤º:")
    print("- ä½¿ç”¨3x3å·ç§¯")
    print("- ç¬¬ä¸€å±‚å¯ä»¥ä½¿ç”¨stride=1")
    print("- è®°å¾—æ·»åŠ è·³è·ƒè¿æ¥")
    print("- æœ€åä½¿ç”¨å…¨å±€å¹³å‡æ± åŒ–")

exercise_mini_resnet()
```

### Exercise 2: Gradient Flow Analysis
### ç»ƒä¹ 2ï¼šæ¢¯åº¦æµåˆ†æ

```python
def exercise_gradient_analysis():
    """
    ç»ƒä¹ ï¼šåˆ†æResNetä¸­çš„æ¢¯åº¦æµ
    Exercise: Analyze gradient flow in ResNet
    """
    print("ç»ƒä¹ ï¼šæ¢¯åº¦æµåˆ†æ (Exercise: Gradient Flow Analysis)")
    print("=" * 50)
    
    print("ä»»åŠ¡:")
    print("1. æ¯”è¾ƒæœ‰æ— è·³è·ƒè¿æ¥çš„æ¢¯åº¦ä¼ æ’­")
    print("2. å¯è§†åŒ–æ¢¯åº¦åœ¨ä¸åŒæ·±åº¦çš„å˜åŒ–")
    print("3. è§£é‡Šä¸ºä»€ä¹ˆResNetè®­ç»ƒæ›´ç¨³å®š")
    
    # ç¤ºä¾‹ä»£ç æ¡†æ¶
    def analyze_gradients(model, input_tensor):
        # TODO: å®ç°æ¢¯åº¦åˆ†æ
        # 1. å‰å‘ä¼ æ’­
        # 2. è®¡ç®—æŸå¤±
        # 3. åå‘ä¼ æ’­
        # 4. è®°å½•æ¯å±‚æ¢¯åº¦å¤§å°
        pass
    
    print("\nè¦åˆ†æçš„æŒ‡æ ‡:")
    print("- æ¯å±‚æ¢¯åº¦çš„L2èŒƒæ•°")
    print("- æ¢¯åº¦æ¶ˆå¤±çš„ç¨‹åº¦")
    print("- è·³è·ƒè¿æ¥çš„è´¡çŒ®")

exercise_gradient_analysis()
```

## Conclusion
## ç»“è®º

ResNet fundamentally changed how we think about deep learning architecture design. By introducing skip connections, it solved the degradation problem and enabled the training of much deeper networks. The key insight - that learning residual mappings is easier than learning complete mappings - has influenced virtually every subsequent deep learning architecture.
ResNetä»æ ¹æœ¬ä¸Šæ”¹å˜äº†æˆ‘ä»¬å¯¹æ·±åº¦å­¦ä¹ æ¶æ„è®¾è®¡çš„æ€è€ƒæ–¹å¼ã€‚é€šè¿‡å¼•å…¥è·³è·ƒè¿æ¥ï¼Œå®ƒè§£å†³äº†é€€åŒ–é—®é¢˜ï¼Œä½¿å¾—è®­ç»ƒæ›´æ·±çš„ç½‘ç»œæˆä¸ºå¯èƒ½ã€‚å…³é”®æ´å¯Ÿâ€”â€”å­¦ä¹ æ®‹å·®æ˜ å°„æ¯”å­¦ä¹ å®Œæ•´æ˜ å°„æ›´å®¹æ˜“â€”â€”å‡ ä¹å½±å“äº†éšåçš„æ¯ä¸€ä¸ªæ·±åº¦å­¦ä¹ æ¶æ„ã€‚

**Remember the analogy:** ResNet is like adding elevators to a tall building - it doesn't change the destination, but it makes the journey much more efficient!
**è®°ä½ç±»æ¯”ï¼š** ResNetå°±åƒåœ¨é«˜æ¥¼ä¸­æ·»åŠ ç”µæ¢¯â€”â€”å®ƒä¸ä¼šæ”¹å˜ç›®çš„åœ°ï¼Œä½†ä¼šè®©æ—…ç¨‹æ›´åŠ é«˜æ•ˆï¼ 