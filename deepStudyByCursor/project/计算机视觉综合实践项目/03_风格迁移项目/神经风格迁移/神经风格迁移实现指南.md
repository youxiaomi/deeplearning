# ç¥ç»é£æ ¼è¿ç§»å®ç°æŒ‡å—
# Neural Style Transfer Implementation Guide

**è®©AIæˆä¸ºè‰ºæœ¯å®¶ - æ¢µé«˜ã€æ¯•åŠ ç´¢çš„æ•°å­—åŒ–ä¼ æ‰¿**
**Making AI an Artist - Digital Heritage of Van Gogh and Picasso**

---

## ğŸ¯ é¡¹ç›®æ¦‚è¿° | Project Overview

ç¥ç»é£æ ¼è¿ç§»æ˜¯æ·±åº¦å­¦ä¹ ä¸è‰ºæœ¯å®Œç¾ç»“åˆçš„å…¸å‹åº”ç”¨ï¼å®ƒè®©æˆ‘ä»¬èƒ½å¤Ÿå°†ä¸€å¹…ç”»çš„è‰ºæœ¯é£æ ¼"è¿ç§»"åˆ°å¦ä¸€å¹…å›¾åƒä¸Šï¼Œåˆ›é€ å‡ºä»¤äººæƒŠå¹çš„è‰ºæœ¯ä½œå“ã€‚

Neural Style Transfer is a perfect combination of deep learning and art! It allows us to "transfer" the artistic style of one painting to another image, creating stunning artwork.

### é­”æ³•åŸç† | Magic Principles
- **å†…å®¹åˆ†ç¦»**: CNNèƒ½å¤Ÿåˆ†ç¦»å›¾åƒçš„å†…å®¹å’Œé£æ ¼
- **Content Separation**: CNN can separate content and style of images
- **é£æ ¼è¡¨ç¤º**: ç”¨ç»Ÿè®¡ç‰¹å¾æ•æ‰è‰ºæœ¯é£æ ¼
- **Style Representation**: Use statistical features to capture artistic style
- **ä¼˜åŒ–ç”Ÿæˆ**: é€šè¿‡æ¢¯åº¦ä¸‹é™"ç”»å‡º"æ–°å›¾åƒ
- **Optimization Generation**: "Paint" new images through gradient descent

## ğŸ¨ è‰ºæœ¯ä¸ç§‘å­¦çš„ç¢°æ’ | Collision of Art and Science

**ä¸ºä»€ä¹ˆé£æ ¼è¿ç§»å¦‚æ­¤è¿·äººï¼Ÿ**
**Why is style transfer so fascinating?**

æƒ³è±¡ä¸€ä¸‹ï¼Œå¦‚æœä½ èƒ½æ‹¥æœ‰æ¢µé«˜çš„ç”»ç¬”ï¼Œæ¯•åŠ ç´¢çš„è‰²å½©æ„Ÿï¼Œè¾¾èŠ¬å¥‡çš„æ„å›¾æŠ€å·§... é£æ ¼è¿ç§»è®©è¿™ä¸ªæ¢¦æƒ³æˆä¸ºç°å®ï¼å®ƒä¸ä»…ä»…æ˜¯æŠ€æœ¯ï¼Œæ›´æ˜¯è‰ºæœ¯åˆ›ä½œçš„å…¨æ–°æ–¹å¼ã€‚

Imagine if you could have Van Gogh's brush, Picasso's color sense, Da Vinci's composition skills... Style transfer makes this dream come true! It's not just technology, but a completely new way of artistic creation.

## ğŸ“š æ ¸å¿ƒç†è®ºæ·±åº¦è§£æ | Deep Core Theory Analysis

### ğŸ§  Gatysç®—æ³•çš„å¤©æ‰æ´å¯Ÿ | Gatys Algorithm's Genius Insight

Leon Gatysåœ¨2015å¹´çš„å¼€åˆ›æ€§å‘ç°ï¼š**CNNçš„ä¸åŒå±‚åŒ…å«ä¸åŒç±»å‹çš„ä¿¡æ¯**
Leon Gatys' groundbreaking discovery in 2015: **Different layers of CNN contain different types of information**

```
æµ…å±‚ (Early Layers):    è¾¹ç¼˜ã€çº¹ç†ã€é¢œè‰² | Edges, textures, colors
ä¸­å±‚ (Middle Layers):   å½¢çŠ¶ã€ç‰©ä½“éƒ¨åˆ† | Shapes, object parts  
æ·±å±‚ (Deep Layers):     é«˜çº§è¯­ä¹‰ã€ç‰©ä½“ | High-level semantics, objects
```

### ğŸ”¢ æ•°å­¦åŸºç¡€ | Mathematical Foundation

#### 1. å†…å®¹è¡¨ç¤º | Content Representation
```python
# å†…å®¹æŸå¤±ï¼šæ¯”è¾ƒç‰¹å¾å›¾çš„å·®å¼‚
# Content Loss: Compare feature map differences
def content_loss(content_features, generated_features):
    """
    å†…å®¹æŸå¤±å‡½æ•°
    Content Loss Function
    """
    return torch.mean((content_features - generated_features) ** 2)
```

#### 2. é£æ ¼è¡¨ç¤º - GramçŸ©é˜µ | Style Representation - Gram Matrix
```python
def gram_matrix(features):
    """
    è®¡ç®—GramçŸ©é˜µæ¥è¡¨ç¤ºé£æ ¼
    Calculate Gram matrix to represent style
    
    GramçŸ©é˜µæ•æ‰äº†ä¸åŒç‰¹å¾é€šé“ä¹‹é—´çš„ç›¸å…³æ€§
    The Gram matrix captures correlations between different feature channels
    """
    batch_size, channels, height, width = features.size()
    
    # é‡å¡‘ç‰¹å¾å›¾ | Reshape feature maps
    features = features.view(batch_size * channels, height * width)
    
    # è®¡ç®—GramçŸ©é˜µ | Calculate Gram matrix
    gram = torch.mm(features, features.t())
    
    # å½’ä¸€åŒ– | Normalize
    return gram / (batch_size * channels * height * width)

def style_loss(style_features, generated_features):
    """
    é£æ ¼æŸå¤±å‡½æ•°
    Style Loss Function
    """
    style_gram = gram_matrix(style_features)
    generated_gram = gram_matrix(generated_features)
    
    return torch.mean((style_gram - generated_gram) ** 2)
```

#### 3. æ€»æŸå¤±å‡½æ•° | Total Loss Function
```python
def total_loss(content_features, style_features, generated_features, 
               alpha=1, beta=1000):
    """
    æ€»æŸå¤± = Î±Ã—å†…å®¹æŸå¤± + Î²Ã—é£æ ¼æŸå¤±
    Total Loss = Î±Ã—Content Loss + Î²Ã—Style Loss
    
    Î±: å†…å®¹æƒé‡ | Content weight
    Î²: é£æ ¼æƒé‡ | Style weight
    """
    content_l = content_loss(content_features, generated_features)
    style_l = style_loss(style_features, generated_features)
    
    return alpha * content_l + beta * style_l
```

## ğŸ› ï¸ å®Œæ•´å®ç°ä»£ç  | Complete Implementation Code

### ç¬¬ä¸€æ­¥: ç‰¹å¾æå–ç½‘ç»œ | Step 1: Feature Extraction Network

```python
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
import torchvision.transforms as transforms
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np

class VGGFeatureExtractor(nn.Module):
    """
    åŸºäºVGG19çš„ç‰¹å¾æå–å™¨
    VGG19-based Feature Extractor
    """
    def __init__(self):
        super(VGGFeatureExtractor, self).__init__()
        
        # åŠ è½½é¢„è®­ç»ƒçš„VGG19æ¨¡å‹
        # Load pre-trained VGG19 model
        vgg = models.vgg19(pretrained=True).features
        
        # é€‰æ‹©ç‰¹å®šå±‚ç”¨äºå†…å®¹å’Œé£æ ¼è¡¨ç¤º
        # Select specific layers for content and style representation
        self.content_layers = ['conv_4']  # ç”¨äºå†…å®¹ | For content
        self.style_layers = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']  # ç”¨äºé£æ ¼ | For style
        
        # æ„å»ºç‰¹å¾æå–ç½‘ç»œ
        # Build feature extraction network
        self.features = nn.ModuleDict()
        conv_count = 0
        
        for i, layer in enumerate(vgg):
            if isinstance(layer, nn.Conv2d):
                conv_count += 1
                name = f'conv_{conv_count}'
            elif isinstance(layer, nn.ReLU):
                name = f'relu_{conv_count}'
                # ä½¿ç”¨inplace=Falseä»¥ä¾¿æ¢¯åº¦å›ä¼ 
                # Use inplace=False for gradient backpropagation
                layer = nn.ReLU(inplace=False)
            elif isinstance(layer, nn.MaxPool2d):
                name = f'pool_{conv_count}'
            elif isinstance(layer, nn.BatchNorm2d):
                name = f'bn_{conv_count}'
            
            self.features[name] = layer
        
        # å†»ç»“VGGå‚æ•°
        # Freeze VGG parameters
        for param in self.parameters():
            param.requires_grad = False
    
    def forward(self, x):
        """
        å‰å‘ä¼ æ’­ï¼Œè¿”å›æ‰€éœ€å±‚çš„ç‰¹å¾
        Forward pass, return features from desired layers
        """
        content_features = {}
        style_features = {}
        
        for name, layer in self.features.items():
            x = layer(x)
            
            if name in self.content_layers:
                content_features[name] = x
            if name in self.style_layers:
                style_features[name] = x
        
        return content_features, style_features
```

### ç¬¬äºŒæ­¥: å›¾åƒé¢„å¤„ç† | Step 2: Image Preprocessing

```python
def load_image(image_path, max_size=512):
    """
    åŠ è½½å¹¶é¢„å¤„ç†å›¾åƒ
    Load and preprocess image
    """
    image = Image.open(image_path).convert('RGB')
    
    # è°ƒæ•´å›¾åƒå¤§å°
    # Resize image
    size = min(max_size, max(image.size))
    transform = transforms.Compose([
        transforms.Resize(size),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])
    ])
    
    # æ·»åŠ batchç»´åº¦
    # Add batch dimension
    image = transform(image).unsqueeze(0)
    
    return image

def save_image(tensor, filename):
    """
    ä¿å­˜å¼ é‡ä¸ºå›¾åƒ
    Save tensor as image
    """
    # åå½’ä¸€åŒ–
    # Denormalize
    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
    
    image = tensor.squeeze(0) * std + mean
    image = torch.clamp(image, 0, 1)
    
    # è½¬æ¢ä¸ºPILå›¾åƒå¹¶ä¿å­˜
    # Convert to PIL image and save
    transform = transforms.ToPILImage()
    image = transform(image)
    image.save(filename)
    
    return image

def display_images(content_img, style_img, generated_img):
    """
    æ˜¾ç¤ºå†…å®¹å›¾åƒã€é£æ ¼å›¾åƒå’Œç”Ÿæˆå›¾åƒ
    Display content, style, and generated images
    """
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    images = [content_img, style_img, generated_img]
    titles = ['Content Image', 'Style Image', 'Generated Image']
    
    for i, (img, title) in enumerate(zip(images, titles)):
        if isinstance(img, torch.Tensor):
            img = save_image(img, f'temp_{i}.jpg')
        
        axes[i].imshow(img)
        axes[i].set_title(title, fontsize=14, fontweight='bold')
        axes[i].axis('off')
    
    plt.tight_layout()
    plt.show()
```

### ç¬¬ä¸‰æ­¥: é£æ ¼è¿ç§»ä¸»å‡½æ•° | Step 3: Main Style Transfer Function

```python
def neural_style_transfer(content_path, style_path, output_path,
                         num_iterations=1000, content_weight=1, style_weight=1000000,
                         learning_rate=0.01):
    """
    ç¥ç»é£æ ¼è¿ç§»ä¸»å‡½æ•°
    Main Neural Style Transfer Function
    """
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    # 1. åŠ è½½å›¾åƒ
    # 1. Load images
    content_image = load_image(content_path).to(device)
    style_image = load_image(style_path).to(device)
    
    # 2. åˆå§‹åŒ–ç”Ÿæˆå›¾åƒï¼ˆä»å†…å®¹å›¾åƒå¼€å§‹ï¼‰
    # 2. Initialize generated image (start from content image)
    generated_image = content_image.clone().requires_grad_(True)
    
    # 3. åˆ›å»ºç‰¹å¾æå–å™¨
    # 3. Create feature extractor
    feature_extractor = VGGFeatureExtractor().to(device)
    
    # 4. æå–ç›®æ ‡ç‰¹å¾
    # 4. Extract target features
    content_features, _ = feature_extractor(content_image)
    _, style_features = feature_extractor(style_image)
    
    # 5. ä¼˜åŒ–å™¨
    # 5. Optimizer
    optimizer = optim.LBFGS([generated_image], lr=learning_rate)
    
    # 6. è®­ç»ƒå¾ªç¯
    # 6. Training loop
    losses = []
    
    def closure():
        """
        LBFGSä¼˜åŒ–å™¨éœ€è¦çš„é—­åŒ…å‡½æ•°
        Closure function required by LBFGS optimizer
        """
        optimizer.zero_grad()
        
        # æå–ç”Ÿæˆå›¾åƒçš„ç‰¹å¾
        # Extract features from generated image
        gen_content_features, gen_style_features = feature_extractor(generated_image)
        
        # è®¡ç®—å†…å®¹æŸå¤±
        # Calculate content loss
        content_loss_total = 0
        for layer in feature_extractor.content_layers:
            content_loss_total += content_loss(
                content_features[layer], 
                gen_content_features[layer]
            )
        
        # è®¡ç®—é£æ ¼æŸå¤±
        # Calculate style loss
        style_loss_total = 0
        for layer in feature_extractor.style_layers:
            style_loss_total += style_loss(
                style_features[layer], 
                gen_style_features[layer]
            )
        
        # æ€»æŸå¤±
        # Total loss
        total_loss_value = content_weight * content_loss_total + style_weight * style_loss_total
        
        total_loss_value.backward()
        return total_loss_value
    
    print("å¼€å§‹é£æ ¼è¿ç§»...")
    print("Starting style transfer...")
    
    for iteration in range(num_iterations):
        loss = optimizer.step(closure)
        losses.append(loss.item())
        
        # é™åˆ¶åƒç´ å€¼èŒƒå›´
        # Clamp pixel values
        with torch.no_grad():
            generated_image.clamp_(0, 1)
        
        if iteration % 100 == 0:
            print(f'Iteration {iteration}, Loss: {loss.item():.4f}')
            
            # ä¿å­˜ä¸­é—´ç»“æœ
            # Save intermediate results
            if iteration % 500 == 0:
                temp_image = save_image(generated_image, f'temp_iter_{iteration}.jpg')
    
    # 7. ä¿å­˜æœ€ç»ˆç»“æœ
    # 7. Save final result
    final_image = save_image(generated_image, output_path)
    
    return final_image, losses

# æŸå¤±å‡½æ•°å®ç°
# Loss function implementations
def content_loss(content_features, generated_features):
    return torch.mean((content_features - generated_features) ** 2)

def gram_matrix(features):
    batch_size, channels, height, width = features.size()
    features = features.view(batch_size * channels, height * width)
    gram = torch.mm(features, features.t())
    return gram / (batch_size * channels * height * width)

def style_loss(style_features, generated_features):
    style_gram = gram_matrix(style_features)
    generated_gram = gram_matrix(generated_features)
    return torch.mean((style_gram - generated_gram) ** 2)
```

### ç¬¬å››æ­¥: ä½¿ç”¨ç¤ºä¾‹ | Step 4: Usage Example

```python
def main():
    """
    ä¸»å‡½æ•°ç¤ºä¾‹
    Main function example
    """
    # å›¾åƒè·¯å¾„
    # Image paths
    content_path = 'images/content.jpg'
    style_path = 'images/style.jpg'
    output_path = 'results/stylized_image.jpg'
    
    # æ‰§è¡Œé£æ ¼è¿ç§»
    # Perform style transfer
    generated_image, losses = neural_style_transfer(
        content_path=content_path,
        style_path=style_path,
        output_path=output_path,
        num_iterations=1000,
        content_weight=1,
        style_weight=1000000,
        learning_rate=0.01
    )
    
    # æ˜¾ç¤ºç»“æœ
    # Display results
    content_img = Image.open(content_path)
    style_img = Image.open(style_path)
    display_images(content_img, style_img, generated_image)
    
    # ç»˜åˆ¶æŸå¤±æ›²çº¿
    # Plot loss curve
    plt.figure(figsize=(10, 6))
    plt.plot(losses)
    plt.title('Training Loss Over Time')
    plt.xlabel('Iteration')
    plt.ylabel('Loss')
    plt.grid(True)
    plt.show()

if __name__ == "__main__":
    main()
```

## ğŸ¨ é«˜çº§æŠ€å·§ä¸ä¼˜åŒ– | Advanced Techniques and Optimizations

### 1. å¤šå°ºåº¦é£æ ¼è¿ç§» | Multi-scale Style Transfer

```python
def multi_scale_style_transfer(content_path, style_path, scales=[512, 1024]):
    """
    å¤šå°ºåº¦é£æ ¼è¿ç§»ï¼Œä»å°å›¾å¼€å§‹é€æ­¥æ”¾å¤§
    Multi-scale style transfer, starting from small images and gradually scaling up
    """
    results = []
    
    for i, scale in enumerate(scales):
        print(f"Processing scale: {scale}x{scale}")
        
        # è°ƒæ•´å›¾åƒå¤§å°
        # Resize images
        content_img = load_image(content_path, max_size=scale)
        style_img = load_image(style_path, max_size=scale)
        
        # å¦‚æœä¸æ˜¯ç¬¬ä¸€ä¸ªå°ºåº¦ï¼Œä½¿ç”¨ä¸Šä¸€ä¸ªç»“æœä½œä¸ºåˆå§‹åŒ–
        # If not the first scale, use previous result as initialization
        if i > 0:
            prev_result = results[-1]
            # ä¸Šé‡‡æ ·åˆ°å½“å‰å°ºåº¦
            # Upsample to current scale
            generated_img = torch.nn.functional.interpolate(
                prev_result, size=(scale, scale), mode='bilinear'
            )
        else:
            generated_img = content_img.clone()
        
        # æ‰§è¡Œé£æ ¼è¿ç§»
        # Perform style transfer
        result, _ = neural_style_transfer(
            content_img, style_img, generated_img,
            num_iterations=500 if i > 0 else 1000
        )
        
        results.append(result)
    
    return results[-1]
```

### 2. å±€éƒ¨é£æ ¼æ§åˆ¶ | Local Style Control

```python
def region_based_style_transfer(content_path, style_path, mask_path):
    """
    åŸºäºåŒºåŸŸçš„é£æ ¼è¿ç§»
    Region-based style transfer
    """
    # åŠ è½½é®ç½©
    # Load mask
    mask = Image.open(mask_path).convert('L')
    mask = transforms.ToTensor()(mask).unsqueeze(0)
    
    # æ­£å¸¸é£æ ¼è¿ç§»
    # Normal style transfer
    generated_image, _ = neural_style_transfer(content_path, style_path)
    
    # æ ¹æ®é®ç½©æ··åˆåŸå›¾å’Œé£æ ¼åŒ–å›¾åƒ
    # Blend original and stylized images based on mask
    content_image = load_image(content_path)
    final_image = mask * generated_image + (1 - mask) * content_image
    
    return final_image
```

### 3. é£æ ¼å¼ºåº¦æ§åˆ¶ | Style Intensity Control

```python
def controllable_style_transfer(content_path, style_path, style_strength=1.0):
    """
    å¯æ§åˆ¶é£æ ¼å¼ºåº¦çš„è¿ç§»
    Controllable style intensity transfer
    """
    # è°ƒæ•´é£æ ¼æƒé‡
    # Adjust style weight
    base_style_weight = 1000000
    adjusted_style_weight = base_style_weight * style_strength
    
    generated_image, _ = neural_style_transfer(
        content_path=content_path,
        style_path=style_path,
        style_weight=adjusted_style_weight
    )
    
    return generated_image
```

## ğŸ“Š æ€§èƒ½åˆ†æä¸è¯„ä¼° | Performance Analysis and Evaluation

### è´¨é‡è¯„ä¼°æŒ‡æ ‡ | Quality Evaluation Metrics

```python
def evaluate_style_transfer_quality(original, stylized, style):
    """
    è¯„ä¼°é£æ ¼è¿ç§»è´¨é‡
    Evaluate style transfer quality
    """
    # 1. å†…å®¹ä¿æŒåº¦ (Content Preservation)
    content_similarity = calculate_content_similarity(original, stylized)
    
    # 2. é£æ ¼ç›¸ä¼¼åº¦ (Style Similarity)
    style_similarity = calculate_style_similarity(stylized, style)
    
    # 3. è‰ºæœ¯è´¨é‡ (Artistic Quality) - ä¸»è§‚è¯„ä¼°
    artistic_score = subjective_artistic_evaluation(stylized)
    
    return {
        'content_preservation': content_similarity,
        'style_similarity': style_similarity,
        'artistic_quality': artistic_score
    }

def calculate_content_similarity(img1, img2):
    """è®¡ç®—å†…å®¹ç›¸ä¼¼åº¦"""
    # ä½¿ç”¨SSIM (Structural Similarity Index)
    from skimage.metrics import structural_similarity as ssim
    
    # è½¬æ¢ä¸ºç°åº¦å›¾
    gray1 = transforms.Grayscale()(img1)
    gray2 = transforms.Grayscale()(img2)
    
    # è®¡ç®—SSIM
    ssim_score = ssim(gray1.numpy(), gray2.numpy(), data_range=1.0)
    return ssim_score
```

## ğŸ¯ å®é™…åº”ç”¨åœºæ™¯ | Real-world Applications

### 1. è‰ºæœ¯åˆ›ä½œå·¥å…· | Artistic Creation Tool

```python
class ArtisticStyleStudio:
    """
    è‰ºæœ¯é£æ ¼å·¥ä½œå®¤ - é›†æˆå¤šç§é£æ ¼è¿ç§»åŠŸèƒ½
    Artistic Style Studio - Integrated multiple style transfer functions
    """
    def __init__(self):
        self.famous_styles = {
            'van_gogh': 'styles/starry_night.jpg',
            'picasso': 'styles/guernica.jpg',
            'monet': 'styles/water_lilies.jpg',
            'kandinsky': 'styles/composition_vii.jpg'
        }
    
    def apply_artist_style(self, content_image, artist_name):
        """åº”ç”¨å¤§å¸ˆé£æ ¼"""
        if artist_name not in self.famous_styles:
            raise ValueError(f"Artist {artist_name} not available")
        
        style_path = self.famous_styles[artist_name]
        return neural_style_transfer(content_image, style_path)
    
    def create_art_collection(self, content_image):
        """åˆ›å»ºè‰ºæœ¯ä½œå“é›†"""
        collection = {}
        for artist, style_path in self.famous_styles.items():
            print(f"Creating {artist} style...")
            result = neural_style_transfer(content_image, style_path)
            collection[artist] = result
        
        return collection
```

### 2. è§†é¢‘é£æ ¼è¿ç§» | Video Style Transfer

```python
def video_style_transfer(video_path, style_path, output_path):
    """
    è§†é¢‘é£æ ¼è¿ç§»
    Video style transfer
    """
    import cv2
    
    # æ‰“å¼€è§†é¢‘
    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    
    # åˆ›å»ºè§†é¢‘å†™å…¥å™¨
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
    
    frame_count = 0
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        # è½¬æ¢æ ¼å¼å¹¶åº”ç”¨é£æ ¼è¿ç§»
        frame_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
        
        # ä¿å­˜ä¸´æ—¶å¸§
        temp_frame_path = f'temp_frame_{frame_count}.jpg'
        frame_pil.save(temp_frame_path)
        
        # åº”ç”¨é£æ ¼è¿ç§»
        stylized_frame, _ = neural_style_transfer(
            temp_frame_path, style_path,
            num_iterations=100  # å‡å°‘è¿­ä»£æ¬¡æ•°ä»¥æé«˜é€Ÿåº¦
        )
        
        # è½¬æ¢å›OpenCVæ ¼å¼
        stylized_array = np.array(stylized_frame)
        stylized_bgr = cv2.cvtColor(stylized_array, cv2.COLOR_RGB2BGR)
        
        # å†™å…¥è§†é¢‘
        out.write(stylized_bgr)
        
        frame_count += 1
        print(f"Processed frame {frame_count}")
    
    cap.release()
    out.release()
```

## ğŸš€ åˆ›æ–°æ‰©å±•é¡¹ç›® | Innovative Extension Projects

### 1. äº¤äº’å¼é£æ ¼è¿ç§» | Interactive Style Transfer

```python
import gradio as gr

def create_interactive_style_transfer():
    """
    åˆ›å»ºäº¤äº’å¼é£æ ¼è¿ç§»ç•Œé¢
    Create interactive style transfer interface
    """
    def transfer_style(content_img, style_img, style_weight):
        # ä¿å­˜ä¸Šä¼ çš„å›¾åƒ
        content_img.save('temp_content.jpg')
        style_img.save('temp_style.jpg')
        
        # æ‰§è¡Œé£æ ¼è¿ç§»
        result, _ = neural_style_transfer(
            'temp_content.jpg', 'temp_style.jpg',
            style_weight=style_weight * 1000000
        )
        
        return result
    
    # åˆ›å»ºGradioç•Œé¢
    interface = gr.Interface(
        fn=transfer_style,
        inputs=[
            gr.Image(type="pil", label="Content Image"),
            gr.Image(type="pil", label="Style Image"),
            gr.Slider(0.1, 2.0, value=1.0, label="Style Strength")
        ],
        outputs=gr.Image(type="pil", label="Stylized Image"),
        title="Neural Style Transfer Studio",
        description="Upload a content image and style image to create amazing art!"
    )
    
    return interface
```

### 2. é£æ ¼æ··åˆ | Style Blending

```python
def multi_style_transfer(content_path, style_paths, weights):
    """
    å¤šé£æ ¼æ··åˆè¿ç§»
    Multi-style blending transfer
    """
    assert len(style_paths) == len(weights), "Style paths and weights must have same length"
    assert abs(sum(weights) - 1.0) < 1e-6, "Weights must sum to 1.0"
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    content_image = load_image(content_path).to(device)
    generated_image = content_image.clone().requires_grad_(True)
    
    # åŠ è½½æ‰€æœ‰é£æ ¼å›¾åƒ
    style_images = [load_image(path).to(device) for path in style_paths]
    
    feature_extractor = VGGFeatureExtractor().to(device)
    
    # æå–å†…å®¹ç‰¹å¾
    content_features, _ = feature_extractor(content_image)
    
    # æå–æ‰€æœ‰é£æ ¼ç‰¹å¾
    style_features_list = []
    for style_img in style_images:
        _, style_feats = feature_extractor(style_img)
        style_features_list.append(style_feats)
    
    optimizer = optim.LBFGS([generated_image])
    
    def closure():
        optimizer.zero_grad()
        
        gen_content_features, gen_style_features = feature_extractor(generated_image)
        
        # å†…å®¹æŸå¤±
        content_loss_total = 0
        for layer in feature_extractor.content_layers:
            content_loss_total += content_loss(
                content_features[layer], 
                gen_content_features[layer]
            )
        
        # æ··åˆé£æ ¼æŸå¤±
        style_loss_total = 0
        for layer in feature_extractor.style_layers:
            layer_style_loss = 0
            for style_feats, weight in zip(style_features_list, weights):
                layer_style_loss += weight * style_loss(
                    style_feats[layer], 
                    gen_style_features[layer]
                )
            style_loss_total += layer_style_loss
        
        total = content_loss_total + 1000000 * style_loss_total
        total.backward()
        return total
    
    # ä¼˜åŒ–å¾ªç¯
    for i in range(1000):
        optimizer.step(closure)
        if i % 100 == 0:
            print(f"Iteration {i}")
    
    return generated_image
```

---

**ğŸ¯ é¡¹ç›®å®è·µå»ºè®® | Project Practice Suggestions:**

### åˆå­¦è€…è·¯å¾„ | Beginner Path
1. **ç†è§£åŸç†**: æ·±å…¥å­¦ä¹ Gatysè®ºæ–‡ï¼Œç†è§£GramçŸ©é˜µçš„ä½œç”¨
2. **ç®€å•å®ç°**: å…ˆå®ç°åŸºç¡€ç‰ˆæœ¬ï¼Œç¡®ä¿èƒ½ç”Ÿæˆç»“æœ
3. **å‚æ•°è°ƒä¼˜**: å°è¯•ä¸åŒçš„å†…å®¹/é£æ ¼æƒé‡æ¯”ä¾‹
4. **æ•ˆæœåˆ†æ**: åˆ†æä¸åŒé£æ ¼å›¾åƒçš„è¿ç§»æ•ˆæœ

### è¿›é˜¶æŒ‘æˆ˜ | Advanced Challenges
1. **æ€§èƒ½ä¼˜åŒ–**: å®ç°å¿«é€Ÿé£æ ¼è¿ç§»ç½‘ç»œ
2. **è´¨é‡æå‡**: æ·»åŠ æ„ŸçŸ¥æŸå¤±ã€æ€»å˜å·®æŸå¤±
3. **åˆ›æ–°åº”ç”¨**: å¼€å‘ç§»åŠ¨ç«¯å®æ—¶é£æ ¼è¿ç§»åº”ç”¨
4. **è‰ºæœ¯ç ”ç©¶**: åˆ†æä¸åŒè‰ºæœ¯é£æ ¼çš„æ•°å­¦ç‰¹å¾

**å…³é”®æé†’ | Key Reminder**: 
é£æ ¼è¿ç§»ä¸ä»…ä»…æ˜¯æŠ€æœ¯ï¼Œæ›´æ˜¯è‰ºæœ¯ä¸ç§‘å­¦çš„å®Œç¾ç»“åˆã€‚é€šè¿‡è¿™ä¸ªé¡¹ç›®ï¼Œä½ ä¸ä»…ä¼šæŒæ¡æ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œè¿˜ä¼šå¯¹è‰ºæœ¯æœ‰æ›´æ·±çš„ç†è§£ï¼
Style transfer is not just technology, but a perfect combination of art and science. Through this project, you will not only master deep learning techniques, but also gain a deeper understanding of art! 