# ä¼˜åŒ–ç®—æ³•å®è·µé¡¹ç›®æ¦‚è¿°
# Optimization Algorithms Practice Project Overview

**è®©è®­ç»ƒæ›´å¿«æ›´ç¨³å®š - æŒæ¡æ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒå¼•æ“**
**Make Training Faster and More Stable - Master the Core Engine of Deep Learning**

---

## ğŸ¯ é¡¹ç›®ç›®æ ‡ | Project Goals

ä¼˜åŒ–ç®—æ³•æ˜¯æ·±åº¦å­¦ä¹ çš„å¿ƒè„ï¼ä¸€ä¸ªå¥½çš„ä¼˜åŒ–å™¨èƒ½è®©æ¨¡å‹æ”¶æ•›æ›´å¿«ã€æ€§èƒ½æ›´å¥½ã€è®­ç»ƒæ›´ç¨³å®šã€‚é€šè¿‡è¿™ä¸ªé¡¹ç›®ï¼Œä½ å°†æŒæ¡ï¼š
Optimization algorithms are the heart of deep learning! A good optimizer can make models converge faster, perform better, and train more stably. Through this project, you will master:

- **ç»å…¸ä¼˜åŒ–ç®—æ³•** | **Classic Optimization Algorithms**: SGDã€Adamç­‰ä¼˜åŒ–å™¨çš„åŸç†ä¸å®ç°
- **é«˜çº§ä¼˜åŒ–æŠ€æœ¯** | **Advanced Optimization Techniques**: å­¦ä¹ ç‡è°ƒåº¦ã€æ¢¯åº¦è£å‰ªç­‰å…³é”®æŠ€æœ¯
- **åˆ†å¸ƒå¼è®­ç»ƒ** | **Distributed Training**: å¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒçš„å¹¶è¡ŒåŒ–ç­–ç•¥
- **ä¼˜åŒ–ç­–ç•¥è®¾è®¡** | **Optimization Strategy Design**: é’ˆå¯¹ä¸åŒä»»åŠ¡çš„ä¼˜åŒ–æ–¹æ¡ˆè®¾è®¡

## ğŸ”¬ ä¸ºä»€ä¹ˆä¼˜åŒ–ç®—æ³•å¦‚æ­¤é‡è¦ï¼Ÿ| Why are Optimization Algorithms So Important?

**ä¼˜åŒ–ç®—æ³•å†³å®šäº†AIè®­ç»ƒçš„æ•ˆç‡å’Œè´¨é‡ï¼**
**Optimization algorithms determine the efficiency and quality of AI training!**

ä»ImageNetåˆ†ç±»è®­ç»ƒæ—¶é—´ä»å‡ å‘¨ç¼©çŸ­åˆ°å‡ å°æ—¶ï¼Œä»GPT-3éœ€è¦æ•°åƒå¼ GPUåˆ°GPT-4çš„é«˜æ•ˆè®­ç»ƒï¼Œä»ResNetçš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜åˆ°Transformerçš„ç¨³å®šè®­ç»ƒï¼Œä¼˜åŒ–ç®—æ³•çš„è¿›æ­¥æ¨åŠ¨äº†æ•´ä¸ªæ·±åº¦å­¦ä¹ çš„å‘å±•ã€‚æŒæ¡ä¼˜åŒ–ç®—æ³•ï¼Œå°±æŒæ¡äº†è®©AIæ›´å¿«æ›´å¥½åœ°å­¦ä¹ çš„é’¥åŒ™ã€‚

From ImageNet classification training time reduced from weeks to hours, from GPT-3 requiring thousands of GPUs to GPT-4's efficient training, from ResNet's gradient vanishing problems to Transformer's stable training, advances in optimization algorithms have driven the development of deep learning. Mastering optimization algorithms means mastering the key to making AI learn faster and better.

### ä¼˜åŒ–ç®—æ³•çš„å‘å±•å†ç¨‹ | Evolution of Optimization Algorithms
```
1951: éšæœºæ¢¯åº¦ä¸‹é™ | Stochastic Gradient Descent
1964: åŠ¨é‡æ–¹æ³• | Momentum Methods
1987: AdaGradè‡ªé€‚åº”å­¦ä¹ ç‡ | AdaGrad Adaptive Learning Rate
2012: RMSpropå‡æ–¹æ ¹ä¼ æ’­ | RMSprop Root Mean Square Propagation
2014: Adamè‡ªé€‚åº”çŸ©ä¼°è®¡ | Adam Adaptive Moment Estimation
2019: AdamWæƒé‡è¡°å‡ä¿®æ­£ | AdamW Weight Decay Correction
2020: å¤§è§„æ¨¡åˆ†å¸ƒå¼ä¼˜åŒ– | Large-scale Distributed Optimization
```

## ğŸ“š é¡¹ç›®ç»“æ„æ·±åº¦è§£æ | Deep Project Structure Analysis

### 01_ç»å…¸ä¼˜åŒ–ç®—æ³• | Classic Optimization Algorithms

**ç†è§£ä¼˜åŒ–çš„æ•°å­¦æœ¬è´¨ï¼**
**Understand the mathematical essence of optimization!**

#### SGDéšæœºæ¢¯åº¦ä¸‹é™ | SGD Stochastic Gradient Descent

**é¡¹ç›®æ ¸å¿ƒ | Project Core:**
SGDæ˜¯æ·±åº¦å­¦ä¹ çš„åŸºçŸ³ä¼˜åŒ–ç®—æ³•ï¼Œç†è§£SGDçš„åŸç†å’Œå˜ç§æ˜¯æŒæ¡æ‰€æœ‰ä¼˜åŒ–å™¨çš„å‰æã€‚

SGD is the foundational optimization algorithm in deep learning. Understanding SGD's principles and variants is the prerequisite for mastering all optimizers.

**æ•°å­¦åŸç† | Mathematical Principles:**

**æ ‡å‡†SGDæ›´æ–°è§„åˆ™ | Standard SGD Update Rule:**
```
Î¸_{t+1} = Î¸_t - Î· âˆ‡L(Î¸_t)
```

**å¸¦åŠ¨é‡çš„SGD | SGD with Momentum:**
```
v_t = Î² v_{t-1} + Î· âˆ‡L(Î¸_t)
Î¸_{t+1} = Î¸_t - v_t
```

**NesterovåŠ é€Ÿæ¢¯åº¦ | Nesterov Accelerated Gradient:**
```
v_t = Î² v_{t-1} + Î· âˆ‡L(Î¸_t - Î² v_{t-1})
Î¸_{t+1} = Î¸_t - v_t
```

**å®Œæ•´å®ç° | Complete Implementation:**
```python
import numpy as np
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
from typing import Dict, List, Optional, Tuple
import math

class SGDOptimizer:
    """
    SGDä¼˜åŒ–å™¨çš„å®Œæ•´å®ç°
    Complete implementation of SGD optimizer
    """
    def __init__(self, parameters, lr=0.01, momentum=0.0, dampening=0.0, 
                 weight_decay=0.0, nesterov=False):
        """
        Args:
            parameters: æ¨¡å‹å‚æ•° | Model parameters
            lr: å­¦ä¹ ç‡ | Learning rate
            momentum: åŠ¨é‡å› å­ | Momentum factor
            dampening: åŠ¨é‡è¡°å‡ | Momentum dampening
            weight_decay: æƒé‡è¡°å‡ | Weight decay
            nesterov: æ˜¯å¦ä½¿ç”¨NesterovåŠ¨é‡ | Whether to use Nesterov momentum
        """
        self.param_groups = [{'params': list(parameters)}]
        self.defaults = {
            'lr': lr,
            'momentum': momentum,
            'dampening': dampening,
            'weight_decay': weight_decay,
            'nesterov': nesterov
        }
        
        # ä¸ºæ¯ä¸ªå‚æ•°ç»„è®¾ç½®é»˜è®¤å€¼ | Set defaults for each parameter group
        for group in self.param_groups:
            group.setdefault('lr', lr)
            group.setdefault('momentum', momentum)
            group.setdefault('dampening', dampening)
            group.setdefault('weight_decay', weight_decay)
            group.setdefault('nesterov', nesterov)
        
        # çŠ¶æ€å­—å…¸ | State dictionary
        self.state = {}
        
        # ä¼˜åŒ–å†å² | Optimization history
        self.loss_history = []
        self.lr_history = []
        
    def zero_grad(self):
        """æ¸…é›¶æ¢¯åº¦ | Zero gradients"""
        for group in self.param_groups:
            for p in group['params']:
                if p.grad is not None:
                    p.grad.zero_()
    
    def step(self, closure=None):
        """
        æ‰§è¡Œä¸€æ­¥ä¼˜åŒ–
        Perform one optimization step
        """
        loss = None
        if closure is not None:
            loss = closure()
        
        for group in self.param_groups:
            weight_decay = group['weight_decay']
            momentum = group['momentum']
            dampening = group['dampening']
            nesterov = group['nesterov']
            lr = group['lr']
            
            for p in group['params']:
                if p.grad is None:
                    continue
                
                # è·å–æ¢¯åº¦ | Get gradient
                grad = p.grad.data
                
                # æƒé‡è¡°å‡ | Weight decay
                if weight_decay != 0:
                    grad = grad.add(p.data, alpha=weight_decay)
                
                # å‚æ•°çŠ¶æ€ | Parameter state
                param_state = self.state.setdefault(id(p), {})
                
                # åº”ç”¨åŠ¨é‡ | Apply momentum
                if momentum != 0:
                    if 'momentum_buffer' not in param_state:
                        buf = param_state['momentum_buffer'] = torch.zeros_like(p.data)
                        buf.mul_(momentum).add_(grad)
                    else:
                        buf = param_state['momentum_buffer']
                        buf.mul_(momentum).add_(grad, alpha=1 - dampening)
                    
                    if nesterov:
                        # NesterovåŠ¨é‡ | Nesterov momentum
                        grad = grad.add(buf, alpha=momentum)
                    else:
                        # æ ‡å‡†åŠ¨é‡ | Standard momentum
                        grad = buf
                
                # æ›´æ–°å‚æ•° | Update parameters
                p.data.add_(grad, alpha=-lr)
        
        return loss
    
    def state_dict(self):
        """è¿”å›ä¼˜åŒ–å™¨çŠ¶æ€ | Return optimizer state"""
        return {
            'state': self.state,
            'param_groups': self.param_groups
        }
    
    def load_state_dict(self, state_dict):
        """åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€ | Load optimizer state"""
        self.state = state_dict['state']
        self.param_groups = state_dict['param_groups']

class AdaptiveLearningRate:
    """
    è‡ªé€‚åº”å­¦ä¹ ç‡è°ƒåº¦å™¨
    Adaptive Learning Rate Scheduler
    """
    def __init__(self, optimizer, mode='min', factor=0.1, patience=10, 
                 threshold=1e-4, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-8):
        self.optimizer = optimizer
        self.mode = mode
        self.factor = factor
        self.patience = patience
        self.threshold = threshold
        self.threshold_mode = threshold_mode
        self.cooldown = cooldown
        self.min_lr = min_lr
        self.eps = eps
        
        self.best = None
        self.num_bad_epochs = 0
        self.mode_worse = None
        self.cooldown_counter = 0
        self.last_epoch = 0
        
        self._init_is_better()
        
    def _init_is_better(self):
        """åˆå§‹åŒ–æ¯”è¾ƒå‡½æ•° | Initialize comparison function"""
        if self.mode == 'min':
            self.mode_worse = float('inf')
        else:
            self.mode_worse = -float('inf')
    
    def _is_better(self, current, best):
        """åˆ¤æ–­å½“å‰å€¼æ˜¯å¦æ›´å¥½ | Check if current value is better"""
        if self.mode == 'min' and self.threshold_mode == 'rel':
            return current < best - best * self.threshold
        elif self.mode == 'min' and self.threshold_mode == 'abs':
            return current < best - self.threshold
        elif self.mode == 'max' and self.threshold_mode == 'rel':
            return current > best + best * self.threshold
        else:  # mode == 'max' and threshold_mode == 'abs'
            return current > best + self.threshold
    
    def step(self, metrics):
        """
        æ ¹æ®æŒ‡æ ‡è°ƒæ•´å­¦ä¹ ç‡
        Adjust learning rate based on metrics
        """
        current = metrics
        self.last_epoch += 1
        
        if self.best is None:
            self.best = current
        
        if self.cooldown_counter > 0:
            self.cooldown_counter -= 1
            self.num_bad_epochs = 0
        
        if self._is_better(current, self.best):
            self.best = current
            self.num_bad_epochs = 0
        else:
            self.num_bad_epochs += 1
        
        if self.cooldown_counter == 0 and self.num_bad_epochs > self.patience:
            self._reduce_lr()
            self.cooldown_counter = self.cooldown
            self.num_bad_epochs = 0
    
    def _reduce_lr(self):
        """å‡å°‘å­¦ä¹ ç‡ | Reduce learning rate"""
        for group in self.optimizer.param_groups:
            old_lr = group['lr']
            new_lr = max(old_lr * self.factor, self.min_lr)
            if old_lr - new_lr > self.eps:
                group['lr'] = new_lr
                print(f'å­¦ä¹ ç‡ä» {old_lr:.6f} é™è‡³ {new_lr:.6f}')
                print(f'Learning rate reduced from {old_lr:.6f} to {new_lr:.6f}')

def sgd_optimization_demo():
    """
    SGDä¼˜åŒ–ç®—æ³•æ¼”ç¤º
    SGD optimization algorithm demo
    """
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„äºŒæ¬¡å‡½æ•°ä¼˜åŒ–é—®é¢˜ | Create a simple quadratic optimization problem
    def rosenbrock_function(x, y):
        """Rosenbrockå‡½æ•° - ç»å…¸ä¼˜åŒ–æµ‹è¯•å‡½æ•°"""
        return (1 - x)**2 + 100 * (y - x**2)**2
    
    # åˆå§‹åŒ–å‚æ•° | Initialize parameters
    x = torch.tensor([-1.0], requires_grad=True)
    y = torch.tensor([1.0], requires_grad=True)
    
    # æµ‹è¯•ä¸åŒçš„SGDå˜ç§ | Test different SGD variants
    optimizers = {
        'SGD': SGDOptimizer([x, y], lr=0.001),
        'SGD+Momentum': SGDOptimizer([x, y], lr=0.001, momentum=0.9),
        'SGD+Nesterov': SGDOptimizer([x, y], lr=0.001, momentum=0.9, nesterov=True)
    }
    
    # é‡ç½®å‚æ•°å‡½æ•° | Reset parameters function
    def reset_params():
        x.data.fill_(-1.0)
        y.data.fill_(1.0)
        if x.grad is not None:
            x.grad.zero_()
        if y.grad is not None:
            y.grad.zero_()
    
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    
    for idx, (opt_name, optimizer) in enumerate(optimizers.items()):
        reset_params()
        
        # ä¼˜åŒ–å†å² | Optimization history
        x_history = []
        y_history = []
        loss_history = []
        
        # ä¼˜åŒ–å¾ªç¯ | Optimization loop
        for iteration in range(1000):
            optimizer.zero_grad()
            
            # è®¡ç®—æŸå¤± | Compute loss
            loss = rosenbrock_function(x, y)
            
            # åå‘ä¼ æ’­ | Backpropagation
            loss.backward()
            
            # è®°å½•å†å² | Record history
            x_history.append(x.item())
            y_history.append(y.item())
            loss_history.append(loss.item())
            
            # ä¼˜åŒ–æ­¥éª¤ | Optimization step
            optimizer.step()
            
            # æ—©åœæ¡ä»¶ | Early stopping condition
            if loss.item() < 1e-6:
                break
        
        # ç»˜åˆ¶ä¼˜åŒ–è·¯å¾„ | Plot optimization path
        ax = axes[idx]
        
        # ç»˜åˆ¶Rosenbrockå‡½æ•°çš„ç­‰é«˜çº¿ | Plot Rosenbrock function contours
        x_range = np.linspace(-2, 2, 100)
        y_range = np.linspace(-1, 3, 100)
        X, Y = np.meshgrid(x_range, y_range)
        Z = (1 - X)**2 + 100 * (Y - X**2)**2
        
        contour = ax.contour(X, Y, Z, levels=np.logspace(-1, 3, 20), cmap='viridis', alpha=0.6)
        ax.clabel(contour, inline=True, fontsize=8)
        
        # ç»˜åˆ¶ä¼˜åŒ–è·¯å¾„ | Plot optimization path
        ax.plot(x_history, y_history, 'r-', linewidth=2, alpha=0.8, label='Optimization Path')
        ax.plot(x_history[0], y_history[0], 'go', markersize=8, label='Start')
        ax.plot(x_history[-1], y_history[-1], 'ro', markersize=8, label='End')
        ax.plot(1, 1, 'b*', markersize=15, label='Global Minimum')
        
        ax.set_title(f'{opt_name} Optimization\nFinal Loss: {loss_history[-1]:.6f}')
        ax.set_xlabel('X')
        ax.set_ylabel('Y')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        print(f"{opt_name}: æ”¶æ•›åˆ° ({x.item():.4f}, {y.item():.4f}), æŸå¤±: {loss_history[-1]:.6f}")
        print(f"{opt_name}: Converged to ({x.item():.4f}, {y.item():.4f}), Loss: {loss_history[-1]:.6f}")
    
    plt.tight_layout()
    plt.show()
    
    return optimizers

if __name__ == "__main__":
    sgd_optimization_demo()
```

#### Adamè‡ªé€‚åº”ä¼˜åŒ– | Adam Adaptive Optimization

**é¡¹ç›®ç‰¹è‰² | Project Features:**
Adamç»“åˆäº†åŠ¨é‡å’Œè‡ªé€‚åº”å­¦ä¹ ç‡çš„ä¼˜åŠ¿ï¼Œæ˜¯ç°ä»£æ·±åº¦å­¦ä¹ ä¸­æœ€å¹¿æ³›ä½¿ç”¨çš„ä¼˜åŒ–å™¨ã€‚

Adam combines the advantages of momentum and adaptive learning rates, being the most widely used optimizer in modern deep learning.

**Adamç®—æ³•åŸç† | Adam Algorithm Principles:**

**Adamæ›´æ–°è§„åˆ™ | Adam Update Rules:**
```python
# ä¸€é˜¶çŸ©ä¼°è®¡ | First moment estimate
m_t = Î²â‚ m_{t-1} + (1 - Î²â‚) g_t

# äºŒé˜¶çŸ©ä¼°è®¡ | Second moment estimate  
v_t = Î²â‚‚ v_{t-1} + (1 - Î²â‚‚) g_tÂ²

# åå·®ä¿®æ­£ | Bias correction
mÌ‚_t = m_t / (1 - Î²â‚^t)
vÌ‚_t = v_t / (1 - Î²â‚‚^t)

# å‚æ•°æ›´æ–° | Parameter update
Î¸_{t+1} = Î¸_t - Î± mÌ‚_t / (âˆšvÌ‚_t + Îµ)
```

**å®Œæ•´Adamå®ç° | Complete Adam Implementation:**
```python
class AdamOptimizer:
    """
    Adamä¼˜åŒ–å™¨çš„å®Œæ•´å®ç°
    Complete implementation of Adam optimizer
    """
    def __init__(self, parameters, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, 
                 weight_decay=0, amsgrad=False):
        """
        Args:
            parameters: æ¨¡å‹å‚æ•° | Model parameters
            lr: å­¦ä¹ ç‡ | Learning rate
            betas: (Î²â‚, Î²â‚‚) æŒ‡æ•°è¡°å‡ç‡ | Exponential decay rates
            eps: æ•°å€¼ç¨³å®šæ€§å¸¸æ•° | Numerical stability constant
            weight_decay: æƒé‡è¡°å‡ | Weight decay
            amsgrad: æ˜¯å¦ä½¿ç”¨AMSGradå˜ç§ | Whether to use AMSGrad variant
        """
        self.param_groups = [{'params': list(parameters)}]
        self.defaults = {
            'lr': lr,
            'betas': betas,
            'eps': eps,
            'weight_decay': weight_decay,
            'amsgrad': amsgrad
        }
        
        # ä¸ºæ¯ä¸ªå‚æ•°ç»„è®¾ç½®é»˜è®¤å€¼ | Set defaults for each parameter group
        for group in self.param_groups:
            group.setdefault('lr', lr)
            group.setdefault('betas', betas)
            group.setdefault('eps', eps)
            group.setdefault('weight_decay', weight_decay)
            group.setdefault('amsgrad', amsgrad)
        
        # çŠ¶æ€å­—å…¸ | State dictionary
        self.state = {}
        
    def zero_grad(self):
        """æ¸…é›¶æ¢¯åº¦ | Zero gradients"""
        for group in self.param_groups:
            for p in group['params']:
                if p.grad is not None:
                    p.grad.zero_()
    
    def step(self, closure=None):
        """
        æ‰§è¡Œä¸€æ­¥ä¼˜åŒ–
        Perform one optimization step
        """
        loss = None
        if closure is not None:
            loss = closure()
        
        for group in self.param_groups:
            lr = group['lr']
            beta1, beta2 = group['betas']
            eps = group['eps']
            weight_decay = group['weight_decay']
            amsgrad = group['amsgrad']
            
            for p in group['params']:
                if p.grad is None:
                    continue
                
                # è·å–æ¢¯åº¦ | Get gradient
                grad = p.grad.data
                
                # æƒé‡è¡°å‡ | Weight decay
                if weight_decay != 0:
                    grad = grad.add(p.data, alpha=weight_decay)
                
                # å‚æ•°çŠ¶æ€ | Parameter state
                param_state = self.state.setdefault(id(p), {})
                
                # åˆå§‹åŒ–çŠ¶æ€ | Initialize state
                if len(param_state) == 0:
                    param_state['step'] = 0
                    param_state['exp_avg'] = torch.zeros_like(p.data)  # m_t
                    param_state['exp_avg_sq'] = torch.zeros_like(p.data)  # v_t
                    if amsgrad:
                        param_state['max_exp_avg_sq'] = torch.zeros_like(p.data)
                
                exp_avg, exp_avg_sq = param_state['exp_avg'], param_state['exp_avg_sq']
                if amsgrad:
                    max_exp_avg_sq = param_state['max_exp_avg_sq']
                
                param_state['step'] += 1
                bias_correction1 = 1 - beta1 ** param_state['step']
                bias_correction2 = 1 - beta2 ** param_state['step']
                
                # æ›´æ–°ä¸€é˜¶å’ŒäºŒé˜¶çŸ©ä¼°è®¡ | Update first and second moment estimates
                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)
                
                if amsgrad:
                    # ç»´æŠ¤äºŒé˜¶çŸ©çš„æœ€å¤§å€¼ | Maintain maximum of second moment
                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)
                    denom = (max_exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(eps)
                else:
                    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(eps)
                
                # è®¡ç®—æ­¥é•¿ | Compute step size
                step_size = lr / bias_correction1
                
                # æ›´æ–°å‚æ•° | Update parameters
                p.data.addcdiv_(exp_avg, denom, value=-step_size)
        
        return loss

class AdamWOptimizer(AdamOptimizer):
    """
    AdamWä¼˜åŒ–å™¨ï¼šæ­£ç¡®çš„æƒé‡è¡°å‡å®ç°
    AdamW optimizer: Correct weight decay implementation
    """
    def step(self, closure=None):
        """
        AdamWçš„å…³é”®åŒºåˆ«ï¼šæƒé‡è¡°å‡ä¸é€šè¿‡æ¢¯åº¦ï¼Œè€Œæ˜¯ç›´æ¥ä½œç”¨äºå‚æ•°
        Key difference in AdamW: weight decay acts directly on parameters, not through gradients
        """
        loss = None
        if closure is not None:
            loss = closure()
        
        for group in self.param_groups:
            lr = group['lr']
            beta1, beta2 = group['betas']
            eps = group['eps']
            weight_decay = group['weight_decay']
            amsgrad = group['amsgrad']
            
            for p in group['params']:
                if p.grad is None:
                    continue
                
                # è·å–æ¢¯åº¦ï¼ˆä¸åŒ…å«æƒé‡è¡°å‡ï¼‰| Get gradient (without weight decay)
                grad = p.grad.data
                
                # å‚æ•°çŠ¶æ€ | Parameter state
                param_state = self.state.setdefault(id(p), {})
                
                # åˆå§‹åŒ–çŠ¶æ€ | Initialize state
                if len(param_state) == 0:
                    param_state['step'] = 0
                    param_state['exp_avg'] = torch.zeros_like(p.data)
                    param_state['exp_avg_sq'] = torch.zeros_like(p.data)
                    if amsgrad:
                        param_state['max_exp_avg_sq'] = torch.zeros_like(p.data)
                
                exp_avg, exp_avg_sq = param_state['exp_avg'], param_state['exp_avg_sq']
                if amsgrad:
                    max_exp_avg_sq = param_state['max_exp_avg_sq']
                
                param_state['step'] += 1
                bias_correction1 = 1 - beta1 ** param_state['step']
                bias_correction2 = 1 - beta2 ** param_state['step']
                
                # æ›´æ–°ä¸€é˜¶å’ŒäºŒé˜¶çŸ©ä¼°è®¡ | Update first and second moment estimates
                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)
                
                if amsgrad:
                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)
                    denom = (max_exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(eps)
                else:
                    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(eps)
                
                # è®¡ç®—æ­¥é•¿ | Compute step size
                step_size = lr / bias_correction1
                
                # AdamWï¼šå…ˆåº”ç”¨æƒé‡è¡°å‡ï¼Œå†åº”ç”¨Adamæ›´æ–° | AdamW: apply weight decay first, then Adam update
                if weight_decay != 0:
                    p.data.mul_(1 - lr * weight_decay)
                
                # æ›´æ–°å‚æ•° | Update parameters
                p.data.addcdiv_(exp_avg, denom, value=-step_size)
        
        return loss

def adam_vs_sgd_comparison():
    """
    Adam vs SGD å¯¹æ¯”å®éªŒ
    Adam vs SGD comparison experiment
    """
    # åˆ›å»ºä¸€ä¸ªæ›´å¤æ‚çš„ä¼˜åŒ–é—®é¢˜ | Create a more complex optimization problem
    class ComplexLoss(nn.Module):
        def __init__(self):
            super().__init__()
            self.params = nn.Parameter(torch.randn(10, requires_grad=True))
        
        def forward(self):
            # å¤æ‚çš„éå‡¸æŸå¤±å‡½æ•° | Complex non-convex loss function
            x = self.params
            loss = torch.sum((x - torch.arange(10, dtype=torch.float32))**2)
            loss += 0.1 * torch.sum(torch.sin(10 * x)**2)
            loss += 0.01 * torch.sum(x**4)
            return loss
    
    # åˆ›å»ºæ¨¡å‹å®ä¾‹ | Create model instances
    models = {
        'SGD': ComplexLoss(),
        'SGD+Momentum': ComplexLoss(),
        'Adam': ComplexLoss(),
        'AdamW': ComplexLoss()
    }
    
    # åˆå§‹åŒ–ç›¸åŒçš„å‚æ•° | Initialize same parameters
    init_params = torch.randn(10)
    for model in models.values():
        model.params.data.copy_(init_params)
    
    # åˆ›å»ºä¼˜åŒ–å™¨ | Create optimizers
    optimizers = {
        'SGD': SGDOptimizer(models['SGD'].parameters(), lr=0.01),
        'SGD+Momentum': SGDOptimizer(models['SGD+Momentum'].parameters(), lr=0.01, momentum=0.9),
        'Adam': AdamOptimizer(models['Adam'].parameters(), lr=0.01),
        'AdamW': AdamWOptimizer(models['AdamW'].parameters(), lr=0.01, weight_decay=0.01)
    }
    
    # è®­ç»ƒå†å² | Training history
    history = {name: [] for name in models.keys()}
    
    # ä¼˜åŒ–å¾ªç¯ | Optimization loop
    num_iterations = 1000
    for iteration in range(num_iterations):
        for name in models.keys():
            model = models[name]
            optimizer = optimizers[name]
            
            optimizer.zero_grad()
            loss = model()
            loss.backward()
            optimizer.step()
            
            history[name].append(loss.item())
    
    # ç»˜åˆ¶ç»“æœ | Plot results
    plt.figure(figsize=(15, 5))
    
    # æŸå¤±æ›²çº¿ | Loss curves
    plt.subplot(1, 3, 1)
    for name, losses in history.items():
        plt.plot(losses, label=name, linewidth=2)
    plt.xlabel('Iteration')
    plt.ylabel('Loss')
    plt.title('Optimization Comparison')
    plt.legend()
    plt.yscale('log')
    plt.grid(True, alpha=0.3)
    
    # æœ€ç»ˆå‚æ•°å¯¹æ¯” | Final parameters comparison
    plt.subplot(1, 3, 2)
    target = torch.arange(10, dtype=torch.float32)
    x_pos = np.arange(10)
    width = 0.2
    
    for i, (name, model) in enumerate(models.items()):
        plt.bar(x_pos + i * width, model.params.data.numpy(), 
               width, label=name, alpha=0.7)
    
    plt.bar(x_pos + len(models) * width, target.numpy(), 
           width, label='Target', alpha=0.7, color='red')
    plt.xlabel('Parameter Index')
    plt.ylabel('Parameter Value')
    plt.title('Final Parameters')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # æ”¶æ•›é€Ÿåº¦å¯¹æ¯” | Convergence speed comparison
    plt.subplot(1, 3, 3)
    convergence_threshold = 1e-3
    for name, losses in history.items():
        convergence_step = next((i for i, loss in enumerate(losses) 
                               if loss < convergence_threshold), len(losses))
        plt.bar(name, convergence_step, alpha=0.7)
    plt.ylabel('Steps to Convergence')
    plt.title('Convergence Speed')
    plt.xticks(rotation=45)
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # æ‰“å°æœ€ç»ˆç»“æœ | Print final results
    print("\n=== ä¼˜åŒ–ç»“æœå¯¹æ¯” | Optimization Results Comparison ===")
    for name, model in models.items():
        final_loss = history[name][-1]
        param_error = torch.norm(model.params.data - target).item()
        print(f"{name}: æœ€ç»ˆæŸå¤± {final_loss:.6f}, å‚æ•°è¯¯å·® {param_error:.6f}")
        print(f"{name}: Final loss {final_loss:.6f}, Parameter error {param_error:.6f}")

if __name__ == "__main__":
    adam_vs_sgd_comparison()
```

### 02_é«˜çº§ä¼˜åŒ–æŠ€æœ¯ | Advanced Optimization Techniques

**æŒæ¡è®­ç»ƒç¨³å®šæ€§çš„å…³é”®æŠ€æœ¯ï¼**
**Master key techniques for training stability!**

#### å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥ | Learning Rate Scheduling Strategies

**é¡¹ç›®ä»·å€¼ | Project Value:**
å­¦ä¹ ç‡è°ƒåº¦æ˜¯ä¼˜åŒ–æˆåŠŸçš„å…³é”®å› ç´ ï¼Œåˆé€‚çš„å­¦ä¹ ç‡ç­–ç•¥èƒ½æ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½å’Œè®­ç»ƒç¨³å®šæ€§ã€‚

Learning rate scheduling is a key factor for optimization success. Appropriate learning rate strategies can significantly improve model performance and training stability.

**å¤šç§å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥ | Multiple Learning Rate Scheduling Strategies:**
```python
import math
from typing import Union, List

class LearningRateScheduler:
    """
    å­¦ä¹ ç‡è°ƒåº¦å™¨åŸºç±»
    Base class for learning rate schedulers
    """
    def __init__(self, optimizer, last_epoch=-1):
        self.optimizer = optimizer
        self.last_epoch = last_epoch
        
        # ä¿å­˜åˆå§‹å­¦ä¹ ç‡ | Save initial learning rates
        if last_epoch == -1:
            for group in optimizer.param_groups:
                group.setdefault('initial_lr', group['lr'])
        else:
            for i, group in enumerate(optimizer.param_groups):
                if 'initial_lr' not in group:
                    raise KeyError(f"å‚æ•°ç»„ {i} æ²¡æœ‰æŒ‡å®š 'initial_lr'")
        
        self.base_lrs = [group['initial_lr'] for group in optimizer.param_groups]
        self.step(last_epoch + 1)
    
    def get_lr(self):
        """è®¡ç®—å½“å‰å­¦ä¹ ç‡ | Compute current learning rates"""
        raise NotImplementedError
    
    def step(self, epoch=None):
        """æ›´æ–°å­¦ä¹ ç‡ | Update learning rates"""
        if epoch is None:
            epoch = self.last_epoch + 1
        self.last_epoch = epoch
        
        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):
            param_group['lr'] = lr

class StepLR(LearningRateScheduler):
    """
    é˜¶æ¢¯å¼å­¦ä¹ ç‡è¡°å‡
    Step learning rate decay
    """
    def __init__(self, optimizer, step_size, gamma=0.1, last_epoch=-1):
        self.step_size = step_size
        self.gamma = gamma
        super().__init__(optimizer, last_epoch)
    
    def get_lr(self):
        return [base_lr * self.gamma ** (self.last_epoch // self.step_size)
                for base_lr in self.base_lrs]

class ExponentialLR(LearningRateScheduler):
    """
    æŒ‡æ•°å­¦ä¹ ç‡è¡°å‡
    Exponential learning rate decay
    """
    def __init__(self, optimizer, gamma, last_epoch=-1):
        self.gamma = gamma
        super().__init__(optimizer, last_epoch)
    
    def get_lr(self):
        return [base_lr * self.gamma ** self.last_epoch
                for base_lr in self.base_lrs]

class CosineAnnealingLR(LearningRateScheduler):
    """
    ä½™å¼¦é€€ç«å­¦ä¹ ç‡
    Cosine annealing learning rate
    """
    def __init__(self, optimizer, T_max, eta_min=0, last_epoch=-1):
        self.T_max = T_max
        self.eta_min = eta_min
        super().__init__(optimizer, last_epoch)
    
    def get_lr(self):
        return [self.eta_min + (base_lr - self.eta_min) *
                (1 + math.cos(math.pi * self.last_epoch / self.T_max)) / 2
                for base_lr in self.base_lrs]

class WarmupCosineAnnealingLR(LearningRateScheduler):
    """
    å¸¦é¢„çƒ­çš„ä½™å¼¦é€€ç«å­¦ä¹ ç‡
    Cosine annealing with warmup
    """
    def __init__(self, optimizer, warmup_epochs, T_max, eta_min=0, last_epoch=-1):
        self.warmup_epochs = warmup_epochs
        self.T_max = T_max
        self.eta_min = eta_min
        super().__init__(optimizer, last_epoch)
    
    def get_lr(self):
        if self.last_epoch < self.warmup_epochs:
            # çº¿æ€§é¢„çƒ­ | Linear warmup
            return [base_lr * self.last_epoch / self.warmup_epochs
                    for base_lr in self.base_lrs]
        else:
            # ä½™å¼¦é€€ç« | Cosine annealing
            progress = (self.last_epoch - self.warmup_epochs) / (self.T_max - self.warmup_epochs)
            return [self.eta_min + (base_lr - self.eta_min) *
                    (1 + math.cos(math.pi * progress)) / 2
                    for base_lr in self.base_lrs]

class CyclicLR(LearningRateScheduler):
    """
    å¾ªç¯å­¦ä¹ ç‡
    Cyclic learning rate
    """
    def __init__(self, optimizer, base_lr, max_lr, step_size_up=2000, 
                 step_size_down=None, mode='triangular', gamma=1.0, last_epoch=-1):
        self.base_lr = base_lr
        self.max_lr = max_lr
        self.step_size_up = step_size_up
        self.step_size_down = step_size_down or step_size_up
        self.mode = mode
        self.gamma = gamma
        
        self.cycle_size = step_size_up + self.step_size_down
        super().__init__(optimizer, last_epoch)
    
    def get_lr(self):
        cycle = 1 + self.last_epoch // self.cycle_size
        x = 1 + self.last_epoch / self.cycle_size - cycle
        
        if x <= self.step_size_up / self.cycle_size:
            # ä¸Šå‡é˜¶æ®µ | Ascending phase
            scale_factor = x * self.cycle_size / self.step_size_up
        else:
            # ä¸‹é™é˜¶æ®µ | Descending phase
            scale_factor = (self.step_size_down - (x * self.cycle_size - self.step_size_up)) / self.step_size_down
        
        if self.mode == 'triangular':
            lr_delta = (self.max_lr - self.base_lr) * max(0, scale_factor)
        elif self.mode == 'exp_range':
            lr_delta = (self.max_lr - self.base_lr) * max(0, scale_factor) * (self.gamma ** self.last_epoch)
        
        return [self.base_lr + lr_delta for _ in self.base_lrs]

class OneCycleLR(LearningRateScheduler):
    """
    ä¸€å‘¨æœŸå­¦ä¹ ç‡ç­–ç•¥
    One cycle learning rate policy
    """
    def __init__(self, optimizer, max_lr, total_steps, pct_start=0.3, 
                 anneal_strategy='cos', last_epoch=-1):
        self.max_lr = max_lr
        self.total_steps = total_steps
        self.pct_start = pct_start
        self.anneal_strategy = anneal_strategy
        super().__init__(optimizer, last_epoch)
    
    def get_lr(self):
        step_ratio = self.last_epoch / self.total_steps
        
        if step_ratio < self.pct_start:
            # ä¸Šå‡é˜¶æ®µ | Ascending phase
            lr_ratio = step_ratio / self.pct_start
            return [base_lr + (self.max_lr - base_lr) * lr_ratio
                    for base_lr in self.base_lrs]
        else:
            # ä¸‹é™é˜¶æ®µ | Descending phase
            remaining_ratio = (step_ratio - self.pct_start) / (1 - self.pct_start)
            
            if self.anneal_strategy == 'cos':
                lr_ratio = (1 + math.cos(math.pi * remaining_ratio)) / 2
            else:  # linear
                lr_ratio = 1 - remaining_ratio
            
            return [base_lr + (self.max_lr - base_lr) * lr_ratio
                    for base_lr in self.base_lrs]

def learning_rate_scheduling_demo():
    """
    å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥æ¼”ç¤º
    Learning rate scheduling demo
    """
    # åˆ›å»ºç®€å•æ¨¡å‹å’Œä¼˜åŒ–å™¨ | Create simple model and optimizer
    model = nn.Linear(10, 1)
    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)
    
    # æµ‹è¯•ä¸åŒçš„è°ƒåº¦å™¨ | Test different schedulers
    schedulers = {
        'StepLR': StepLR(optimizer, step_size=30, gamma=0.1),
        'ExponentialLR': ExponentialLR(optimizer, gamma=0.95),
        'CosineAnnealing': CosineAnnealingLR(optimizer, T_max=100),
        'WarmupCosine': WarmupCosineAnnealingLR(optimizer, warmup_epochs=10, T_max=100),
        'CyclicLR': CyclicLR(optimizer, base_lr=0.01, max_lr=0.1, step_size_up=20),
        'OneCycleLR': OneCycleLR(optimizer, max_lr=0.1, total_steps=100)
    }
    
    # è®°å½•å­¦ä¹ ç‡å†å² | Record learning rate history
    lr_histories = {name: [] for name in schedulers.keys()}
    
    # é‡ç½®ä¼˜åŒ–å™¨å‡½æ•° | Reset optimizer function
    def reset_optimizer():
        for group in optimizer.param_groups:
            group['lr'] = 0.1
    
    # ä¸ºæ¯ä¸ªè°ƒåº¦å™¨è®°å½•å­¦ä¹ ç‡å˜åŒ– | Record learning rate changes for each scheduler
    epochs = 100
    for scheduler_name, scheduler in schedulers.items():
        reset_optimizer()
        scheduler.__init__(optimizer, **scheduler.__dict__)
        
        for epoch in range(epochs):
            lr_histories[scheduler_name].append(optimizer.param_groups[0]['lr'])
            scheduler.step()
    
    # ç»˜åˆ¶å­¦ä¹ ç‡æ›²çº¿ | Plot learning rate curves
    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    axes = axes.flatten()
    
    for idx, (name, history) in enumerate(lr_histories.items()):
        ax = axes[idx]
        ax.plot(history, linewidth=2, color=f'C{idx}')
        ax.set_title(f'{name} Learning Rate Schedule', fontweight='bold')
        ax.set_xlabel('Epoch')
        ax.set_ylabel('Learning Rate')
        ax.grid(True, alpha=0.3)
        ax.set_yscale('log')
    
    plt.tight_layout()
    plt.show()
    
    # æ¯”è¾ƒä¸åŒè°ƒåº¦å™¨çš„ç‰¹ç‚¹ | Compare characteristics of different schedulers
    print("=== å­¦ä¹ ç‡è°ƒåº¦å™¨ç‰¹ç‚¹å¯¹æ¯” | Learning Rate Scheduler Characteristics ===")
    for name, history in lr_histories.items():
        initial_lr = history[0]
        final_lr = history[-1]
        max_lr = max(history)
        min_lr = min(history)
        
        print(f"\n{name}:")
        print(f"  åˆå§‹å­¦ä¹ ç‡: {initial_lr:.6f} | Initial LR: {initial_lr:.6f}")
        print(f"  æœ€ç»ˆå­¦ä¹ ç‡: {final_lr:.6f} | Final LR: {final_lr:.6f}")
        print(f"  æœ€å¤§å­¦ä¹ ç‡: {max_lr:.6f} | Max LR: {max_lr:.6f}")
        print(f"  æœ€å°å­¦ä¹ ç‡: {min_lr:.6f} | Min LR: {min_lr:.6f}")

if __name__ == "__main__":
    learning_rate_scheduling_demo()
```

#### æ¢¯åº¦è£å‰ªæŠ€æœ¯ | Gradient Clipping Techniques

**é¡¹ç›®æ ¸å¿ƒ | Project Core:**
æ¢¯åº¦è£å‰ªæ˜¯é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ã€ç¨³å®šè®­ç»ƒçš„é‡è¦æŠ€æœ¯ï¼Œç‰¹åˆ«åœ¨RNNå’ŒTransformerè®­ç»ƒä¸­å¿…ä¸å¯å°‘ã€‚

Gradient clipping is an essential technique for preventing gradient explosion and stabilizing training, especially crucial in RNN and Transformer training.

**æ¢¯åº¦è£å‰ªå®ç° | Gradient Clipping Implementation:**
```python
class GradientClipper:
    """
    æ¢¯åº¦è£å‰ªå·¥å…·ç±»
    Gradient clipping utility class
    """
    
    @staticmethod
    def clip_grad_norm(parameters, max_norm, norm_type=2.0, error_if_nonfinite=False):
        """
        æŒ‰èŒƒæ•°è£å‰ªæ¢¯åº¦
        Clip gradients by norm
        
        Args:
            parameters: æ¨¡å‹å‚æ•° | Model parameters
            max_norm: æœ€å¤§èŒƒæ•° | Maximum norm
            norm_type: èŒƒæ•°ç±»å‹ | Norm type
            error_if_nonfinite: é‡åˆ°éæœ‰é™å€¼æ˜¯å¦æŠ¥é”™ | Whether to error on non-finite values
        """
        if isinstance(parameters, torch.Tensor):
            parameters = [parameters]
        
        parameters = [p for p in parameters if p.grad is not None]
        
        if len(parameters) == 0:
            return torch.tensor(0.0)
        
        device = parameters[0].grad.device
        
        if norm_type == float('inf'):
            # æ— ç©·èŒƒæ•° | Infinity norm
            norms = [p.grad.detach().abs().max().to(device) for p in parameters]
            total_norm = norms[0] if len(norms) == 1 else torch.max(torch.stack(norms))
        else:
            # pèŒƒæ•° | p-norm
            total_norm = torch.norm(
                torch.stack([torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters]), 
                norm_type
            )
        
        if error_if_nonfinite and torch.logical_or(total_norm.isnan(), total_norm.isinf()):
            raise RuntimeError(f'æ¢¯åº¦èŒƒæ•°ä¸º {total_norm}ï¼Œè¿™æ˜¯éæœ‰é™å€¼')
        
        clip_coef = max_norm / (total_norm + 1e-6)
        
        # åªæœ‰å½“éœ€è¦è£å‰ªæ—¶æ‰è¿›è¡Œæ“ä½œ | Only clip when necessary
        if clip_coef < 1:
            for p in parameters:
                p.grad.detach().mul_(clip_coef.to(p.grad.device))
        
        return total_norm
    
    @staticmethod
    def clip_grad_value(parameters, clip_value):
        """
        æŒ‰å€¼è£å‰ªæ¢¯åº¦
        Clip gradients by value
        
        Args:
            parameters: æ¨¡å‹å‚æ•° | Model parameters
            clip_value: è£å‰ªå€¼ | Clipping value
        """
        if isinstance(parameters, torch.Tensor):
            parameters = [parameters]
        
        for p in parameters:
            if p.grad is not None:
                p.grad.data.clamp_(-clip_value, clip_value)

class AdaptiveGradientClipper:
    """
    è‡ªé€‚åº”æ¢¯åº¦è£å‰ªå™¨
    Adaptive gradient clipper
    """
    def __init__(self, parameters, percentile=10, history_size=1000):
        """
        Args:
            parameters: æ¨¡å‹å‚æ•° | Model parameters
            percentile: ç™¾åˆ†ä½æ•°é˜ˆå€¼ | Percentile threshold
            history_size: å†å²è®°å½•å¤§å° | History size
        """
        self.parameters = list(parameters)
        self.percentile = percentile
        self.history_size = history_size
        self.grad_norms_history = []
    
    def step(self):
        """
        è‡ªé€‚åº”æ¢¯åº¦è£å‰ªæ­¥éª¤
        Adaptive gradient clipping step
        """
        # è®¡ç®—å½“å‰æ¢¯åº¦èŒƒæ•° | Compute current gradient norm
        total_norm = torch.norm(
            torch.stack([torch.norm(p.grad.detach(), 2.0) for p in self.parameters if p.grad is not None]), 
            2.0
        )
        
        # è®°å½•å†å² | Record history
        self.grad_norms_history.append(total_norm.item())
        if len(self.grad_norms_history) > self.history_size:
            self.grad_norms_history.pop(0)
        
        # è®¡ç®—è‡ªé€‚åº”é˜ˆå€¼ | Compute adaptive threshold
        if len(self.grad_norms_history) >= 10:
            threshold = np.percentile(self.grad_norms_history, 100 - self.percentile)
            
            # åº”ç”¨è£å‰ª | Apply clipping
            if total_norm > threshold:
                GradientClipper.clip_grad_norm(self.parameters, threshold)
                return True, total_norm.item(), threshold
        
        return False, total_norm.item(), None

def gradient_clipping_demo():
    """
    æ¢¯åº¦è£å‰ªæŠ€æœ¯æ¼”ç¤º
    Gradient clipping techniques demo
    """
    # åˆ›å»ºä¸€ä¸ªå®¹æ˜“äº§ç”Ÿæ¢¯åº¦çˆ†ç‚¸çš„æ¨¡å‹ | Create a model prone to gradient explosion
    class ProblematicRNN(nn.Module):
        def __init__(self, input_size=10, hidden_size=50, num_layers=3):
            super().__init__()
            self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)
            self.fc = nn.Linear(hidden_size, 1)
            
            # åˆå§‹åŒ–æƒé‡ä½¿å…¶å®¹æ˜“æ¢¯åº¦çˆ†ç‚¸ | Initialize weights to be prone to gradient explosion
            for name, param in self.named_parameters():
                if 'weight' in name:
                    nn.init.uniform_(param, -2, 2)  # è¾ƒå¤§çš„åˆå§‹æƒé‡
        
        def forward(self, x):
            out, _ = self.rnn(x)
            return self.fc(out[:, -1, :])
    
    # åˆ›å»ºæ•°æ® | Create data
    batch_size, seq_len, input_size = 32, 20, 10
    X = torch.randn(batch_size, seq_len, input_size)
    y = torch.randn(batch_size, 1)
    
    # æµ‹è¯•ä¸åŒçš„æ¢¯åº¦è£å‰ªç­–ç•¥ | Test different gradient clipping strategies
    models = {
        'No Clipping': ProblematicRNN(),
        'Norm Clipping': ProblematicRNN(),
        'Value Clipping': ProblematicRNN(),
        'Adaptive Clipping': ProblematicRNN()
    }
    
    # åˆå§‹åŒ–ç›¸åŒçš„æƒé‡ | Initialize same weights
    state_dict = models['No Clipping'].state_dict()
    for model in models.values():
        model.load_state_dict(state_dict)
    
    # åˆ›å»ºä¼˜åŒ–å™¨ | Create optimizers
    optimizers = {name: torch.optim.SGD(model.parameters(), lr=0.01) 
                  for name, model in models.items()}
    
    # åˆ›å»ºè‡ªé€‚åº”æ¢¯åº¦è£å‰ªå™¨ | Create adaptive gradient clipper
    adaptive_clipper = AdaptiveGradientClipper(models['Adaptive Clipping'].parameters())
    
    # è®­ç»ƒå†å² | Training history
    histories = {
        'loss': {name: [] for name in models.keys()},
        'grad_norm': {name: [] for name in models.keys()},
        'clipped': {name: [] for name in models.keys()}
    }
    
    # è®­ç»ƒå¾ªç¯ | Training loop
    criterion = nn.MSELoss()
    num_epochs = 100
    
    for epoch in range(num_epochs):
        for name, model in models.items():
            optimizer = optimizers[name]
            
            # å‰å‘ä¼ æ’­ | Forward pass
            optimizer.zero_grad()
            output = model(X)
            loss = criterion(output, y)
            
            # åå‘ä¼ æ’­ | Backward pass
            loss.backward()
            
            # è®¡ç®—æ¢¯åº¦èŒƒæ•° | Compute gradient norm
            total_norm = torch.norm(
                torch.stack([torch.norm(p.grad.detach(), 2.0) for p in model.parameters() if p.grad is not None]), 
                2.0
            ).item()
            
            # åº”ç”¨ä¸åŒçš„æ¢¯åº¦è£å‰ªç­–ç•¥ | Apply different gradient clipping strategies
            clipped = False
            if name == 'Norm Clipping':
                if total_norm > 1.0:
                    GradientClipper.clip_grad_norm(model.parameters(), max_norm=1.0)
                    clipped = True
            elif name == 'Value Clipping':
                if total_norm > 1.0:
                    GradientClipper.clip_grad_value(model.parameters(), clip_value=0.5)
                    clipped = True
            elif name == 'Adaptive Clipping':
                clipped, _, _ = adaptive_clipper.step()
            
            # æ›´æ–°å‚æ•° | Update parameters
            optimizer.step()
            
            # è®°å½•å†å² | Record history
            histories['loss'][name].append(loss.item())
            histories['grad_norm'][name].append(total_norm)
            histories['clipped'][name].append(clipped)
    
    # ç»˜åˆ¶ç»“æœ | Plot results
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # æŸå¤±æ›²çº¿ | Loss curves
    ax1 = axes[0, 0]
    for name, losses in histories['loss'].items():
        ax1.plot(losses, label=name, linewidth=2)
    ax1.set_title('Training Loss')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.legend()
    ax1.set_yscale('log')
    ax1.grid(True, alpha=0.3)
    
    # æ¢¯åº¦èŒƒæ•° | Gradient norms
    ax2 = axes[0, 1]
    for name, norms in histories['grad_norm'].items():
        ax2.plot(norms, label=name, linewidth=2)
    ax2.set_title('Gradient Norm')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Gradient Norm')
    ax2.legend()
    ax2.set_yscale('log')
    ax2.grid(True, alpha=0.3)
    
    # è£å‰ªé¢‘ç‡ | Clipping frequency
    ax3 = axes[1, 0]
    clipping_rates = {}
    for name, clipped_list in histories['clipped'].items():
        clipping_rate = sum(clipped_list) / len(clipped_list) * 100
        clipping_rates[name] = clipping_rate
    
    names = list(clipping_rates.keys())
    rates = list(clipping_rates.values())
    ax3.bar(names, rates, alpha=0.7)
    ax3.set_title('Gradient Clipping Frequency (%)')
    ax3.set_ylabel('Clipping Rate (%)')
    ax3.tick_params(axis='x', rotation=45)
    ax3.grid(True, alpha=0.3)
    
    # æœ€ç»ˆæ€§èƒ½å¯¹æ¯” | Final performance comparison
    ax4 = axes[1, 1]
    final_losses = {name: losses[-10:] for name, losses in histories['loss'].items()}
    avg_final_losses = {name: np.mean(losses) for name, losses in final_losses.items()}
    
    names = list(avg_final_losses.keys())
    losses = list(avg_final_losses.values())
    bars = ax4.bar(names, losses, alpha=0.7)
    ax4.set_title('Average Final Loss (Last 10 Epochs)')
    ax4.set_ylabel('Average Loss')
    ax4.tick_params(axis='x', rotation=45)
    ax4.grid(True, alpha=0.3)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾ | Add value labels
    for bar, loss in zip(bars, losses):
        height = bar.get_height()
        ax4.text(bar.get_x() + bar.get_width()/2., height,
                f'{loss:.4f}', ha='center', va='bottom')
    
    plt.tight_layout()
    plt.show()
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯ | Print statistics
    print("=== æ¢¯åº¦è£å‰ªæ•ˆæœç»Ÿè®¡ | Gradient Clipping Effects Statistics ===")
    for name in models.keys():
        final_loss = np.mean(histories['loss'][name][-10:])
        max_grad_norm = max(histories['grad_norm'][name])
        avg_grad_norm = np.mean(histories['grad_norm'][name])
        clipping_rate = sum(histories['clipped'][name]) / len(histories['clipped'][name]) * 100
        
        print(f"\n{name}:")
        print(f"  æœ€ç»ˆæŸå¤±: {final_loss:.6f} | Final Loss: {final_loss:.6f}")
        print(f"  æœ€å¤§æ¢¯åº¦èŒƒæ•°: {max_grad_norm:.6f} | Max Grad Norm: {max_grad_norm:.6f}")
        print(f"  å¹³å‡æ¢¯åº¦èŒƒæ•°: {avg_grad_norm:.6f} | Avg Grad Norm: {avg_grad_norm:.6f}")
        print(f"  è£å‰ªé¢‘ç‡: {clipping_rate:.2f}% | Clipping Rate: {clipping_rate:.2f}%")

if __name__ == "__main__":
    gradient_clipping_demo()
```

### 03_åˆ†å¸ƒå¼è®­ç»ƒ | Distributed Training

**æ‰©å±•åˆ°å¤§è§„æ¨¡è®­ç»ƒï¼**
**Scale to large-scale training!**

#### æ•°æ®å¹¶è¡Œè®­ç»ƒ | Data Parallel Training

**é¡¹ç›®çªç ´ | Project Breakthrough:**
æ•°æ®å¹¶è¡Œæ˜¯æœ€å¸¸ç”¨çš„åˆ†å¸ƒå¼è®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡åœ¨å¤šä¸ªè®¾å¤‡ä¸Šå¤åˆ¶æ¨¡å‹ï¼Œå¹¶è¡Œå¤„ç†ä¸åŒçš„æ•°æ®æ‰¹æ¬¡ã€‚

Data parallelism is the most commonly used distributed training method, replicating the model across multiple devices and processing different data batches in parallel.

**æ•°æ®å¹¶è¡Œå®ç° | Data Parallel Implementation:**
```python
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel as DDP
import os

class DistributedTrainer:
    """
    åˆ†å¸ƒå¼è®­ç»ƒç®¡ç†å™¨
    Distributed training manager
    """
    def __init__(self, model, train_dataset, val_dataset=None, 
                 batch_size=32, num_epochs=10, lr=0.001):
        self.model = model
        self.train_dataset = train_dataset
        self.val_dataset = val_dataset
        self.batch_size = batch_size
        self.num_epochs = num_epochs
        self.lr = lr
        
        # åˆ†å¸ƒå¼è®¾ç½® | Distributed setup
        self.world_size = None
        self.rank = None
        self.local_rank = None
        self.device = None
    
    def setup_distributed(self, rank, world_size):
        """
        è®¾ç½®åˆ†å¸ƒå¼ç¯å¢ƒ
        Setup distributed environment
        """
        os.environ['MASTER_ADDR'] = 'localhost'
        os.environ['MASTER_PORT'] = '12355'
        
        # åˆå§‹åŒ–è¿›ç¨‹ç»„ | Initialize process group
        dist.init_process_group("nccl", rank=rank, world_size=world_size)
        
        self.rank = rank
        self.world_size = world_size
        self.local_rank = rank
        self.device = torch.device(f'cuda:{rank}')
        
        # è®¾ç½®å½“å‰è®¾å¤‡ | Set current device
        torch.cuda.set_device(self.device)
        
        print(f"è¿›ç¨‹ {rank}/{world_size} åœ¨è®¾å¤‡ {self.device} ä¸Šåˆå§‹åŒ–å®Œæˆ")
        print(f"Process {rank}/{world_size} initialized on device {self.device}")
    
    def cleanup_distributed(self):
        """æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ | Cleanup distributed environment"""
        dist.destroy_process_group()
    
    def create_data_loaders(self):
        """
        åˆ›å»ºåˆ†å¸ƒå¼æ•°æ®åŠ è½½å™¨
        Create distributed data loaders
        """
        # åˆ†å¸ƒå¼é‡‡æ ·å™¨ | Distributed sampler
        train_sampler = torch.utils.data.distributed.DistributedSampler(
            self.train_dataset,
            num_replicas=self.world_size,
            rank=self.rank,
            shuffle=True
        )
        
        train_loader = torch.utils.data.DataLoader(
            self.train_dataset,
            batch_size=self.batch_size,
            sampler=train_sampler,
            num_workers=2,
            pin_memory=True
        )
        
        val_loader = None
        if self.val_dataset:
            val_sampler = torch.utils.data.distributed.DistributedSampler(
                self.val_dataset,
                num_replicas=self.world_size,
                rank=self.rank,
                shuffle=False
            )
            
            val_loader = torch.utils.data.DataLoader(
                self.val_dataset,
                batch_size=self.batch_size,
                sampler=val_sampler,
                num_workers=2,
                pin_memory=True
            )
        
        return train_loader, val_loader, train_sampler
    
    def train_distributed(self, rank, world_size):
        """
        åˆ†å¸ƒå¼è®­ç»ƒä¸»å‡½æ•°
        Main distributed training function
        """
        # è®¾ç½®åˆ†å¸ƒå¼ç¯å¢ƒ | Setup distributed environment
        self.setup_distributed(rank, world_size)
        
        # ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡ | Move model to device
        self.model = self.model.to(self.device)
        
        # åŒ…è£…ä¸ºåˆ†å¸ƒå¼æ¨¡å‹ | Wrap as distributed model
        ddp_model = DDP(self.model, device_ids=[self.local_rank])
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨ | Create data loaders
        train_loader, val_loader, train_sampler = self.create_data_loaders()
        
        # åˆ›å»ºä¼˜åŒ–å™¨ | Create optimizer
        optimizer = AdamOptimizer(ddp_model.parameters(), lr=self.lr)
        criterion = nn.CrossEntropyLoss()
        
        # è®­ç»ƒå¾ªç¯ | Training loop
        for epoch in range(self.num_epochs):
            # è®¾ç½®epochç”¨äºæ­£ç¡®çš„æ•°æ®shuffling | Set epoch for proper data shuffling
            train_sampler.set_epoch(epoch)
            
            # è®­ç»ƒä¸€ä¸ªepoch | Train one epoch
            train_loss = self._train_epoch(ddp_model, train_loader, optimizer, criterion)
            
            # éªŒè¯ | Validation
            val_loss = None
            if val_loader:
                val_loss = self._validate_epoch(ddp_model, val_loader, criterion)
            
            # åªåœ¨ä¸»è¿›ç¨‹æ‰“å° | Only print on main process
            if self.rank == 0:
                print(f"Epoch {epoch+1}/{self.num_epochs}")
                print(f"  è®­ç»ƒæŸå¤±: {train_loss:.4f} | Train Loss: {train_loss:.4f}")
                if val_loss:
                    print(f"  éªŒè¯æŸå¤±: {val_loss:.4f} | Val Loss: {val_loss:.4f}")
        
        # æ¸…ç† | Cleanup
        self.cleanup_distributed()
    
    def _train_epoch(self, model, data_loader, optimizer, criterion):
        """
        è®­ç»ƒä¸€ä¸ªepoch
        Train one epoch
        """
        model.train()
        total_loss = 0
        num_batches = 0
        
        for batch_idx, (data, target) in enumerate(data_loader):
            data, target = data.to(self.device), target.to(self.device)
            
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            num_batches += 1
        
        # è®¡ç®—å¹³å‡æŸå¤±å¹¶åŒæ­¥æ‰€æœ‰è¿›ç¨‹ | Compute average loss and sync across all processes
        avg_loss = total_loss / num_batches
        loss_tensor = torch.tensor(avg_loss, device=self.device)
        dist.all_reduce(loss_tensor, op=dist.ReduceOp.SUM)
        avg_loss = loss_tensor.item() / self.world_size
        
        return avg_loss
    
    def _validate_epoch(self, model, data_loader, criterion):
        """
        éªŒè¯ä¸€ä¸ªepoch
        Validate one epoch
        """
        model.eval()
        total_loss = 0
        num_batches = 0
        
        with torch.no_grad():
            for data, target in data_loader:
                data, target = data.to(self.device), target.to(self.device)
                output = model(data)
                loss = criterion(output, target)
                
                total_loss += loss.item()
                num_batches += 1
        
        # åŒæ­¥éªŒè¯æŸå¤± | Sync validation loss
        avg_loss = total_loss / num_batches
        loss_tensor = torch.tensor(avg_loss, device=self.device)
        dist.all_reduce(loss_tensor, op=dist.ReduceOp.SUM)
        avg_loss = loss_tensor.item() / self.world_size
        
        return avg_loss

def distributed_training_demo():
    """
    åˆ†å¸ƒå¼è®­ç»ƒæ¼”ç¤º
    Distributed training demo
    """
    # æ£€æŸ¥å¯ç”¨GPUæ•°é‡ | Check available GPU count
    if not torch.cuda.is_available():
        print("CUDAä¸å¯ç”¨ï¼Œè·³è¿‡åˆ†å¸ƒå¼è®­ç»ƒæ¼”ç¤º")
        print("CUDA not available, skipping distributed training demo")
        return
    
    world_size = torch.cuda.device_count()
    if world_size < 2:
        print(f"åªæœ‰ {world_size} ä¸ªGPUï¼Œéœ€è¦è‡³å°‘2ä¸ªGPUè¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒæ¼”ç¤º")
        print(f"Only {world_size} GPU available, need at least 2 GPUs for distributed training demo")
        return
    
    # åˆ›å»ºç¤ºä¾‹æ¨¡å‹å’Œæ•°æ® | Create example model and data
    model = nn.Sequential(
        nn.Linear(784, 256),
        nn.ReLU(),
        nn.Linear(256, 128),
        nn.ReLU(),
        nn.Linear(128, 10)
    )
    
    # åˆ›å»ºè™šæ‹Ÿæ•°æ®é›† | Create dummy dataset
    class DummyDataset(torch.utils.data.Dataset):
        def __init__(self, size=1000):
            self.size = size
            self.data = torch.randn(size, 784)
            self.targets = torch.randint(0, 10, (size,))
        
        def __len__(self):
            return self.size
        
        def __getitem__(self, idx):
            return self.data[idx], self.targets[idx]
    
    train_dataset = DummyDataset(1000)
    val_dataset = DummyDataset(200)
    
    # åˆ›å»ºåˆ†å¸ƒå¼è®­ç»ƒå™¨ | Create distributed trainer
    trainer = DistributedTrainer(
        model=model,
        train_dataset=train_dataset,
        val_dataset=val_dataset,
        batch_size=32,
        num_epochs=5,
        lr=0.001
    )
    
    print(f"å¼€å§‹åœ¨ {world_size} ä¸ªGPUä¸Šè¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒ...")
    print(f"Starting distributed training on {world_size} GPUs...")
    
    # å¯åŠ¨å¤šè¿›ç¨‹è®­ç»ƒ | Launch multi-process training
    mp.spawn(
        trainer.train_distributed,
        args=(world_size,),
        nprocs=world_size,
        join=True
    )
    
    print("åˆ†å¸ƒå¼è®­ç»ƒå®Œæˆï¼")
    print("Distributed training completed!")

if __name__ == "__main__":
    distributed_training_demo()
```

---

**ğŸ¯ é¡¹ç›®å®Œæˆæ£€æŸ¥æ¸…å• | Project Completion Checklist:**

### ä¼˜åŒ–ç®—æ³•ç†è§£ | Optimization Algorithm Understanding
- [ ] æ·±å…¥ç†è§£SGDã€Adamç­‰ç»å…¸ä¼˜åŒ–å™¨çš„æ•°å­¦åŸç†
- [ ] æŒæ¡ä¸åŒä¼˜åŒ–å™¨çš„é€‚ç”¨åœºæ™¯å’Œè°ƒå‚æŠ€å·§
- [ ] ç†è§£å­¦ä¹ ç‡è°ƒåº¦å’Œæ¢¯åº¦è£å‰ªçš„é‡è¦æ€§
- [ ] èƒ½å¤Ÿæ ¹æ®ä»»åŠ¡ç‰¹ç‚¹é€‰æ‹©åˆé€‚çš„ä¼˜åŒ–ç­–ç•¥

### é«˜çº§æŠ€æœ¯å®ç° | Advanced Technique Implementation
- [ ] ä»é›¶å®ç°ä¸»æµä¼˜åŒ–ç®—æ³•å’Œå˜ç§
- [ ] æŒæ¡å¤šç§å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥çš„è®¾è®¡å’Œå®ç°
- [ ] å®ç°æœ‰æ•ˆçš„æ¢¯åº¦è£å‰ªå’Œç¨³å®šæ€§æ§åˆ¶
- [ ] ç†è§£å¹¶å®ç°åˆ†å¸ƒå¼è®­ç»ƒçš„æ ¸å¿ƒæŠ€æœ¯

### å·¥ç¨‹å®è·µèƒ½åŠ› | Engineering Practice Capability
- [ ] èƒ½å¤Ÿè¯Šæ–­å’Œè§£å†³è®­ç»ƒä¸­çš„ä¼˜åŒ–é—®é¢˜
- [ ] æŒæ¡å¤§è§„æ¨¡æ¨¡å‹çš„åˆ†å¸ƒå¼è®­ç»ƒæ–¹æ³•
- [ ] å…·å¤‡ä¼˜åŒ–ç­–ç•¥çš„æ€§èƒ½åˆ†æå’Œè°ƒä¼˜èƒ½åŠ›
- [ ] ç†è§£ä¸åŒä¼˜åŒ–æŠ€æœ¯çš„è®¡ç®—å’Œå†…å­˜å¼€é”€

**è®°ä½**: ä¼˜åŒ–ç®—æ³•æ˜¯æ·±åº¦å­¦ä¹ çš„å¿ƒè„ï¼Œå†³å®šäº†æ¨¡å‹è®­ç»ƒçš„æ•ˆç‡å’Œè´¨é‡ã€‚é€šè¿‡è¿™ä¸ªé¡¹ç›®ï¼Œä½ å°†æŒæ¡è®©AIè®­ç»ƒæ›´å¿«ã€æ›´ç¨³å®šã€æ›´é«˜æ•ˆçš„æ ¸å¿ƒæŠ€æœ¯ï¼

**Remember**: Optimization algorithms are the heart of deep learning, determining the efficiency and quality of model training. Through this project, you will master the core technologies to make AI training faster, more stable, and more efficient! 