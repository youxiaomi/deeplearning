# 优化算法实践项目概述
# Optimization Algorithms Practice Project Overview

**让训练更快更稳定 - 掌握深度学习的核心引擎**
**Make Training Faster and More Stable - Master the Core Engine of Deep Learning**

---

## 🎯 项目目标 | Project Goals

优化算法是深度学习的心脏！一个好的优化器能让模型收敛更快、性能更好、训练更稳定。通过这个项目，你将掌握：
Optimization algorithms are the heart of deep learning! A good optimizer can make models converge faster, perform better, and train more stably. Through this project, you will master:

- **经典优化算法** | **Classic Optimization Algorithms**: SGD、Adam等优化器的原理与实现
- **高级优化技术** | **Advanced Optimization Techniques**: 学习率调度、梯度裁剪等关键技术
- **分布式训练** | **Distributed Training**: 大规模模型训练的并行化策略
- **优化策略设计** | **Optimization Strategy Design**: 针对不同任务的优化方案设计

## 🔬 为什么优化算法如此重要？| Why are Optimization Algorithms So Important?

**优化算法决定了AI训练的效率和质量！**
**Optimization algorithms determine the efficiency and quality of AI training!**

从ImageNet分类训练时间从几周缩短到几小时，从GPT-3需要数千张GPU到GPT-4的高效训练，从ResNet的梯度消失问题到Transformer的稳定训练，优化算法的进步推动了整个深度学习的发展。掌握优化算法，就掌握了让AI更快更好地学习的钥匙。

From ImageNet classification training time reduced from weeks to hours, from GPT-3 requiring thousands of GPUs to GPT-4's efficient training, from ResNet's gradient vanishing problems to Transformer's stable training, advances in optimization algorithms have driven the development of deep learning. Mastering optimization algorithms means mastering the key to making AI learn faster and better.

### 优化算法的发展历程 | Evolution of Optimization Algorithms
```
1951: 随机梯度下降 | Stochastic Gradient Descent
1964: 动量方法 | Momentum Methods
1987: AdaGrad自适应学习率 | AdaGrad Adaptive Learning Rate
2012: RMSprop均方根传播 | RMSprop Root Mean Square Propagation
2014: Adam自适应矩估计 | Adam Adaptive Moment Estimation
2019: AdamW权重衰减修正 | AdamW Weight Decay Correction
2020: 大规模分布式优化 | Large-scale Distributed Optimization
```

## 📚 项目结构深度解析 | Deep Project Structure Analysis

### 01_经典优化算法 | Classic Optimization Algorithms

**理解优化的数学本质！**
**Understand the mathematical essence of optimization!**

#### SGD随机梯度下降 | SGD Stochastic Gradient Descent

**项目核心 | Project Core:**
SGD是深度学习的基石优化算法，理解SGD的原理和变种是掌握所有优化器的前提。

SGD is the foundational optimization algorithm in deep learning. Understanding SGD's principles and variants is the prerequisite for mastering all optimizers.

**数学原理 | Mathematical Principles:**

**标准SGD更新规则 | Standard SGD Update Rule:**
```
θ_{t+1} = θ_t - η ∇L(θ_t)
```

**带动量的SGD | SGD with Momentum:**
```
v_t = β v_{t-1} + η ∇L(θ_t)
θ_{t+1} = θ_t - v_t
```

**Nesterov加速梯度 | Nesterov Accelerated Gradient:**
```
v_t = β v_{t-1} + η ∇L(θ_t - β v_{t-1})
θ_{t+1} = θ_t - v_t
```

**完整实现 | Complete Implementation:**
```python
import numpy as np
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
from typing import Dict, List, Optional, Tuple
import math

class SGDOptimizer:
    """
    SGD优化器的完整实现
    Complete implementation of SGD optimizer
    """
    def __init__(self, parameters, lr=0.01, momentum=0.0, dampening=0.0, 
                 weight_decay=0.0, nesterov=False):
        """
        Args:
            parameters: 模型参数 | Model parameters
            lr: 学习率 | Learning rate
            momentum: 动量因子 | Momentum factor
            dampening: 动量衰减 | Momentum dampening
            weight_decay: 权重衰减 | Weight decay
            nesterov: 是否使用Nesterov动量 | Whether to use Nesterov momentum
        """
        self.param_groups = [{'params': list(parameters)}]
        self.defaults = {
            'lr': lr,
            'momentum': momentum,
            'dampening': dampening,
            'weight_decay': weight_decay,
            'nesterov': nesterov
        }
        
        # 为每个参数组设置默认值 | Set defaults for each parameter group
        for group in self.param_groups:
            group.setdefault('lr', lr)
            group.setdefault('momentum', momentum)
            group.setdefault('dampening', dampening)
            group.setdefault('weight_decay', weight_decay)
            group.setdefault('nesterov', nesterov)
        
        # 状态字典 | State dictionary
        self.state = {}
        
        # 优化历史 | Optimization history
        self.loss_history = []
        self.lr_history = []
        
    def zero_grad(self):
        """清零梯度 | Zero gradients"""
        for group in self.param_groups:
            for p in group['params']:
                if p.grad is not None:
                    p.grad.zero_()
    
    def step(self, closure=None):
        """
        执行一步优化
        Perform one optimization step
        """
        loss = None
        if closure is not None:
            loss = closure()
        
        for group in self.param_groups:
            weight_decay = group['weight_decay']
            momentum = group['momentum']
            dampening = group['dampening']
            nesterov = group['nesterov']
            lr = group['lr']
            
            for p in group['params']:
                if p.grad is None:
                    continue
                
                # 获取梯度 | Get gradient
                grad = p.grad.data
                
                # 权重衰减 | Weight decay
                if weight_decay != 0:
                    grad = grad.add(p.data, alpha=weight_decay)
                
                # 参数状态 | Parameter state
                param_state = self.state.setdefault(id(p), {})
                
                # 应用动量 | Apply momentum
                if momentum != 0:
                    if 'momentum_buffer' not in param_state:
                        buf = param_state['momentum_buffer'] = torch.zeros_like(p.data)
                        buf.mul_(momentum).add_(grad)
                    else:
                        buf = param_state['momentum_buffer']
                        buf.mul_(momentum).add_(grad, alpha=1 - dampening)
                    
                    if nesterov:
                        # Nesterov动量 | Nesterov momentum
                        grad = grad.add(buf, alpha=momentum)
                    else:
                        # 标准动量 | Standard momentum
                        grad = buf
                
                # 更新参数 | Update parameters
                p.data.add_(grad, alpha=-lr)
        
        return loss
    
    def state_dict(self):
        """返回优化器状态 | Return optimizer state"""
        return {
            'state': self.state,
            'param_groups': self.param_groups
        }
    
    def load_state_dict(self, state_dict):
        """加载优化器状态 | Load optimizer state"""
        self.state = state_dict['state']
        self.param_groups = state_dict['param_groups']

class AdaptiveLearningRate:
    """
    自适应学习率调度器
    Adaptive Learning Rate Scheduler
    """
    def __init__(self, optimizer, mode='min', factor=0.1, patience=10, 
                 threshold=1e-4, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-8):
        self.optimizer = optimizer
        self.mode = mode
        self.factor = factor
        self.patience = patience
        self.threshold = threshold
        self.threshold_mode = threshold_mode
        self.cooldown = cooldown
        self.min_lr = min_lr
        self.eps = eps
        
        self.best = None
        self.num_bad_epochs = 0
        self.mode_worse = None
        self.cooldown_counter = 0
        self.last_epoch = 0
        
        self._init_is_better()
        
    def _init_is_better(self):
        """初始化比较函数 | Initialize comparison function"""
        if self.mode == 'min':
            self.mode_worse = float('inf')
        else:
            self.mode_worse = -float('inf')
    
    def _is_better(self, current, best):
        """判断当前值是否更好 | Check if current value is better"""
        if self.mode == 'min' and self.threshold_mode == 'rel':
            return current < best - best * self.threshold
        elif self.mode == 'min' and self.threshold_mode == 'abs':
            return current < best - self.threshold
        elif self.mode == 'max' and self.threshold_mode == 'rel':
            return current > best + best * self.threshold
        else:  # mode == 'max' and threshold_mode == 'abs'
            return current > best + self.threshold
    
    def step(self, metrics):
        """
        根据指标调整学习率
        Adjust learning rate based on metrics
        """
        current = metrics
        self.last_epoch += 1
        
        if self.best is None:
            self.best = current
        
        if self.cooldown_counter > 0:
            self.cooldown_counter -= 1
            self.num_bad_epochs = 0
        
        if self._is_better(current, self.best):
            self.best = current
            self.num_bad_epochs = 0
        else:
            self.num_bad_epochs += 1
        
        if self.cooldown_counter == 0 and self.num_bad_epochs > self.patience:
            self._reduce_lr()
            self.cooldown_counter = self.cooldown
            self.num_bad_epochs = 0
    
    def _reduce_lr(self):
        """减少学习率 | Reduce learning rate"""
        for group in self.optimizer.param_groups:
            old_lr = group['lr']
            new_lr = max(old_lr * self.factor, self.min_lr)
            if old_lr - new_lr > self.eps:
                group['lr'] = new_lr
                print(f'学习率从 {old_lr:.6f} 降至 {new_lr:.6f}')
                print(f'Learning rate reduced from {old_lr:.6f} to {new_lr:.6f}')

def sgd_optimization_demo():
    """
    SGD优化算法演示
    SGD optimization algorithm demo
    """
    # 创建一个简单的二次函数优化问题 | Create a simple quadratic optimization problem
    def rosenbrock_function(x, y):
        """Rosenbrock函数 - 经典优化测试函数"""
        return (1 - x)**2 + 100 * (y - x**2)**2
    
    # 初始化参数 | Initialize parameters
    x = torch.tensor([-1.0], requires_grad=True)
    y = torch.tensor([1.0], requires_grad=True)
    
    # 测试不同的SGD变种 | Test different SGD variants
    optimizers = {
        'SGD': SGDOptimizer([x, y], lr=0.001),
        'SGD+Momentum': SGDOptimizer([x, y], lr=0.001, momentum=0.9),
        'SGD+Nesterov': SGDOptimizer([x, y], lr=0.001, momentum=0.9, nesterov=True)
    }
    
    # 重置参数函数 | Reset parameters function
    def reset_params():
        x.data.fill_(-1.0)
        y.data.fill_(1.0)
        if x.grad is not None:
            x.grad.zero_()
        if y.grad is not None:
            y.grad.zero_()
    
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    
    for idx, (opt_name, optimizer) in enumerate(optimizers.items()):
        reset_params()
        
        # 优化历史 | Optimization history
        x_history = []
        y_history = []
        loss_history = []
        
        # 优化循环 | Optimization loop
        for iteration in range(1000):
            optimizer.zero_grad()
            
            # 计算损失 | Compute loss
            loss = rosenbrock_function(x, y)
            
            # 反向传播 | Backpropagation
            loss.backward()
            
            # 记录历史 | Record history
            x_history.append(x.item())
            y_history.append(y.item())
            loss_history.append(loss.item())
            
            # 优化步骤 | Optimization step
            optimizer.step()
            
            # 早停条件 | Early stopping condition
            if loss.item() < 1e-6:
                break
        
        # 绘制优化路径 | Plot optimization path
        ax = axes[idx]
        
        # 绘制Rosenbrock函数的等高线 | Plot Rosenbrock function contours
        x_range = np.linspace(-2, 2, 100)
        y_range = np.linspace(-1, 3, 100)
        X, Y = np.meshgrid(x_range, y_range)
        Z = (1 - X)**2 + 100 * (Y - X**2)**2
        
        contour = ax.contour(X, Y, Z, levels=np.logspace(-1, 3, 20), cmap='viridis', alpha=0.6)
        ax.clabel(contour, inline=True, fontsize=8)
        
        # 绘制优化路径 | Plot optimization path
        ax.plot(x_history, y_history, 'r-', linewidth=2, alpha=0.8, label='Optimization Path')
        ax.plot(x_history[0], y_history[0], 'go', markersize=8, label='Start')
        ax.plot(x_history[-1], y_history[-1], 'ro', markersize=8, label='End')
        ax.plot(1, 1, 'b*', markersize=15, label='Global Minimum')
        
        ax.set_title(f'{opt_name} Optimization\nFinal Loss: {loss_history[-1]:.6f}')
        ax.set_xlabel('X')
        ax.set_ylabel('Y')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        print(f"{opt_name}: 收敛到 ({x.item():.4f}, {y.item():.4f}), 损失: {loss_history[-1]:.6f}")
        print(f"{opt_name}: Converged to ({x.item():.4f}, {y.item():.4f}), Loss: {loss_history[-1]:.6f}")
    
    plt.tight_layout()
    plt.show()
    
    return optimizers

if __name__ == "__main__":
    sgd_optimization_demo()
```

#### Adam自适应优化 | Adam Adaptive Optimization

**项目特色 | Project Features:**
Adam结合了动量和自适应学习率的优势，是现代深度学习中最广泛使用的优化器。

Adam combines the advantages of momentum and adaptive learning rates, being the most widely used optimizer in modern deep learning.

**Adam算法原理 | Adam Algorithm Principles:**

**Adam更新规则 | Adam Update Rules:**
```python
# 一阶矩估计 | First moment estimate
m_t = β₁ m_{t-1} + (1 - β₁) g_t

# 二阶矩估计 | Second moment estimate  
v_t = β₂ v_{t-1} + (1 - β₂) g_t²

# 偏差修正 | Bias correction
m̂_t = m_t / (1 - β₁^t)
v̂_t = v_t / (1 - β₂^t)

# 参数更新 | Parameter update
θ_{t+1} = θ_t - α m̂_t / (√v̂_t + ε)
```

**完整Adam实现 | Complete Adam Implementation:**
```python
class AdamOptimizer:
    """
    Adam优化器的完整实现
    Complete implementation of Adam optimizer
    """
    def __init__(self, parameters, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, 
                 weight_decay=0, amsgrad=False):
        """
        Args:
            parameters: 模型参数 | Model parameters
            lr: 学习率 | Learning rate
            betas: (β₁, β₂) 指数衰减率 | Exponential decay rates
            eps: 数值稳定性常数 | Numerical stability constant
            weight_decay: 权重衰减 | Weight decay
            amsgrad: 是否使用AMSGrad变种 | Whether to use AMSGrad variant
        """
        self.param_groups = [{'params': list(parameters)}]
        self.defaults = {
            'lr': lr,
            'betas': betas,
            'eps': eps,
            'weight_decay': weight_decay,
            'amsgrad': amsgrad
        }
        
        # 为每个参数组设置默认值 | Set defaults for each parameter group
        for group in self.param_groups:
            group.setdefault('lr', lr)
            group.setdefault('betas', betas)
            group.setdefault('eps', eps)
            group.setdefault('weight_decay', weight_decay)
            group.setdefault('amsgrad', amsgrad)
        
        # 状态字典 | State dictionary
        self.state = {}
        
    def zero_grad(self):
        """清零梯度 | Zero gradients"""
        for group in self.param_groups:
            for p in group['params']:
                if p.grad is not None:
                    p.grad.zero_()
    
    def step(self, closure=None):
        """
        执行一步优化
        Perform one optimization step
        """
        loss = None
        if closure is not None:
            loss = closure()
        
        for group in self.param_groups:
            lr = group['lr']
            beta1, beta2 = group['betas']
            eps = group['eps']
            weight_decay = group['weight_decay']
            amsgrad = group['amsgrad']
            
            for p in group['params']:
                if p.grad is None:
                    continue
                
                # 获取梯度 | Get gradient
                grad = p.grad.data
                
                # 权重衰减 | Weight decay
                if weight_decay != 0:
                    grad = grad.add(p.data, alpha=weight_decay)
                
                # 参数状态 | Parameter state
                param_state = self.state.setdefault(id(p), {})
                
                # 初始化状态 | Initialize state
                if len(param_state) == 0:
                    param_state['step'] = 0
                    param_state['exp_avg'] = torch.zeros_like(p.data)  # m_t
                    param_state['exp_avg_sq'] = torch.zeros_like(p.data)  # v_t
                    if amsgrad:
                        param_state['max_exp_avg_sq'] = torch.zeros_like(p.data)
                
                exp_avg, exp_avg_sq = param_state['exp_avg'], param_state['exp_avg_sq']
                if amsgrad:
                    max_exp_avg_sq = param_state['max_exp_avg_sq']
                
                param_state['step'] += 1
                bias_correction1 = 1 - beta1 ** param_state['step']
                bias_correction2 = 1 - beta2 ** param_state['step']
                
                # 更新一阶和二阶矩估计 | Update first and second moment estimates
                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)
                
                if amsgrad:
                    # 维护二阶矩的最大值 | Maintain maximum of second moment
                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)
                    denom = (max_exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(eps)
                else:
                    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(eps)
                
                # 计算步长 | Compute step size
                step_size = lr / bias_correction1
                
                # 更新参数 | Update parameters
                p.data.addcdiv_(exp_avg, denom, value=-step_size)
        
        return loss

class AdamWOptimizer(AdamOptimizer):
    """
    AdamW优化器：正确的权重衰减实现
    AdamW optimizer: Correct weight decay implementation
    """
    def step(self, closure=None):
        """
        AdamW的关键区别：权重衰减不通过梯度，而是直接作用于参数
        Key difference in AdamW: weight decay acts directly on parameters, not through gradients
        """
        loss = None
        if closure is not None:
            loss = closure()
        
        for group in self.param_groups:
            lr = group['lr']
            beta1, beta2 = group['betas']
            eps = group['eps']
            weight_decay = group['weight_decay']
            amsgrad = group['amsgrad']
            
            for p in group['params']:
                if p.grad is None:
                    continue
                
                # 获取梯度（不包含权重衰减）| Get gradient (without weight decay)
                grad = p.grad.data
                
                # 参数状态 | Parameter state
                param_state = self.state.setdefault(id(p), {})
                
                # 初始化状态 | Initialize state
                if len(param_state) == 0:
                    param_state['step'] = 0
                    param_state['exp_avg'] = torch.zeros_like(p.data)
                    param_state['exp_avg_sq'] = torch.zeros_like(p.data)
                    if amsgrad:
                        param_state['max_exp_avg_sq'] = torch.zeros_like(p.data)
                
                exp_avg, exp_avg_sq = param_state['exp_avg'], param_state['exp_avg_sq']
                if amsgrad:
                    max_exp_avg_sq = param_state['max_exp_avg_sq']
                
                param_state['step'] += 1
                bias_correction1 = 1 - beta1 ** param_state['step']
                bias_correction2 = 1 - beta2 ** param_state['step']
                
                # 更新一阶和二阶矩估计 | Update first and second moment estimates
                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)
                
                if amsgrad:
                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)
                    denom = (max_exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(eps)
                else:
                    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(eps)
                
                # 计算步长 | Compute step size
                step_size = lr / bias_correction1
                
                # AdamW：先应用权重衰减，再应用Adam更新 | AdamW: apply weight decay first, then Adam update
                if weight_decay != 0:
                    p.data.mul_(1 - lr * weight_decay)
                
                # 更新参数 | Update parameters
                p.data.addcdiv_(exp_avg, denom, value=-step_size)
        
        return loss

def adam_vs_sgd_comparison():
    """
    Adam vs SGD 对比实验
    Adam vs SGD comparison experiment
    """
    # 创建一个更复杂的优化问题 | Create a more complex optimization problem
    class ComplexLoss(nn.Module):
        def __init__(self):
            super().__init__()
            self.params = nn.Parameter(torch.randn(10, requires_grad=True))
        
        def forward(self):
            # 复杂的非凸损失函数 | Complex non-convex loss function
            x = self.params
            loss = torch.sum((x - torch.arange(10, dtype=torch.float32))**2)
            loss += 0.1 * torch.sum(torch.sin(10 * x)**2)
            loss += 0.01 * torch.sum(x**4)
            return loss
    
    # 创建模型实例 | Create model instances
    models = {
        'SGD': ComplexLoss(),
        'SGD+Momentum': ComplexLoss(),
        'Adam': ComplexLoss(),
        'AdamW': ComplexLoss()
    }
    
    # 初始化相同的参数 | Initialize same parameters
    init_params = torch.randn(10)
    for model in models.values():
        model.params.data.copy_(init_params)
    
    # 创建优化器 | Create optimizers
    optimizers = {
        'SGD': SGDOptimizer(models['SGD'].parameters(), lr=0.01),
        'SGD+Momentum': SGDOptimizer(models['SGD+Momentum'].parameters(), lr=0.01, momentum=0.9),
        'Adam': AdamOptimizer(models['Adam'].parameters(), lr=0.01),
        'AdamW': AdamWOptimizer(models['AdamW'].parameters(), lr=0.01, weight_decay=0.01)
    }
    
    # 训练历史 | Training history
    history = {name: [] for name in models.keys()}
    
    # 优化循环 | Optimization loop
    num_iterations = 1000
    for iteration in range(num_iterations):
        for name in models.keys():
            model = models[name]
            optimizer = optimizers[name]
            
            optimizer.zero_grad()
            loss = model()
            loss.backward()
            optimizer.step()
            
            history[name].append(loss.item())
    
    # 绘制结果 | Plot results
    plt.figure(figsize=(15, 5))
    
    # 损失曲线 | Loss curves
    plt.subplot(1, 3, 1)
    for name, losses in history.items():
        plt.plot(losses, label=name, linewidth=2)
    plt.xlabel('Iteration')
    plt.ylabel('Loss')
    plt.title('Optimization Comparison')
    plt.legend()
    plt.yscale('log')
    plt.grid(True, alpha=0.3)
    
    # 最终参数对比 | Final parameters comparison
    plt.subplot(1, 3, 2)
    target = torch.arange(10, dtype=torch.float32)
    x_pos = np.arange(10)
    width = 0.2
    
    for i, (name, model) in enumerate(models.items()):
        plt.bar(x_pos + i * width, model.params.data.numpy(), 
               width, label=name, alpha=0.7)
    
    plt.bar(x_pos + len(models) * width, target.numpy(), 
           width, label='Target', alpha=0.7, color='red')
    plt.xlabel('Parameter Index')
    plt.ylabel('Parameter Value')
    plt.title('Final Parameters')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # 收敛速度对比 | Convergence speed comparison
    plt.subplot(1, 3, 3)
    convergence_threshold = 1e-3
    for name, losses in history.items():
        convergence_step = next((i for i, loss in enumerate(losses) 
                               if loss < convergence_threshold), len(losses))
        plt.bar(name, convergence_step, alpha=0.7)
    plt.ylabel('Steps to Convergence')
    plt.title('Convergence Speed')
    plt.xticks(rotation=45)
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # 打印最终结果 | Print final results
    print("\n=== 优化结果对比 | Optimization Results Comparison ===")
    for name, model in models.items():
        final_loss = history[name][-1]
        param_error = torch.norm(model.params.data - target).item()
        print(f"{name}: 最终损失 {final_loss:.6f}, 参数误差 {param_error:.6f}")
        print(f"{name}: Final loss {final_loss:.6f}, Parameter error {param_error:.6f}")

if __name__ == "__main__":
    adam_vs_sgd_comparison()
```

### 02_高级优化技术 | Advanced Optimization Techniques

**掌握训练稳定性的关键技术！**
**Master key techniques for training stability!**

#### 学习率调度策略 | Learning Rate Scheduling Strategies

**项目价值 | Project Value:**
学习率调度是优化成功的关键因素，合适的学习率策略能显著提升模型性能和训练稳定性。

Learning rate scheduling is a key factor for optimization success. Appropriate learning rate strategies can significantly improve model performance and training stability.

**多种学习率调度策略 | Multiple Learning Rate Scheduling Strategies:**
```python
import math
from typing import Union, List

class LearningRateScheduler:
    """
    学习率调度器基类
    Base class for learning rate schedulers
    """
    def __init__(self, optimizer, last_epoch=-1):
        self.optimizer = optimizer
        self.last_epoch = last_epoch
        
        # 保存初始学习率 | Save initial learning rates
        if last_epoch == -1:
            for group in optimizer.param_groups:
                group.setdefault('initial_lr', group['lr'])
        else:
            for i, group in enumerate(optimizer.param_groups):
                if 'initial_lr' not in group:
                    raise KeyError(f"参数组 {i} 没有指定 'initial_lr'")
        
        self.base_lrs = [group['initial_lr'] for group in optimizer.param_groups]
        self.step(last_epoch + 1)
    
    def get_lr(self):
        """计算当前学习率 | Compute current learning rates"""
        raise NotImplementedError
    
    def step(self, epoch=None):
        """更新学习率 | Update learning rates"""
        if epoch is None:
            epoch = self.last_epoch + 1
        self.last_epoch = epoch
        
        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):
            param_group['lr'] = lr

class StepLR(LearningRateScheduler):
    """
    阶梯式学习率衰减
    Step learning rate decay
    """
    def __init__(self, optimizer, step_size, gamma=0.1, last_epoch=-1):
        self.step_size = step_size
        self.gamma = gamma
        super().__init__(optimizer, last_epoch)
    
    def get_lr(self):
        return [base_lr * self.gamma ** (self.last_epoch // self.step_size)
                for base_lr in self.base_lrs]

class ExponentialLR(LearningRateScheduler):
    """
    指数学习率衰减
    Exponential learning rate decay
    """
    def __init__(self, optimizer, gamma, last_epoch=-1):
        self.gamma = gamma
        super().__init__(optimizer, last_epoch)
    
    def get_lr(self):
        return [base_lr * self.gamma ** self.last_epoch
                for base_lr in self.base_lrs]

class CosineAnnealingLR(LearningRateScheduler):
    """
    余弦退火学习率
    Cosine annealing learning rate
    """
    def __init__(self, optimizer, T_max, eta_min=0, last_epoch=-1):
        self.T_max = T_max
        self.eta_min = eta_min
        super().__init__(optimizer, last_epoch)
    
    def get_lr(self):
        return [self.eta_min + (base_lr - self.eta_min) *
                (1 + math.cos(math.pi * self.last_epoch / self.T_max)) / 2
                for base_lr in self.base_lrs]

class WarmupCosineAnnealingLR(LearningRateScheduler):
    """
    带预热的余弦退火学习率
    Cosine annealing with warmup
    """
    def __init__(self, optimizer, warmup_epochs, T_max, eta_min=0, last_epoch=-1):
        self.warmup_epochs = warmup_epochs
        self.T_max = T_max
        self.eta_min = eta_min
        super().__init__(optimizer, last_epoch)
    
    def get_lr(self):
        if self.last_epoch < self.warmup_epochs:
            # 线性预热 | Linear warmup
            return [base_lr * self.last_epoch / self.warmup_epochs
                    for base_lr in self.base_lrs]
        else:
            # 余弦退火 | Cosine annealing
            progress = (self.last_epoch - self.warmup_epochs) / (self.T_max - self.warmup_epochs)
            return [self.eta_min + (base_lr - self.eta_min) *
                    (1 + math.cos(math.pi * progress)) / 2
                    for base_lr in self.base_lrs]

class CyclicLR(LearningRateScheduler):
    """
    循环学习率
    Cyclic learning rate
    """
    def __init__(self, optimizer, base_lr, max_lr, step_size_up=2000, 
                 step_size_down=None, mode='triangular', gamma=1.0, last_epoch=-1):
        self.base_lr = base_lr
        self.max_lr = max_lr
        self.step_size_up = step_size_up
        self.step_size_down = step_size_down or step_size_up
        self.mode = mode
        self.gamma = gamma
        
        self.cycle_size = step_size_up + self.step_size_down
        super().__init__(optimizer, last_epoch)
    
    def get_lr(self):
        cycle = 1 + self.last_epoch // self.cycle_size
        x = 1 + self.last_epoch / self.cycle_size - cycle
        
        if x <= self.step_size_up / self.cycle_size:
            # 上升阶段 | Ascending phase
            scale_factor = x * self.cycle_size / self.step_size_up
        else:
            # 下降阶段 | Descending phase
            scale_factor = (self.step_size_down - (x * self.cycle_size - self.step_size_up)) / self.step_size_down
        
        if self.mode == 'triangular':
            lr_delta = (self.max_lr - self.base_lr) * max(0, scale_factor)
        elif self.mode == 'exp_range':
            lr_delta = (self.max_lr - self.base_lr) * max(0, scale_factor) * (self.gamma ** self.last_epoch)
        
        return [self.base_lr + lr_delta for _ in self.base_lrs]

class OneCycleLR(LearningRateScheduler):
    """
    一周期学习率策略
    One cycle learning rate policy
    """
    def __init__(self, optimizer, max_lr, total_steps, pct_start=0.3, 
                 anneal_strategy='cos', last_epoch=-1):
        self.max_lr = max_lr
        self.total_steps = total_steps
        self.pct_start = pct_start
        self.anneal_strategy = anneal_strategy
        super().__init__(optimizer, last_epoch)
    
    def get_lr(self):
        step_ratio = self.last_epoch / self.total_steps
        
        if step_ratio < self.pct_start:
            # 上升阶段 | Ascending phase
            lr_ratio = step_ratio / self.pct_start
            return [base_lr + (self.max_lr - base_lr) * lr_ratio
                    for base_lr in self.base_lrs]
        else:
            # 下降阶段 | Descending phase
            remaining_ratio = (step_ratio - self.pct_start) / (1 - self.pct_start)
            
            if self.anneal_strategy == 'cos':
                lr_ratio = (1 + math.cos(math.pi * remaining_ratio)) / 2
            else:  # linear
                lr_ratio = 1 - remaining_ratio
            
            return [base_lr + (self.max_lr - base_lr) * lr_ratio
                    for base_lr in self.base_lrs]

def learning_rate_scheduling_demo():
    """
    学习率调度策略演示
    Learning rate scheduling demo
    """
    # 创建简单模型和优化器 | Create simple model and optimizer
    model = nn.Linear(10, 1)
    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)
    
    # 测试不同的调度器 | Test different schedulers
    schedulers = {
        'StepLR': StepLR(optimizer, step_size=30, gamma=0.1),
        'ExponentialLR': ExponentialLR(optimizer, gamma=0.95),
        'CosineAnnealing': CosineAnnealingLR(optimizer, T_max=100),
        'WarmupCosine': WarmupCosineAnnealingLR(optimizer, warmup_epochs=10, T_max=100),
        'CyclicLR': CyclicLR(optimizer, base_lr=0.01, max_lr=0.1, step_size_up=20),
        'OneCycleLR': OneCycleLR(optimizer, max_lr=0.1, total_steps=100)
    }
    
    # 记录学习率历史 | Record learning rate history
    lr_histories = {name: [] for name in schedulers.keys()}
    
    # 重置优化器函数 | Reset optimizer function
    def reset_optimizer():
        for group in optimizer.param_groups:
            group['lr'] = 0.1
    
    # 为每个调度器记录学习率变化 | Record learning rate changes for each scheduler
    epochs = 100
    for scheduler_name, scheduler in schedulers.items():
        reset_optimizer()
        scheduler.__init__(optimizer, **scheduler.__dict__)
        
        for epoch in range(epochs):
            lr_histories[scheduler_name].append(optimizer.param_groups[0]['lr'])
            scheduler.step()
    
    # 绘制学习率曲线 | Plot learning rate curves
    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    axes = axes.flatten()
    
    for idx, (name, history) in enumerate(lr_histories.items()):
        ax = axes[idx]
        ax.plot(history, linewidth=2, color=f'C{idx}')
        ax.set_title(f'{name} Learning Rate Schedule', fontweight='bold')
        ax.set_xlabel('Epoch')
        ax.set_ylabel('Learning Rate')
        ax.grid(True, alpha=0.3)
        ax.set_yscale('log')
    
    plt.tight_layout()
    plt.show()
    
    # 比较不同调度器的特点 | Compare characteristics of different schedulers
    print("=== 学习率调度器特点对比 | Learning Rate Scheduler Characteristics ===")
    for name, history in lr_histories.items():
        initial_lr = history[0]
        final_lr = history[-1]
        max_lr = max(history)
        min_lr = min(history)
        
        print(f"\n{name}:")
        print(f"  初始学习率: {initial_lr:.6f} | Initial LR: {initial_lr:.6f}")
        print(f"  最终学习率: {final_lr:.6f} | Final LR: {final_lr:.6f}")
        print(f"  最大学习率: {max_lr:.6f} | Max LR: {max_lr:.6f}")
        print(f"  最小学习率: {min_lr:.6f} | Min LR: {min_lr:.6f}")

if __name__ == "__main__":
    learning_rate_scheduling_demo()
```

#### 梯度裁剪技术 | Gradient Clipping Techniques

**项目核心 | Project Core:**
梯度裁剪是防止梯度爆炸、稳定训练的重要技术，特别在RNN和Transformer训练中必不可少。

Gradient clipping is an essential technique for preventing gradient explosion and stabilizing training, especially crucial in RNN and Transformer training.

**梯度裁剪实现 | Gradient Clipping Implementation:**
```python
class GradientClipper:
    """
    梯度裁剪工具类
    Gradient clipping utility class
    """
    
    @staticmethod
    def clip_grad_norm(parameters, max_norm, norm_type=2.0, error_if_nonfinite=False):
        """
        按范数裁剪梯度
        Clip gradients by norm
        
        Args:
            parameters: 模型参数 | Model parameters
            max_norm: 最大范数 | Maximum norm
            norm_type: 范数类型 | Norm type
            error_if_nonfinite: 遇到非有限值是否报错 | Whether to error on non-finite values
        """
        if isinstance(parameters, torch.Tensor):
            parameters = [parameters]
        
        parameters = [p for p in parameters if p.grad is not None]
        
        if len(parameters) == 0:
            return torch.tensor(0.0)
        
        device = parameters[0].grad.device
        
        if norm_type == float('inf'):
            # 无穷范数 | Infinity norm
            norms = [p.grad.detach().abs().max().to(device) for p in parameters]
            total_norm = norms[0] if len(norms) == 1 else torch.max(torch.stack(norms))
        else:
            # p范数 | p-norm
            total_norm = torch.norm(
                torch.stack([torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters]), 
                norm_type
            )
        
        if error_if_nonfinite and torch.logical_or(total_norm.isnan(), total_norm.isinf()):
            raise RuntimeError(f'梯度范数为 {total_norm}，这是非有限值')
        
        clip_coef = max_norm / (total_norm + 1e-6)
        
        # 只有当需要裁剪时才进行操作 | Only clip when necessary
        if clip_coef < 1:
            for p in parameters:
                p.grad.detach().mul_(clip_coef.to(p.grad.device))
        
        return total_norm
    
    @staticmethod
    def clip_grad_value(parameters, clip_value):
        """
        按值裁剪梯度
        Clip gradients by value
        
        Args:
            parameters: 模型参数 | Model parameters
            clip_value: 裁剪值 | Clipping value
        """
        if isinstance(parameters, torch.Tensor):
            parameters = [parameters]
        
        for p in parameters:
            if p.grad is not None:
                p.grad.data.clamp_(-clip_value, clip_value)

class AdaptiveGradientClipper:
    """
    自适应梯度裁剪器
    Adaptive gradient clipper
    """
    def __init__(self, parameters, percentile=10, history_size=1000):
        """
        Args:
            parameters: 模型参数 | Model parameters
            percentile: 百分位数阈值 | Percentile threshold
            history_size: 历史记录大小 | History size
        """
        self.parameters = list(parameters)
        self.percentile = percentile
        self.history_size = history_size
        self.grad_norms_history = []
    
    def step(self):
        """
        自适应梯度裁剪步骤
        Adaptive gradient clipping step
        """
        # 计算当前梯度范数 | Compute current gradient norm
        total_norm = torch.norm(
            torch.stack([torch.norm(p.grad.detach(), 2.0) for p in self.parameters if p.grad is not None]), 
            2.0
        )
        
        # 记录历史 | Record history
        self.grad_norms_history.append(total_norm.item())
        if len(self.grad_norms_history) > self.history_size:
            self.grad_norms_history.pop(0)
        
        # 计算自适应阈值 | Compute adaptive threshold
        if len(self.grad_norms_history) >= 10:
            threshold = np.percentile(self.grad_norms_history, 100 - self.percentile)
            
            # 应用裁剪 | Apply clipping
            if total_norm > threshold:
                GradientClipper.clip_grad_norm(self.parameters, threshold)
                return True, total_norm.item(), threshold
        
        return False, total_norm.item(), None

def gradient_clipping_demo():
    """
    梯度裁剪技术演示
    Gradient clipping techniques demo
    """
    # 创建一个容易产生梯度爆炸的模型 | Create a model prone to gradient explosion
    class ProblematicRNN(nn.Module):
        def __init__(self, input_size=10, hidden_size=50, num_layers=3):
            super().__init__()
            self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)
            self.fc = nn.Linear(hidden_size, 1)
            
            # 初始化权重使其容易梯度爆炸 | Initialize weights to be prone to gradient explosion
            for name, param in self.named_parameters():
                if 'weight' in name:
                    nn.init.uniform_(param, -2, 2)  # 较大的初始权重
        
        def forward(self, x):
            out, _ = self.rnn(x)
            return self.fc(out[:, -1, :])
    
    # 创建数据 | Create data
    batch_size, seq_len, input_size = 32, 20, 10
    X = torch.randn(batch_size, seq_len, input_size)
    y = torch.randn(batch_size, 1)
    
    # 测试不同的梯度裁剪策略 | Test different gradient clipping strategies
    models = {
        'No Clipping': ProblematicRNN(),
        'Norm Clipping': ProblematicRNN(),
        'Value Clipping': ProblematicRNN(),
        'Adaptive Clipping': ProblematicRNN()
    }
    
    # 初始化相同的权重 | Initialize same weights
    state_dict = models['No Clipping'].state_dict()
    for model in models.values():
        model.load_state_dict(state_dict)
    
    # 创建优化器 | Create optimizers
    optimizers = {name: torch.optim.SGD(model.parameters(), lr=0.01) 
                  for name, model in models.items()}
    
    # 创建自适应梯度裁剪器 | Create adaptive gradient clipper
    adaptive_clipper = AdaptiveGradientClipper(models['Adaptive Clipping'].parameters())
    
    # 训练历史 | Training history
    histories = {
        'loss': {name: [] for name in models.keys()},
        'grad_norm': {name: [] for name in models.keys()},
        'clipped': {name: [] for name in models.keys()}
    }
    
    # 训练循环 | Training loop
    criterion = nn.MSELoss()
    num_epochs = 100
    
    for epoch in range(num_epochs):
        for name, model in models.items():
            optimizer = optimizers[name]
            
            # 前向传播 | Forward pass
            optimizer.zero_grad()
            output = model(X)
            loss = criterion(output, y)
            
            # 反向传播 | Backward pass
            loss.backward()
            
            # 计算梯度范数 | Compute gradient norm
            total_norm = torch.norm(
                torch.stack([torch.norm(p.grad.detach(), 2.0) for p in model.parameters() if p.grad is not None]), 
                2.0
            ).item()
            
            # 应用不同的梯度裁剪策略 | Apply different gradient clipping strategies
            clipped = False
            if name == 'Norm Clipping':
                if total_norm > 1.0:
                    GradientClipper.clip_grad_norm(model.parameters(), max_norm=1.0)
                    clipped = True
            elif name == 'Value Clipping':
                if total_norm > 1.0:
                    GradientClipper.clip_grad_value(model.parameters(), clip_value=0.5)
                    clipped = True
            elif name == 'Adaptive Clipping':
                clipped, _, _ = adaptive_clipper.step()
            
            # 更新参数 | Update parameters
            optimizer.step()
            
            # 记录历史 | Record history
            histories['loss'][name].append(loss.item())
            histories['grad_norm'][name].append(total_norm)
            histories['clipped'][name].append(clipped)
    
    # 绘制结果 | Plot results
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # 损失曲线 | Loss curves
    ax1 = axes[0, 0]
    for name, losses in histories['loss'].items():
        ax1.plot(losses, label=name, linewidth=2)
    ax1.set_title('Training Loss')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.legend()
    ax1.set_yscale('log')
    ax1.grid(True, alpha=0.3)
    
    # 梯度范数 | Gradient norms
    ax2 = axes[0, 1]
    for name, norms in histories['grad_norm'].items():
        ax2.plot(norms, label=name, linewidth=2)
    ax2.set_title('Gradient Norm')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Gradient Norm')
    ax2.legend()
    ax2.set_yscale('log')
    ax2.grid(True, alpha=0.3)
    
    # 裁剪频率 | Clipping frequency
    ax3 = axes[1, 0]
    clipping_rates = {}
    for name, clipped_list in histories['clipped'].items():
        clipping_rate = sum(clipped_list) / len(clipped_list) * 100
        clipping_rates[name] = clipping_rate
    
    names = list(clipping_rates.keys())
    rates = list(clipping_rates.values())
    ax3.bar(names, rates, alpha=0.7)
    ax3.set_title('Gradient Clipping Frequency (%)')
    ax3.set_ylabel('Clipping Rate (%)')
    ax3.tick_params(axis='x', rotation=45)
    ax3.grid(True, alpha=0.3)
    
    # 最终性能对比 | Final performance comparison
    ax4 = axes[1, 1]
    final_losses = {name: losses[-10:] for name, losses in histories['loss'].items()}
    avg_final_losses = {name: np.mean(losses) for name, losses in final_losses.items()}
    
    names = list(avg_final_losses.keys())
    losses = list(avg_final_losses.values())
    bars = ax4.bar(names, losses, alpha=0.7)
    ax4.set_title('Average Final Loss (Last 10 Epochs)')
    ax4.set_ylabel('Average Loss')
    ax4.tick_params(axis='x', rotation=45)
    ax4.grid(True, alpha=0.3)
    
    # 添加数值标签 | Add value labels
    for bar, loss in zip(bars, losses):
        height = bar.get_height()
        ax4.text(bar.get_x() + bar.get_width()/2., height,
                f'{loss:.4f}', ha='center', va='bottom')
    
    plt.tight_layout()
    plt.show()
    
    # 打印统计信息 | Print statistics
    print("=== 梯度裁剪效果统计 | Gradient Clipping Effects Statistics ===")
    for name in models.keys():
        final_loss = np.mean(histories['loss'][name][-10:])
        max_grad_norm = max(histories['grad_norm'][name])
        avg_grad_norm = np.mean(histories['grad_norm'][name])
        clipping_rate = sum(histories['clipped'][name]) / len(histories['clipped'][name]) * 100
        
        print(f"\n{name}:")
        print(f"  最终损失: {final_loss:.6f} | Final Loss: {final_loss:.6f}")
        print(f"  最大梯度范数: {max_grad_norm:.6f} | Max Grad Norm: {max_grad_norm:.6f}")
        print(f"  平均梯度范数: {avg_grad_norm:.6f} | Avg Grad Norm: {avg_grad_norm:.6f}")
        print(f"  裁剪频率: {clipping_rate:.2f}% | Clipping Rate: {clipping_rate:.2f}%")

if __name__ == "__main__":
    gradient_clipping_demo()
```

### 03_分布式训练 | Distributed Training

**扩展到大规模训练！**
**Scale to large-scale training!**

#### 数据并行训练 | Data Parallel Training

**项目突破 | Project Breakthrough:**
数据并行是最常用的分布式训练方法，通过在多个设备上复制模型，并行处理不同的数据批次。

Data parallelism is the most commonly used distributed training method, replicating the model across multiple devices and processing different data batches in parallel.

**数据并行实现 | Data Parallel Implementation:**
```python
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel as DDP
import os

class DistributedTrainer:
    """
    分布式训练管理器
    Distributed training manager
    """
    def __init__(self, model, train_dataset, val_dataset=None, 
                 batch_size=32, num_epochs=10, lr=0.001):
        self.model = model
        self.train_dataset = train_dataset
        self.val_dataset = val_dataset
        self.batch_size = batch_size
        self.num_epochs = num_epochs
        self.lr = lr
        
        # 分布式设置 | Distributed setup
        self.world_size = None
        self.rank = None
        self.local_rank = None
        self.device = None
    
    def setup_distributed(self, rank, world_size):
        """
        设置分布式环境
        Setup distributed environment
        """
        os.environ['MASTER_ADDR'] = 'localhost'
        os.environ['MASTER_PORT'] = '12355'
        
        # 初始化进程组 | Initialize process group
        dist.init_process_group("nccl", rank=rank, world_size=world_size)
        
        self.rank = rank
        self.world_size = world_size
        self.local_rank = rank
        self.device = torch.device(f'cuda:{rank}')
        
        # 设置当前设备 | Set current device
        torch.cuda.set_device(self.device)
        
        print(f"进程 {rank}/{world_size} 在设备 {self.device} 上初始化完成")
        print(f"Process {rank}/{world_size} initialized on device {self.device}")
    
    def cleanup_distributed(self):
        """清理分布式环境 | Cleanup distributed environment"""
        dist.destroy_process_group()
    
    def create_data_loaders(self):
        """
        创建分布式数据加载器
        Create distributed data loaders
        """
        # 分布式采样器 | Distributed sampler
        train_sampler = torch.utils.data.distributed.DistributedSampler(
            self.train_dataset,
            num_replicas=self.world_size,
            rank=self.rank,
            shuffle=True
        )
        
        train_loader = torch.utils.data.DataLoader(
            self.train_dataset,
            batch_size=self.batch_size,
            sampler=train_sampler,
            num_workers=2,
            pin_memory=True
        )
        
        val_loader = None
        if self.val_dataset:
            val_sampler = torch.utils.data.distributed.DistributedSampler(
                self.val_dataset,
                num_replicas=self.world_size,
                rank=self.rank,
                shuffle=False
            )
            
            val_loader = torch.utils.data.DataLoader(
                self.val_dataset,
                batch_size=self.batch_size,
                sampler=val_sampler,
                num_workers=2,
                pin_memory=True
            )
        
        return train_loader, val_loader, train_sampler
    
    def train_distributed(self, rank, world_size):
        """
        分布式训练主函数
        Main distributed training function
        """
        # 设置分布式环境 | Setup distributed environment
        self.setup_distributed(rank, world_size)
        
        # 移动模型到设备 | Move model to device
        self.model = self.model.to(self.device)
        
        # 包装为分布式模型 | Wrap as distributed model
        ddp_model = DDP(self.model, device_ids=[self.local_rank])
        
        # 创建数据加载器 | Create data loaders
        train_loader, val_loader, train_sampler = self.create_data_loaders()
        
        # 创建优化器 | Create optimizer
        optimizer = AdamOptimizer(ddp_model.parameters(), lr=self.lr)
        criterion = nn.CrossEntropyLoss()
        
        # 训练循环 | Training loop
        for epoch in range(self.num_epochs):
            # 设置epoch用于正确的数据shuffling | Set epoch for proper data shuffling
            train_sampler.set_epoch(epoch)
            
            # 训练一个epoch | Train one epoch
            train_loss = self._train_epoch(ddp_model, train_loader, optimizer, criterion)
            
            # 验证 | Validation
            val_loss = None
            if val_loader:
                val_loss = self._validate_epoch(ddp_model, val_loader, criterion)
            
            # 只在主进程打印 | Only print on main process
            if self.rank == 0:
                print(f"Epoch {epoch+1}/{self.num_epochs}")
                print(f"  训练损失: {train_loss:.4f} | Train Loss: {train_loss:.4f}")
                if val_loss:
                    print(f"  验证损失: {val_loss:.4f} | Val Loss: {val_loss:.4f}")
        
        # 清理 | Cleanup
        self.cleanup_distributed()
    
    def _train_epoch(self, model, data_loader, optimizer, criterion):
        """
        训练一个epoch
        Train one epoch
        """
        model.train()
        total_loss = 0
        num_batches = 0
        
        for batch_idx, (data, target) in enumerate(data_loader):
            data, target = data.to(self.device), target.to(self.device)
            
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            num_batches += 1
        
        # 计算平均损失并同步所有进程 | Compute average loss and sync across all processes
        avg_loss = total_loss / num_batches
        loss_tensor = torch.tensor(avg_loss, device=self.device)
        dist.all_reduce(loss_tensor, op=dist.ReduceOp.SUM)
        avg_loss = loss_tensor.item() / self.world_size
        
        return avg_loss
    
    def _validate_epoch(self, model, data_loader, criterion):
        """
        验证一个epoch
        Validate one epoch
        """
        model.eval()
        total_loss = 0
        num_batches = 0
        
        with torch.no_grad():
            for data, target in data_loader:
                data, target = data.to(self.device), target.to(self.device)
                output = model(data)
                loss = criterion(output, target)
                
                total_loss += loss.item()
                num_batches += 1
        
        # 同步验证损失 | Sync validation loss
        avg_loss = total_loss / num_batches
        loss_tensor = torch.tensor(avg_loss, device=self.device)
        dist.all_reduce(loss_tensor, op=dist.ReduceOp.SUM)
        avg_loss = loss_tensor.item() / self.world_size
        
        return avg_loss

def distributed_training_demo():
    """
    分布式训练演示
    Distributed training demo
    """
    # 检查可用GPU数量 | Check available GPU count
    if not torch.cuda.is_available():
        print("CUDA不可用，跳过分布式训练演示")
        print("CUDA not available, skipping distributed training demo")
        return
    
    world_size = torch.cuda.device_count()
    if world_size < 2:
        print(f"只有 {world_size} 个GPU，需要至少2个GPU进行分布式训练演示")
        print(f"Only {world_size} GPU available, need at least 2 GPUs for distributed training demo")
        return
    
    # 创建示例模型和数据 | Create example model and data
    model = nn.Sequential(
        nn.Linear(784, 256),
        nn.ReLU(),
        nn.Linear(256, 128),
        nn.ReLU(),
        nn.Linear(128, 10)
    )
    
    # 创建虚拟数据集 | Create dummy dataset
    class DummyDataset(torch.utils.data.Dataset):
        def __init__(self, size=1000):
            self.size = size
            self.data = torch.randn(size, 784)
            self.targets = torch.randint(0, 10, (size,))
        
        def __len__(self):
            return self.size
        
        def __getitem__(self, idx):
            return self.data[idx], self.targets[idx]
    
    train_dataset = DummyDataset(1000)
    val_dataset = DummyDataset(200)
    
    # 创建分布式训练器 | Create distributed trainer
    trainer = DistributedTrainer(
        model=model,
        train_dataset=train_dataset,
        val_dataset=val_dataset,
        batch_size=32,
        num_epochs=5,
        lr=0.001
    )
    
    print(f"开始在 {world_size} 个GPU上进行分布式训练...")
    print(f"Starting distributed training on {world_size} GPUs...")
    
    # 启动多进程训练 | Launch multi-process training
    mp.spawn(
        trainer.train_distributed,
        args=(world_size,),
        nprocs=world_size,
        join=True
    )
    
    print("分布式训练完成！")
    print("Distributed training completed!")

if __name__ == "__main__":
    distributed_training_demo()
```

---

**🎯 项目完成检查清单 | Project Completion Checklist:**

### 优化算法理解 | Optimization Algorithm Understanding
- [ ] 深入理解SGD、Adam等经典优化器的数学原理
- [ ] 掌握不同优化器的适用场景和调参技巧
- [ ] 理解学习率调度和梯度裁剪的重要性
- [ ] 能够根据任务特点选择合适的优化策略

### 高级技术实现 | Advanced Technique Implementation
- [ ] 从零实现主流优化算法和变种
- [ ] 掌握多种学习率调度策略的设计和实现
- [ ] 实现有效的梯度裁剪和稳定性控制
- [ ] 理解并实现分布式训练的核心技术

### 工程实践能力 | Engineering Practice Capability
- [ ] 能够诊断和解决训练中的优化问题
- [ ] 掌握大规模模型的分布式训练方法
- [ ] 具备优化策略的性能分析和调优能力
- [ ] 理解不同优化技术的计算和内存开销

**记住**: 优化算法是深度学习的心脏，决定了模型训练的效率和质量。通过这个项目，你将掌握让AI训练更快、更稳定、更高效的核心技术！

**Remember**: Optimization algorithms are the heart of deep learning, determining the efficiency and quality of model training. Through this project, you will master the core technologies to make AI training faster, more stable, and more efficient! 