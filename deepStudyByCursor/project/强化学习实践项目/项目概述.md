# å¼ºåŒ–å­¦ä¹ å®è·µé¡¹ç›®æ¦‚è¿°
# Reinforcement Learning Practice Project Overview

**è®©AIå­¦ä¼šå†³ç­– - ä»æ¸¸æˆåˆ°ç°å®ä¸–ç•Œçš„æ™ºèƒ½å†³ç­–**
**Teaching AI to Make Decisions - From Games to Real-world Intelligent Decision Making**

---

## ğŸ¯ é¡¹ç›®ç›®æ ‡ | Project Goals

å¼ºåŒ–å­¦ä¹ æ˜¯AIæœ€æ¿€åŠ¨äººå¿ƒçš„é¢†åŸŸä¹‹ä¸€ï¼å®ƒè®©æœºå™¨èƒ½å¤Ÿé€šè¿‡ä¸ç¯å¢ƒäº¤äº’æ¥å­¦ä¹ æœ€ä¼˜å†³ç­–ç­–ç•¥ã€‚é€šè¿‡è¿™ä¸ªé¡¹ç›®ï¼Œä½ å°†æŒæ¡ï¼š
Reinforcement Learning is one of the most exciting fields in AI! It enables machines to learn optimal decision-making strategies through interaction with environments. Through this project, you will master:

- **é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹** | **Markov Decision Processes**: å†³ç­–é—®é¢˜çš„æ•°å­¦å»ºæ¨¡
- **ä»·å€¼å‡½æ•°ä¸ç­–ç•¥** | **Value Functions & Policies**: è¯„ä¼°å’Œæ”¹è¿›å†³ç­–ç­–ç•¥
- **æ·±åº¦å¼ºåŒ–å­¦ä¹ ** | **Deep Reinforcement Learning**: ç»“åˆæ·±åº¦å­¦ä¹ çš„å¼ºåŒ–å­¦ä¹ 
- **å®é™…åº”ç”¨** | **Real Applications**: æ¸¸æˆAIã€è‡ªåŠ¨é©¾é©¶ã€æ¨èç³»ç»Ÿç­‰

## ğŸ”¬ ä¸ºä»€ä¹ˆå¼ºåŒ–å­¦ä¹ å¦‚æ­¤é‡è¦ï¼Ÿ| Why is Reinforcement Learning So Important?

**å¼ºåŒ–å­¦ä¹ æ­£åœ¨åˆ›é€ çœŸæ­£çš„äººå·¥æ™ºèƒ½ï¼**
**Reinforcement Learning is creating true artificial intelligence!**

ä»AlphaGoå‡»è´¥ä¸–ç•Œå›´æ£‹å† å†›ï¼Œåˆ°è‡ªåŠ¨é©¾é©¶æ±½è½¦çš„è·¯å¾„è§„åˆ’ï¼Œä»ä¸ªæ€§åŒ–æ¨èçš„ç­–ç•¥ä¼˜åŒ–ï¼Œåˆ°æœºå™¨äººçš„çµæ´»æ§åˆ¶ï¼Œå¼ºåŒ–å­¦ä¹ è®©AIæ‹¥æœ‰äº†"æ™ºèƒ½å†³ç­–"çš„èƒ½åŠ›ã€‚è¿™ä¸ä»…ä»…æ˜¯ç®—æ³•ï¼Œæ›´æ˜¯é€šå‘é€šç”¨äººå·¥æ™ºèƒ½çš„å…³é”®è·¯å¾„ã€‚

From AlphaGo defeating world Go champions to path planning in autonomous vehicles, from strategy optimization in personalized recommendations to flexible robot control, reinforcement learning gives AI the ability to make "intelligent decisions". This is not just algorithms, but a key path to artificial general intelligence.

### å¼ºåŒ–å­¦ä¹ çš„å‘å±•å†ç¨‹ | Evolution of Reinforcement Learning
```
1950s: åŠ¨æ€è§„åˆ’åŸºç¡€ | Dynamic Programming Foundations
1980s: æ—¶é—´å·®åˆ†å­¦ä¹  | Temporal Difference Learning
1990s: Qå­¦ä¹ ç®—æ³• | Q-Learning Algorithm
2000s: ç­–ç•¥æ¢¯åº¦æ–¹æ³• | Policy Gradient Methods
2010s: æ·±åº¦å¼ºåŒ–å­¦ä¹  | Deep Reinforcement Learning
2020s: å¤šæ™ºèƒ½ä½“ä¸å…ƒå­¦ä¹  | Multi-Agent & Meta-Learning
```

## ğŸ“š é¡¹ç›®ç»“æ„æ·±åº¦è§£æ | Deep Project Structure Analysis

### 01_åŸºç¡€å¼ºåŒ–å­¦ä¹  | Basic Reinforcement Learning

**ç†è§£æ™ºèƒ½å†³ç­–çš„æ•°å­¦åŸºç¡€ï¼**
**Understand the mathematical foundations of intelligent decision-making!**

#### Qå­¦ä¹ ç®—æ³•å®ç° | Q-Learning Algorithm Implementation

**é¡¹ç›®æ ¸å¿ƒ | Project Core:**
Qå­¦ä¹ æ˜¯å¼ºåŒ–å­¦ä¹ çš„ç»å…¸ç®—æ³•ï¼Œé€šè¿‡å­¦ä¹ çŠ¶æ€-åŠ¨ä½œä»·å€¼å‡½æ•°æ¥æ‰¾åˆ°æœ€ä¼˜ç­–ç•¥ã€‚

Q-learning is a classic reinforcement learning algorithm that finds optimal policies by learning state-action value functions.

**æ•°å­¦åŸç† | Mathematical Principles:**

**è´å°”æ›¼æ–¹ç¨‹ | Bellman Equation:**
```
Q*(s, a) = R(s, a) + Î³ * max Q*(s', a')
```

**Qå­¦ä¹ æ›´æ–°è§„åˆ™ | Q-Learning Update Rule:**
```
Q(s, a) â† Q(s, a) + Î±[R + Î³ * max Q(s', a') - Q(s, a)]
```

å…¶ä¸­ï¼š
- `s`: å½“å‰çŠ¶æ€ | current state
- `a`: å½“å‰åŠ¨ä½œ | current action
- `R`: å³æ—¶å¥–åŠ± | immediate reward
- `Î³`: æŠ˜æ‰£å› å­ | discount factor
- `Î±`: å­¦ä¹ ç‡ | learning rate

**å®Œæ•´å®ç° | Complete Implementation:**
```python
import numpy as np
import matplotlib.pyplot as plt
from collections import defaultdict
import random

class QLearningAgent:
    """
    Qå­¦ä¹ æ™ºèƒ½ä½“å®ç°
    Q-Learning Agent Implementation
    """
    def __init__(self, num_states, num_actions, learning_rate=0.1, 
                 discount_factor=0.95, epsilon=0.1):
        self.num_states = num_states
        self.num_actions = num_actions
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = epsilon
        
        # åˆå§‹åŒ–Qè¡¨ | Initialize Q-table
        self.q_table = np.zeros((num_states, num_actions))
        
        # è®°å½•è®­ç»ƒå†å² | Record training history
        self.episode_rewards = []
        self.episode_steps = []
    
    def choose_action(self, state, training=True):
        """
        é€‰æ‹©åŠ¨ä½œï¼šÎµ-è´ªå©ªç­–ç•¥
        Choose action: Îµ-greedy policy
        """
        if training and random.random() < self.epsilon:
            # æ¢ç´¢ï¼šéšæœºé€‰æ‹©åŠ¨ä½œ | Exploration: random action
            return random.randint(0, self.num_actions - 1)
        else:
            # åˆ©ç”¨ï¼šé€‰æ‹©Qå€¼æœ€å¤§çš„åŠ¨ä½œ | Exploitation: action with max Q-value
            return np.argmax(self.q_table[state])
    
    def update_q_table(self, state, action, reward, next_state, done):
        """
        æ›´æ–°Qè¡¨
        Update Q-table
        """
        current_q = self.q_table[state, action]
        
        if done:
            # ç»ˆæ­¢çŠ¶æ€ï¼Œæ²¡æœ‰æœªæ¥å¥–åŠ± | Terminal state, no future reward
            target_q = reward
        else:
            # ä½¿ç”¨è´å°”æ›¼æ–¹ç¨‹è®¡ç®—ç›®æ ‡Qå€¼ | Use Bellman equation for target Q-value
            max_next_q = np.max(self.q_table[next_state])
            target_q = reward + self.discount_factor * max_next_q
        
        # Qå­¦ä¹ æ›´æ–° | Q-learning update
        self.q_table[state, action] += self.learning_rate * (target_q - current_q)
    
    def train(self, environment, num_episodes=1000):
        """
        è®­ç»ƒæ™ºèƒ½ä½“
        Train the agent
        """
        print(f"å¼€å§‹è®­ç»ƒQå­¦ä¹ æ™ºèƒ½ä½“ï¼Œå…±{num_episodes}è½®...")
        print(f"Starting Q-Learning training for {num_episodes} episodes...")
        
        for episode in range(num_episodes):
            state = environment.reset()
            total_reward = 0
            steps = 0
            
            while True:
                # é€‰æ‹©åŠ¨ä½œ | Choose action
                action = self.choose_action(state)
                
                # æ‰§è¡ŒåŠ¨ä½œ | Execute action
                next_state, reward, done, info = environment.step(action)
                
                # æ›´æ–°Qè¡¨ | Update Q-table
                self.update_q_table(state, action, reward, next_state, done)
                
                # æ›´æ–°çŠ¶æ€å’Œç»Ÿè®¡ | Update state and statistics
                state = next_state
                total_reward += reward
                steps += 1
                
                if done:
                    break
            
            # è®°å½•æœ¬è½®ç»“æœ | Record episode results
            self.episode_rewards.append(total_reward)
            self.episode_steps.append(steps)
            
            # åŠ¨æ€è°ƒæ•´æ¢ç´¢ç‡ | Dynamically adjust exploration rate
            if episode % 100 == 0:
                self.epsilon = max(0.01, self.epsilon * 0.995)
                avg_reward = np.mean(self.episode_rewards[-100:])
                print(f"Episode {episode}, Average Reward: {avg_reward:.2f}, Epsilon: {self.epsilon:.3f}")
        
        print("è®­ç»ƒå®Œæˆï¼| Training completed!")
        return self.q_table
    
    def evaluate(self, environment, num_episodes=100):
        """
        è¯„ä¼°æ™ºèƒ½ä½“æ€§èƒ½
        Evaluate agent performance
        """
        print(f"è¯„ä¼°æ™ºèƒ½ä½“æ€§èƒ½ï¼Œå…±{num_episodes}è½®...")
        print(f"Evaluating agent performance for {num_episodes} episodes...")
        
        eval_rewards = []
        
        for episode in range(num_episodes):
            state = environment.reset()
            total_reward = 0
            
            while True:
                # ä½¿ç”¨è´ªå©ªç­–ç•¥ï¼ˆä¸æ¢ç´¢ï¼‰| Use greedy policy (no exploration)
                action = self.choose_action(state, training=False)
                state, reward, done, info = environment.step(action)
                total_reward += reward
                
                if done:
                    break
            
            eval_rewards.append(total_reward)
        
        avg_reward = np.mean(eval_rewards)
        std_reward = np.std(eval_rewards)
        
        print(f"è¯„ä¼°ç»“æœ | Evaluation Results:")
        print(f"å¹³å‡å¥–åŠ± | Average Reward: {avg_reward:.2f} Â± {std_reward:.2f}")
        
        return eval_rewards
    
    def visualize_training(self):
        """
        å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹
        Visualize training process
        """
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
        
        # å¥–åŠ±æ›²çº¿ | Reward curve
        window_size = 100
        smoothed_rewards = []
        for i in range(len(self.episode_rewards)):
            start = max(0, i - window_size)
            smoothed_rewards.append(np.mean(self.episode_rewards[start:i+1]))
        
        ax1.plot(smoothed_rewards)
        ax1.set_title('Training Rewards (Smoothed)')
        ax1.set_xlabel('Episode')
        ax1.set_ylabel('Average Reward')
        ax1.grid(True)
        
        # æ­¥æ•°æ›²çº¿ | Steps curve
        smoothed_steps = []
        for i in range(len(self.episode_steps)):
            start = max(0, i - window_size)
            smoothed_steps.append(np.mean(self.episode_steps[start:i+1]))
        
        ax2.plot(smoothed_steps)
        ax2.set_title('Episode Steps (Smoothed)')
        ax2.set_xlabel('Episode')
        ax2.set_ylabel('Average Steps')
        ax2.grid(True)
        
        plt.tight_layout()
        plt.show()
        
        return fig
```

#### ç­–ç•¥æ¢¯åº¦æ–¹æ³• | Policy Gradient Methods

**é¡¹ç›®ç‰¹è‰² | Project Features:**
ç­–ç•¥æ¢¯åº¦ç›´æ¥ä¼˜åŒ–ç­–ç•¥å‡½æ•°ï¼Œé€‚ç”¨äºè¿ç»­åŠ¨ä½œç©ºé—´å’Œéšæœºç­–ç•¥ã€‚

Policy gradient directly optimizes the policy function, suitable for continuous action spaces and stochastic policies.

**REINFORCEç®—æ³•å®ç° | REINFORCE Algorithm Implementation:**
```python
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.distributions import Categorical

class PolicyNetwork(nn.Module):
    """
    ç­–ç•¥ç½‘ç»œï¼šå°†çŠ¶æ€æ˜ å°„åˆ°åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ
    Policy Network: Maps states to action probability distributions
    """
    def __init__(self, state_size, action_size, hidden_size=128):
        super(PolicyNetwork, self).__init__()
        self.fc1 = nn.Linear(state_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, action_size)
        
    def forward(self, state):
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        action_probs = F.softmax(self.fc3(x), dim=-1)
        return action_probs

class REINFORCEAgent:
    """
    REINFORCEç®—æ³•å®ç°
    REINFORCE Algorithm Implementation
    """
    def __init__(self, state_size, action_size, learning_rate=1e-3, gamma=0.99):
        self.state_size = state_size
        self.action_size = action_size
        self.gamma = gamma
        
        # ç­–ç•¥ç½‘ç»œ | Policy network
        self.policy_net = PolicyNetwork(state_size, action_size)
        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=learning_rate)
        
        # å­˜å‚¨è½¨è¿¹ | Store trajectory
        self.states = []
        self.actions = []
        self.rewards = []
        self.log_probs = []
        
        # è®­ç»ƒå†å² | Training history
        self.episode_rewards = []
    
    def choose_action(self, state):
        """
        æ ¹æ®å½“å‰ç­–ç•¥é€‰æ‹©åŠ¨ä½œ
        Choose action according to current policy
        """
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        action_probs = self.policy_net(state_tensor)
        
        # åˆ›å»ºåˆ†å¸ƒå¹¶é‡‡æ · | Create distribution and sample
        distribution = Categorical(action_probs)
        action = distribution.sample()
        
        # å­˜å‚¨ç”¨äºè®­ç»ƒ | Store for training
        self.log_probs.append(distribution.log_prob(action))
        
        return action.item()
    
    def store_transition(self, state, action, reward):
        """
        å­˜å‚¨çŠ¶æ€è½¬ç§»
        Store state transition
        """
        self.states.append(state)
        self.actions.append(action)
        self.rewards.append(reward)
    
    def calculate_returns(self):
        """
        è®¡ç®—å›æŠ¥ï¼ˆä»åå¾€å‰ï¼‰
        Calculate returns (backwards)
        """
        returns = []
        R = 0
        
        # ä»ç»ˆæ­¢çŠ¶æ€å¾€å›è®¡ç®— | Calculate backwards from terminal state
        for reward in reversed(self.rewards):
            R = reward + self.gamma * R
            returns.insert(0, R)
        
        return returns
    
    def update_policy(self):
        """
        æ›´æ–°ç­–ç•¥ç½‘ç»œ
        Update policy network
        """
        if len(self.rewards) == 0:
            return
        
        # è®¡ç®—å›æŠ¥ | Calculate returns
        returns = self.calculate_returns()
        returns = torch.FloatTensor(returns)
        
        # æ ‡å‡†åŒ–å›æŠ¥ | Normalize returns
        returns = (returns - returns.mean()) / (returns.std() + 1e-8)
        
        # è®¡ç®—ç­–ç•¥æŸå¤± | Calculate policy loss
        policy_loss = []
        for log_prob, R in zip(self.log_probs, returns):
            policy_loss.append(-log_prob * R)
        
        policy_loss = torch.cat(policy_loss).sum()
        
        # åå‘ä¼ æ’­ | Backpropagation
        self.optimizer.zero_grad()
        policy_loss.backward()
        self.optimizer.step()
        
        # æ¸…ç©ºè½¨è¿¹ | Clear trajectory
        self.states.clear()
        self.actions.clear()
        self.rewards.clear()
        self.log_probs.clear()
    
    def train(self, environment, num_episodes=2000):
        """
        è®­ç»ƒç­–ç•¥ç½‘ç»œ
        Train policy network
        """
        print(f"å¼€å§‹REINFORCEè®­ç»ƒï¼Œå…±{num_episodes}è½®...")
        print(f"Starting REINFORCE training for {num_episodes} episodes...")
        
        for episode in range(num_episodes):
            state = environment.reset()
            total_reward = 0
            
            while True:
                # é€‰æ‹©åŠ¨ä½œ | Choose action
                action = self.choose_action(state)
                
                # æ‰§è¡ŒåŠ¨ä½œ | Execute action
                next_state, reward, done, info = environment.step(action)
                
                # å­˜å‚¨è½¬ç§» | Store transition
                self.store_transition(state, action, reward)
                
                state = next_state
                total_reward += reward
                
                if done:
                    break
            
            # æ›´æ–°ç­–ç•¥ | Update policy
            self.update_policy()
            
            # è®°å½•å¥–åŠ± | Record reward
            self.episode_rewards.append(total_reward)
            
            # æ‰“å°è¿›åº¦ | Print progress
            if episode % 100 == 0:
                avg_reward = np.mean(self.episode_rewards[-100:])
                print(f"Episode {episode}, Average Reward: {avg_reward:.2f}")
        
        print("REINFORCEè®­ç»ƒå®Œæˆï¼| REINFORCE training completed!")
```

### 02_æ·±åº¦å¼ºåŒ–å­¦ä¹  | Deep Reinforcement Learning

**å°†æ·±åº¦å­¦ä¹ çš„åŠ›é‡æ³¨å…¥å¼ºåŒ–å­¦ä¹ ï¼**
**Inject the power of deep learning into reinforcement learning!**

#### DQNæ·±åº¦Qç½‘ç»œ | DQN Deep Q-Network

**é¡¹ç›®çªç ´ | Project Breakthrough:**
DQNæ˜¯æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„é‡Œç¨‹ç¢‘ï¼Œè§£å†³äº†Qå­¦ä¹ åœ¨é«˜ç»´çŠ¶æ€ç©ºé—´çš„é—®é¢˜ã€‚

DQN is a milestone in deep reinforcement learning, solving Q-learning problems in high-dimensional state spaces.

**æ ¸å¿ƒåˆ›æ–° | Core Innovations:**

1. **ç»éªŒå›æ”¾ | Experience Replay**
2. **ç›®æ ‡ç½‘ç»œ | Target Network**
3. **åŒé‡DQN | Double DQN**
4. **ä¼˜å…ˆç»éªŒå›æ”¾ | Prioritized Experience Replay**

**å®Œæ•´DQNå®ç° | Complete DQN Implementation:**
```python
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import random
from collections import deque, namedtuple

# ç»éªŒå…ƒç»„ | Experience tuple
Experience = namedtuple('Experience', 
                       ['state', 'action', 'reward', 'next_state', 'done'])

class DQNNetwork(nn.Module):
    """
    æ·±åº¦Qç½‘ç»œæ¶æ„
    Deep Q-Network Architecture
    """
    def __init__(self, state_size, action_size, hidden_size=512):
        super(DQNNetwork, self).__init__()
        
        self.fc1 = nn.Linear(state_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, hidden_size)
        self.fc4 = nn.Linear(hidden_size, action_size)
        
        # æƒé‡åˆå§‹åŒ– | Weight initialization
        self.apply(self.init_weights)
    
    def init_weights(self, layer):
        if isinstance(layer, nn.Linear):
            torch.nn.init.xavier_uniform_(layer.weight)
            layer.bias.data.fill_(0.01)
    
    def forward(self, state):
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        q_values = self.fc4(x)
        return q_values

class ReplayBuffer:
    """
    ç»éªŒå›æ”¾ç¼“å†²åŒº
    Experience Replay Buffer
    """
    def __init__(self, capacity=100000):
        self.buffer = deque(maxlen=capacity)
    
    def push(self, state, action, reward, next_state, done):
        """æ·»åŠ ç»éªŒ | Add experience"""
        experience = Experience(state, action, reward, next_state, done)
        self.buffer.append(experience)
    
    def sample(self, batch_size):
        """é‡‡æ ·æ‰¹æ¬¡ç»éªŒ | Sample batch of experiences"""
        experiences = random.sample(self.buffer, batch_size)
        
        states = torch.FloatTensor([e.state for e in experiences])
        actions = torch.LongTensor([e.action for e in experiences])
        rewards = torch.FloatTensor([e.reward for e in experiences])
        next_states = torch.FloatTensor([e.next_state for e in experiences])
        dones = torch.BoolTensor([e.done for e in experiences])
        
        return states, actions, rewards, next_states, dones
    
    def __len__(self):
        return len(self.buffer)

class DQNAgent:
    """
    DQNæ™ºèƒ½ä½“å®ç°
    DQN Agent Implementation
    """
    def __init__(self, state_size, action_size, learning_rate=1e-4, 
                 gamma=0.99, epsilon=1.0, epsilon_decay=0.995, 
                 epsilon_min=0.01, memory_size=100000, batch_size=64):
        
        self.state_size = state_size
        self.action_size = action_size
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min
        self.batch_size = batch_size
        
        # è®¾å¤‡é€‰æ‹© | Device selection
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # ç¥ç»ç½‘ç»œ | Neural networks
        self.q_network = DQNNetwork(state_size, action_size).to(self.device)
        self.target_network = DQNNetwork(state_size, action_size).to(self.device)
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)
        
        # ç»éªŒå›æ”¾ | Experience replay
        self.memory = ReplayBuffer(memory_size)
        
        # è®­ç»ƒç»Ÿè®¡ | Training statistics
        self.episode_rewards = []
        self.losses = []
        self.update_target_every = 1000  # ç›®æ ‡ç½‘ç»œæ›´æ–°é¢‘ç‡
        self.step_count = 0
    
    def act(self, state, training=True):
        """
        é€‰æ‹©åŠ¨ä½œ
        Choose action
        """
        if training and random.random() <= self.epsilon:
            return random.choice(range(self.action_size))
        
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        q_values = self.q_network(state_tensor)
        return q_values.cpu().data.numpy().argmax()
    
    def remember(self, state, action, reward, next_state, done):
        """
        å­˜å‚¨ç»éªŒ
        Store experience
        """
        self.memory.push(state, action, reward, next_state, done)
    
    def replay(self):
        """
        ç»éªŒå›æ”¾è®­ç»ƒ
        Experience replay training
        """
        if len(self.memory) < self.batch_size:
            return
        
        # é‡‡æ ·æ‰¹æ¬¡æ•°æ® | Sample batch data
        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)
        
        states = states.to(self.device)
        actions = actions.to(self.device)
        rewards = rewards.to(self.device)
        next_states = next_states.to(self.device)
        dones = dones.to(self.device)
        
        # å½“å‰Qå€¼ | Current Q-values
        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))
        
        # ä¸‹ä¸€çŠ¶æ€çš„æœ€å¤§Qå€¼ï¼ˆä½¿ç”¨ç›®æ ‡ç½‘ç»œï¼‰| Max Q-values for next states (using target network)
        with torch.no_grad():
            next_q_values = self.target_network(next_states).max(1)[0].detach()
            target_q_values = rewards + (self.gamma * next_q_values * ~dones)
        
        # è®¡ç®—æŸå¤± | Calculate loss
        loss = F.mse_loss(current_q_values.squeeze(), target_q_values)
        
        # åå‘ä¼ æ’­ | Backpropagation
        self.optimizer.zero_grad()
        loss.backward()
        
        # æ¢¯åº¦è£å‰ª | Gradient clipping
        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)
        
        self.optimizer.step()
        
        # è®°å½•æŸå¤± | Record loss
        self.losses.append(loss.item())
        
        # æ›´æ–°ç›®æ ‡ç½‘ç»œ | Update target network
        self.step_count += 1
        if self.step_count % self.update_target_every == 0:
            self.update_target_network()
        
        # è¡°å‡æ¢ç´¢ç‡ | Decay exploration rate
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
    
    def update_target_network(self):
        """
        æ›´æ–°ç›®æ ‡ç½‘ç»œ
        Update target network
        """
        self.target_network.load_state_dict(self.q_network.state_dict())
    
    def train(self, environment, num_episodes=2000):
        """
        è®­ç»ƒDQNæ™ºèƒ½ä½“
        Train DQN agent
        """
        print(f"å¼€å§‹DQNè®­ç»ƒï¼Œå…±{num_episodes}è½®...")
        print(f"Starting DQN training for {num_episodes} episodes...")
        
        for episode in range(num_episodes):
            state = environment.reset()
            total_reward = 0
            steps = 0
            
            while True:
                # é€‰æ‹©åŠ¨ä½œ | Choose action
                action = self.act(state)
                
                # æ‰§è¡ŒåŠ¨ä½œ | Execute action
                next_state, reward, done, info = environment.step(action)
                
                # å­˜å‚¨ç»éªŒ | Store experience
                self.remember(state, action, reward, next_state, done)
                
                # ç»éªŒå›æ”¾å­¦ä¹  | Experience replay learning
                self.replay()
                
                state = next_state
                total_reward += reward
                steps += 1
                
                if done:
                    break
            
            # è®°å½•æœ¬è½®ç»“æœ | Record episode results
            self.episode_rewards.append(total_reward)
            
            # æ‰“å°è¿›åº¦ | Print progress
            if episode % 100 == 0:
                avg_reward = np.mean(self.episode_rewards[-100:])
                avg_loss = np.mean(self.losses[-100:]) if self.losses else 0
                print(f"Episode {episode}")
                print(f"  Average Reward: {avg_reward:.2f}")
                print(f"  Average Loss: {avg_loss:.4f}")
                print(f"  Epsilon: {self.epsilon:.3f}")
                print(f"  Memory Size: {len(self.memory)}")
        
        print("DQNè®­ç»ƒå®Œæˆï¼| DQN training completed!")
        return self.q_network
```

#### Actor-Criticç®—æ³• | Actor-Critic Algorithm

**é¡¹ç›®ä»·å€¼ | Project Value:**
Actor-Criticç»“åˆäº†ä»·å€¼å‡½æ•°å’Œç­–ç•¥å‡½æ•°çš„ä¼˜åŠ¿ï¼Œæ˜¯ç°ä»£å¼ºåŒ–å­¦ä¹ çš„æ ¸å¿ƒæ–¹æ³•ã€‚

Actor-Critic combines the advantages of value functions and policy functions, being a core method in modern reinforcement learning.

**A2Cç®—æ³•å®ç° | A2C Algorithm Implementation:**
```python
class ActorCriticNetwork(nn.Module):
    """
    Actor-Criticç½‘ç»œæ¶æ„
    Actor-Critic Network Architecture
    """
    def __init__(self, state_size, action_size, hidden_size=256):
        super(ActorCriticNetwork, self).__init__()
        
        # å…±äº«ç‰¹å¾å±‚ | Shared feature layers
        self.shared_fc1 = nn.Linear(state_size, hidden_size)
        self.shared_fc2 = nn.Linear(hidden_size, hidden_size)
        
        # Actoråˆ†æ”¯ï¼ˆç­–ç•¥ç½‘ç»œï¼‰| Actor branch (policy network)
        self.actor_fc = nn.Linear(hidden_size, hidden_size)
        self.actor_output = nn.Linear(hidden_size, action_size)
        
        # Criticåˆ†æ”¯ï¼ˆä»·å€¼ç½‘ç»œï¼‰| Critic branch (value network)
        self.critic_fc = nn.Linear(hidden_size, hidden_size)
        self.critic_output = nn.Linear(hidden_size, 1)
        
    def forward(self, state):
        # å…±äº«ç‰¹å¾æå– | Shared feature extraction
        x = F.relu(self.shared_fc1(state))
        x = F.relu(self.shared_fc2(x))
        
        # Actorè¾“å‡ºï¼ˆåŠ¨ä½œæ¦‚ç‡ï¼‰| Actor output (action probabilities)
        actor_x = F.relu(self.actor_fc(x))
        action_probs = F.softmax(self.actor_output(actor_x), dim=-1)
        
        # Criticè¾“å‡ºï¼ˆçŠ¶æ€ä»·å€¼ï¼‰| Critic output (state value)
        critic_x = F.relu(self.critic_fc(x))
        state_value = self.critic_output(critic_x)
        
        return action_probs, state_value

class A2CAgent:
    """
    Advantage Actor-Criticæ™ºèƒ½ä½“
    Advantage Actor-Critic Agent
    """
    def __init__(self, state_size, action_size, learning_rate=1e-3, 
                 gamma=0.99, value_loss_coef=0.5, entropy_coef=0.01):
        
        self.state_size = state_size
        self.action_size = action_size
        self.gamma = gamma
        self.value_loss_coef = value_loss_coef
        self.entropy_coef = entropy_coef
        
        # è®¾å¤‡é€‰æ‹© | Device selection
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # Actor-Criticç½‘ç»œ | Actor-Critic network
        self.network = ActorCriticNetwork(state_size, action_size).to(self.device)
        self.optimizer = optim.Adam(self.network.parameters(), lr=learning_rate)
        
        # å­˜å‚¨è½¨è¿¹ | Store trajectory
        self.states = []
        self.actions = []
        self.rewards = []
        self.values = []
        self.log_probs = []
        self.entropies = []
        
        # è®­ç»ƒå†å² | Training history
        self.episode_rewards = []
        self.actor_losses = []
        self.critic_losses = []
    
    def act(self, state):
        """
        é€‰æ‹©åŠ¨ä½œå¹¶è®¡ç®—ä»·å€¼
        Choose action and compute value
        """
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        action_probs, state_value = self.network(state_tensor)
        
        # åˆ›å»ºåˆ†å¸ƒ | Create distribution
        distribution = Categorical(action_probs)
        action = distribution.sample()
        
        # è®¡ç®—ç†µï¼ˆç”¨äºé¼“åŠ±æ¢ç´¢ï¼‰| Calculate entropy (for exploration)
        entropy = distribution.entropy()
        
        # å­˜å‚¨ç”¨äºè®­ç»ƒ | Store for training
        self.log_probs.append(distribution.log_prob(action))
        self.entropies.append(entropy)
        self.values.append(state_value)
        
        return action.item(), state_value.item()
    
    def store_transition(self, state, action, reward):
        """
        å­˜å‚¨çŠ¶æ€è½¬ç§»
        Store state transition
        """
        self.states.append(state)
        self.actions.append(action)
        self.rewards.append(reward)
    
    def calculate_advantages(self, next_value=0):
        """
        è®¡ç®—ä¼˜åŠ¿å‡½æ•°
        Calculate advantage function
        """
        returns = []
        advantages = []
        
        # è®¡ç®—å›æŠ¥ | Calculate returns
        R = next_value
        for reward in reversed(self.rewards):
            R = reward + self.gamma * R
            returns.insert(0, R)
        
        returns = torch.FloatTensor(returns).to(self.device)
        values = torch.cat(self.values).to(self.device)
        
        # è®¡ç®—ä¼˜åŠ¿ A(s,a) = R - V(s) | Calculate advantage A(s,a) = R - V(s)
        advantages = returns - values
        
        return returns, advantages
    
    def update_network(self, next_value=0):
        """
        æ›´æ–°ç½‘ç»œå‚æ•°
        Update network parameters
        """
        if len(self.rewards) == 0:
            return
        
        # è®¡ç®—ä¼˜åŠ¿ | Calculate advantages
        returns, advantages = self.calculate_advantages(next_value)
        
        # æ ‡å‡†åŒ–ä¼˜åŠ¿ | Normalize advantages
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        # è®¡ç®—æŸå¤± | Calculate losses
        log_probs = torch.cat(self.log_probs).to(self.device)
        entropies = torch.cat(self.entropies).to(self.device)
        
        # ActoræŸå¤±ï¼ˆç­–ç•¥æ¢¯åº¦ï¼‰| Actor loss (policy gradient)
        actor_loss = -(log_probs * advantages.detach()).mean()
        
        # CriticæŸå¤±ï¼ˆä»·å€¼å‡½æ•°ï¼‰| Critic loss (value function)
        critic_loss = F.mse_loss(torch.cat(self.values).to(self.device), returns)
        
        # ç†µæŸå¤±ï¼ˆé¼“åŠ±æ¢ç´¢ï¼‰| Entropy loss (encourage exploration)
        entropy_loss = -entropies.mean()
        
        # æ€»æŸå¤± | Total loss
        total_loss = (actor_loss + 
                     self.value_loss_coef * critic_loss + 
                     self.entropy_coef * entropy_loss)
        
        # åå‘ä¼ æ’­ | Backpropagation
        self.optimizer.zero_grad()
        total_loss.backward()
        
        # æ¢¯åº¦è£å‰ª | Gradient clipping
        torch.nn.utils.clip_grad_norm_(self.network.parameters(), 0.5)
        
        self.optimizer.step()
        
        # è®°å½•æŸå¤± | Record losses
        self.actor_losses.append(actor_loss.item())
        self.critic_losses.append(critic_loss.item())
        
        # æ¸…ç©ºè½¨è¿¹ | Clear trajectory
        self.clear_trajectory()
    
    def clear_trajectory(self):
        """æ¸…ç©ºè½¨è¿¹æ•°æ® | Clear trajectory data"""
        self.states.clear()
        self.actions.clear()
        self.rewards.clear()
        self.values.clear()
        self.log_probs.clear()
        self.entropies.clear()
    
    def train(self, environment, num_episodes=2000):
        """
        è®­ç»ƒA2Cæ™ºèƒ½ä½“
        Train A2C agent
        """
        print(f"å¼€å§‹A2Cè®­ç»ƒï¼Œå…±{num_episodes}è½®...")
        print(f"Starting A2C training for {num_episodes} episodes...")
        
        for episode in range(num_episodes):
            state = environment.reset()
            total_reward = 0
            
            while True:
                # é€‰æ‹©åŠ¨ä½œ | Choose action
                action, value = self.act(state)
                
                # æ‰§è¡ŒåŠ¨ä½œ | Execute action
                next_state, reward, done, info = environment.step(action)
                
                # å­˜å‚¨è½¬ç§» | Store transition
                self.store_transition(state, action, reward)
                
                state = next_state
                total_reward += reward
                
                if done:
                    # å›åˆç»“æŸï¼Œæ›´æ–°ç½‘ç»œ | Episode ended, update network
                    self.update_network(next_value=0)
                    break
            
            # è®°å½•å¥–åŠ± | Record reward
            self.episode_rewards.append(total_reward)
            
            # æ‰“å°è¿›åº¦ | Print progress
            if episode % 100 == 0:
                avg_reward = np.mean(self.episode_rewards[-100:])
                avg_actor_loss = np.mean(self.actor_losses[-100:]) if self.actor_losses else 0
                avg_critic_loss = np.mean(self.critic_losses[-100:]) if self.critic_losses else 0
                
                print(f"Episode {episode}")
                print(f"  Average Reward: {avg_reward:.2f}")
                print(f"  Actor Loss: {avg_actor_loss:.4f}")
                print(f"  Critic Loss: {avg_critic_loss:.4f}")
        
        print("A2Cè®­ç»ƒå®Œæˆï¼| A2C training completed!")
```

## ğŸ¯ å®é™…åº”ç”¨åœºæ™¯ | Real-world Application Scenarios

### ğŸ® æ¸¸æˆAIå¼€å‘ | Game AI Development

```python
class GameAI:
    """
    æ¸¸æˆAIç³»ç»Ÿ
    Game AI System
    """
    def __init__(self, game_type="atari"):
        self.game_type = game_type
        self.agent = None
        
    def create_agent_for_game(self, game_name):
        """
        ä¸ºç‰¹å®šæ¸¸æˆåˆ›å»ºæ™ºèƒ½ä½“
        Create agent for specific game
        """
        if game_name in ["Pong", "Breakout", "SpaceInvaders"]:
            # ä½¿ç”¨DQNå¤„ç†Atariæ¸¸æˆ
            self.agent = DQNAgent(state_size=84*84*4, action_size=4)
        elif game_name in ["CartPole", "MountainCar"]:
            # ä½¿ç”¨A2Cå¤„ç†ç»å…¸æ§åˆ¶é—®é¢˜
            self.agent = A2CAgent(state_size=4, action_size=2)
        
        return self.agent
```

### ğŸš— è‡ªåŠ¨é©¾é©¶å†³ç­– | Autonomous Driving Decision

```python
class AutonomousDrivingAgent:
    """
    è‡ªåŠ¨é©¾é©¶å†³ç­–ç³»ç»Ÿ
    Autonomous driving decision system
    """
    def __init__(self):
        # çŠ¶æ€ï¼šé€Ÿåº¦ã€ä½ç½®ã€å‘¨å›´è½¦è¾†ä¿¡æ¯ç­‰
        # State: speed, position, surrounding vehicle info, etc.
        self.state_size = 20
        
        # åŠ¨ä½œï¼šåŠ é€Ÿã€å‡é€Ÿã€å˜é“ç­‰
        # Actions: accelerate, decelerate, change lanes, etc.
        self.action_size = 5
        
        self.agent = A2CAgent(self.state_size, self.action_size)
    
    def make_driving_decision(self, traffic_state):
        """
        åšå‡ºé©¾é©¶å†³ç­–
        Make driving decision
        """
        action, _ = self.agent.act(traffic_state)
        return self.interpret_action(action)
    
    def interpret_action(self, action):
        """è§£é‡ŠåŠ¨ä½œå«ä¹‰ | Interpret action meaning"""
        actions = ["maintain_speed", "accelerate", "decelerate", 
                  "change_left", "change_right"]
        return actions[action]
```

---

**ğŸ¯ é¡¹ç›®å®Œæˆæ£€æŸ¥æ¸…å• | Project Completion Checklist:**

### ç†è®ºç†è§£ | Theoretical Understanding
- [ ] æ·±å…¥ç†è§£é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹çš„æ•°å­¦æ¡†æ¶
- [ ] æŒæ¡ä»·å€¼å‡½æ•°ã€ç­–ç•¥å‡½æ•°å’Œä¼˜åŠ¿å‡½æ•°çš„æ¦‚å¿µ
- [ ] ç†è§£æ¢ç´¢ä¸åˆ©ç”¨çš„å¹³è¡¡ç­–ç•¥
- [ ] æŒæ¡ç­–ç•¥æ¢¯åº¦å®šç†å’ŒActor-Criticæ–¹æ³•

### ç®—æ³•å®ç° | Algorithm Implementation
- [ ] ä»é›¶å®ç°Qå­¦ä¹ å’ŒSARSAç®—æ³•
- [ ] å®ç°DQNåŠå…¶æ”¹è¿›ç‰ˆæœ¬ï¼ˆDouble DQNã€Dueling DQNï¼‰
- [ ] å®ç°ç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼ˆREINFORCEã€A2Cã€PPOï¼‰
- [ ] æŒæ¡ç»éªŒå›æ”¾å’Œç›®æ ‡ç½‘ç»œç­‰å…³é”®æŠ€æœ¯

### å®é™…åº”ç”¨ | Practical Applications
- [ ] åœ¨ç»å…¸æ§åˆ¶ä»»åŠ¡ä¸Šè·å¾—è‰¯å¥½æ€§èƒ½
- [ ] æˆåŠŸè®­ç»ƒæ¸¸æˆAIï¼ˆå¦‚Atariæ¸¸æˆï¼‰
- [ ] å®ç°å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿ
- [ ] å°†RLåº”ç”¨åˆ°å®é™…é—®é¢˜ï¼ˆæ¨èã€è‡ªåŠ¨é©¾é©¶ç­‰ï¼‰

**è®°ä½**: å¼ºåŒ–å­¦ä¹ æ˜¯è®©AIå­¦ä¼šå†³ç­–çš„å…³é”®æŠ€æœ¯ã€‚é€šè¿‡è¿™ä¸ªé¡¹ç›®ï¼Œä½ å°†æŒæ¡ä»ç®€å•çš„Qå­¦ä¹ åˆ°å¤æ‚çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„å®Œæ•´æŠ€æœ¯æ ˆï¼

**Remember**: Reinforcement learning is the key technology for teaching AI to make decisions. Through this project, you will master the complete technology stack from simple Q-learning to complex deep reinforcement learning! 