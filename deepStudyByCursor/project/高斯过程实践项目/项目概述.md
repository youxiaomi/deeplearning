# 高斯过程实践项目概述
# Gaussian Processes Practice Project Overview

**不确定性建模的艺术 - 让AI拥有"知道自己不知道"的智慧**
**The Art of Uncertainty Modeling - Giving AI the Wisdom to "Know What It Doesn't Know"**

---

## 🎯 项目目标 | Project Goals

高斯过程是机器学习中最优雅的概率模型之一！它不仅能进行预测，还能量化预测的不确定性。通过这个项目，你将掌握：
Gaussian Processes are one of the most elegant probabilistic models in machine learning! They not only make predictions but also quantify prediction uncertainty. Through this project, you will master:

- **高斯过程回归** | **Gaussian Process Regression**: 非参数贝叶斯回归的强大工具
- **核函数设计** | **Kernel Function Design**: 编码先验知识的数学语言
- **贝叶斯优化** | **Bayesian Optimization**: 高效的全局优化方法
- **不确定性量化** | **Uncertainty Quantification**: 可信AI的核心技术

## 🔬 为什么高斯过程如此重要？| Why are Gaussian Processes So Important?

**高斯过程让AI拥有"谦逊"的智慧！**
**Gaussian Processes give AI the wisdom of "humility"!**

在医疗诊断中，AI需要知道自己的诊断有多可靠；在自动驾驶中，AI需要识别未知的危险情况；在金融投资中，AI需要评估预测的风险。高斯过程为AI提供了表达不确定性的数学框架，这对于构建可信、安全的AI系统至关重要。

In medical diagnosis, AI needs to know how reliable its diagnosis is; in autonomous driving, AI needs to identify unknown dangerous situations; in financial investment, AI needs to assess prediction risks. Gaussian Processes provide AI with a mathematical framework for expressing uncertainty, which is crucial for building trustworthy and safe AI systems.

### 高斯过程的发展历程 | Evolution of Gaussian Processes
```
1940s: 维纳过程理论 | Wiener Process Theory
1960s: 克里金插值方法 | Kriging Interpolation Methods
1990s: 机器学习中的GP | GP in Machine Learning
2000s: 贝叶斯优化兴起 | Rise of Bayesian Optimization
2010s: 深度高斯过程 | Deep Gaussian Processes
2020s: 可扩展GP方法 | Scalable GP Methods
```

## 📚 项目结构深度解析 | Deep Project Structure Analysis

### 01_高斯过程基础 | Gaussian Process Fundamentals

**理解无穷维的优雅数学！**
**Understand the elegant mathematics of infinite dimensions!**

#### GP回归实现 | GP Regression Implementation

**项目核心 | Project Core:**
高斯过程将函数视为随机变量，通过均值函数和协方差函数（核函数）来完全定义函数的分布。

Gaussian Processes treat functions as random variables, completely defining the distribution of functions through mean functions and covariance functions (kernel functions).

**数学原理 | Mathematical Principles:**

**高斯过程定义 | Gaussian Process Definition:**
```
f(x) ~ GP(m(x), k(x, x'))
```

其中：
- `m(x)`: 均值函数 | mean function
- `k(x, x')`: 核函数/协方差函数 | kernel/covariance function

**预测公式 | Prediction Formula:**
```python
# 给定训练数据 (X, y)，预测新点 x*
# Given training data (X, y), predict at new point x*

# 后验均值 | Posterior mean
μ(x*) = k(x*, X) @ K^(-1) @ y

# 后验方差 | Posterior variance  
σ²(x*) = k(x*, x*) - k(x*, X) @ K^(-1) @ k(X, x*)
```

**完整实现 | Complete Implementation:**
```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.linalg import cholesky, solve_triangular
from scipy.optimize import minimize
import warnings
warnings.filterwarnings('ignore')

class GaussianProcess:
    """
    高斯过程回归实现
    Gaussian Process Regression Implementation
    """
    def __init__(self, kernel, noise_variance=1e-6, mean_function=None):
        self.kernel = kernel
        self.noise_variance = noise_variance
        self.mean_function = mean_function if mean_function else lambda x: np.zeros(x.shape[0])
        
        # 训练数据 | Training data
        self.X_train = None
        self.y_train = None
        self.K_inv = None
        self.L = None  # Cholesky分解 | Cholesky decomposition
        
    def fit(self, X_train, y_train):
        """
        训练高斯过程模型
        Train Gaussian Process model
        """
        self.X_train = np.array(X_train)
        self.y_train = np.array(y_train)
        
        if self.X_train.ndim == 1:
            self.X_train = self.X_train.reshape(-1, 1)
        
        # 计算协方差矩阵 | Compute covariance matrix
        K = self.kernel(self.X_train, self.X_train)
        
        # 添加噪声项 | Add noise term
        K += self.noise_variance * np.eye(K.shape[0])
        
        # Cholesky分解用于数值稳定性 | Cholesky decomposition for numerical stability
        try:
            self.L = cholesky(K, lower=True)
        except np.linalg.LinAlgError:
            # 如果Cholesky分解失败，添加更多噪声 | If Cholesky fails, add more noise
            K += 1e-3 * np.eye(K.shape[0])
            self.L = cholesky(K, lower=True)
        
        # 计算 K^(-1) @ y 用于预测 | Compute K^(-1) @ y for prediction
        alpha = solve_triangular(self.L, self.y_train, lower=True)
        self.alpha = solve_triangular(self.L.T, alpha, lower=False)
        
        return self
    
    def predict(self, X_test, return_std=True):
        """
        进行预测
        Make predictions
        """
        X_test = np.array(X_test)
        if X_test.ndim == 1:
            X_test = X_test.reshape(-1, 1)
        
        # 计算测试点与训练点的协方差 | Compute covariance between test and train points
        K_star = self.kernel(X_test, self.X_train)
        
        # 后验均值 | Posterior mean
        mean_pred = K_star @ self.alpha
        
        if return_std:
            # 计算后验方差 | Compute posterior variance
            v = solve_triangular(self.L, K_star.T, lower=True)
            
            # 测试点之间的协方差 | Covariance between test points
            K_star_star = self.kernel(X_test, X_test)
            
            # 后验方差 | Posterior variance
            var_pred = np.diag(K_star_star) - np.sum(v**2, axis=0)
            
            # 确保方差非负 | Ensure variance is non-negative
            var_pred = np.maximum(var_pred, 0)
            std_pred = np.sqrt(var_pred)
            
            return mean_pred, std_pred
        
        return mean_pred
    
    def sample_posterior(self, X_test, n_samples=5):
        """
        从后验分布采样函数
        Sample functions from posterior distribution
        """
        X_test = np.array(X_test)
        if X_test.ndim == 1:
            X_test = X_test.reshape(-1, 1)
        
        # 获取后验均值和协方差 | Get posterior mean and covariance
        mean_pred, std_pred = self.predict(X_test, return_std=True)
        
        # 计算完整的后验协方差矩阵 | Compute full posterior covariance matrix
        K_star = self.kernel(X_test, self.X_train)
        K_star_star = self.kernel(X_test, X_test)
        
        v = solve_triangular(self.L, K_star.T, lower=True)
        cov_pred = K_star_star - v.T @ v
        
        # 添加小的噪声以确保数值稳定性 | Add small noise for numerical stability
        cov_pred += 1e-6 * np.eye(cov_pred.shape[0])
        
        # 从多元正态分布采样 | Sample from multivariate normal
        samples = np.random.multivariate_normal(mean_pred, cov_pred, n_samples)
        
        return samples
    
    def log_marginal_likelihood(self):
        """
        计算对数边际似然
        Compute log marginal likelihood
        """
        if self.L is None:
            raise ValueError("模型尚未训练 | Model not trained yet")
        
        # 对数边际似然的三个组成部分 | Three components of log marginal likelihood
        # 1. 数据拟合项 | Data fit term
        alpha = solve_triangular(self.L, self.y_train, lower=True)
        data_fit = -0.5 * np.sum(alpha**2)
        
        # 2. 复杂度惩罚项 | Complexity penalty term
        complexity_penalty = -np.sum(np.log(np.diag(self.L)))
        
        # 3. 归一化常数 | Normalization constant
        normalization = -0.5 * len(self.y_train) * np.log(2 * np.pi)
        
        return data_fit + complexity_penalty + normalization

# 核函数实现 | Kernel function implementations
class RBFKernel:
    """
    径向基函数核（高斯核）
    Radial Basis Function (Gaussian) Kernel
    """
    def __init__(self, length_scale=1.0, signal_variance=1.0):
        self.length_scale = length_scale
        self.signal_variance = signal_variance
    
    def __call__(self, X1, X2):
        """
        计算核矩阵
        Compute kernel matrix
        """
        # 计算平方欧几里得距离 | Compute squared Euclidean distances
        X1 = X1 / self.length_scale
        X2 = X2 / self.length_scale
        
        sqdist = np.sum(X1**2, axis=1).reshape(-1, 1) + np.sum(X2**2, axis=1) - 2 * np.dot(X1, X2.T)
        
        return self.signal_variance * np.exp(-0.5 * sqdist)

class MaternKernel:
    """
    Matérn核函数
    Matérn Kernel Function
    """
    def __init__(self, length_scale=1.0, signal_variance=1.0, nu=1.5):
        self.length_scale = length_scale
        self.signal_variance = signal_variance
        self.nu = nu
    
    def __call__(self, X1, X2):
        """
        计算Matérn核矩阵
        Compute Matérn kernel matrix
        """
        from scipy.special import gamma, kv
        
        # 计算距离 | Compute distances
        X1_scaled = X1 / self.length_scale
        X2_scaled = X2 / self.length_scale
        
        dists = np.sqrt(np.sum((X1_scaled[:, np.newaxis, :] - X2_scaled[np.newaxis, :, :]) ** 2, axis=2))
        
        # Matérn核公式 | Matérn kernel formula
        sqrt_2nu_times_r = np.sqrt(2 * self.nu) * dists
        
        # 避免在距离为0时的数值问题 | Avoid numerical issues when distance is 0
        sqrt_2nu_times_r = np.maximum(sqrt_2nu_times_r, 1e-10)
        
        # Matérn核 | Matérn kernel
        kernel_matrix = (2 ** (1 - self.nu) / gamma(self.nu)) * \
                       (sqrt_2nu_times_r ** self.nu) * \
                       kv(self.nu, sqrt_2nu_times_r)
        
        # 处理距离为0的情况 | Handle case when distance is 0
        kernel_matrix[dists == 0] = 1.0
        
        return self.signal_variance * kernel_matrix

class PeriodicKernel:
    """
    周期核函数
    Periodic Kernel Function
    """
    def __init__(self, length_scale=1.0, signal_variance=1.0, period=1.0):
        self.length_scale = length_scale
        self.signal_variance = signal_variance
        self.period = period
    
    def __call__(self, X1, X2):
        """
        计算周期核矩阵
        Compute periodic kernel matrix
        """
        # 计算距离 | Compute distances
        dists = np.sqrt(np.sum((X1[:, np.newaxis, :] - X2[np.newaxis, :, :]) ** 2, axis=2))
        
        # 周期核公式 | Periodic kernel formula
        kernel_matrix = np.exp(-2 * np.sin(np.pi * dists / self.period) ** 2 / self.length_scale ** 2)
        
        return self.signal_variance * kernel_matrix

# 示例使用 | Example usage
def gp_regression_demo():
    """
    高斯过程回归演示
    Gaussian Process Regression Demo
    """
    # 生成示例数据 | Generate example data
    np.random.seed(42)
    X_train = np.array([1, 3, 5, 6, 7, 8]).reshape(-1, 1)
    y_train = np.array([1, 3, 2, 5, 4, 6]) + 0.1 * np.random.randn(6)
    
    # 测试点 | Test points
    X_test = np.linspace(0, 10, 100).reshape(-1, 1)
    
    # 创建不同的核函数 | Create different kernels
    kernels = {
        'RBF': RBFKernel(length_scale=1.0, signal_variance=1.0),
        'Matérn': MaternKernel(length_scale=1.0, signal_variance=1.0, nu=1.5),
        'Periodic': PeriodicKernel(length_scale=1.0, signal_variance=1.0, period=3.0)
    }
    
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    
    for i, (kernel_name, kernel) in enumerate(kernels.items()):
        # 训练高斯过程 | Train Gaussian Process
        gp = GaussianProcess(kernel=kernel, noise_variance=0.01)
        gp.fit(X_train, y_train)
        
        # 预测 | Predict
        mean_pred, std_pred = gp.predict(X_test, return_std=True)
        
        # 从后验采样 | Sample from posterior
        samples = gp.sample_posterior(X_test, n_samples=3)
        
        # 绘图 | Plot
        ax = axes[i]
        
        # 训练数据 | Training data
        ax.scatter(X_train.flatten(), y_train, c='red', s=50, zorder=5, label='Training Data')
        
        # 预测均值 | Prediction mean
        ax.plot(X_test.flatten(), mean_pred, 'blue', linewidth=2, label='Prediction Mean')
        
        # 不确定性区间 | Uncertainty interval
        ax.fill_between(X_test.flatten(), 
                       mean_pred - 2 * std_pred, 
                       mean_pred + 2 * std_pred, 
                       alpha=0.3, color='blue', label='95% Confidence')
        
        # 后验样本 | Posterior samples
        for j, sample in enumerate(samples):
            ax.plot(X_test.flatten(), sample, '--', alpha=0.7, 
                   label=f'Sample {j+1}' if i == 0 else '')
        
        ax.set_title(f'{kernel_name} Kernel', fontsize=14, fontweight='bold')
        ax.set_xlabel('X')
        ax.set_ylabel('Y')
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # 计算对数边际似然 | Compute log marginal likelihood
    for kernel_name, kernel in kernels.items():
        gp = GaussianProcess(kernel=kernel, noise_variance=0.01)
        gp.fit(X_train, y_train)
        lml = gp.log_marginal_likelihood()
        print(f"{kernel_name} Kernel - Log Marginal Likelihood: {lml:.4f}")

if __name__ == "__main__":
    gp_regression_demo()
```

#### 核函数设计 | Kernel Function Design

**项目特色 | Project Features:**
核函数是高斯过程的灵魂，它编码了对函数形状的先验知识。设计合适的核函数是GP成功应用的关键。

Kernel functions are the soul of Gaussian Processes, encoding prior knowledge about function shapes. Designing appropriate kernel functions is key to successful GP applications.

**核函数的组合与设计 | Kernel Combination and Design:**
```python
class CompositeKernel:
    """
    复合核函数：支持核函数的加法和乘法组合
    Composite Kernel: Supports additive and multiplicative combinations of kernels
    """
    def __init__(self, kernels, operations):
        """
        Args:
            kernels: 核函数列表 | List of kernel functions
            operations: 操作列表 ('+' 或 '*') | List of operations ('+' or '*')
        """
        self.kernels = kernels
        self.operations = operations
        
        if len(kernels) != len(operations) + 1:
            raise ValueError("核函数数量应该比操作数量多1")
    
    def __call__(self, X1, X2):
        """
        计算复合核矩阵
        Compute composite kernel matrix
        """
        result = self.kernels[0](X1, X2)
        
        for i, operation in enumerate(self.operations):
            kernel_matrix = self.kernels[i + 1](X1, X2)
            
            if operation == '+':
                result = result + kernel_matrix
            elif operation == '*':
                result = result * kernel_matrix
            else:
                raise ValueError(f"不支持的操作: {operation}")
        
        return result

class LocalPeriodicKernel:
    """
    局部周期核：RBF核与周期核的乘积，用于建模局部周期性
    Local Periodic Kernel: Product of RBF and Periodic kernels for local periodicity
    """
    def __init__(self, length_scale=1.0, signal_variance=1.0, period=1.0, local_length=1.0):
        self.rbf_kernel = RBFKernel(length_scale=local_length, signal_variance=1.0)
        self.periodic_kernel = PeriodicKernel(length_scale=length_scale, 
                                             signal_variance=1.0, period=period)
        self.signal_variance = signal_variance
    
    def __call__(self, X1, X2):
        """
        计算局部周期核
        Compute local periodic kernel
        """
        rbf_matrix = self.rbf_kernel(X1, X2)
        periodic_matrix = self.periodic_kernel(X1, X2)
        
        return self.signal_variance * rbf_matrix * periodic_matrix

class LinearKernel:
    """
    线性核函数
    Linear Kernel Function
    """
    def __init__(self, signal_variance=1.0, offset=0.0):
        self.signal_variance = signal_variance
        self.offset = offset
    
    def __call__(self, X1, X2):
        """
        计算线性核矩阵
        Compute linear kernel matrix
        """
        return self.signal_variance * ((X1 - self.offset) @ (X2 - self.offset).T)

class PolynomialKernel:
    """
    多项式核函数
    Polynomial Kernel Function
    """
    def __init__(self, signal_variance=1.0, offset=1.0, degree=2):
        self.signal_variance = signal_variance
        self.offset = offset
        self.degree = degree
    
    def __call__(self, X1, X2):
        """
        计算多项式核矩阵
        Compute polynomial kernel matrix
        """
        linear_kernel = (X1 @ X2.T) + self.offset
        return self.signal_variance * (linear_kernel ** self.degree)

def kernel_design_demo():
    """
    核函数设计演示
    Kernel Function Design Demo
    """
    # 生成不同类型的数据 | Generate different types of data
    np.random.seed(42)
    
    # 1. 趋势 + 周期数据 | Trend + Periodic data
    X = np.linspace(0, 10, 50).reshape(-1, 1)
    y_trend_periodic = 0.5 * X.flatten() + 2 * np.sin(2 * np.pi * X.flatten() / 3) + 0.2 * np.random.randn(50)
    
    # 2. 局部周期数据 | Local periodic data
    y_local_periodic = np.exp(-0.1 * (X.flatten() - 5)**2) * np.sin(2 * np.pi * X.flatten()) + 0.1 * np.random.randn(50)
    
    # 3. 多项式数据 | Polynomial data
    y_polynomial = 0.1 * X.flatten()**2 - 0.05 * X.flatten()**3 + 0.3 * np.random.randn(50)
    
    datasets = [
        (y_trend_periodic, "Trend + Periodic"),
        (y_local_periodic, "Local Periodic"),
        (y_polynomial, "Polynomial")
    ]
    
    # 设计对应的核函数 | Design corresponding kernels
    kernels = [
        # 趋势 + 周期：线性核 + 周期核 | Trend + Periodic: Linear + Periodic
        CompositeKernel([
            LinearKernel(signal_variance=0.5),
            PeriodicKernel(length_scale=1.0, signal_variance=1.0, period=3.0)
        ], ['+']),
        
        # 局部周期：局部周期核 | Local Periodic: Local Periodic Kernel
        LocalPeriodicKernel(length_scale=2.0, signal_variance=1.0, period=1.0, local_length=3.0),
        
        # 多项式：RBF核 + 多项式核 | Polynomial: RBF + Polynomial
        CompositeKernel([
            RBFKernel(length_scale=2.0, signal_variance=0.5),
            PolynomialKernel(signal_variance=0.5, degree=3)
        ], ['+'])
    ]
    
    fig, axes = plt.subplots(3, 1, figsize=(12, 15))
    
    X_test = np.linspace(-1, 11, 100).reshape(-1, 1)
    
    for i, ((y_data, title), kernel) in enumerate(zip(datasets, kernels)):
        # 训练高斯过程 | Train Gaussian Process
        gp = GaussianProcess(kernel=kernel, noise_variance=0.1)
        gp.fit(X, y_data)
        
        # 预测 | Predict
        mean_pred, std_pred = gp.predict(X_test, return_std=True)
        
        # 绘图 | Plot
        ax = axes[i]
        
        # 训练数据 | Training data
        ax.scatter(X.flatten(), y_data, c='red', s=30, zorder=5, label='Training Data')
        
        # 预测均值 | Prediction mean
        ax.plot(X_test.flatten(), mean_pred, 'blue', linewidth=2, label='GP Prediction')
        
        # 不确定性区间 | Uncertainty interval
        ax.fill_between(X_test.flatten(), 
                       mean_pred - 2 * std_pred, 
                       mean_pred + 2 * std_pred, 
                       alpha=0.3, color='blue', label='95% Confidence')
        
        ax.set_title(f'{title} - Custom Kernel Design', fontsize=14, fontweight='bold')
        ax.set_xlabel('X')
        ax.set_ylabel('Y')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # 显示对数边际似然 | Show log marginal likelihood
        lml = gp.log_marginal_likelihood()
        ax.text(0.02, 0.98, f'Log ML: {lml:.2f}', transform=ax.transAxes, 
               verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat'))
    
    plt.tight_layout()
    plt.show()

if __name__ == "__main__":
    kernel_design_demo()
```

### 02_贝叶斯优化 | Bayesian Optimization

**高效的全局优化艺术！**
**The art of efficient global optimization!**

#### 超参数优化 | Hyperparameter Optimization

**项目价值 | Project Value:**
贝叶斯优化使用高斯过程来建模目标函数，通过采集函数来平衡探索和利用，实现高效的全局优化。

Bayesian Optimization uses Gaussian Processes to model objective functions, balancing exploration and exploitation through acquisition functions for efficient global optimization.

**贝叶斯优化实现 | Bayesian Optimization Implementation:**
```python
from scipy.stats import norm
from scipy.optimize import minimize

class BayesianOptimizer:
    """
    贝叶斯优化器
    Bayesian Optimizer
    """
    def __init__(self, objective_function, bounds, kernel=None, 
                 acquisition='ei', xi=0.01, n_initial=5):
        """
        Args:
            objective_function: 目标函数 | Objective function
            bounds: 搜索空间边界 | Search space bounds [(low, high), ...]
            kernel: 高斯过程核函数 | GP kernel function
            acquisition: 采集函数类型 | Acquisition function type
            xi: 探索参数 | Exploration parameter
            n_initial: 初始随机采样数量 | Number of initial random samples
        """
        self.objective_function = objective_function
        self.bounds = np.array(bounds)
        self.kernel = kernel if kernel else RBFKernel(length_scale=1.0)
        self.acquisition = acquisition
        self.xi = xi
        self.n_initial = n_initial
        
        # 观测数据 | Observed data
        self.X_observed = []
        self.y_observed = []
        
        # 高斯过程模型 | Gaussian Process model
        self.gp = None
        
        # 最优值记录 | Best value record
        self.best_x = None
        self.best_y = None
        self.optimization_history = []
    
    def _random_sampling(self, n_samples):
        """
        在搜索空间内随机采样
        Random sampling within search space
        """
        samples = []
        for _ in range(n_samples):
            sample = []
            for low, high in self.bounds:
                sample.append(np.random.uniform(low, high))
            samples.append(sample)
        return np.array(samples)
    
    def _acquisition_function(self, X):
        """
        计算采集函数值
        Compute acquisition function values
        """
        if len(self.X_observed) == 0:
            return np.ones(X.shape[0])
        
        # 预测均值和方差 | Predict mean and variance
        mu, sigma = self.gp.predict(X, return_std=True)
        
        # 避免除零 | Avoid division by zero
        sigma = np.maximum(sigma, 1e-9)
        
        if self.acquisition == 'ei':
            # Expected Improvement | 期望改进
            return self._expected_improvement(mu, sigma)
        elif self.acquisition == 'pi':
            # Probability of Improvement | 改进概率
            return self._probability_of_improvement(mu, sigma)
        elif self.acquisition == 'ucb':
            # Upper Confidence Bound | 置信上界
            return self._upper_confidence_bound(mu, sigma)
        else:
            raise ValueError(f"不支持的采集函数: {self.acquisition}")
    
    def _expected_improvement(self, mu, sigma):
        """
        期望改进采集函数
        Expected Improvement acquisition function
        """
        if self.best_y is None:
            return np.ones_like(mu)
        
        # 计算改进量 | Compute improvement
        improvement = mu - self.best_y - self.xi
        Z = improvement / sigma
        
        # 期望改进公式 | Expected improvement formula
        ei = improvement * norm.cdf(Z) + sigma * norm.pdf(Z)
        
        return ei
    
    def _probability_of_improvement(self, mu, sigma):
        """
        改进概率采集函数
        Probability of Improvement acquisition function
        """
        if self.best_y is None:
            return np.ones_like(mu)
        
        # 改进概率公式 | Probability of improvement formula
        Z = (mu - self.best_y - self.xi) / sigma
        pi = norm.cdf(Z)
        
        return pi
    
    def _upper_confidence_bound(self, mu, sigma, beta=2.0):
        """
        置信上界采集函数
        Upper Confidence Bound acquisition function
        """
        return mu + beta * sigma
    
    def _optimize_acquisition(self):
        """
        优化采集函数找到下一个采样点
        Optimize acquisition function to find next sampling point
        """
        def objective(x):
            # 最小化负采集函数值 | Minimize negative acquisition function
            return -self._acquisition_function(x.reshape(1, -1))[0]
        
        best_x = None
        best_acquisition = -np.inf
        
        # 多起点优化 | Multi-start optimization
        n_restarts = 10
        for _ in range(n_restarts):
            # 随机起点 | Random starting point
            x0 = []
            for low, high in self.bounds:
                x0.append(np.random.uniform(low, high))
            x0 = np.array(x0)
            
            # 优化 | Optimize
            result = minimize(objective, x0, method='L-BFGS-B', 
                            bounds=self.bounds)
            
            if result.success and -result.fun > best_acquisition:
                best_acquisition = -result.fun
                best_x = result.x
        
        return best_x
    
    def optimize(self, n_iterations=20, verbose=True):
        """
        执行贝叶斯优化
        Perform Bayesian Optimization
        """
        if verbose:
            print("开始贝叶斯优化...")
            print("Starting Bayesian Optimization...")
        
        # 初始随机采样 | Initial random sampling
        X_init = self._random_sampling(self.n_initial)
        
        for x in X_init:
            y = self.objective_function(x)
            self.X_observed.append(x)
            self.y_observed.append(y)
            
            # 更新最优值 | Update best value
            if self.best_y is None or y > self.best_y:
                self.best_y = y
                self.best_x = x.copy()
            
            self.optimization_history.append({
                'iteration': len(self.optimization_history),
                'x': x.copy(),
                'y': y,
                'best_y': self.best_y
            })
        
        # 贝叶斯优化主循环 | Main Bayesian optimization loop
        for iteration in range(n_iterations):
            # 训练高斯过程 | Train Gaussian Process
            self.gp = GaussianProcess(kernel=self.kernel, noise_variance=1e-6)
            self.gp.fit(np.array(self.X_observed), np.array(self.y_observed))
            
            # 优化采集函数 | Optimize acquisition function
            next_x = self._optimize_acquisition()
            
            if next_x is None:
                if verbose:
                    print("无法找到下一个采样点，优化终止")
                break
            
            # 评估目标函数 | Evaluate objective function
            next_y = self.objective_function(next_x)
            
            # 更新观测数据 | Update observed data
            self.X_observed.append(next_x)
            self.y_observed.append(next_y)
            
            # 更新最优值 | Update best value
            if next_y > self.best_y:
                self.best_y = next_y
                self.best_x = next_x.copy()
            
            # 记录优化历史 | Record optimization history
            self.optimization_history.append({
                'iteration': len(self.optimization_history),
                'x': next_x.copy(),
                'y': next_y,
                'best_y': self.best_y
            })
            
            if verbose and (iteration + 1) % 5 == 0:
                print(f"Iteration {iteration + 1}, Best Y: {self.best_y:.6f}")
        
        if verbose:
            print(f"贝叶斯优化完成！最优值: {self.best_y:.6f}")
            print(f"Bayesian Optimization completed! Best value: {self.best_y:.6f}")
        
        return self.best_x, self.best_y
    
    def plot_optimization_progress(self):
        """
        绘制优化进度
        Plot optimization progress
        """
        iterations = [h['iteration'] for h in self.optimization_history]
        best_values = [h['best_y'] for h in self.optimization_history]
        current_values = [h['y'] for h in self.optimization_history]
        
        plt.figure(figsize=(12, 5))
        
        plt.subplot(1, 2, 1)
        plt.plot(iterations, current_values, 'o-', alpha=0.7, label='Current Values')
        plt.plot(iterations, best_values, 'r-', linewidth=2, label='Best Value')
        plt.xlabel('Iteration')
        plt.ylabel('Objective Value')
        plt.title('Optimization Progress')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # 如果是一维问题，绘制采集函数 | If 1D problem, plot acquisition function
        if len(self.bounds) == 1:
            plt.subplot(1, 2, 2)
            
            # 绘制目标函数和高斯过程预测 | Plot objective function and GP prediction
            x_plot = np.linspace(self.bounds[0, 0], self.bounds[0, 1], 200).reshape(-1, 1)
            
            if self.gp is not None:
                mu, sigma = self.gp.predict(x_plot, return_std=True)
                
                plt.plot(x_plot, mu, 'b-', label='GP Mean')
                plt.fill_between(x_plot.flatten(), mu - 2*sigma, mu + 2*sigma, 
                               alpha=0.3, color='blue', label='95% Confidence')
                
                # 绘制采集函数 | Plot acquisition function
                acquisition_values = self._acquisition_function(x_plot)
                plt.plot(x_plot, acquisition_values, 'g-', label='Acquisition Function')
            
            # 绘制观测点 | Plot observed points
            X_obs = np.array(self.X_observed)
            y_obs = np.array(self.y_observed)
            plt.scatter(X_obs.flatten(), y_obs, c='red', s=50, zorder=5, label='Observations')
            
            plt.xlabel('X')
            plt.ylabel('Y')
            plt.title('Gaussian Process and Acquisition Function')
            plt.legend()
            plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()

# 测试函数 | Test functions
def test_function_1d(x):
    """一维测试函数"""
    return -(x[0] - 0.3)**2 * np.sin(15 * x[0]) + 0.5

def test_function_2d(x):
    """二维测试函数（Branin函数）"""
    x1, x2 = x[0], x[1]
    a = 1
    b = 5.1 / (4 * np.pi**2)
    c = 5 / np.pi
    r = 6
    s = 10
    t = 1 / (8 * np.pi)
    
    return -(a * (x2 - b * x1**2 + c * x1 - r)**2 + s * (1 - t) * np.cos(x1) + s)

def bayesian_optimization_demo():
    """
    贝叶斯优化演示
    Bayesian Optimization Demo
    """
    print("=== 一维贝叶斯优化演示 ===")
    print("=== 1D Bayesian Optimization Demo ===")
    
    # 一维优化 | 1D optimization
    bounds_1d = [(-1, 1)]
    optimizer_1d = BayesianOptimizer(
        objective_function=test_function_1d,
        bounds=bounds_1d,
        acquisition='ei',
        n_initial=3
    )
    
    best_x_1d, best_y_1d = optimizer_1d.optimize(n_iterations=15, verbose=True)
    print(f"最优解: x = {best_x_1d[0]:.4f}, f(x) = {best_y_1d:.4f}")
    
    optimizer_1d.plot_optimization_progress()
    
    print("\n=== 二维贝叶斯优化演示 ===")
    print("=== 2D Bayesian Optimization Demo ===")
    
    # 二维优化 | 2D optimization
    bounds_2d = [(-5, 10), (0, 15)]
    optimizer_2d = BayesianOptimizer(
        objective_function=test_function_2d,
        bounds=bounds_2d,
        acquisition='ei',
        n_initial=5
    )
    
    best_x_2d, best_y_2d = optimizer_2d.optimize(n_iterations=25, verbose=True)
    print(f"最优解: x = [{best_x_2d[0]:.4f}, {best_x_2d[1]:.4f}], f(x) = {best_y_2d:.4f}")
    
    optimizer_2d.plot_optimization_progress()

if __name__ == "__main__":
    bayesian_optimization_demo()
```

#### 神经架构搜索 | Neural Architecture Search

**项目前沿 | Project Frontier:**
神经架构搜索使用贝叶斯优化来自动设计神经网络架构，是AutoML的重要组成部分。

Neural Architecture Search uses Bayesian Optimization to automatically design neural network architectures, being an important component of AutoML.

**NAS实现框架 | NAS Implementation Framework:**
```python
class NeuralArchitectureSearch:
    """
    神经架构搜索系统
    Neural Architecture Search System
    """
    def __init__(self, search_space, performance_estimator, max_epochs=10):
        self.search_space = search_space
        self.performance_estimator = performance_estimator
        self.max_epochs = max_epochs
        
        # 架构编码器 | Architecture encoder
        self.architecture_encoder = self._create_architecture_encoder()
        
        # 贝叶斯优化器 | Bayesian optimizer
        self.optimizer = None
        
    def _create_architecture_encoder(self):
        """
        创建架构编码器，将架构转换为向量
        Create architecture encoder to convert architectures to vectors
        """
        # 搜索空间维度 | Search space dimensions
        encoding_dims = []
        
        for param_name, param_range in self.search_space.items():
            if isinstance(param_range, list):
                encoding_dims.append(len(param_range))
            else:
                encoding_dims.append(1)  # 连续参数
        
        return encoding_dims
    
    def encode_architecture(self, architecture):
        """
        将架构编码为向量
        Encode architecture as vector
        """
        encoded = []
        
        for param_name, param_range in self.search_space.items():
            value = architecture[param_name]
            
            if isinstance(param_range, list):
                # 分类参数：one-hot编码 | Categorical parameter: one-hot encoding
                encoded.append(param_range.index(value))
            else:
                # 连续参数：归一化 | Continuous parameter: normalize
                low, high = param_range
                normalized = (value - low) / (high - low)
                encoded.append(normalized)
        
        return np.array(encoded)
    
    def decode_architecture(self, encoded_vector):
        """
        将向量解码为架构
        Decode vector to architecture
        """
        architecture = {}
        idx = 0
        
        for param_name, param_range in self.search_space.items():
            if isinstance(param_range, list):
                # 分类参数 | Categorical parameter
                param_idx = int(round(encoded_vector[idx]))
                param_idx = max(0, min(len(param_range) - 1, param_idx))
                architecture[param_name] = param_range[param_idx]
            else:
                # 连续参数 | Continuous parameter
                low, high = param_range
                normalized_value = max(0, min(1, encoded_vector[idx]))
                architecture[param_name] = low + normalized_value * (high - low)
            
            idx += 1
        
        return architecture
    
    def objective_function(self, encoded_vector):
        """
        目标函数：评估架构性能
        Objective function: evaluate architecture performance
        """
        # 解码架构 | Decode architecture
        architecture = self.decode_architecture(encoded_vector)
        
        # 评估性能 | Evaluate performance
        performance = self.performance_estimator(architecture)
        
        return performance
    
    def search(self, n_iterations=50):
        """
        执行神经架构搜索
        Perform Neural Architecture Search
        """
        print("开始神经架构搜索...")
        print("Starting Neural Architecture Search...")
        
        # 设置搜索边界 | Set search bounds
        bounds = []
        for param_name, param_range in self.search_space.items():
            if isinstance(param_range, list):
                bounds.append((0, len(param_range) - 1))
            else:
                bounds.append((0, 1))  # 归一化范围
        
        # 创建贝叶斯优化器 | Create Bayesian optimizer
        self.optimizer = BayesianOptimizer(
            objective_function=self.objective_function,
            bounds=bounds,
            acquisition='ei',
            n_initial=10
        )
        
        # 执行搜索 | Perform search
        best_encoded, best_performance = self.optimizer.optimize(
            n_iterations=n_iterations, verbose=True
        )
        
        # 解码最优架构 | Decode best architecture
        best_architecture = self.decode_architecture(best_encoded)
        
        print(f"搜索完成！最优架构性能: {best_performance:.4f}")
        print(f"Search completed! Best architecture performance: {best_performance:.4f}")
        print(f"最优架构: {best_architecture}")
        print(f"Best architecture: {best_architecture}")
        
        return best_architecture, best_performance

# 简化的性能估计器示例 | Simplified performance estimator example
def simple_performance_estimator(architecture):
    """
    简化的架构性能估计器
    Simplified architecture performance estimator
    """
    # 模拟训练时间和准确率的权衡 | Simulate trade-off between training time and accuracy
    
    # 基础准确率 | Base accuracy
    accuracy = 0.7
    
    # 层数影响 | Number of layers effect
    n_layers = architecture['n_layers']
    accuracy += 0.05 * min(n_layers, 5)  # 更多层提高准确率，但有上限
    
    # 隐藏单元数影响 | Hidden units effect
    hidden_units = architecture['hidden_units']
    accuracy += 0.1 * (hidden_units / 512)  # 更多单元提高准确率
    
    # 学习率影响 | Learning rate effect
    learning_rate = architecture['learning_rate']
    optimal_lr = 0.001
    lr_penalty = abs(np.log10(learning_rate) - np.log10(optimal_lr)) * 0.02
    accuracy -= lr_penalty
    
    # 激活函数影响 | Activation function effect
    activation_bonus = {
        'relu': 0.02,
        'tanh': 0.01,
        'sigmoid': 0.005
    }
    accuracy += activation_bonus.get(architecture['activation'], 0)
    
    # 添加随机噪声模拟实验不确定性 | Add random noise to simulate experimental uncertainty
    noise = np.random.normal(0, 0.01)
    accuracy += noise
    
    return max(0, min(1, accuracy))

def nas_demo():
    """
    神经架构搜索演示
    Neural Architecture Search Demo
    """
    # 定义搜索空间 | Define search space
    search_space = {
        'n_layers': [2, 3, 4, 5, 6],  # 层数选择
        'hidden_units': (64, 512),    # 隐藏单元数范围
        'learning_rate': (1e-4, 1e-2), # 学习率范围
        'activation': ['relu', 'tanh', 'sigmoid']  # 激活函数选择
    }
    
    # 创建NAS系统 | Create NAS system
    nas = NeuralArchitectureSearch(
        search_space=search_space,
        performance_estimator=simple_performance_estimator
    )
    
    # 执行搜索 | Perform search
    best_arch, best_perf = nas.search(n_iterations=30)
    
    # 可视化搜索过程 | Visualize search process
    nas.optimizer.plot_optimization_progress()

if __name__ == "__main__":
    nas_demo()
```

---

**🎯 项目完成检查清单 | Project Completion Checklist:**

### 理论理解 | Theoretical Understanding
- [ ] 深入理解高斯过程的数学基础和概率解释
- [ ] 掌握不同核函数的特性和适用场景
- [ ] 理解贝叶斯优化的原理和采集函数设计
- [ ] 能够分析GP模型的不确定性量化能力

### 编程实现 | Programming Implementation
- [ ] 从零实现高斯过程回归算法
- [ ] 设计和组合各种核函数
- [ ] 实现完整的贝叶斯优化框架
- [ ] 掌握数值稳定性和计算效率的优化技巧

### 实际应用 | Practical Applications
- [ ] 在真实数据上获得良好的回归和优化效果
- [ ] 成功应用到超参数优化问题
- [ ] 理解并处理高维和复杂优化场景
- [ ] 分析模型的不确定性和置信区间

**记住**: 高斯过程是机器学习中最优雅的概率模型，它教会AI"知道自己不知道"的谦逊智慧。通过这个项目，你将掌握不确定性建模和贝叶斯优化的核心技术！

**Remember**: Gaussian Processes are the most elegant probabilistic models in machine learning, teaching AI the humble wisdom of "knowing what it doesn't know". Through this project, you will master the core technologies of uncertainty modeling and Bayesian optimization! 