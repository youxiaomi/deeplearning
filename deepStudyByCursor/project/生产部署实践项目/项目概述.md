# 生产部署实践项目概述
# Production Deployment Practice Project Overview

**从实验室到生产环境 - AI系统工程化的完整实践**
**From Laboratory to Production - Complete Practice of AI System Engineering**

---

## 🎯 项目目标 | Project Goals

生产部署是AI技术落地的最后一公里！一个优秀的AI模型只有成功部署到生产环境，才能真正创造价值。通过这个项目，你将掌握：
Production deployment is the last mile for AI technology implementation! An excellent AI model can only create real value when successfully deployed to production environment. Through this project, you will master:

- **模型服务化** | **Model Serving**: Flask/FastAPI等Web服务和容器化部署
- **系统监控** | **System Monitoring**: 性能监控、模型漂移检测等运维技术
- **A/B测试** | **A/B Testing**: 实验设计、效果评估等科学验证方法
- **生产运维** | **Production Operations**: 高可用、可扩展的AI系统架构

## 🔬 为什么生产部署如此重要？| Why is Production Deployment So Important?

**部署决定了AI的商业价值！**
**Deployment determines the business value of AI!**

从OpenAI的API服务支撑数十万开发者，到Tesla的自动驾驶系统每天处理数百万次决策，从Netflix的推荐系统影响数亿用户选择，到Google搜索的毫秒级响应服务全球用户，生产部署技术决定了AI能否真正改变世界。

From OpenAI's API service supporting hundreds of thousands of developers, to Tesla's autonomous driving system processing millions of decisions daily, from Netflix's recommendation system influencing hundreds of millions of users' choices, to Google Search's millisecond response serving global users, production deployment technology determines whether AI can truly change the world.

### 生产部署的发展历程 | Evolution of Production Deployment
```
2010s: 传统Web服务 | Traditional Web Services
2014: Docker容器化 | Docker Containerization
2016: 微服务架构 | Microservices Architecture
2018: Kubernetes编排 | Kubernetes Orchestration
2020: MLOps兴起 | Rise of MLOps
2022: 边缘计算部署 | Edge Computing Deployment
2023: 大模型服务化 | Large Model Serving
```

## 📚 项目结构深度解析 | Deep Project Structure Analysis

### 01_模型服务化 | Model Serving

**让AI模型成为可调用的服务！**
**Turn AI models into callable services!**

#### Flask API服务 | Flask API Service

**项目核心 | Project Core:**
Flask是轻量级的Web框架，非常适合快速构建AI模型的REST API服务，支持模型推理、批量处理等功能。

Flask is a lightweight web framework, very suitable for quickly building REST API services for AI models, supporting model inference, batch processing and other functions.

**完整Flask服务实现 | Complete Flask Service Implementation:**
```python
from flask import Flask, request, jsonify, render_template
from flask_cors import CORS
import torch
import torch.nn as nn
import torchvision.transforms as transforms
from PIL import Image
import io
import base64
import numpy as np
import json
import logging
import time
from datetime import datetime
import threading
import queue
import redis
from werkzeug.middleware.proxy_fix import ProxyFix
import os

# 配置日志 | Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class ModelServer:
    """
    AI模型服务器
    AI Model Server
    """
    def __init__(self, model_path, device='cpu'):
        self.device = torch.device(device)
        self.model = self._load_model(model_path)
        self.model.eval()
        
        # 预处理管道 | Preprocessing pipeline
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
        # 性能统计 | Performance statistics
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'total_inference_time': 0,
            'avg_inference_time': 0,
            'start_time': datetime.now()
        }
        
        # 请求队列（用于批处理）| Request queue (for batch processing)
        self.request_queue = queue.Queue(maxsize=100)
        self.batch_size = 8
        self.batch_timeout = 0.1  # 100ms
        
        # 启动批处理线程 | Start batch processing thread
        self.batch_thread = threading.Thread(target=self._batch_processor, daemon=True)
        self.batch_thread.start()
        
        logger.info(f"模型服务器初始化完成，设备: {self.device}")
        logger.info(f"Model server initialized, device: {self.device}")
    
    def _load_model(self, model_path):
        """
        加载模型
        Load model
        """
        try:
            # 这里可以加载各种类型的模型 | Can load various types of models here
            if model_path.endswith('.pth'):
                model = torch.load(model_path, map_location=self.device)
            elif model_path.endswith('.onnx'):
                import onnxruntime as ort
                model = ort.InferenceSession(model_path)
            else:
                # 简单示例模型 | Simple example model
                model = nn.Sequential(
                    nn.Conv2d(3, 32, 3, padding=1),
                    nn.ReLU(),
                    nn.AdaptiveAvgPool2d((1, 1)),
                    nn.Flatten(),
                    nn.Linear(32, 10)
                )
            
            return model.to(self.device)
        except Exception as e:
            logger.error(f"模型加载失败: {e}")
            logger.error(f"Model loading failed: {e}")
            raise
    
    def _preprocess_image(self, image_data):
        """
        图像预处理
        Image preprocessing
        """
        try:
            # 处理base64编码的图像 | Handle base64 encoded images
            if isinstance(image_data, str):
                if image_data.startswith('data:image'):
                    image_data = image_data.split(',')[1]
                image_data = base64.b64decode(image_data)
            
            # 打开图像 | Open image
            image = Image.open(io.BytesIO(image_data)).convert('RGB')
            
            # 应用变换 | Apply transforms
            tensor = self.transform(image).unsqueeze(0)
            
            return tensor.to(self.device)
        
        except Exception as e:
            logger.error(f"图像预处理失败: {e}")
            logger.error(f"Image preprocessing failed: {e}")
            raise
    
    def predict_single(self, image_data):
        """
        单个图像预测
        Single image prediction
        """
        start_time = time.time()
        
        try:
            # 预处理 | Preprocessing
            input_tensor = self._preprocess_image(image_data)
            
            # 推理 | Inference
            with torch.no_grad():
                outputs = self.model(input_tensor)
                probabilities = torch.softmax(outputs, dim=1)
                predicted_class = torch.argmax(probabilities, dim=1).item()
                confidence = probabilities[0][predicted_class].item()
            
            inference_time = time.time() - start_time
            
            # 更新统计 | Update statistics
            self._update_stats(inference_time, success=True)
            
            result = {
                'predicted_class': predicted_class,
                'confidence': float(confidence),
                'probabilities': probabilities[0].tolist(),
                'inference_time': inference_time
            }
            
            logger.info(f"预测完成: 类别={predicted_class}, 置信度={confidence:.3f}")
            logger.info(f"Prediction completed: class={predicted_class}, confidence={confidence:.3f}")
            
            return result
            
        except Exception as e:
            self._update_stats(0, success=False)
            logger.error(f"预测失败: {e}")
            logger.error(f"Prediction failed: {e}")
            raise
    
    def predict_batch(self, image_list):
        """
        批量图像预测
        Batch image prediction
        """
        start_time = time.time()
        
        try:
            # 预处理所有图像 | Preprocess all images
            input_tensors = []
            for image_data in image_list:
                tensor = self._preprocess_image(image_data)
                input_tensors.append(tensor)
            
            # 合并为批次 | Combine into batch
            batch_tensor = torch.cat(input_tensors, dim=0)
            
            # 批量推理 | Batch inference
            with torch.no_grad():
                outputs = self.model(batch_tensor)
                probabilities = torch.softmax(outputs, dim=1)
                predicted_classes = torch.argmax(probabilities, dim=1)
            
            inference_time = time.time() - start_time
            
            # 构建结果 | Build results
            results = []
            for i in range(len(image_list)):
                result = {
                    'predicted_class': predicted_classes[i].item(),
                    'confidence': probabilities[i][predicted_classes[i]].item(),
                    'probabilities': probabilities[i].tolist()
                }
                results.append(result)
            
            # 更新统计 | Update statistics
            self._update_stats(inference_time, success=True, batch_size=len(image_list))
            
            logger.info(f"批量预测完成: {len(image_list)} 张图像")
            logger.info(f"Batch prediction completed: {len(image_list)} images")
            
            return {
                'results': results,
                'batch_size': len(image_list),
                'total_inference_time': inference_time,
                'avg_inference_time': inference_time / len(image_list)
            }
            
        except Exception as e:
            self._update_stats(0, success=False)
            logger.error(f"批量预测失败: {e}")
            logger.error(f"Batch prediction failed: {e}")
            raise
    
    def _batch_processor(self):
        """
        批处理器线程
        Batch processor thread
        """
        while True:
            batch = []
            start_time = time.time()
            
            # 收集批次 | Collect batch
            try:
                while len(batch) < self.batch_size:
                    timeout = self.batch_timeout - (time.time() - start_time)
                    if timeout <= 0:
                        break
                    
                    try:
                        item = self.request_queue.get(timeout=timeout)
                        batch.append(item)
                    except queue.Empty:
                        break
                
                # 处理批次 | Process batch
                if batch:
                    self._process_batch(batch)
                    
            except Exception as e:
                logger.error(f"批处理错误: {e}")
                logger.error(f"Batch processing error: {e}")
    
    def _process_batch(self, batch):
        """
        处理一个批次
        Process a batch
        """
        try:
            image_list = [item['image'] for item in batch]
            results = self.predict_batch(image_list)
            
            # 返回结果给各个请求 | Return results to individual requests
            for i, item in enumerate(batch):
                result_queue = item['result_queue']
                result_queue.put({
                    'status': 'success',
                    'result': results['results'][i]
                })
                
        except Exception as e:
            # 错误处理 | Error handling
            for item in batch:
                result_queue = item['result_queue']
                result_queue.put({
                    'status': 'error',
                    'error': str(e)
                })
    
    def _update_stats(self, inference_time, success=True, batch_size=1):
        """
        更新性能统计
        Update performance statistics
        """
        self.stats['total_requests'] += batch_size
        
        if success:
            self.stats['successful_requests'] += batch_size
            self.stats['total_inference_time'] += inference_time
            self.stats['avg_inference_time'] = (
                self.stats['total_inference_time'] / self.stats['successful_requests']
            )
        else:
            self.stats['failed_requests'] += batch_size
    
    def get_stats(self):
        """
        获取性能统计
        Get performance statistics
        """
        uptime = datetime.now() - self.stats['start_time']
        
        return {
            **self.stats,
            'uptime_seconds': uptime.total_seconds(),
            'requests_per_second': self.stats['total_requests'] / uptime.total_seconds() if uptime.total_seconds() > 0 else 0,
            'success_rate': self.stats['successful_requests'] / self.stats['total_requests'] if self.stats['total_requests'] > 0 else 0
        }

# 创建Flask应用 | Create Flask application
app = Flask(__name__)
CORS(app)  # 允许跨域请求 | Allow cross-origin requests
app.wsgi_app = ProxyFix(app.wsgi_app)  # 代理支持 | Proxy support

# 初始化模型服务器 | Initialize model server
model_server = None

def init_model_server():
    """初始化模型服务器 | Initialize model server"""
    global model_server
    try:
        model_path = os.getenv('MODEL_PATH', 'model.pth')
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        model_server = ModelServer(model_path, device)
        logger.info("模型服务器初始化成功")
        logger.info("Model server initialized successfully")
    except Exception as e:
        logger.error(f"模型服务器初始化失败: {e}")
        logger.error(f"Model server initialization failed: {e}")
        raise

@app.route('/', methods=['GET'])
def home():
    """
    首页
    Home page
    """
    return render_template('index.html')

@app.route('/health', methods=['GET'])
def health_check():
    """
    健康检查
    Health check
    """
    try:
        stats = model_server.get_stats() if model_server else {}
        return jsonify({
            'status': 'healthy',
            'timestamp': datetime.now().isoformat(),
            'stats': stats
        })
    except Exception as e:
        return jsonify({
            'status': 'unhealthy',
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }), 500

@app.route('/predict', methods=['POST'])
def predict():
    """
    单图像预测接口
    Single image prediction endpoint
    """
    try:
        # 检查模型服务器 | Check model server
        if not model_server:
            return jsonify({'error': '模型服务器未初始化'}), 500
        
        # 获取请求数据 | Get request data
        if 'image' not in request.files and 'image' not in request.json:
            return jsonify({'error': '缺少图像数据'}), 400
        
        # 处理文件上传 | Handle file upload
        if 'image' in request.files:
            file = request.files['image']
            if file.filename == '':
                return jsonify({'error': '未选择文件'}), 400
            image_data = file.read()
        else:
            # 处理JSON中的base64图像 | Handle base64 image in JSON
            image_data = request.json['image']
        
        # 执行预测 | Perform prediction
        result = model_server.predict_single(image_data)
        
        return jsonify({
            'status': 'success',
            'result': result,
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"预测请求失败: {e}")
        logger.error(f"Prediction request failed: {e}")
        return jsonify({
            'status': 'error',
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }), 500

@app.route('/predict/batch', methods=['POST'])
def predict_batch():
    """
    批量预测接口
    Batch prediction endpoint
    """
    try:
        if not model_server:
            return jsonify({'error': '模型服务器未初始化'}), 500
        
        # 获取图像列表 | Get image list
        data = request.get_json()
        if 'images' not in data:
            return jsonify({'error': '缺少图像数据'}), 400
        
        image_list = data['images']
        if len(image_list) == 0:
            return jsonify({'error': '图像列表为空'}), 400
        
        if len(image_list) > 32:  # 限制批量大小 | Limit batch size
            return jsonify({'error': '批量大小超过限制(32)'}), 400
        
        # 执行批量预测 | Perform batch prediction
        result = model_server.predict_batch(image_list)
        
        return jsonify({
            'status': 'success',
            'result': result,
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"批量预测请求失败: {e}")
        logger.error(f"Batch prediction request failed: {e}")
        return jsonify({
            'status': 'error',
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }), 500

@app.route('/stats', methods=['GET'])
def get_stats():
    """
    获取服务统计信息
    Get service statistics
    """
    try:
        if not model_server:
            return jsonify({'error': '模型服务器未初始化'}), 500
        
        stats = model_server.get_stats()
        
        return jsonify({
            'status': 'success',
            'stats': stats,
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        return jsonify({
            'status': 'error',
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }), 500

@app.route('/model/info', methods=['GET'])
def model_info():
    """
    获取模型信息
    Get model information
    """
    try:
        if not model_server:
            return jsonify({'error': '模型服务器未初始化'}), 500
        
        # 计算模型参数数量 | Calculate model parameters
        total_params = sum(p.numel() for p in model_server.model.parameters())
        trainable_params = sum(p.numel() for p in model_server.model.parameters() if p.requires_grad)
        
        model_info = {
            'model_type': type(model_server.model).__name__,
            'device': str(model_server.device),
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'model_size_mb': total_params * 4 / (1024 * 1024),  # 假设float32
        }
        
        return jsonify({
            'status': 'success',
            'model_info': model_info,
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        return jsonify({
            'status': 'error',
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }), 500

@app.errorhandler(404)
def not_found(error):
    """404错误处理 | 404 error handler"""
    return jsonify({'error': '接口不存在'}), 404

@app.errorhandler(500)
def internal_error(error):
    """500错误处理 | 500 error handler"""
    return jsonify({'error': '内部服务器错误'}), 500

# HTML模板 | HTML template
HTML_TEMPLATE = '''
<!DOCTYPE html>
<html>
<head>
    <title>AI Model API Server</title>
    <meta charset="utf-8">
    <style>
        body { font-family: Arial, sans-serif; margin: 40px; }
        .container { max-width: 800px; margin: 0 auto; }
        .api-section { margin: 20px 0; padding: 20px; border: 1px solid #ddd; }
        .stats { background: #f5f5f5; padding: 15px; margin: 10px 0; }
        button { padding: 10px 20px; margin: 5px; }
        input[type="file"] { margin: 10px 0; }
        .result { margin: 10px 0; padding: 10px; background: #e8f5e8; }
        .error { background: #ffe8e8; }
    </style>
</head>
<body>
    <div class="container">
        <h1>🤖 AI Model API Server</h1>
        
        <div class="api-section">
            <h2>📊 Server Stats</h2>
            <button onclick="loadStats()">刷新统计</button>
            <div id="stats" class="stats">点击刷新查看统计信息</div>
        </div>
        
        <div class="api-section">
            <h2>🔮 Image Prediction</h2>
            <input type="file" id="imageFile" accept="image/*">
            <button onclick="predictImage()">预测</button>
            <div id="prediction-result" class="result" style="display:none;"></div>
        </div>
        
        <div class="api-section">
            <h2>📱 API Endpoints</h2>
            <ul>
                <li><strong>GET /health</strong> - 健康检查</li>
                <li><strong>POST /predict</strong> - 单图像预测</li>
                <li><strong>POST /predict/batch</strong> - 批量预测</li>
                <li><strong>GET /stats</strong> - 服务统计</li>
                <li><strong>GET /model/info</strong> - 模型信息</li>
            </ul>
        </div>
    </div>

    <script>
        function loadStats() {
            fetch('/stats')
                .then(response => response.json())
                .then(data => {
                    if (data.status === 'success') {
                        const stats = data.stats;
                        document.getElementById('stats').innerHTML = `
                            <strong>Service Statistics:</strong><br>
                            Total Requests: ${stats.total_requests}<br>
                            Successful: ${stats.successful_requests}<br>
                            Failed: ${stats.failed_requests}<br>
                            Success Rate: ${(stats.success_rate * 100).toFixed(1)}%<br>
                            Avg Inference Time: ${stats.avg_inference_time.toFixed(3)}s<br>
                            Uptime: ${stats.uptime_seconds.toFixed(0)}s<br>
                            RPS: ${stats.requests_per_second.toFixed(2)}
                        `;
                    }
                })
                .catch(error => {
                    document.getElementById('stats').innerHTML = 'Error loading stats: ' + error;
                });
        }
        
        function predictImage() {
            const fileInput = document.getElementById('imageFile');
            const resultDiv = document.getElementById('prediction-result');
            
            if (!fileInput.files[0]) {
                alert('请选择图像文件');
                return;
            }
            
            const formData = new FormData();
            formData.append('image', fileInput.files[0]);
            
            fetch('/predict', {
                method: 'POST',
                body: formData
            })
            .then(response => response.json())
            .then(data => {
                resultDiv.style.display = 'block';
                if (data.status === 'success') {
                    const result = data.result;
                    resultDiv.className = 'result';
                    resultDiv.innerHTML = `
                        <strong>Prediction Result:</strong><br>
                        Predicted Class: ${result.predicted_class}<br>
                        Confidence: ${(result.confidence * 100).toFixed(1)}%<br>
                        Inference Time: ${result.inference_time.toFixed(3)}s
                    `;
                } else {
                    resultDiv.className = 'result error';
                    resultDiv.innerHTML = 'Error: ' + data.error;
                }
            })
            .catch(error => {
                resultDiv.style.display = 'block';
                resultDiv.className = 'result error';
                resultDiv.innerHTML = 'Request failed: ' + error;
            });
        }
        
        // 页面加载时自动刷新统计 | Auto refresh stats on page load
        window.onload = function() {
            loadStats();
        };
    </script>
</body>
</html>
'''

# 创建模板目录和文件 | Create template directory and file
if not os.path.exists('templates'):
    os.makedirs('templates')

with open('templates/index.html', 'w', encoding='utf-8') as f:
    f.write(HTML_TEMPLATE)

if __name__ == '__main__':
    # 初始化模型服务器 | Initialize model server
    init_model_server()
    
    # 启动Flask应用 | Start Flask application
    port = int(os.getenv('PORT', 5000))
    debug = os.getenv('DEBUG', 'False').lower() == 'true'
    
    logger.info(f"启动AI模型API服务器，端口: {port}")
    logger.info(f"Starting AI model API server on port: {port}")
    
    app.run(host='0.0.0.0', port=port, debug=debug, threaded=True)
```

#### Docker容器化 | Docker Containerization

**项目价值 | Project Value:**
Docker容器化使AI服务具备环境一致性、快速部署、易于扩展等优势，是现代云原生应用的标准实践。

Docker containerization provides AI services with environment consistency, fast deployment, easy scaling and other advantages, being the standard practice for modern cloud-native applications.

**完整Docker化方案 | Complete Dockerization Solution:**

**Dockerfile:**
```dockerfile
# 多阶段构建 | Multi-stage build
FROM python:3.9-slim as builder

# 设置工作目录 | Set working directory
WORKDIR /app

# 安装系统依赖 | Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    curl \
    && rm -rf /var/lib/apt/lists/*

# 复制requirements文件 | Copy requirements file
COPY requirements.txt .

# 安装Python依赖 | Install Python dependencies
RUN pip install --no-cache-dir --user -r requirements.txt

# 生产阶段 | Production stage
FROM python:3.9-slim

# 创建非root用户 | Create non-root user
RUN useradd --create-home --shell /bin/bash app

# 设置工作目录 | Set working directory
WORKDIR /app

# 安装运行时依赖 | Install runtime dependencies
RUN apt-get update && apt-get install -y \
    curl \
    && rm -rf /var/lib/apt/lists/*

# 从builder阶段复制Python包 | Copy Python packages from builder stage
COPY --from=builder /root/.local /home/app/.local

# 复制应用代码 | Copy application code
COPY --chown=app:app . .

# 设置环境变量 | Set environment variables
ENV PATH=/home/app/.local/bin:$PATH
ENV PYTHONUNBUFFERED=1
ENV MODEL_PATH=/app/models/model.pth
ENV PORT=5000

# 创建必要目录 | Create necessary directories
RUN mkdir -p /app/models /app/logs /app/data && \
    chown -R app:app /app

# 切换到app用户 | Switch to app user
USER app

# 健康检查 | Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:$PORT/health || exit 1

# 暴露端口 | Expose port
EXPOSE $PORT

# 启动命令 | Start command
CMD ["python", "app.py"]
```

**requirements.txt:**
```txt
torch>=1.12.0
torchvision>=0.13.0
flask>=2.2.0
flask-cors>=3.0.10
pillow>=9.0.0
numpy>=1.21.0
redis>=4.3.0
gunicorn>=20.1.0
requests>=2.28.0
prometheus-client>=0.14.0
```

**docker-compose.yml:**
```yaml
version: '3.8'

services:
  # AI模型服务 | AI Model Service
  ai-model-api:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "5000:5000"
    environment:
      - MODEL_PATH=/app/models/model.pth
      - REDIS_URL=redis://redis:6379
      - DEBUG=false
    volumes:
      - ./models:/app/models:ro
      - ./logs:/app/logs
    depends_on:
      - redis
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 1G
          cpus: '0.5'
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Redis缓存 | Redis Cache
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
    restart: unless-stopped

  # Nginx负载均衡器 | Nginx Load Balancer
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - ./ssl:/etc/nginx/ssl:ro
    depends_on:
      - ai-model-api
    restart: unless-stopped

  # Prometheus监控 | Prometheus Monitoring
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
    restart: unless-stopped

  # Grafana可视化 | Grafana Visualization
  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin123
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./grafana/datasources:/etc/grafana/provisioning/datasources:ro
    depends_on:
      - prometheus
    restart: unless-stopped

volumes:
  redis_data:
  prometheus_data:
  grafana_data:

networks:
  default:
    driver: bridge
```

**部署脚本 | Deployment Scripts:**
```bash
#!/bin/bash
# deploy.sh - 部署脚本

set -e

echo "🚀 开始部署AI模型服务..."
echo "🚀 Starting AI model service deployment..."

# 检查Docker和docker-compose
echo "检查Docker环境..."
if ! command -v docker &> /dev/null; then
    echo "❌ Docker未安装"
    exit 1
fi

if ! command -v docker-compose &> /dev/null; then
    echo "❌ docker-compose未安装"
    exit 1
fi

# 创建必要目录
echo "创建目录结构..."
mkdir -p models logs ssl grafana/dashboards grafana/datasources

# 构建镜像
echo "🔨 构建Docker镜像..."
docker-compose build

# 启动服务
echo "🌟 启动服务..."
docker-compose up -d

# 等待服务启动
echo "⏳ 等待服务启动..."
sleep 30

# 健康检查
echo "🔍 检查服务健康状态..."
for service in ai-model-api redis nginx prometheus grafana; do
    if docker-compose ps $service | grep -q "Up"; then
        echo "✅ $service 运行正常"
    else
        echo "❌ $service 启动失败"
        docker-compose logs $service
        exit 1
    fi
done

# 测试API
echo "🧪 测试API端点..."
if curl -f http://localhost/health > /dev/null 2>&1; then
    echo "✅ API服务正常"
else
    echo "❌ API服务异常"
    exit 1
fi

echo "🎉 部署完成！"
echo "📱 服务地址:"
echo "  - API服务: http://localhost"
echo "  - Prometheus: http://localhost:9090"
echo "  - Grafana: http://localhost:3000 (admin/admin123)"

echo "📊 查看服务状态: docker-compose ps"
echo "📋 查看日志: docker-compose logs -f"
echo "🛑 停止服务: docker-compose down"
```

### 02_模型监控 | Model Monitoring

**让AI系统状态一目了然！**
**Make AI system status clear at a glance!**

#### 性能监控 | Performance Monitoring

**项目核心 | Project Core:**
性能监控通过收集系统指标、响应时间、错误率等数据，实时监控AI服务的健康状态和性能表现。

Performance monitoring collects system metrics, response times, error rates and other data to monitor the health and performance of AI services in real-time.

**Prometheus监控集成 | Prometheus Monitoring Integration:**
```python
from prometheus_client import Counter, Histogram, Gauge, start_http_server
import time
import threading
import psutil
import torch

class ModelMonitor:
    """
    模型性能监控器
    Model Performance Monitor
    """
    def __init__(self, model_server, port=8000):
        self.model_server = model_server
        self.port = port
        
        # Prometheus指标 | Prometheus metrics
        self.request_count = Counter(
            'ai_model_requests_total', 
            'Total number of requests',
            ['method', 'status']
        )
        
        self.request_duration = Histogram(
            'ai_model_request_duration_seconds',
            'Request duration in seconds',
            ['method']
        )
        
        self.inference_duration = Histogram(
            'ai_model_inference_duration_seconds',
            'Model inference duration in seconds'
        )
        
        self.batch_size = Histogram(
            'ai_model_batch_size',
            'Batch size for predictions'
        )
        
        # 系统指标 | System metrics
        self.cpu_usage = Gauge('system_cpu_usage_percent', 'CPU usage percentage')
        self.memory_usage = Gauge('system_memory_usage_bytes', 'Memory usage in bytes')
        self.gpu_usage = Gauge('system_gpu_usage_percent', 'GPU usage percentage')
        self.gpu_memory = Gauge('system_gpu_memory_bytes', 'GPU memory usage in bytes')
        
        # 模型指标 | Model metrics
        self.model_accuracy = Gauge('ai_model_accuracy', 'Model accuracy')
        self.prediction_confidence = Histogram(
            'ai_model_prediction_confidence',
            'Prediction confidence scores'
        )
        
        # 启动指标服务器 | Start metrics server
        start_http_server(self.port)
        
        # 启动系统监控线程 | Start system monitoring thread
        self.monitoring_thread = threading.Thread(target=self._monitor_system, daemon=True)
        self.monitoring_thread.start()
        
        print(f"性能监控服务启动在端口 {self.port}")
        print(f"Performance monitoring service started on port {self.port}")
    
    def record_request(self, method, status, duration):
        """
        记录请求指标
        Record request metrics
        """
        self.request_count.labels(method=method, status=status).inc()
        self.request_duration.labels(method=method).observe(duration)
    
    def record_inference(self, duration, batch_size=1, confidence=None):
        """
        记录推理指标
        Record inference metrics
        """
        self.inference_duration.observe(duration)
        self.batch_size.observe(batch_size)
        
        if confidence is not None:
            self.prediction_confidence.observe(confidence)
    
    def update_model_accuracy(self, accuracy):
        """
        更新模型准确率
        Update model accuracy
        """
        self.model_accuracy.set(accuracy)
    
    def _monitor_system(self):
        """
        系统资源监控
        System resource monitoring
        """
        while True:
            try:
                # CPU使用率 | CPU usage
                cpu_percent = psutil.cpu_percent(interval=1)
                self.cpu_usage.set(cpu_percent)
                
                # 内存使用 | Memory usage
                memory = psutil.virtual_memory()
                self.memory_usage.set(memory.used)
                
                # GPU监控 | GPU monitoring
                if torch.cuda.is_available():
                    for i in range(torch.cuda.device_count()):
                        # GPU使用率 | GPU utilization
                        gpu_usage = torch.cuda.utilization(i)
                        self.gpu_usage.set(gpu_usage)
                        
                        # GPU内存 | GPU memory
                        gpu_memory = torch.cuda.memory_allocated(i)
                        self.gpu_memory.set(gpu_memory)
                
                time.sleep(10)  # 每10秒更新一次 | Update every 10 seconds
                
            except Exception as e:
                print(f"系统监控错误: {e}")
                print(f"System monitoring error: {e}")
                time.sleep(10)

# 集成监控到Flask应用 | Integrate monitoring into Flask app
def create_monitored_app():
    """
    创建带监控的Flask应用
    Create Flask app with monitoring
    """
    app = Flask(__name__)
    
    # 初始化监控 | Initialize monitoring
    monitor = ModelMonitor(model_server)
    
    @app.before_request
    def before_request():
        """请求开始时间记录 | Record request start time"""
        request.start_time = time.time()
    
    @app.after_request
    def after_request(response):
        """请求结束指标记录 | Record request end metrics"""
        if hasattr(request, 'start_time'):
            duration = time.time() - request.start_time
            method = request.endpoint or 'unknown'
            status = 'success' if response.status_code < 400 else 'error'
            
            monitor.record_request(method, status, duration)
        
        return response
    
    return app, monitor
```

#### 模型漂移检测 | Model Drift Detection

**项目特色 | Project Features:**
模型漂移检测通过监控数据分布变化和模型性能退化，及时发现模型在生产环境中的质量问题。

Model drift detection monitors data distribution changes and model performance degradation to timely discover model quality issues in production environments.

**漂移检测实现 | Drift Detection Implementation:**
```python
import numpy as np
from scipy import stats
from sklearn.metrics import accuracy_score, precision_score, recall_score
import json
import sqlite3
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

class ModelDriftDetector:
    """
    模型漂移检测器
    Model Drift Detector
    """
    def __init__(self, reference_data, db_path='model_monitoring.db'):
        self.reference_data = reference_data
        self.db_path = db_path
        
        # 初始化数据库 | Initialize database
        self._init_database()
        
        # 漂移阈值 | Drift thresholds
        self.thresholds = {
            'data_drift': 0.05,  # KS test p-value
            'performance_drift': 0.1,  # 性能下降阈值
            'prediction_drift': 0.05  # 预测分布变化阈值
        }
        
        # 计算参考数据统计 | Calculate reference data statistics
        self.reference_stats = self._calculate_reference_stats()
        
        print("模型漂移检测器初始化完成")
        print("Model drift detector initialized")
    
    def _init_database(self):
        """
        初始化监控数据库
        Initialize monitoring database
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # 创建表 | Create tables
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS drift_monitoring (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp DATETIME,
                metric_type TEXT,
                metric_name TEXT,
                value REAL,
                threshold REAL,
                is_drift BOOLEAN,
                severity TEXT
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS model_performance (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp DATETIME,
                accuracy REAL,
                precision REAL,
                recall REAL,
                avg_confidence REAL,
                prediction_count INTEGER
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def _calculate_reference_stats(self):
        """
        计算参考数据统计信息
        Calculate reference data statistics
        """
        stats = {}
        
        if isinstance(self.reference_data, dict):
            for feature_name, feature_data in self.reference_data.items():
                stats[feature_name] = {
                    'mean': np.mean(feature_data),
                    'std': np.std(feature_data),
                    'min': np.min(feature_data),
                    'max': np.max(feature_data),
                    'quantiles': np.percentile(feature_data, [25, 50, 75])
                }
        
        return stats
    
    def detect_data_drift(self, current_data, feature_names=None):
        """
        检测数据漂移
        Detect data drift
        """
        drift_results = {}
        
        if feature_names is None:
            feature_names = list(self.reference_data.keys())
        
        for feature_name in feature_names:
            if feature_name not in self.reference_data:
                continue
            
            reference_values = self.reference_data[feature_name]
            current_values = current_data.get(feature_name, [])
            
            if len(current_values) == 0:
                continue
            
            # Kolmogorov-Smirnov测试 | Kolmogorov-Smirnov test
            ks_statistic, p_value = stats.ks_2samp(reference_values, current_values)
            
            # 判断是否漂移 | Determine if drift occurs
            is_drift = p_value < self.thresholds['data_drift']
            severity = self._get_drift_severity(p_value, self.thresholds['data_drift'])
            
            drift_results[feature_name] = {
                'ks_statistic': ks_statistic,
                'p_value': p_value,
                'is_drift': is_drift,
                'severity': severity,
                'threshold': self.thresholds['data_drift']
            }
            
            # 记录到数据库 | Record to database
            self._log_drift_metric(
                'data_drift', feature_name, p_value, 
                self.thresholds['data_drift'], is_drift, severity
            )
        
        return drift_results
    
    def detect_prediction_drift(self, current_predictions, reference_predictions=None):
        """
        检测预测漂移
        Detect prediction drift
        """
        if reference_predictions is None:
            # 使用历史预测数据 | Use historical prediction data
            reference_predictions = self._get_recent_predictions(days=7)
        
        if len(reference_predictions) == 0 or len(current_predictions) == 0:
            return None
        
        # 比较预测分布 | Compare prediction distributions
        ks_statistic, p_value = stats.ks_2samp(reference_predictions, current_predictions)
        
        is_drift = p_value < self.thresholds['prediction_drift']
        severity = self._get_drift_severity(p_value, self.thresholds['prediction_drift'])
        
        result = {
            'ks_statistic': ks_statistic,
            'p_value': p_value,
            'is_drift': is_drift,
            'severity': severity,
            'threshold': self.thresholds['prediction_drift'],
            'current_mean': np.mean(current_predictions),
            'reference_mean': np.mean(reference_predictions),
            'current_std': np.std(current_predictions),
            'reference_std': np.std(reference_predictions)
        }
        
        # 记录到数据库 | Record to database
        self._log_drift_metric(
            'prediction_drift', 'predictions', p_value,
            self.thresholds['prediction_drift'], is_drift, severity
        )
        
        return result
    
    def detect_performance_drift(self, current_performance, reference_performance=None):
        """
        检测性能漂移
        Detect performance drift
        """
        if reference_performance is None:
            # 使用历史性能数据 | Use historical performance data
            reference_performance = self._get_recent_performance(days=7)
        
        if reference_performance is None:
            return None
        
        # 计算性能下降 | Calculate performance degradation
        performance_drop = reference_performance - current_performance
        relative_drop = performance_drop / reference_performance if reference_performance > 0 else 0
        
        is_drift = relative_drop > self.thresholds['performance_drift']
        severity = self._get_performance_severity(relative_drop)
        
        result = {
            'current_performance': current_performance,
            'reference_performance': reference_performance,
            'absolute_drop': performance_drop,
            'relative_drop': relative_drop,
            'is_drift': is_drift,
            'severity': severity,
            'threshold': self.thresholds['performance_drift']
        }
        
        # 记录到数据库 | Record to database
        self._log_drift_metric(
            'performance_drift', 'accuracy', relative_drop,
            self.thresholds['performance_drift'], is_drift, severity
        )
        
        return result
    
    def log_model_performance(self, accuracy, precision, recall, avg_confidence, prediction_count):
        """
        记录模型性能
        Log model performance
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO model_performance 
            (timestamp, accuracy, precision, recall, avg_confidence, prediction_count)
            VALUES (?, ?, ?, ?, ?, ?)
        ''', (datetime.now(), accuracy, precision, recall, avg_confidence, prediction_count))
        
        conn.commit()
        conn.close()
    
    def get_drift_summary(self, days=7):
        """
        获取漂移检测摘要
        Get drift detection summary
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # 获取最近的漂移检测结果 | Get recent drift detection results
        cursor.execute('''
            SELECT metric_type, metric_name, is_drift, severity, COUNT(*) as count
            FROM drift_monitoring
            WHERE timestamp > datetime('now', '-{} days')
            GROUP BY metric_type, metric_name, is_drift, severity
        '''.format(days))
        
        drift_summary = cursor.fetchall()
        
        # 获取性能趋势 | Get performance trends
        cursor.execute('''
            SELECT timestamp, accuracy, precision, recall
            FROM model_performance
            WHERE timestamp > datetime('now', '-{} days')
            ORDER BY timestamp
        '''.format(days))
        
        performance_history = cursor.fetchall()
        
        conn.close()
        
        return {
            'drift_summary': drift_summary,
            'performance_history': performance_history
        }
    
    def _get_drift_severity(self, p_value, threshold):
        """
        计算漂移严重程度
        Calculate drift severity
        """
        if p_value >= threshold:
            return 'none'
        elif p_value >= threshold / 2:
            return 'low'
        elif p_value >= threshold / 10:
            return 'medium'
        else:
            return 'high'
    
    def _get_performance_severity(self, relative_drop):
        """
        计算性能下降严重程度
        Calculate performance degradation severity
        """
        if relative_drop <= 0.05:
            return 'none'
        elif relative_drop <= 0.1:
            return 'low'
        elif relative_drop <= 0.2:
            return 'medium'
        else:
            return 'high'
    
    def _log_drift_metric(self, metric_type, metric_name, value, threshold, is_drift, severity):
        """
        记录漂移指标到数据库
        Log drift metric to database
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO drift_monitoring 
            (timestamp, metric_type, metric_name, value, threshold, is_drift, severity)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        ''', (datetime.now(), metric_type, metric_name, value, threshold, is_drift, severity))
        
        conn.commit()
        conn.close()
    
    def _get_recent_predictions(self, days=7):
        """
        获取最近的预测数据
        Get recent prediction data
        """
        # 这里应该从实际的预测日志中获取数据
        # This should get data from actual prediction logs
        return []
    
    def _get_recent_performance(self, days=7):
        """
        获取最近的性能数据
        Get recent performance data
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT AVG(accuracy) as avg_accuracy
            FROM model_performance
            WHERE timestamp > datetime('now', '-{} days')
        '''.format(days))
        
        result = cursor.fetchone()
        conn.close()
        
        return result[0] if result and result[0] is not None else None

def drift_detection_demo():
    """
    漂移检测演示
    Drift detection demo
    """
    print("=== 模型漂移检测演示 | Model Drift Detection Demo ===")
    
    # 生成参考数据 | Generate reference data
    np.random.seed(42)
    reference_data = {
        'feature_1': np.random.normal(0, 1, 1000),
        'feature_2': np.random.exponential(2, 1000),
        'feature_3': np.random.uniform(-1, 1, 1000)
    }
    
    # 创建漂移检测器 | Create drift detector
    detector = ModelDriftDetector(reference_data)
    
    # 模拟当前数据（有漂移）| Simulate current data (with drift)
    current_data = {
        'feature_1': np.random.normal(0.5, 1.2, 500),  # 均值和方差都有变化
        'feature_2': np.random.exponential(1.5, 500),   # 参数变化
        'feature_3': np.random.uniform(-0.5, 1.5, 500)  # 范围变化
    }
    
    # 检测数据漂移 | Detect data drift
    print("1. 数据漂移检测 | Data Drift Detection")
    data_drift_results = detector.detect_data_drift(current_data)
    
    for feature, result in data_drift_results.items():
        print(f"  {feature}:")
        print(f"    KS统计量: {result['ks_statistic']:.4f}")
        print(f"    p值: {result['p_value']:.4f}")
        print(f"    是否漂移: {'是' if result['is_drift'] else '否'}")
        print(f"    严重程度: {result['severity']}")
    
    # 模拟预测漂移 | Simulate prediction drift
    print("\n2. 预测漂移检测 | Prediction Drift Detection")
    reference_predictions = np.random.beta(2, 5, 1000)  # 参考预测分布
    current_predictions = np.random.beta(3, 3, 500)     # 当前预测分布（不同）
    
    prediction_drift = detector.detect_prediction_drift(
        current_predictions, reference_predictions
    )
    
    if prediction_drift:
        print(f"  KS统计量: {prediction_drift['ks_statistic']:.4f}")
        print(f"  p值: {prediction_drift['p_value']:.4f}")
        print(f"  是否漂移: {'是' if prediction_drift['is_drift'] else '否'}")
        print(f"  严重程度: {prediction_drift['severity']}")
    
    # 模拟性能漂移 | Simulate performance drift
    print("\n3. 性能漂移检测 | Performance Drift Detection")
    reference_accuracy = 0.95
    current_accuracy = 0.82  # 性能下降
    
    performance_drift = detector.detect_performance_drift(
        current_accuracy, reference_accuracy
    )
    
    if performance_drift:
        print(f"  当前性能: {performance_drift['current_performance']:.3f}")
        print(f"  参考性能: {performance_drift['reference_performance']:.3f}")
        print(f"  相对下降: {performance_drift['relative_drop']:.3f}")
        print(f"  是否漂移: {'是' if performance_drift['is_drift'] else '否'}")
        print(f"  严重程度: {performance_drift['severity']}")
    
    # 获取漂移摘要 | Get drift summary
    print("\n4. 漂移检测摘要 | Drift Detection Summary")
    summary = detector.get_drift_summary(days=1)
    
    print("  漂移检测记录:")
    for record in summary['drift_summary']:
        print(f"    {record[0]} - {record[1]}: {record[4]} 次")
    
    return detector

if __name__ == "__main__":
    drift_detection_demo()
```

---

**🎯 项目完成检查清单 | Project Completion Checklist:**

### 部署技术理解 | Deployment Technology Understanding
- [ ] 深入理解Flask/FastAPI等Web框架的服务化模式
- [ ] 掌握Docker容器化的原理和最佳实践
- [ ] 理解微服务架构和负载均衡的设计
- [ ] 能够设计高可用、可扩展的AI服务架构

### 监控运维能力 | Monitoring and Operations Capability
- [ ] 实现完整的性能监控和指标收集系统
- [ ] 掌握模型漂移检测的理论和实践方法
- [ ] 能够设计和实施A/B测试验证框架
- [ ] 具备生产环境的故障诊断和问题解决能力

### 工程实践技能 | Engineering Practice Skills
- [ ] 能够将AI模型部署到云平台和边缘设备
- [ ] 掌握CI/CD流水线的设计和实现
- [ ] 理解安全性、可靠性、可维护性的工程要求
- [ ] 具备端到端的AI系统设计和交付能力

**记住**: 生产部署是AI技术创造价值的最后一公里。通过这个项目，你将掌握让AI系统在生产环境中稳定、高效、可靠运行的全套技术！

**Remember**: Production deployment is the last mile for AI technology to create value. Through this project, you will master the complete set of technologies to make AI systems run stably, efficiently, and reliably in production environments! 