# ç”Ÿäº§éƒ¨ç½²å®è·µé¡¹ç›®æ¦‚è¿°
# Production Deployment Practice Project Overview

**ä»å®éªŒå®¤åˆ°ç”Ÿäº§ç¯å¢ƒ - AIç³»ç»Ÿå·¥ç¨‹åŒ–çš„å®Œæ•´å®è·µ**
**From Laboratory to Production - Complete Practice of AI System Engineering**

---

## ğŸ¯ é¡¹ç›®ç›®æ ‡ | Project Goals

ç”Ÿäº§éƒ¨ç½²æ˜¯AIæŠ€æœ¯è½åœ°çš„æœ€åä¸€å…¬é‡Œï¼ä¸€ä¸ªä¼˜ç§€çš„AIæ¨¡å‹åªæœ‰æˆåŠŸéƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒï¼Œæ‰èƒ½çœŸæ­£åˆ›é€ ä»·å€¼ã€‚é€šè¿‡è¿™ä¸ªé¡¹ç›®ï¼Œä½ å°†æŒæ¡ï¼š
Production deployment is the last mile for AI technology implementation! An excellent AI model can only create real value when successfully deployed to production environment. Through this project, you will master:

- **æ¨¡å‹æœåŠ¡åŒ–** | **Model Serving**: Flask/FastAPIç­‰WebæœåŠ¡å’Œå®¹å™¨åŒ–éƒ¨ç½²
- **ç³»ç»Ÿç›‘æ§** | **System Monitoring**: æ€§èƒ½ç›‘æ§ã€æ¨¡å‹æ¼‚ç§»æ£€æµ‹ç­‰è¿ç»´æŠ€æœ¯
- **A/Bæµ‹è¯•** | **A/B Testing**: å®éªŒè®¾è®¡ã€æ•ˆæœè¯„ä¼°ç­‰ç§‘å­¦éªŒè¯æ–¹æ³•
- **ç”Ÿäº§è¿ç»´** | **Production Operations**: é«˜å¯ç”¨ã€å¯æ‰©å±•çš„AIç³»ç»Ÿæ¶æ„

## ğŸ”¬ ä¸ºä»€ä¹ˆç”Ÿäº§éƒ¨ç½²å¦‚æ­¤é‡è¦ï¼Ÿ| Why is Production Deployment So Important?

**éƒ¨ç½²å†³å®šäº†AIçš„å•†ä¸šä»·å€¼ï¼**
**Deployment determines the business value of AI!**

ä»OpenAIçš„APIæœåŠ¡æ”¯æ’‘æ•°åä¸‡å¼€å‘è€…ï¼Œåˆ°Teslaçš„è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿæ¯å¤©å¤„ç†æ•°ç™¾ä¸‡æ¬¡å†³ç­–ï¼Œä»Netflixçš„æ¨èç³»ç»Ÿå½±å“æ•°äº¿ç”¨æˆ·é€‰æ‹©ï¼Œåˆ°Googleæœç´¢çš„æ¯«ç§’çº§å“åº”æœåŠ¡å…¨çƒç”¨æˆ·ï¼Œç”Ÿäº§éƒ¨ç½²æŠ€æœ¯å†³å®šäº†AIèƒ½å¦çœŸæ­£æ”¹å˜ä¸–ç•Œã€‚

From OpenAI's API service supporting hundreds of thousands of developers, to Tesla's autonomous driving system processing millions of decisions daily, from Netflix's recommendation system influencing hundreds of millions of users' choices, to Google Search's millisecond response serving global users, production deployment technology determines whether AI can truly change the world.

### ç”Ÿäº§éƒ¨ç½²çš„å‘å±•å†ç¨‹ | Evolution of Production Deployment
```
2010s: ä¼ ç»ŸWebæœåŠ¡ | Traditional Web Services
2014: Dockerå®¹å™¨åŒ– | Docker Containerization
2016: å¾®æœåŠ¡æ¶æ„ | Microservices Architecture
2018: Kubernetesç¼–æ’ | Kubernetes Orchestration
2020: MLOpså…´èµ· | Rise of MLOps
2022: è¾¹ç¼˜è®¡ç®—éƒ¨ç½² | Edge Computing Deployment
2023: å¤§æ¨¡å‹æœåŠ¡åŒ– | Large Model Serving
```

## ğŸ“š é¡¹ç›®ç»“æ„æ·±åº¦è§£æ | Deep Project Structure Analysis

### 01_æ¨¡å‹æœåŠ¡åŒ– | Model Serving

**è®©AIæ¨¡å‹æˆä¸ºå¯è°ƒç”¨çš„æœåŠ¡ï¼**
**Turn AI models into callable services!**

#### Flask APIæœåŠ¡ | Flask API Service

**é¡¹ç›®æ ¸å¿ƒ | Project Core:**
Flaskæ˜¯è½»é‡çº§çš„Webæ¡†æ¶ï¼Œéå¸¸é€‚åˆå¿«é€Ÿæ„å»ºAIæ¨¡å‹çš„REST APIæœåŠ¡ï¼Œæ”¯æŒæ¨¡å‹æ¨ç†ã€æ‰¹é‡å¤„ç†ç­‰åŠŸèƒ½ã€‚

Flask is a lightweight web framework, very suitable for quickly building REST API services for AI models, supporting model inference, batch processing and other functions.

**å®Œæ•´FlaskæœåŠ¡å®ç° | Complete Flask Service Implementation:**
```python
from flask import Flask, request, jsonify, render_template
from flask_cors import CORS
import torch
import torch.nn as nn
import torchvision.transforms as transforms
from PIL import Image
import io
import base64
import numpy as np
import json
import logging
import time
from datetime import datetime
import threading
import queue
import redis
from werkzeug.middleware.proxy_fix import ProxyFix
import os

# é…ç½®æ—¥å¿— | Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class ModelServer:
    """
    AIæ¨¡å‹æœåŠ¡å™¨
    AI Model Server
    """
    def __init__(self, model_path, device='cpu'):
        self.device = torch.device(device)
        self.model = self._load_model(model_path)
        self.model.eval()
        
        # é¢„å¤„ç†ç®¡é“ | Preprocessing pipeline
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
        # æ€§èƒ½ç»Ÿè®¡ | Performance statistics
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'total_inference_time': 0,
            'avg_inference_time': 0,
            'start_time': datetime.now()
        }
        
        # è¯·æ±‚é˜Ÿåˆ—ï¼ˆç”¨äºæ‰¹å¤„ç†ï¼‰| Request queue (for batch processing)
        self.request_queue = queue.Queue(maxsize=100)
        self.batch_size = 8
        self.batch_timeout = 0.1  # 100ms
        
        # å¯åŠ¨æ‰¹å¤„ç†çº¿ç¨‹ | Start batch processing thread
        self.batch_thread = threading.Thread(target=self._batch_processor, daemon=True)
        self.batch_thread.start()
        
        logger.info(f"æ¨¡å‹æœåŠ¡å™¨åˆå§‹åŒ–å®Œæˆï¼Œè®¾å¤‡: {self.device}")
        logger.info(f"Model server initialized, device: {self.device}")
    
    def _load_model(self, model_path):
        """
        åŠ è½½æ¨¡å‹
        Load model
        """
        try:
            # è¿™é‡Œå¯ä»¥åŠ è½½å„ç§ç±»å‹çš„æ¨¡å‹ | Can load various types of models here
            if model_path.endswith('.pth'):
                model = torch.load(model_path, map_location=self.device)
            elif model_path.endswith('.onnx'):
                import onnxruntime as ort
                model = ort.InferenceSession(model_path)
            else:
                # ç®€å•ç¤ºä¾‹æ¨¡å‹ | Simple example model
                model = nn.Sequential(
                    nn.Conv2d(3, 32, 3, padding=1),
                    nn.ReLU(),
                    nn.AdaptiveAvgPool2d((1, 1)),
                    nn.Flatten(),
                    nn.Linear(32, 10)
                )
            
            return model.to(self.device)
        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            logger.error(f"Model loading failed: {e}")
            raise
    
    def _preprocess_image(self, image_data):
        """
        å›¾åƒé¢„å¤„ç†
        Image preprocessing
        """
        try:
            # å¤„ç†base64ç¼–ç çš„å›¾åƒ | Handle base64 encoded images
            if isinstance(image_data, str):
                if image_data.startswith('data:image'):
                    image_data = image_data.split(',')[1]
                image_data = base64.b64decode(image_data)
            
            # æ‰“å¼€å›¾åƒ | Open image
            image = Image.open(io.BytesIO(image_data)).convert('RGB')
            
            # åº”ç”¨å˜æ¢ | Apply transforms
            tensor = self.transform(image).unsqueeze(0)
            
            return tensor.to(self.device)
        
        except Exception as e:
            logger.error(f"å›¾åƒé¢„å¤„ç†å¤±è´¥: {e}")
            logger.error(f"Image preprocessing failed: {e}")
            raise
    
    def predict_single(self, image_data):
        """
        å•ä¸ªå›¾åƒé¢„æµ‹
        Single image prediction
        """
        start_time = time.time()
        
        try:
            # é¢„å¤„ç† | Preprocessing
            input_tensor = self._preprocess_image(image_data)
            
            # æ¨ç† | Inference
            with torch.no_grad():
                outputs = self.model(input_tensor)
                probabilities = torch.softmax(outputs, dim=1)
                predicted_class = torch.argmax(probabilities, dim=1).item()
                confidence = probabilities[0][predicted_class].item()
            
            inference_time = time.time() - start_time
            
            # æ›´æ–°ç»Ÿè®¡ | Update statistics
            self._update_stats(inference_time, success=True)
            
            result = {
                'predicted_class': predicted_class,
                'confidence': float(confidence),
                'probabilities': probabilities[0].tolist(),
                'inference_time': inference_time
            }
            
            logger.info(f"é¢„æµ‹å®Œæˆ: ç±»åˆ«={predicted_class}, ç½®ä¿¡åº¦={confidence:.3f}")
            logger.info(f"Prediction completed: class={predicted_class}, confidence={confidence:.3f}")
            
            return result
            
        except Exception as e:
            self._update_stats(0, success=False)
            logger.error(f"é¢„æµ‹å¤±è´¥: {e}")
            logger.error(f"Prediction failed: {e}")
            raise
    
    def predict_batch(self, image_list):
        """
        æ‰¹é‡å›¾åƒé¢„æµ‹
        Batch image prediction
        """
        start_time = time.time()
        
        try:
            # é¢„å¤„ç†æ‰€æœ‰å›¾åƒ | Preprocess all images
            input_tensors = []
            for image_data in image_list:
                tensor = self._preprocess_image(image_data)
                input_tensors.append(tensor)
            
            # åˆå¹¶ä¸ºæ‰¹æ¬¡ | Combine into batch
            batch_tensor = torch.cat(input_tensors, dim=0)
            
            # æ‰¹é‡æ¨ç† | Batch inference
            with torch.no_grad():
                outputs = self.model(batch_tensor)
                probabilities = torch.softmax(outputs, dim=1)
                predicted_classes = torch.argmax(probabilities, dim=1)
            
            inference_time = time.time() - start_time
            
            # æ„å»ºç»“æœ | Build results
            results = []
            for i in range(len(image_list)):
                result = {
                    'predicted_class': predicted_classes[i].item(),
                    'confidence': probabilities[i][predicted_classes[i]].item(),
                    'probabilities': probabilities[i].tolist()
                }
                results.append(result)
            
            # æ›´æ–°ç»Ÿè®¡ | Update statistics
            self._update_stats(inference_time, success=True, batch_size=len(image_list))
            
            logger.info(f"æ‰¹é‡é¢„æµ‹å®Œæˆ: {len(image_list)} å¼ å›¾åƒ")
            logger.info(f"Batch prediction completed: {len(image_list)} images")
            
            return {
                'results': results,
                'batch_size': len(image_list),
                'total_inference_time': inference_time,
                'avg_inference_time': inference_time / len(image_list)
            }
            
        except Exception as e:
            self._update_stats(0, success=False)
            logger.error(f"æ‰¹é‡é¢„æµ‹å¤±è´¥: {e}")
            logger.error(f"Batch prediction failed: {e}")
            raise
    
    def _batch_processor(self):
        """
        æ‰¹å¤„ç†å™¨çº¿ç¨‹
        Batch processor thread
        """
        while True:
            batch = []
            start_time = time.time()
            
            # æ”¶é›†æ‰¹æ¬¡ | Collect batch
            try:
                while len(batch) < self.batch_size:
                    timeout = self.batch_timeout - (time.time() - start_time)
                    if timeout <= 0:
                        break
                    
                    try:
                        item = self.request_queue.get(timeout=timeout)
                        batch.append(item)
                    except queue.Empty:
                        break
                
                # å¤„ç†æ‰¹æ¬¡ | Process batch
                if batch:
                    self._process_batch(batch)
                    
            except Exception as e:
                logger.error(f"æ‰¹å¤„ç†é”™è¯¯: {e}")
                logger.error(f"Batch processing error: {e}")
    
    def _process_batch(self, batch):
        """
        å¤„ç†ä¸€ä¸ªæ‰¹æ¬¡
        Process a batch
        """
        try:
            image_list = [item['image'] for item in batch]
            results = self.predict_batch(image_list)
            
            # è¿”å›ç»“æœç»™å„ä¸ªè¯·æ±‚ | Return results to individual requests
            for i, item in enumerate(batch):
                result_queue = item['result_queue']
                result_queue.put({
                    'status': 'success',
                    'result': results['results'][i]
                })
                
        except Exception as e:
            # é”™è¯¯å¤„ç† | Error handling
            for item in batch:
                result_queue = item['result_queue']
                result_queue.put({
                    'status': 'error',
                    'error': str(e)
                })
    
    def _update_stats(self, inference_time, success=True, batch_size=1):
        """
        æ›´æ–°æ€§èƒ½ç»Ÿè®¡
        Update performance statistics
        """
        self.stats['total_requests'] += batch_size
        
        if success:
            self.stats['successful_requests'] += batch_size
            self.stats['total_inference_time'] += inference_time
            self.stats['avg_inference_time'] = (
                self.stats['total_inference_time'] / self.stats['successful_requests']
            )
        else:
            self.stats['failed_requests'] += batch_size
    
    def get_stats(self):
        """
        è·å–æ€§èƒ½ç»Ÿè®¡
        Get performance statistics
        """
        uptime = datetime.now() - self.stats['start_time']
        
        return {
            **self.stats,
            'uptime_seconds': uptime.total_seconds(),
            'requests_per_second': self.stats['total_requests'] / uptime.total_seconds() if uptime.total_seconds() > 0 else 0,
            'success_rate': self.stats['successful_requests'] / self.stats['total_requests'] if self.stats['total_requests'] > 0 else 0
        }

# åˆ›å»ºFlaskåº”ç”¨ | Create Flask application
app = Flask(__name__)
CORS(app)  # å…è®¸è·¨åŸŸè¯·æ±‚ | Allow cross-origin requests
app.wsgi_app = ProxyFix(app.wsgi_app)  # ä»£ç†æ”¯æŒ | Proxy support

# åˆå§‹åŒ–æ¨¡å‹æœåŠ¡å™¨ | Initialize model server
model_server = None

def init_model_server():
    """åˆå§‹åŒ–æ¨¡å‹æœåŠ¡å™¨ | Initialize model server"""
    global model_server
    try:
        model_path = os.getenv('MODEL_PATH', 'model.pth')
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        model_server = ModelServer(model_path, device)
        logger.info("æ¨¡å‹æœåŠ¡å™¨åˆå§‹åŒ–æˆåŠŸ")
        logger.info("Model server initialized successfully")
    except Exception as e:
        logger.error(f"æ¨¡å‹æœåŠ¡å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        logger.error(f"Model server initialization failed: {e}")
        raise

@app.route('/', methods=['GET'])
def home():
    """
    é¦–é¡µ
    Home page
    """
    return render_template('index.html')

@app.route('/health', methods=['GET'])
def health_check():
    """
    å¥åº·æ£€æŸ¥
    Health check
    """
    try:
        stats = model_server.get_stats() if model_server else {}
        return jsonify({
            'status': 'healthy',
            'timestamp': datetime.now().isoformat(),
            'stats': stats
        })
    except Exception as e:
        return jsonify({
            'status': 'unhealthy',
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }), 500

@app.route('/predict', methods=['POST'])
def predict():
    """
    å•å›¾åƒé¢„æµ‹æ¥å£
    Single image prediction endpoint
    """
    try:
        # æ£€æŸ¥æ¨¡å‹æœåŠ¡å™¨ | Check model server
        if not model_server:
            return jsonify({'error': 'æ¨¡å‹æœåŠ¡å™¨æœªåˆå§‹åŒ–'}), 500
        
        # è·å–è¯·æ±‚æ•°æ® | Get request data
        if 'image' not in request.files and 'image' not in request.json:
            return jsonify({'error': 'ç¼ºå°‘å›¾åƒæ•°æ®'}), 400
        
        # å¤„ç†æ–‡ä»¶ä¸Šä¼  | Handle file upload
        if 'image' in request.files:
            file = request.files['image']
            if file.filename == '':
                return jsonify({'error': 'æœªé€‰æ‹©æ–‡ä»¶'}), 400
            image_data = file.read()
        else:
            # å¤„ç†JSONä¸­çš„base64å›¾åƒ | Handle base64 image in JSON
            image_data = request.json['image']
        
        # æ‰§è¡Œé¢„æµ‹ | Perform prediction
        result = model_server.predict_single(image_data)
        
        return jsonify({
            'status': 'success',
            'result': result,
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"é¢„æµ‹è¯·æ±‚å¤±è´¥: {e}")
        logger.error(f"Prediction request failed: {e}")
        return jsonify({
            'status': 'error',
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }), 500

@app.route('/predict/batch', methods=['POST'])
def predict_batch():
    """
    æ‰¹é‡é¢„æµ‹æ¥å£
    Batch prediction endpoint
    """
    try:
        if not model_server:
            return jsonify({'error': 'æ¨¡å‹æœåŠ¡å™¨æœªåˆå§‹åŒ–'}), 500
        
        # è·å–å›¾åƒåˆ—è¡¨ | Get image list
        data = request.get_json()
        if 'images' not in data:
            return jsonify({'error': 'ç¼ºå°‘å›¾åƒæ•°æ®'}), 400
        
        image_list = data['images']
        if len(image_list) == 0:
            return jsonify({'error': 'å›¾åƒåˆ—è¡¨ä¸ºç©º'}), 400
        
        if len(image_list) > 32:  # é™åˆ¶æ‰¹é‡å¤§å° | Limit batch size
            return jsonify({'error': 'æ‰¹é‡å¤§å°è¶…è¿‡é™åˆ¶(32)'}), 400
        
        # æ‰§è¡Œæ‰¹é‡é¢„æµ‹ | Perform batch prediction
        result = model_server.predict_batch(image_list)
        
        return jsonify({
            'status': 'success',
            'result': result,
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"æ‰¹é‡é¢„æµ‹è¯·æ±‚å¤±è´¥: {e}")
        logger.error(f"Batch prediction request failed: {e}")
        return jsonify({
            'status': 'error',
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }), 500

@app.route('/stats', methods=['GET'])
def get_stats():
    """
    è·å–æœåŠ¡ç»Ÿè®¡ä¿¡æ¯
    Get service statistics
    """
    try:
        if not model_server:
            return jsonify({'error': 'æ¨¡å‹æœåŠ¡å™¨æœªåˆå§‹åŒ–'}), 500
        
        stats = model_server.get_stats()
        
        return jsonify({
            'status': 'success',
            'stats': stats,
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        return jsonify({
            'status': 'error',
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }), 500

@app.route('/model/info', methods=['GET'])
def model_info():
    """
    è·å–æ¨¡å‹ä¿¡æ¯
    Get model information
    """
    try:
        if not model_server:
            return jsonify({'error': 'æ¨¡å‹æœåŠ¡å™¨æœªåˆå§‹åŒ–'}), 500
        
        # è®¡ç®—æ¨¡å‹å‚æ•°æ•°é‡ | Calculate model parameters
        total_params = sum(p.numel() for p in model_server.model.parameters())
        trainable_params = sum(p.numel() for p in model_server.model.parameters() if p.requires_grad)
        
        model_info = {
            'model_type': type(model_server.model).__name__,
            'device': str(model_server.device),
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'model_size_mb': total_params * 4 / (1024 * 1024),  # å‡è®¾float32
        }
        
        return jsonify({
            'status': 'success',
            'model_info': model_info,
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        return jsonify({
            'status': 'error',
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }), 500

@app.errorhandler(404)
def not_found(error):
    """404é”™è¯¯å¤„ç† | 404 error handler"""
    return jsonify({'error': 'æ¥å£ä¸å­˜åœ¨'}), 404

@app.errorhandler(500)
def internal_error(error):
    """500é”™è¯¯å¤„ç† | 500 error handler"""
    return jsonify({'error': 'å†…éƒ¨æœåŠ¡å™¨é”™è¯¯'}), 500

# HTMLæ¨¡æ¿ | HTML template
HTML_TEMPLATE = '''
<!DOCTYPE html>
<html>
<head>
    <title>AI Model API Server</title>
    <meta charset="utf-8">
    <style>
        body { font-family: Arial, sans-serif; margin: 40px; }
        .container { max-width: 800px; margin: 0 auto; }
        .api-section { margin: 20px 0; padding: 20px; border: 1px solid #ddd; }
        .stats { background: #f5f5f5; padding: 15px; margin: 10px 0; }
        button { padding: 10px 20px; margin: 5px; }
        input[type="file"] { margin: 10px 0; }
        .result { margin: 10px 0; padding: 10px; background: #e8f5e8; }
        .error { background: #ffe8e8; }
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸ¤– AI Model API Server</h1>
        
        <div class="api-section">
            <h2>ğŸ“Š Server Stats</h2>
            <button onclick="loadStats()">åˆ·æ–°ç»Ÿè®¡</button>
            <div id="stats" class="stats">ç‚¹å‡»åˆ·æ–°æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯</div>
        </div>
        
        <div class="api-section">
            <h2>ğŸ”® Image Prediction</h2>
            <input type="file" id="imageFile" accept="image/*">
            <button onclick="predictImage()">é¢„æµ‹</button>
            <div id="prediction-result" class="result" style="display:none;"></div>
        </div>
        
        <div class="api-section">
            <h2>ğŸ“± API Endpoints</h2>
            <ul>
                <li><strong>GET /health</strong> - å¥åº·æ£€æŸ¥</li>
                <li><strong>POST /predict</strong> - å•å›¾åƒé¢„æµ‹</li>
                <li><strong>POST /predict/batch</strong> - æ‰¹é‡é¢„æµ‹</li>
                <li><strong>GET /stats</strong> - æœåŠ¡ç»Ÿè®¡</li>
                <li><strong>GET /model/info</strong> - æ¨¡å‹ä¿¡æ¯</li>
            </ul>
        </div>
    </div>

    <script>
        function loadStats() {
            fetch('/stats')
                .then(response => response.json())
                .then(data => {
                    if (data.status === 'success') {
                        const stats = data.stats;
                        document.getElementById('stats').innerHTML = `
                            <strong>Service Statistics:</strong><br>
                            Total Requests: ${stats.total_requests}<br>
                            Successful: ${stats.successful_requests}<br>
                            Failed: ${stats.failed_requests}<br>
                            Success Rate: ${(stats.success_rate * 100).toFixed(1)}%<br>
                            Avg Inference Time: ${stats.avg_inference_time.toFixed(3)}s<br>
                            Uptime: ${stats.uptime_seconds.toFixed(0)}s<br>
                            RPS: ${stats.requests_per_second.toFixed(2)}
                        `;
                    }
                })
                .catch(error => {
                    document.getElementById('stats').innerHTML = 'Error loading stats: ' + error;
                });
        }
        
        function predictImage() {
            const fileInput = document.getElementById('imageFile');
            const resultDiv = document.getElementById('prediction-result');
            
            if (!fileInput.files[0]) {
                alert('è¯·é€‰æ‹©å›¾åƒæ–‡ä»¶');
                return;
            }
            
            const formData = new FormData();
            formData.append('image', fileInput.files[0]);
            
            fetch('/predict', {
                method: 'POST',
                body: formData
            })
            .then(response => response.json())
            .then(data => {
                resultDiv.style.display = 'block';
                if (data.status === 'success') {
                    const result = data.result;
                    resultDiv.className = 'result';
                    resultDiv.innerHTML = `
                        <strong>Prediction Result:</strong><br>
                        Predicted Class: ${result.predicted_class}<br>
                        Confidence: ${(result.confidence * 100).toFixed(1)}%<br>
                        Inference Time: ${result.inference_time.toFixed(3)}s
                    `;
                } else {
                    resultDiv.className = 'result error';
                    resultDiv.innerHTML = 'Error: ' + data.error;
                }
            })
            .catch(error => {
                resultDiv.style.display = 'block';
                resultDiv.className = 'result error';
                resultDiv.innerHTML = 'Request failed: ' + error;
            });
        }
        
        // é¡µé¢åŠ è½½æ—¶è‡ªåŠ¨åˆ·æ–°ç»Ÿè®¡ | Auto refresh stats on page load
        window.onload = function() {
            loadStats();
        };
    </script>
</body>
</html>
'''

# åˆ›å»ºæ¨¡æ¿ç›®å½•å’Œæ–‡ä»¶ | Create template directory and file
if not os.path.exists('templates'):
    os.makedirs('templates')

with open('templates/index.html', 'w', encoding='utf-8') as f:
    f.write(HTML_TEMPLATE)

if __name__ == '__main__':
    # åˆå§‹åŒ–æ¨¡å‹æœåŠ¡å™¨ | Initialize model server
    init_model_server()
    
    # å¯åŠ¨Flaskåº”ç”¨ | Start Flask application
    port = int(os.getenv('PORT', 5000))
    debug = os.getenv('DEBUG', 'False').lower() == 'true'
    
    logger.info(f"å¯åŠ¨AIæ¨¡å‹APIæœåŠ¡å™¨ï¼Œç«¯å£: {port}")
    logger.info(f"Starting AI model API server on port: {port}")
    
    app.run(host='0.0.0.0', port=port, debug=debug, threaded=True)
```

#### Dockerå®¹å™¨åŒ– | Docker Containerization

**é¡¹ç›®ä»·å€¼ | Project Value:**
Dockerå®¹å™¨åŒ–ä½¿AIæœåŠ¡å…·å¤‡ç¯å¢ƒä¸€è‡´æ€§ã€å¿«é€Ÿéƒ¨ç½²ã€æ˜“äºæ‰©å±•ç­‰ä¼˜åŠ¿ï¼Œæ˜¯ç°ä»£äº‘åŸç”Ÿåº”ç”¨çš„æ ‡å‡†å®è·µã€‚

Docker containerization provides AI services with environment consistency, fast deployment, easy scaling and other advantages, being the standard practice for modern cloud-native applications.

**å®Œæ•´DockeråŒ–æ–¹æ¡ˆ | Complete Dockerization Solution:**

**Dockerfile:**
```dockerfile
# å¤šé˜¶æ®µæ„å»º | Multi-stage build
FROM python:3.9-slim as builder

# è®¾ç½®å·¥ä½œç›®å½• | Set working directory
WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ– | Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    curl \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶requirementsæ–‡ä»¶ | Copy requirements file
COPY requirements.txt .

# å®‰è£…Pythonä¾èµ– | Install Python dependencies
RUN pip install --no-cache-dir --user -r requirements.txt

# ç”Ÿäº§é˜¶æ®µ | Production stage
FROM python:3.9-slim

# åˆ›å»ºérootç”¨æˆ· | Create non-root user
RUN useradd --create-home --shell /bin/bash app

# è®¾ç½®å·¥ä½œç›®å½• | Set working directory
WORKDIR /app

# å®‰è£…è¿è¡Œæ—¶ä¾èµ– | Install runtime dependencies
RUN apt-get update && apt-get install -y \
    curl \
    && rm -rf /var/lib/apt/lists/*

# ä»builderé˜¶æ®µå¤åˆ¶PythonåŒ… | Copy Python packages from builder stage
COPY --from=builder /root/.local /home/app/.local

# å¤åˆ¶åº”ç”¨ä»£ç  | Copy application code
COPY --chown=app:app . .

# è®¾ç½®ç¯å¢ƒå˜é‡ | Set environment variables
ENV PATH=/home/app/.local/bin:$PATH
ENV PYTHONUNBUFFERED=1
ENV MODEL_PATH=/app/models/model.pth
ENV PORT=5000

# åˆ›å»ºå¿…è¦ç›®å½• | Create necessary directories
RUN mkdir -p /app/models /app/logs /app/data && \
    chown -R app:app /app

# åˆ‡æ¢åˆ°appç”¨æˆ· | Switch to app user
USER app

# å¥åº·æ£€æŸ¥ | Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:$PORT/health || exit 1

# æš´éœ²ç«¯å£ | Expose port
EXPOSE $PORT

# å¯åŠ¨å‘½ä»¤ | Start command
CMD ["python", "app.py"]
```

**requirements.txt:**
```txt
torch>=1.12.0
torchvision>=0.13.0
flask>=2.2.0
flask-cors>=3.0.10
pillow>=9.0.0
numpy>=1.21.0
redis>=4.3.0
gunicorn>=20.1.0
requests>=2.28.0
prometheus-client>=0.14.0
```

**docker-compose.yml:**
```yaml
version: '3.8'

services:
  # AIæ¨¡å‹æœåŠ¡ | AI Model Service
  ai-model-api:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "5000:5000"
    environment:
      - MODEL_PATH=/app/models/model.pth
      - REDIS_URL=redis://redis:6379
      - DEBUG=false
    volumes:
      - ./models:/app/models:ro
      - ./logs:/app/logs
    depends_on:
      - redis
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 1G
          cpus: '0.5'
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Redisç¼“å­˜ | Redis Cache
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
    restart: unless-stopped

  # Nginxè´Ÿè½½å‡è¡¡å™¨ | Nginx Load Balancer
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - ./ssl:/etc/nginx/ssl:ro
    depends_on:
      - ai-model-api
    restart: unless-stopped

  # Prometheusç›‘æ§ | Prometheus Monitoring
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
    restart: unless-stopped

  # Grafanaå¯è§†åŒ– | Grafana Visualization
  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin123
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./grafana/datasources:/etc/grafana/provisioning/datasources:ro
    depends_on:
      - prometheus
    restart: unless-stopped

volumes:
  redis_data:
  prometheus_data:
  grafana_data:

networks:
  default:
    driver: bridge
```

**éƒ¨ç½²è„šæœ¬ | Deployment Scripts:**
```bash
#!/bin/bash
# deploy.sh - éƒ¨ç½²è„šæœ¬

set -e

echo "ğŸš€ å¼€å§‹éƒ¨ç½²AIæ¨¡å‹æœåŠ¡..."
echo "ğŸš€ Starting AI model service deployment..."

# æ£€æŸ¥Dockerå’Œdocker-compose
echo "æ£€æŸ¥Dockerç¯å¢ƒ..."
if ! command -v docker &> /dev/null; then
    echo "âŒ Dockeræœªå®‰è£…"
    exit 1
fi

if ! command -v docker-compose &> /dev/null; then
    echo "âŒ docker-composeæœªå®‰è£…"
    exit 1
fi

# åˆ›å»ºå¿…è¦ç›®å½•
echo "åˆ›å»ºç›®å½•ç»“æ„..."
mkdir -p models logs ssl grafana/dashboards grafana/datasources

# æ„å»ºé•œåƒ
echo "ğŸ”¨ æ„å»ºDockeré•œåƒ..."
docker-compose build

# å¯åŠ¨æœåŠ¡
echo "ğŸŒŸ å¯åŠ¨æœåŠ¡..."
docker-compose up -d

# ç­‰å¾…æœåŠ¡å¯åŠ¨
echo "â³ ç­‰å¾…æœåŠ¡å¯åŠ¨..."
sleep 30

# å¥åº·æ£€æŸ¥
echo "ğŸ” æ£€æŸ¥æœåŠ¡å¥åº·çŠ¶æ€..."
for service in ai-model-api redis nginx prometheus grafana; do
    if docker-compose ps $service | grep -q "Up"; then
        echo "âœ… $service è¿è¡Œæ­£å¸¸"
    else
        echo "âŒ $service å¯åŠ¨å¤±è´¥"
        docker-compose logs $service
        exit 1
    fi
done

# æµ‹è¯•API
echo "ğŸ§ª æµ‹è¯•APIç«¯ç‚¹..."
if curl -f http://localhost/health > /dev/null 2>&1; then
    echo "âœ… APIæœåŠ¡æ­£å¸¸"
else
    echo "âŒ APIæœåŠ¡å¼‚å¸¸"
    exit 1
fi

echo "ğŸ‰ éƒ¨ç½²å®Œæˆï¼"
echo "ğŸ“± æœåŠ¡åœ°å€:"
echo "  - APIæœåŠ¡: http://localhost"
echo "  - Prometheus: http://localhost:9090"
echo "  - Grafana: http://localhost:3000 (admin/admin123)"

echo "ğŸ“Š æŸ¥çœ‹æœåŠ¡çŠ¶æ€: docker-compose ps"
echo "ğŸ“‹ æŸ¥çœ‹æ—¥å¿—: docker-compose logs -f"
echo "ğŸ›‘ åœæ­¢æœåŠ¡: docker-compose down"
```

### 02_æ¨¡å‹ç›‘æ§ | Model Monitoring

**è®©AIç³»ç»ŸçŠ¶æ€ä¸€ç›®äº†ç„¶ï¼**
**Make AI system status clear at a glance!**

#### æ€§èƒ½ç›‘æ§ | Performance Monitoring

**é¡¹ç›®æ ¸å¿ƒ | Project Core:**
æ€§èƒ½ç›‘æ§é€šè¿‡æ”¶é›†ç³»ç»ŸæŒ‡æ ‡ã€å“åº”æ—¶é—´ã€é”™è¯¯ç‡ç­‰æ•°æ®ï¼Œå®æ—¶ç›‘æ§AIæœåŠ¡çš„å¥åº·çŠ¶æ€å’Œæ€§èƒ½è¡¨ç°ã€‚

Performance monitoring collects system metrics, response times, error rates and other data to monitor the health and performance of AI services in real-time.

**Prometheusç›‘æ§é›†æˆ | Prometheus Monitoring Integration:**
```python
from prometheus_client import Counter, Histogram, Gauge, start_http_server
import time
import threading
import psutil
import torch

class ModelMonitor:
    """
    æ¨¡å‹æ€§èƒ½ç›‘æ§å™¨
    Model Performance Monitor
    """
    def __init__(self, model_server, port=8000):
        self.model_server = model_server
        self.port = port
        
        # PrometheusæŒ‡æ ‡ | Prometheus metrics
        self.request_count = Counter(
            'ai_model_requests_total', 
            'Total number of requests',
            ['method', 'status']
        )
        
        self.request_duration = Histogram(
            'ai_model_request_duration_seconds',
            'Request duration in seconds',
            ['method']
        )
        
        self.inference_duration = Histogram(
            'ai_model_inference_duration_seconds',
            'Model inference duration in seconds'
        )
        
        self.batch_size = Histogram(
            'ai_model_batch_size',
            'Batch size for predictions'
        )
        
        # ç³»ç»ŸæŒ‡æ ‡ | System metrics
        self.cpu_usage = Gauge('system_cpu_usage_percent', 'CPU usage percentage')
        self.memory_usage = Gauge('system_memory_usage_bytes', 'Memory usage in bytes')
        self.gpu_usage = Gauge('system_gpu_usage_percent', 'GPU usage percentage')
        self.gpu_memory = Gauge('system_gpu_memory_bytes', 'GPU memory usage in bytes')
        
        # æ¨¡å‹æŒ‡æ ‡ | Model metrics
        self.model_accuracy = Gauge('ai_model_accuracy', 'Model accuracy')
        self.prediction_confidence = Histogram(
            'ai_model_prediction_confidence',
            'Prediction confidence scores'
        )
        
        # å¯åŠ¨æŒ‡æ ‡æœåŠ¡å™¨ | Start metrics server
        start_http_server(self.port)
        
        # å¯åŠ¨ç³»ç»Ÿç›‘æ§çº¿ç¨‹ | Start system monitoring thread
        self.monitoring_thread = threading.Thread(target=self._monitor_system, daemon=True)
        self.monitoring_thread.start()
        
        print(f"æ€§èƒ½ç›‘æ§æœåŠ¡å¯åŠ¨åœ¨ç«¯å£ {self.port}")
        print(f"Performance monitoring service started on port {self.port}")
    
    def record_request(self, method, status, duration):
        """
        è®°å½•è¯·æ±‚æŒ‡æ ‡
        Record request metrics
        """
        self.request_count.labels(method=method, status=status).inc()
        self.request_duration.labels(method=method).observe(duration)
    
    def record_inference(self, duration, batch_size=1, confidence=None):
        """
        è®°å½•æ¨ç†æŒ‡æ ‡
        Record inference metrics
        """
        self.inference_duration.observe(duration)
        self.batch_size.observe(batch_size)
        
        if confidence is not None:
            self.prediction_confidence.observe(confidence)
    
    def update_model_accuracy(self, accuracy):
        """
        æ›´æ–°æ¨¡å‹å‡†ç¡®ç‡
        Update model accuracy
        """
        self.model_accuracy.set(accuracy)
    
    def _monitor_system(self):
        """
        ç³»ç»Ÿèµ„æºç›‘æ§
        System resource monitoring
        """
        while True:
            try:
                # CPUä½¿ç”¨ç‡ | CPU usage
                cpu_percent = psutil.cpu_percent(interval=1)
                self.cpu_usage.set(cpu_percent)
                
                # å†…å­˜ä½¿ç”¨ | Memory usage
                memory = psutil.virtual_memory()
                self.memory_usage.set(memory.used)
                
                # GPUç›‘æ§ | GPU monitoring
                if torch.cuda.is_available():
                    for i in range(torch.cuda.device_count()):
                        # GPUä½¿ç”¨ç‡ | GPU utilization
                        gpu_usage = torch.cuda.utilization(i)
                        self.gpu_usage.set(gpu_usage)
                        
                        # GPUå†…å­˜ | GPU memory
                        gpu_memory = torch.cuda.memory_allocated(i)
                        self.gpu_memory.set(gpu_memory)
                
                time.sleep(10)  # æ¯10ç§’æ›´æ–°ä¸€æ¬¡ | Update every 10 seconds
                
            except Exception as e:
                print(f"ç³»ç»Ÿç›‘æ§é”™è¯¯: {e}")
                print(f"System monitoring error: {e}")
                time.sleep(10)

# é›†æˆç›‘æ§åˆ°Flaskåº”ç”¨ | Integrate monitoring into Flask app
def create_monitored_app():
    """
    åˆ›å»ºå¸¦ç›‘æ§çš„Flaskåº”ç”¨
    Create Flask app with monitoring
    """
    app = Flask(__name__)
    
    # åˆå§‹åŒ–ç›‘æ§ | Initialize monitoring
    monitor = ModelMonitor(model_server)
    
    @app.before_request
    def before_request():
        """è¯·æ±‚å¼€å§‹æ—¶é—´è®°å½• | Record request start time"""
        request.start_time = time.time()
    
    @app.after_request
    def after_request(response):
        """è¯·æ±‚ç»“æŸæŒ‡æ ‡è®°å½• | Record request end metrics"""
        if hasattr(request, 'start_time'):
            duration = time.time() - request.start_time
            method = request.endpoint or 'unknown'
            status = 'success' if response.status_code < 400 else 'error'
            
            monitor.record_request(method, status, duration)
        
        return response
    
    return app, monitor
```

#### æ¨¡å‹æ¼‚ç§»æ£€æµ‹ | Model Drift Detection

**é¡¹ç›®ç‰¹è‰² | Project Features:**
æ¨¡å‹æ¼‚ç§»æ£€æµ‹é€šè¿‡ç›‘æ§æ•°æ®åˆ†å¸ƒå˜åŒ–å’Œæ¨¡å‹æ€§èƒ½é€€åŒ–ï¼ŒåŠæ—¶å‘ç°æ¨¡å‹åœ¨ç”Ÿäº§ç¯å¢ƒä¸­çš„è´¨é‡é—®é¢˜ã€‚

Model drift detection monitors data distribution changes and model performance degradation to timely discover model quality issues in production environments.

**æ¼‚ç§»æ£€æµ‹å®ç° | Drift Detection Implementation:**
```python
import numpy as np
from scipy import stats
from sklearn.metrics import accuracy_score, precision_score, recall_score
import json
import sqlite3
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

class ModelDriftDetector:
    """
    æ¨¡å‹æ¼‚ç§»æ£€æµ‹å™¨
    Model Drift Detector
    """
    def __init__(self, reference_data, db_path='model_monitoring.db'):
        self.reference_data = reference_data
        self.db_path = db_path
        
        # åˆå§‹åŒ–æ•°æ®åº“ | Initialize database
        self._init_database()
        
        # æ¼‚ç§»é˜ˆå€¼ | Drift thresholds
        self.thresholds = {
            'data_drift': 0.05,  # KS test p-value
            'performance_drift': 0.1,  # æ€§èƒ½ä¸‹é™é˜ˆå€¼
            'prediction_drift': 0.05  # é¢„æµ‹åˆ†å¸ƒå˜åŒ–é˜ˆå€¼
        }
        
        # è®¡ç®—å‚è€ƒæ•°æ®ç»Ÿè®¡ | Calculate reference data statistics
        self.reference_stats = self._calculate_reference_stats()
        
        print("æ¨¡å‹æ¼‚ç§»æ£€æµ‹å™¨åˆå§‹åŒ–å®Œæˆ")
        print("Model drift detector initialized")
    
    def _init_database(self):
        """
        åˆå§‹åŒ–ç›‘æ§æ•°æ®åº“
        Initialize monitoring database
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # åˆ›å»ºè¡¨ | Create tables
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS drift_monitoring (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp DATETIME,
                metric_type TEXT,
                metric_name TEXT,
                value REAL,
                threshold REAL,
                is_drift BOOLEAN,
                severity TEXT
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS model_performance (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp DATETIME,
                accuracy REAL,
                precision REAL,
                recall REAL,
                avg_confidence REAL,
                prediction_count INTEGER
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def _calculate_reference_stats(self):
        """
        è®¡ç®—å‚è€ƒæ•°æ®ç»Ÿè®¡ä¿¡æ¯
        Calculate reference data statistics
        """
        stats = {}
        
        if isinstance(self.reference_data, dict):
            for feature_name, feature_data in self.reference_data.items():
                stats[feature_name] = {
                    'mean': np.mean(feature_data),
                    'std': np.std(feature_data),
                    'min': np.min(feature_data),
                    'max': np.max(feature_data),
                    'quantiles': np.percentile(feature_data, [25, 50, 75])
                }
        
        return stats
    
    def detect_data_drift(self, current_data, feature_names=None):
        """
        æ£€æµ‹æ•°æ®æ¼‚ç§»
        Detect data drift
        """
        drift_results = {}
        
        if feature_names is None:
            feature_names = list(self.reference_data.keys())
        
        for feature_name in feature_names:
            if feature_name not in self.reference_data:
                continue
            
            reference_values = self.reference_data[feature_name]
            current_values = current_data.get(feature_name, [])
            
            if len(current_values) == 0:
                continue
            
            # Kolmogorov-Smirnovæµ‹è¯• | Kolmogorov-Smirnov test
            ks_statistic, p_value = stats.ks_2samp(reference_values, current_values)
            
            # åˆ¤æ–­æ˜¯å¦æ¼‚ç§» | Determine if drift occurs
            is_drift = p_value < self.thresholds['data_drift']
            severity = self._get_drift_severity(p_value, self.thresholds['data_drift'])
            
            drift_results[feature_name] = {
                'ks_statistic': ks_statistic,
                'p_value': p_value,
                'is_drift': is_drift,
                'severity': severity,
                'threshold': self.thresholds['data_drift']
            }
            
            # è®°å½•åˆ°æ•°æ®åº“ | Record to database
            self._log_drift_metric(
                'data_drift', feature_name, p_value, 
                self.thresholds['data_drift'], is_drift, severity
            )
        
        return drift_results
    
    def detect_prediction_drift(self, current_predictions, reference_predictions=None):
        """
        æ£€æµ‹é¢„æµ‹æ¼‚ç§»
        Detect prediction drift
        """
        if reference_predictions is None:
            # ä½¿ç”¨å†å²é¢„æµ‹æ•°æ® | Use historical prediction data
            reference_predictions = self._get_recent_predictions(days=7)
        
        if len(reference_predictions) == 0 or len(current_predictions) == 0:
            return None
        
        # æ¯”è¾ƒé¢„æµ‹åˆ†å¸ƒ | Compare prediction distributions
        ks_statistic, p_value = stats.ks_2samp(reference_predictions, current_predictions)
        
        is_drift = p_value < self.thresholds['prediction_drift']
        severity = self._get_drift_severity(p_value, self.thresholds['prediction_drift'])
        
        result = {
            'ks_statistic': ks_statistic,
            'p_value': p_value,
            'is_drift': is_drift,
            'severity': severity,
            'threshold': self.thresholds['prediction_drift'],
            'current_mean': np.mean(current_predictions),
            'reference_mean': np.mean(reference_predictions),
            'current_std': np.std(current_predictions),
            'reference_std': np.std(reference_predictions)
        }
        
        # è®°å½•åˆ°æ•°æ®åº“ | Record to database
        self._log_drift_metric(
            'prediction_drift', 'predictions', p_value,
            self.thresholds['prediction_drift'], is_drift, severity
        )
        
        return result
    
    def detect_performance_drift(self, current_performance, reference_performance=None):
        """
        æ£€æµ‹æ€§èƒ½æ¼‚ç§»
        Detect performance drift
        """
        if reference_performance is None:
            # ä½¿ç”¨å†å²æ€§èƒ½æ•°æ® | Use historical performance data
            reference_performance = self._get_recent_performance(days=7)
        
        if reference_performance is None:
            return None
        
        # è®¡ç®—æ€§èƒ½ä¸‹é™ | Calculate performance degradation
        performance_drop = reference_performance - current_performance
        relative_drop = performance_drop / reference_performance if reference_performance > 0 else 0
        
        is_drift = relative_drop > self.thresholds['performance_drift']
        severity = self._get_performance_severity(relative_drop)
        
        result = {
            'current_performance': current_performance,
            'reference_performance': reference_performance,
            'absolute_drop': performance_drop,
            'relative_drop': relative_drop,
            'is_drift': is_drift,
            'severity': severity,
            'threshold': self.thresholds['performance_drift']
        }
        
        # è®°å½•åˆ°æ•°æ®åº“ | Record to database
        self._log_drift_metric(
            'performance_drift', 'accuracy', relative_drop,
            self.thresholds['performance_drift'], is_drift, severity
        )
        
        return result
    
    def log_model_performance(self, accuracy, precision, recall, avg_confidence, prediction_count):
        """
        è®°å½•æ¨¡å‹æ€§èƒ½
        Log model performance
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO model_performance 
            (timestamp, accuracy, precision, recall, avg_confidence, prediction_count)
            VALUES (?, ?, ?, ?, ?, ?)
        ''', (datetime.now(), accuracy, precision, recall, avg_confidence, prediction_count))
        
        conn.commit()
        conn.close()
    
    def get_drift_summary(self, days=7):
        """
        è·å–æ¼‚ç§»æ£€æµ‹æ‘˜è¦
        Get drift detection summary
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # è·å–æœ€è¿‘çš„æ¼‚ç§»æ£€æµ‹ç»“æœ | Get recent drift detection results
        cursor.execute('''
            SELECT metric_type, metric_name, is_drift, severity, COUNT(*) as count
            FROM drift_monitoring
            WHERE timestamp > datetime('now', '-{} days')
            GROUP BY metric_type, metric_name, is_drift, severity
        '''.format(days))
        
        drift_summary = cursor.fetchall()
        
        # è·å–æ€§èƒ½è¶‹åŠ¿ | Get performance trends
        cursor.execute('''
            SELECT timestamp, accuracy, precision, recall
            FROM model_performance
            WHERE timestamp > datetime('now', '-{} days')
            ORDER BY timestamp
        '''.format(days))
        
        performance_history = cursor.fetchall()
        
        conn.close()
        
        return {
            'drift_summary': drift_summary,
            'performance_history': performance_history
        }
    
    def _get_drift_severity(self, p_value, threshold):
        """
        è®¡ç®—æ¼‚ç§»ä¸¥é‡ç¨‹åº¦
        Calculate drift severity
        """
        if p_value >= threshold:
            return 'none'
        elif p_value >= threshold / 2:
            return 'low'
        elif p_value >= threshold / 10:
            return 'medium'
        else:
            return 'high'
    
    def _get_performance_severity(self, relative_drop):
        """
        è®¡ç®—æ€§èƒ½ä¸‹é™ä¸¥é‡ç¨‹åº¦
        Calculate performance degradation severity
        """
        if relative_drop <= 0.05:
            return 'none'
        elif relative_drop <= 0.1:
            return 'low'
        elif relative_drop <= 0.2:
            return 'medium'
        else:
            return 'high'
    
    def _log_drift_metric(self, metric_type, metric_name, value, threshold, is_drift, severity):
        """
        è®°å½•æ¼‚ç§»æŒ‡æ ‡åˆ°æ•°æ®åº“
        Log drift metric to database
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO drift_monitoring 
            (timestamp, metric_type, metric_name, value, threshold, is_drift, severity)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        ''', (datetime.now(), metric_type, metric_name, value, threshold, is_drift, severity))
        
        conn.commit()
        conn.close()
    
    def _get_recent_predictions(self, days=7):
        """
        è·å–æœ€è¿‘çš„é¢„æµ‹æ•°æ®
        Get recent prediction data
        """
        # è¿™é‡Œåº”è¯¥ä»å®é™…çš„é¢„æµ‹æ—¥å¿—ä¸­è·å–æ•°æ®
        # This should get data from actual prediction logs
        return []
    
    def _get_recent_performance(self, days=7):
        """
        è·å–æœ€è¿‘çš„æ€§èƒ½æ•°æ®
        Get recent performance data
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT AVG(accuracy) as avg_accuracy
            FROM model_performance
            WHERE timestamp > datetime('now', '-{} days')
        '''.format(days))
        
        result = cursor.fetchone()
        conn.close()
        
        return result[0] if result and result[0] is not None else None

def drift_detection_demo():
    """
    æ¼‚ç§»æ£€æµ‹æ¼”ç¤º
    Drift detection demo
    """
    print("=== æ¨¡å‹æ¼‚ç§»æ£€æµ‹æ¼”ç¤º | Model Drift Detection Demo ===")
    
    # ç”Ÿæˆå‚è€ƒæ•°æ® | Generate reference data
    np.random.seed(42)
    reference_data = {
        'feature_1': np.random.normal(0, 1, 1000),
        'feature_2': np.random.exponential(2, 1000),
        'feature_3': np.random.uniform(-1, 1, 1000)
    }
    
    # åˆ›å»ºæ¼‚ç§»æ£€æµ‹å™¨ | Create drift detector
    detector = ModelDriftDetector(reference_data)
    
    # æ¨¡æ‹Ÿå½“å‰æ•°æ®ï¼ˆæœ‰æ¼‚ç§»ï¼‰| Simulate current data (with drift)
    current_data = {
        'feature_1': np.random.normal(0.5, 1.2, 500),  # å‡å€¼å’Œæ–¹å·®éƒ½æœ‰å˜åŒ–
        'feature_2': np.random.exponential(1.5, 500),   # å‚æ•°å˜åŒ–
        'feature_3': np.random.uniform(-0.5, 1.5, 500)  # èŒƒå›´å˜åŒ–
    }
    
    # æ£€æµ‹æ•°æ®æ¼‚ç§» | Detect data drift
    print("1. æ•°æ®æ¼‚ç§»æ£€æµ‹ | Data Drift Detection")
    data_drift_results = detector.detect_data_drift(current_data)
    
    for feature, result in data_drift_results.items():
        print(f"  {feature}:")
        print(f"    KSç»Ÿè®¡é‡: {result['ks_statistic']:.4f}")
        print(f"    på€¼: {result['p_value']:.4f}")
        print(f"    æ˜¯å¦æ¼‚ç§»: {'æ˜¯' if result['is_drift'] else 'å¦'}")
        print(f"    ä¸¥é‡ç¨‹åº¦: {result['severity']}")
    
    # æ¨¡æ‹Ÿé¢„æµ‹æ¼‚ç§» | Simulate prediction drift
    print("\n2. é¢„æµ‹æ¼‚ç§»æ£€æµ‹ | Prediction Drift Detection")
    reference_predictions = np.random.beta(2, 5, 1000)  # å‚è€ƒé¢„æµ‹åˆ†å¸ƒ
    current_predictions = np.random.beta(3, 3, 500)     # å½“å‰é¢„æµ‹åˆ†å¸ƒï¼ˆä¸åŒï¼‰
    
    prediction_drift = detector.detect_prediction_drift(
        current_predictions, reference_predictions
    )
    
    if prediction_drift:
        print(f"  KSç»Ÿè®¡é‡: {prediction_drift['ks_statistic']:.4f}")
        print(f"  på€¼: {prediction_drift['p_value']:.4f}")
        print(f"  æ˜¯å¦æ¼‚ç§»: {'æ˜¯' if prediction_drift['is_drift'] else 'å¦'}")
        print(f"  ä¸¥é‡ç¨‹åº¦: {prediction_drift['severity']}")
    
    # æ¨¡æ‹Ÿæ€§èƒ½æ¼‚ç§» | Simulate performance drift
    print("\n3. æ€§èƒ½æ¼‚ç§»æ£€æµ‹ | Performance Drift Detection")
    reference_accuracy = 0.95
    current_accuracy = 0.82  # æ€§èƒ½ä¸‹é™
    
    performance_drift = detector.detect_performance_drift(
        current_accuracy, reference_accuracy
    )
    
    if performance_drift:
        print(f"  å½“å‰æ€§èƒ½: {performance_drift['current_performance']:.3f}")
        print(f"  å‚è€ƒæ€§èƒ½: {performance_drift['reference_performance']:.3f}")
        print(f"  ç›¸å¯¹ä¸‹é™: {performance_drift['relative_drop']:.3f}")
        print(f"  æ˜¯å¦æ¼‚ç§»: {'æ˜¯' if performance_drift['is_drift'] else 'å¦'}")
        print(f"  ä¸¥é‡ç¨‹åº¦: {performance_drift['severity']}")
    
    # è·å–æ¼‚ç§»æ‘˜è¦ | Get drift summary
    print("\n4. æ¼‚ç§»æ£€æµ‹æ‘˜è¦ | Drift Detection Summary")
    summary = detector.get_drift_summary(days=1)
    
    print("  æ¼‚ç§»æ£€æµ‹è®°å½•:")
    for record in summary['drift_summary']:
        print(f"    {record[0]} - {record[1]}: {record[4]} æ¬¡")
    
    return detector

if __name__ == "__main__":
    drift_detection_demo()
```

---

**ğŸ¯ é¡¹ç›®å®Œæˆæ£€æŸ¥æ¸…å• | Project Completion Checklist:**

### éƒ¨ç½²æŠ€æœ¯ç†è§£ | Deployment Technology Understanding
- [ ] æ·±å…¥ç†è§£Flask/FastAPIç­‰Webæ¡†æ¶çš„æœåŠ¡åŒ–æ¨¡å¼
- [ ] æŒæ¡Dockerå®¹å™¨åŒ–çš„åŸç†å’Œæœ€ä½³å®è·µ
- [ ] ç†è§£å¾®æœåŠ¡æ¶æ„å’Œè´Ÿè½½å‡è¡¡çš„è®¾è®¡
- [ ] èƒ½å¤Ÿè®¾è®¡é«˜å¯ç”¨ã€å¯æ‰©å±•çš„AIæœåŠ¡æ¶æ„

### ç›‘æ§è¿ç»´èƒ½åŠ› | Monitoring and Operations Capability
- [ ] å®ç°å®Œæ•´çš„æ€§èƒ½ç›‘æ§å’ŒæŒ‡æ ‡æ”¶é›†ç³»ç»Ÿ
- [ ] æŒæ¡æ¨¡å‹æ¼‚ç§»æ£€æµ‹çš„ç†è®ºå’Œå®è·µæ–¹æ³•
- [ ] èƒ½å¤Ÿè®¾è®¡å’Œå®æ–½A/Bæµ‹è¯•éªŒè¯æ¡†æ¶
- [ ] å…·å¤‡ç”Ÿäº§ç¯å¢ƒçš„æ•…éšœè¯Šæ–­å’Œé—®é¢˜è§£å†³èƒ½åŠ›

### å·¥ç¨‹å®è·µæŠ€èƒ½ | Engineering Practice Skills
- [ ] èƒ½å¤Ÿå°†AIæ¨¡å‹éƒ¨ç½²åˆ°äº‘å¹³å°å’Œè¾¹ç¼˜è®¾å¤‡
- [ ] æŒæ¡CI/CDæµæ°´çº¿çš„è®¾è®¡å’Œå®ç°
- [ ] ç†è§£å®‰å…¨æ€§ã€å¯é æ€§ã€å¯ç»´æŠ¤æ€§çš„å·¥ç¨‹è¦æ±‚
- [ ] å…·å¤‡ç«¯åˆ°ç«¯çš„AIç³»ç»Ÿè®¾è®¡å’Œäº¤ä»˜èƒ½åŠ›

**è®°ä½**: ç”Ÿäº§éƒ¨ç½²æ˜¯AIæŠ€æœ¯åˆ›é€ ä»·å€¼çš„æœ€åä¸€å…¬é‡Œã€‚é€šè¿‡è¿™ä¸ªé¡¹ç›®ï¼Œä½ å°†æŒæ¡è®©AIç³»ç»Ÿåœ¨ç”Ÿäº§ç¯å¢ƒä¸­ç¨³å®šã€é«˜æ•ˆã€å¯é è¿è¡Œçš„å…¨å¥—æŠ€æœ¯ï¼

**Remember**: Production deployment is the last mile for AI technology to create value. Through this project, you will master the complete set of technologies to make AI systems run stably, efficiently, and reliably in production environments! 