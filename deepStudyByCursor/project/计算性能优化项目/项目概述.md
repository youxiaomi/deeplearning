# 计算性能优化项目概述
# Computational Performance Optimization Project Overview

**让AI更轻量更快速 - 从研究到生产的性能跨越**
**Make AI Lighter and Faster - Performance Leap from Research to Production**

---

## 🎯 项目目标 | Project Goals

性能优化是AI走向实际应用的关键桥梁！通过优化技术，我们能让大模型在移动设备上运行，让推理速度提升数倍，让内存占用大幅降低。通过这个项目，你将掌握：
Performance optimization is the key bridge for AI to reach practical applications! Through optimization techniques, we can make large models run on mobile devices, increase inference speed several times, and significantly reduce memory usage. Through this project, you will master:

- **模型压缩技术** | **Model Compression Techniques**: 知识蒸馏、模型剪枝等轻量化方法
- **推理加速技术** | **Inference Acceleration**: 量化、TensorRT等高性能推理技术
- **内存优化策略** | **Memory Optimization**: 梯度累积、混合精度等内存管理技术
- **部署优化方案** | **Deployment Optimization**: 生产环境的性能调优和系统设计

## 🔬 为什么性能优化如此重要？| Why is Performance Optimization So Important?

**性能优化决定了AI技术的普及程度！**
**Performance optimization determines the adoption level of AI technology!**

从ChatGPT的千亿参数模型到手机上的语音助手，从云端的图像识别到边缘设备的实时检测，性能优化让AI从实验室走向千家万户。掌握性能优化技术，就掌握了让AI真正服务于人类的关键能力。

From ChatGPT's hundred-billion parameter models to voice assistants on phones, from cloud image recognition to real-time detection on edge devices, performance optimization brings AI from laboratories to every household. Mastering performance optimization techniques means mastering the key ability to make AI truly serve humanity.

### 性能优化的发展历程 | Evolution of Performance Optimization
```
2012: AlexNet GPU加速 | AlexNet GPU Acceleration
2015: 模型压缩起步 | Model Compression Beginnings
2017: 知识蒸馏兴起 | Rise of Knowledge Distillation
2018: 量化技术普及 | Quantization Technology Popularization
2020: 混合精度训练 | Mixed Precision Training
2021: 大模型推理优化 | Large Model Inference Optimization
2023: 端侧部署突破 | Edge Deployment Breakthroughs
```

## 📚 项目结构深度解析 | Deep Project Structure Analysis

### 01_模型压缩技术 | Model Compression Techniques

**让大模型变小而不失性能！**
**Make large models smaller without losing performance!**

#### 知识蒸馏 | Knowledge Distillation

**项目核心 | Project Core:**
知识蒸馏通过教师-学生模型的训练范式，将大模型的知识转移到小模型中，实现性能与效率的平衡。

Knowledge distillation transfers knowledge from large models to small models through teacher-student training paradigm, achieving a balance between performance and efficiency.

**数学原理 | Mathematical Principles:**

**蒸馏损失函数 | Distillation Loss Function:**
```
L_total = α * L_hard + (1-α) * L_soft

L_hard = CrossEntropy(y_true, y_student)
L_soft = KLDivergence(softmax(z_teacher/T), softmax(z_student/T))
```

其中：
- `T`: 温度参数 | temperature parameter
- `α`: 硬目标和软目标的权重 | weight between hard and soft targets
- `z_teacher`, `z_student`: 教师和学生模型的logits

**完整实现 | Complete Implementation:**
```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import matplotlib.pyplot as plt
from typing import Tuple, Dict, List

class KnowledgeDistillationLoss(nn.Module):
    """
    知识蒸馏损失函数
    Knowledge Distillation Loss Function
    """
    def __init__(self, temperature=4.0, alpha=0.3):
        """
        Args:
            temperature: 软化温度参数 | Softening temperature parameter
            alpha: 硬目标损失权重 | Hard target loss weight
        """
        super(KnowledgeDistillationLoss, self).__init__()
        self.temperature = temperature
        self.alpha = alpha
        self.hard_loss = nn.CrossEntropyLoss()
        self.soft_loss = nn.KLDivLoss(reduction='batchmean')
    
    def forward(self, student_outputs, teacher_outputs, true_labels):
        """
        计算知识蒸馏损失
        Compute knowledge distillation loss
        """
        # 硬目标损失：学生预测 vs 真实标签 | Hard target loss: student prediction vs true labels
        hard_loss = self.hard_loss(student_outputs, true_labels)
        
        # 软目标损失：学生预测 vs 教师预测 | Soft target loss: student prediction vs teacher prediction
        student_soft = F.log_softmax(student_outputs / self.temperature, dim=1)
        teacher_soft = F.softmax(teacher_outputs / self.temperature, dim=1)
        soft_loss = self.soft_loss(student_soft, teacher_soft) * (self.temperature ** 2)
        
        # 总损失 | Total loss
        total_loss = self.alpha * hard_loss + (1 - self.alpha) * soft_loss
        
        return total_loss, hard_loss, soft_loss

class TeacherModel(nn.Module):
    """
    教师模型：大而强的模型
    Teacher Model: Large and powerful model
    """
    def __init__(self, num_classes=10):
        super(TeacherModel, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            
            nn.Conv2d(64, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            
            nn.Conv2d(128, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
        )
        
        self.classifier = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
            nn.Linear(256, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(512, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(256, num_classes)
        )
    
    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return x
    
    def get_param_count(self):
        """计算参数数量 | Count parameters"""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)

class StudentModel(nn.Module):
    """
    学生模型：小而高效的模型
    Student Model: Small and efficient model
    """
    def __init__(self, num_classes=10):
        super(StudentModel, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 32, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 32, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            
            nn.Conv2d(32, 64, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            
            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
        )
        
        self.classifier = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
            nn.Linear(128, 64),
            nn.ReLU(inplace=True),
            nn.Linear(64, num_classes)
        )
    
    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return x
    
    def get_param_count(self):
        """计算参数数量 | Count parameters"""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)

class KnowledgeDistillationTrainer:
    """
    知识蒸馏训练器
    Knowledge Distillation Trainer
    """
    def __init__(self, teacher_model, student_model, device='cuda'):
        self.teacher_model = teacher_model.to(device)
        self.student_model = student_model.to(device)
        self.device = device
        
        # 冻结教师模型参数 | Freeze teacher model parameters
        for param in self.teacher_model.parameters():
            param.requires_grad = False
        self.teacher_model.eval()
        
        # 训练历史 | Training history
        self.training_history = {
            'total_loss': [],
            'hard_loss': [],
            'soft_loss': [],
            'accuracy': []
        }
    
    def train_student(self, train_loader, val_loader, num_epochs=50, 
                     learning_rate=0.001, temperature=4.0, alpha=0.3):
        """
        训练学生模型
        Train student model
        """
        # 损失函数和优化器 | Loss function and optimizer
        kd_loss = KnowledgeDistillationLoss(temperature=temperature, alpha=alpha)
        optimizer = optim.Adam(self.student_model.parameters(), lr=learning_rate)
        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)
        
        print(f"开始知识蒸馏训练，共{num_epochs}轮...")
        print(f"Starting knowledge distillation training for {num_epochs} epochs...")
        
        for epoch in range(num_epochs):
            # 训练阶段 | Training phase
            self.student_model.train()
            epoch_loss = 0
            epoch_hard_loss = 0
            epoch_soft_loss = 0
            num_batches = 0
            
            for batch_idx, (data, targets) in enumerate(train_loader):
                data, targets = data.to(self.device), targets.to(self.device)
                
                # 获取教师模型输出 | Get teacher model outputs
                with torch.no_grad():
                    teacher_outputs = self.teacher_model(data)
                
                # 获取学生模型输出 | Get student model outputs
                student_outputs = self.student_model(data)
                
                # 计算损失 | Compute loss
                total_loss, hard_loss, soft_loss = kd_loss(
                    student_outputs, teacher_outputs, targets
                )
                
                # 反向传播 | Backpropagation
                optimizer.zero_grad()
                total_loss.backward()
                optimizer.step()
                
                # 累计损失 | Accumulate loss
                epoch_loss += total_loss.item()
                epoch_hard_loss += hard_loss.item()
                epoch_soft_loss += soft_loss.item()
                num_batches += 1
            
            # 验证阶段 | Validation phase
            val_accuracy = self.evaluate_student(val_loader)
            
            # 更新学习率 | Update learning rate
            scheduler.step()
            
            # 记录历史 | Record history
            avg_loss = epoch_loss / num_batches
            avg_hard_loss = epoch_hard_loss / num_batches
            avg_soft_loss = epoch_soft_loss / num_batches
            
            self.training_history['total_loss'].append(avg_loss)
            self.training_history['hard_loss'].append(avg_hard_loss)
            self.training_history['soft_loss'].append(avg_soft_loss)
            self.training_history['accuracy'].append(val_accuracy)
            
            # 打印进度 | Print progress
            if (epoch + 1) % 10 == 0:
                print(f"Epoch {epoch+1}/{num_epochs}")
                print(f"  Total Loss: {avg_loss:.4f}")
                print(f"  Hard Loss: {avg_hard_loss:.4f}")
                print(f"  Soft Loss: {avg_soft_loss:.4f}")
                print(f"  Val Accuracy: {val_accuracy:.2f}%")
        
        print("知识蒸馏训练完成！")
        print("Knowledge distillation training completed!")
    
    def evaluate_student(self, data_loader):
        """
        评估学生模型
        Evaluate student model
        """
        self.student_model.eval()
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, targets in data_loader:
                data, targets = data.to(self.device), targets.to(self.device)
                outputs = self.student_model(data)
                _, predicted = torch.max(outputs.data, 1)
                total += targets.size(0)
                correct += (predicted == targets).sum().item()
        
        accuracy = 100 * correct / total
        return accuracy
    
    def compare_models(self, test_loader):
        """
        比较教师和学生模型的性能
        Compare performance of teacher and student models
        """
        print("=== 模型性能对比 | Model Performance Comparison ===")
        
        # 参数数量对比 | Parameter count comparison
        teacher_params = self.teacher_model.get_param_count()
        student_params = self.student_model.get_param_count()
        compression_ratio = teacher_params / student_params
        
        print(f"教师模型参数数量: {teacher_params:,}")
        print(f"Teacher model parameters: {teacher_params:,}")
        print(f"学生模型参数数量: {student_params:,}")
        print(f"Student model parameters: {student_params:,}")
        print(f"压缩比: {compression_ratio:.2f}x")
        print(f"Compression ratio: {compression_ratio:.2f}x")
        
        # 准确率对比 | Accuracy comparison
        teacher_accuracy = self.evaluate_teacher(test_loader)
        student_accuracy = self.evaluate_student(test_loader)
        
        print(f"教师模型准确率: {teacher_accuracy:.2f}%")
        print(f"Teacher model accuracy: {teacher_accuracy:.2f}%")
        print(f"学生模型准确率: {student_accuracy:.2f}%")
        print(f"Student model accuracy: {student_accuracy:.2f}%")
        print(f"准确率保持率: {student_accuracy/teacher_accuracy*100:.1f}%")
        print(f"Accuracy retention: {student_accuracy/teacher_accuracy*100:.1f}%")
        
        return {
            'teacher_params': teacher_params,
            'student_params': student_params,
            'compression_ratio': compression_ratio,
            'teacher_accuracy': teacher_accuracy,
            'student_accuracy': student_accuracy
        }
    
    def evaluate_teacher(self, data_loader):
        """
        评估教师模型
        Evaluate teacher model
        """
        self.teacher_model.eval()
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, targets in data_loader:
                data, targets = data.to(self.device), targets.to(self.device)
                outputs = self.teacher_model(data)
                _, predicted = torch.max(outputs.data, 1)
                total += targets.size(0)
                correct += (predicted == targets).sum().item()
        
        accuracy = 100 * correct / total
        return accuracy
    
    def visualize_training(self):
        """
        可视化训练过程
        Visualize training process
        """
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        
        epochs = range(1, len(self.training_history['total_loss']) + 1)
        
        # 总损失 | Total loss
        ax1.plot(epochs, self.training_history['total_loss'], 'b-', linewidth=2)
        ax1.set_title('Total Distillation Loss')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.grid(True, alpha=0.3)
        
        # 硬损失和软损失 | Hard loss and soft loss
        ax2.plot(epochs, self.training_history['hard_loss'], 'r-', linewidth=2, label='Hard Loss')
        ax2.plot(epochs, self.training_history['soft_loss'], 'g-', linewidth=2, label='Soft Loss')
        ax2.set_title('Hard vs Soft Loss')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Loss')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 准确率 | Accuracy
        ax3.plot(epochs, self.training_history['accuracy'], 'purple', linewidth=2)
        ax3.set_title('Validation Accuracy')
        ax3.set_xlabel('Epoch')
        ax3.set_ylabel('Accuracy (%)')
        ax3.grid(True, alpha=0.3)
        
        # 损失组件比例 | Loss component ratio
        hard_ratio = np.array(self.training_history['hard_loss']) / np.array(self.training_history['total_loss'])
        soft_ratio = np.array(self.training_history['soft_loss']) / np.array(self.training_history['total_loss'])
        
        ax4.plot(epochs, hard_ratio, 'r-', linewidth=2, label='Hard Loss Ratio')
        ax4.plot(epochs, soft_ratio, 'g-', linewidth=2, label='Soft Loss Ratio')
        ax4.set_title('Loss Component Ratios')
        ax4.set_xlabel('Epoch')
        ax4.set_ylabel('Ratio')
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()

def knowledge_distillation_demo():
    """
    知识蒸馏演示
    Knowledge distillation demo
    """
    # 创建虚拟数据集 | Create dummy dataset
    def create_dummy_data(num_samples=1000, img_size=32):
        """创建虚拟CIFAR-like数据"""
        data = torch.randn(num_samples, 3, img_size, img_size)
        targets = torch.randint(0, 10, (num_samples,))
        return TensorDataset(data, targets)
    
    # 创建数据集 | Create datasets
    train_dataset = create_dummy_data(2000)
    val_dataset = create_dummy_data(500)
    test_dataset = create_dummy_data(500)
    
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
    
    # 创建模型 | Create models
    teacher_model = TeacherModel(num_classes=10)
    student_model = StudentModel(num_classes=10)
    
    print("=== 模型架构对比 | Model Architecture Comparison ===")
    print(f"教师模型参数数量: {teacher_model.get_param_count():,}")
    print(f"Teacher model parameters: {teacher_model.get_param_count():,}")
    print(f"学生模型参数数量: {student_model.get_param_count():,}")
    print(f"Student model parameters: {student_model.get_param_count():,}")
    
    # 检查设备 | Check device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"使用设备: {device}")
    print(f"Using device: {device}")
    
    # 预训练教师模型（简化版）| Pre-train teacher model (simplified)
    print("\n预训练教师模型...")
    print("Pre-training teacher model...")
    
    teacher_model = teacher_model.to(device)
    teacher_optimizer = optim.Adam(teacher_model.parameters(), lr=0.001)
    teacher_criterion = nn.CrossEntropyLoss()
    
    # 简单训练几轮教师模型 | Simple training for teacher model
    teacher_model.train()
    for epoch in range(5):
        for batch_idx, (data, targets) in enumerate(train_loader):
            data, targets = data.to(device), targets.to(device)
            teacher_optimizer.zero_grad()
            outputs = teacher_model(data)
            loss = teacher_criterion(outputs, targets)
            loss.backward()
            teacher_optimizer.step()
        print(f"Teacher training epoch {epoch + 1}/5 completed")
    
    # 创建知识蒸馏训练器 | Create knowledge distillation trainer
    kd_trainer = KnowledgeDistillationTrainer(teacher_model, student_model, device)
    
    # 训练学生模型 | Train student model
    kd_trainer.train_student(
        train_loader, val_loader, 
        num_epochs=30, 
        learning_rate=0.001, 
        temperature=4.0, 
        alpha=0.3
    )
    
    # 比较模型性能 | Compare model performance
    comparison_results = kd_trainer.compare_models(test_loader)
    
    # 可视化训练过程 | Visualize training process
    kd_trainer.visualize_training()
    
    return kd_trainer, comparison_results

if __name__ == "__main__":
    knowledge_distillation_demo()
```

#### 模型剪枝 | Model Pruning

**项目特色 | Project Features:**
模型剪枝通过移除不重要的神经网络连接或神经元，在保持性能的同时大幅减少模型大小和计算量。

Model pruning removes unimportant neural network connections or neurons, significantly reducing model size and computation while maintaining performance.

**剪枝策略实现 | Pruning Strategy Implementation:**
```python
import torch
import torch.nn as nn
import torch.nn.utils.prune as prune
import numpy as np
import matplotlib.pyplot as plt
from collections import OrderedDict

class PruningManager:
    """
    模型剪枝管理器
    Model Pruning Manager
    """
    def __init__(self, model):
        self.model = model
        self.original_params = self._count_parameters()
        self.pruning_history = []
        
    def _count_parameters(self):
        """计算模型参数数量 | Count model parameters"""
        return sum(p.numel() for p in self.model.parameters())
    
    def _count_nonzero_parameters(self):
        """计算非零参数数量 | Count non-zero parameters"""
        return sum((p != 0).sum().item() for p in self.model.parameters())
    
    def magnitude_pruning(self, pruning_ratio=0.2, structured=False):
        """
        基于权重大小的剪枝
        Magnitude-based pruning
        
        Args:
            pruning_ratio: 剪枝比例 | Pruning ratio
            structured: 是否结构化剪枝 | Whether to use structured pruning
        """
        print(f"执行{'结构化' if structured else '非结构化'}幅度剪枝，剪枝比例: {pruning_ratio}")
        print(f"Performing {'structured' if structured else 'unstructured'} magnitude pruning, ratio: {pruning_ratio}")
        
        parameters_to_prune = []
        
        for name, module in self.model.named_modules():
            if isinstance(module, (nn.Conv2d, nn.Linear)):
                if structured:
                    # 结构化剪枝：整个滤波器或神经元 | Structured pruning: entire filters or neurons
                    if isinstance(module, nn.Conv2d):
                        prune.ln_structured(module, name='weight', amount=pruning_ratio, n=2, dim=0)
                    else:
                        prune.ln_structured(module, name='weight', amount=pruning_ratio, n=2, dim=0)
                else:
                    # 非结构化剪枝：随机权重 | Unstructured pruning: random weights
                    prune.l1_unstructured(module, name='weight', amount=pruning_ratio)
                
                parameters_to_prune.append((module, 'weight'))
        
        # 记录剪枝结果 | Record pruning results
        current_params = self._count_nonzero_parameters()
        sparsity = 1 - (current_params / self.original_params)
        
        self.pruning_history.append({
            'method': 'magnitude',
            'ratio': pruning_ratio,
            'structured': structured,
            'remaining_params': current_params,
            'sparsity': sparsity
        })
        
        print(f"剪枝完成，当前稀疏度: {sparsity:.2%}")
        print(f"Pruning completed, current sparsity: {sparsity:.2%}")
        
        return sparsity
    
    def random_pruning(self, pruning_ratio=0.2):
        """
        随机剪枝
        Random pruning
        """
        print(f"执行随机剪枝，剪枝比例: {pruning_ratio}")
        print(f"Performing random pruning, ratio: {pruning_ratio}")
        
        for name, module in self.model.named_modules():
            if isinstance(module, (nn.Conv2d, nn.Linear)):
                prune.random_unstructured(module, name='weight', amount=pruning_ratio)
        
        # 记录剪枝结果 | Record pruning results
        current_params = self._count_nonzero_parameters()
        sparsity = 1 - (current_params / self.original_params)
        
        self.pruning_history.append({
            'method': 'random',
            'ratio': pruning_ratio,
            'structured': False,
            'remaining_params': current_params,
            'sparsity': sparsity
        })
        
        return sparsity
    
    def gradual_pruning(self, initial_sparsity=0.0, final_sparsity=0.8, num_steps=10):
        """
        渐进式剪枝
        Gradual pruning
        """
        print(f"执行渐进式剪枝，从 {initial_sparsity:.1%} 到 {final_sparsity:.1%}，{num_steps} 步")
        print(f"Performing gradual pruning from {initial_sparsity:.1%} to {final_sparsity:.1%}, {num_steps} steps")
        
        sparsity_schedule = np.linspace(initial_sparsity, final_sparsity, num_steps)
        
        for step, target_sparsity in enumerate(sparsity_schedule):
            current_sparsity = 1 - (self._count_nonzero_parameters() / self.original_params)
            
            if target_sparsity > current_sparsity:
                # 计算这一步需要剪枝的比例 | Calculate pruning ratio for this step
                step_ratio = (target_sparsity - current_sparsity) / (1 - current_sparsity)
                
                for name, module in self.model.named_modules():
                    if isinstance(module, (nn.Conv2d, nn.Linear)):
                        prune.l1_unstructured(module, name='weight', amount=step_ratio)
                
                print(f"Step {step + 1}/{num_steps}, Target sparsity: {target_sparsity:.1%}")
        
        final_sparsity = 1 - (self._count_nonzero_parameters() / self.original_params)
        
        self.pruning_history.append({
            'method': 'gradual',
            'steps': num_steps,
            'final_sparsity': final_sparsity,
            'remaining_params': self._count_nonzero_parameters()
        })
        
        return final_sparsity
    
    def remove_pruning(self):
        """
        移除剪枝掩码，使剪枝永久化
        Remove pruning masks to make pruning permanent
        """
        for name, module in self.model.named_modules():
            if isinstance(module, (nn.Conv2d, nn.Linear)):
                try:
                    prune.remove(module, 'weight')
                except ValueError:
                    continue  # 该层没有被剪枝
        
        print("剪枝掩码已移除，剪枝永久化完成")
        print("Pruning masks removed, pruning made permanent")
    
    def analyze_sparsity(self):
        """
        分析模型稀疏度
        Analyze model sparsity
        """
        layer_sparsity = {}
        
        for name, module in self.model.named_modules():
            if isinstance(module, (nn.Conv2d, nn.Linear)):
                if hasattr(module, 'weight_mask'):
                    # 使用剪枝掩码计算稀疏度 | Use pruning mask to calculate sparsity
                    mask = module.weight_mask
                    sparsity = 1 - (mask.sum().item() / mask.numel())
                else:
                    # 直接计算零权重比例 | Directly calculate zero weight ratio
                    weight = module.weight.data
                    sparsity = (weight == 0).sum().item() / weight.numel()
                
                layer_sparsity[name] = sparsity
        
        return layer_sparsity
    
    def visualize_sparsity(self):
        """
        可视化模型稀疏度
        Visualize model sparsity
        """
        layer_sparsity = self.analyze_sparsity()
        
        if not layer_sparsity:
            print("没有检测到剪枝层")
            print("No pruned layers detected")
            return
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # 各层稀疏度 | Layer-wise sparsity
        layers = list(layer_sparsity.keys())
        sparsities = list(layer_sparsity.values())
        
        ax1.bar(range(len(layers)), sparsities, alpha=0.7)
        ax1.set_title('Sparsity by Layer')
        ax1.set_xlabel('Layer')
        ax1.set_ylabel('Sparsity')
        ax1.set_xticks(range(len(layers)))
        ax1.set_xticklabels([f'L{i}' for i in range(len(layers))], rotation=45)
        ax1.grid(True, alpha=0.3)
        
        # 添加数值标签 | Add value labels
        for i, v in enumerate(sparsities):
            ax1.text(i, v + 0.01, f'{v:.1%}', ha='center', va='bottom')
        
        # 剪枝历史 | Pruning history
        if self.pruning_history:
            steps = range(len(self.pruning_history))
            sparsity_history = [entry.get('sparsity', 0) for entry in self.pruning_history]
            
            ax2.plot(steps, sparsity_history, 'o-', linewidth=2, markersize=8)
            ax2.set_title('Pruning Progress')
            ax2.set_xlabel('Pruning Step')
            ax2.set_ylabel('Overall Sparsity')
            ax2.grid(True, alpha=0.3)
            
            # 添加方法标签 | Add method labels
            for i, entry in enumerate(self.pruning_history):
                method = entry.get('method', 'unknown')
                ax2.annotate(method, (i, sparsity_history[i]), 
                           textcoords="offset points", xytext=(0,10), ha='center')
        
        plt.tight_layout()
        plt.show()

class PrunedModelEvaluator:
    """
    剪枝模型评估器
    Pruned Model Evaluator
    """
    def __init__(self, original_model, pruned_model, device='cuda'):
        self.original_model = original_model.to(device)
        self.pruned_model = pruned_model.to(device)
        self.device = device
    
    def compare_performance(self, test_loader, criterion=nn.CrossEntropyLoss()):
        """
        比较原始模型和剪枝模型的性能
        Compare performance of original and pruned models
        """
        print("=== 模型性能对比 | Model Performance Comparison ===")
        
        # 评估原始模型 | Evaluate original model
        original_accuracy, original_loss = self._evaluate_model(
            self.original_model, test_loader, criterion
        )
        
        # 评估剪枝模型 | Evaluate pruned model
        pruned_accuracy, pruned_loss = self._evaluate_model(
            self.pruned_model, test_loader, criterion
        )
        
        # 计算参数数量 | Calculate parameter counts
        original_params = sum(p.numel() for p in self.original_model.parameters())
        pruned_params = sum((p != 0).sum().item() for p in self.pruned_model.parameters())
        
        compression_ratio = original_params / pruned_params if pruned_params > 0 else float('inf')
        accuracy_retention = pruned_accuracy / original_accuracy if original_accuracy > 0 else 0
        
        print(f"原始模型 | Original Model:")
        print(f"  参数数量: {original_params:,}")
        print(f"  准确率: {original_accuracy:.2f}%")
        print(f"  损失: {original_loss:.4f}")
        
        print(f"剪枝模型 | Pruned Model:")
        print(f"  参数数量: {pruned_params:,}")
        print(f"  准确率: {pruned_accuracy:.2f}%")
        print(f"  损失: {pruned_loss:.4f}")
        
        print(f"压缩比: {compression_ratio:.2f}x")
        print(f"准确率保持率: {accuracy_retention:.1%}")
        
        return {
            'original_accuracy': original_accuracy,
            'pruned_accuracy': pruned_accuracy,
            'compression_ratio': compression_ratio,
            'accuracy_retention': accuracy_retention
        }
    
    def _evaluate_model(self, model, data_loader, criterion):
        """
        评估单个模型
        Evaluate single model
        """
        model.eval()
        correct = 0
        total = 0
        total_loss = 0
        num_batches = 0
        
        with torch.no_grad():
            for data, targets in data_loader:
                data, targets = data.to(self.device), targets.to(self.device)
                outputs = model(data)
                loss = criterion(outputs, targets)
                
                _, predicted = torch.max(outputs.data, 1)
                total += targets.size(0)
                correct += (predicted == targets).sum().item()
                total_loss += loss.item()
                num_batches += 1
        
        accuracy = 100 * correct / total
        avg_loss = total_loss / num_batches
        
        return accuracy, avg_loss
    
    def measure_inference_time(self, input_shape=(1, 3, 32, 32), num_runs=100):
        """
        测量推理时间
        Measure inference time
        """
        dummy_input = torch.randn(input_shape).to(self.device)
        
        # 预热 | Warmup
        with torch.no_grad():
            for _ in range(10):
                _ = self.original_model(dummy_input)
                _ = self.pruned_model(dummy_input)
        
        torch.cuda.synchronize() if self.device.type == 'cuda' else None
        
        # 测量原始模型 | Measure original model
        start_time = torch.cuda.Event(enable_timing=True) if self.device.type == 'cuda' else None
        end_time = torch.cuda.Event(enable_timing=True) if self.device.type == 'cuda' else None
        
        if self.device.type == 'cuda':
            start_time.record()
            
        import time
        cpu_start = time.time()
        
        with torch.no_grad():
            for _ in range(num_runs):
                _ = self.original_model(dummy_input)
        
        cpu_end = time.time()
        
        if self.device.type == 'cuda':
            end_time.record()
            torch.cuda.synchronize()
            original_time = start_time.elapsed_time(end_time) / num_runs
        else:
            original_time = (cpu_end - cpu_start) * 1000 / num_runs
        
        # 测量剪枝模型 | Measure pruned model
        if self.device.type == 'cuda':
            start_time.record()
            
        cpu_start = time.time()
        
        with torch.no_grad():
            for _ in range(num_runs):
                _ = self.pruned_model(dummy_input)
        
        cpu_end = time.time()
        
        if self.device.type == 'cuda':
            end_time.record()
            torch.cuda.synchronize()
            pruned_time = start_time.elapsed_time(end_time) / num_runs
        else:
            pruned_time = (cpu_end - cpu_start) * 1000 / num_runs
        
        speedup = original_time / pruned_time if pruned_time > 0 else float('inf')
        
        print(f"推理时间对比 | Inference Time Comparison:")
        print(f"  原始模型: {original_time:.2f} ms")
        print(f"  剪枝模型: {pruned_time:.2f} ms")
        print(f"  加速比: {speedup:.2f}x")
        
        return original_time, pruned_time, speedup

def model_pruning_demo():
    """
    模型剪枝演示
    Model pruning demo
    """
    # 创建简单的CNN模型 | Create simple CNN model
    class SimpleCNN(nn.Module):
        def __init__(self, num_classes=10):
            super(SimpleCNN, self).__init__()
            self.features = nn.Sequential(
                nn.Conv2d(3, 32, 3, padding=1),
                nn.ReLU(inplace=True),
                nn.Conv2d(32, 64, 3, padding=1),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(2, 2),
                nn.Conv2d(64, 128, 3, padding=1),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(2, 2),
            )
            self.classifier = nn.Sequential(
                nn.AdaptiveAvgPool2d((1, 1)),
                nn.Flatten(),
                nn.Linear(128, 64),
                nn.ReLU(inplace=True),
                nn.Linear(64, num_classes)
            )
        
        def forward(self, x):
            x = self.features(x)
            x = self.classifier(x)
            return x
    
    # 创建模型和数据 | Create model and data
    model = SimpleCNN()
    
    # 创建虚拟数据 | Create dummy data
    from torch.utils.data import DataLoader, TensorDataset
    
    test_data = torch.randn(500, 3, 32, 32)
    test_targets = torch.randint(0, 10, (500,))
    test_dataset = TensorDataset(test_data, test_targets)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
    
    # 复制模型用于对比 | Copy model for comparison
    import copy
    original_model = copy.deepcopy(model)
    
    print("=== 模型剪枝演示 | Model Pruning Demo ===")
    print(f"原始模型参数数量: {sum(p.numel() for p in model.parameters()):,}")
    
    # 创建剪枝管理器 | Create pruning manager
    pruning_manager = PruningManager(model)
    
    # 执行不同的剪枝策略 | Perform different pruning strategies
    print("\n1. 幅度剪枝 | Magnitude Pruning")
    pruning_manager.magnitude_pruning(pruning_ratio=0.3, structured=False)
    
    print("\n2. 渐进式剪枝 | Gradual Pruning")
    pruning_manager.gradual_pruning(initial_sparsity=0.3, final_sparsity=0.7, num_steps=5)
    
    # 分析稀疏度 | Analyze sparsity
    print("\n3. 稀疏度分析 | Sparsity Analysis")
    layer_sparsity = pruning_manager.analyze_sparsity()
    for layer, sparsity in layer_sparsity.items():
        print(f"  {layer}: {sparsity:.1%}")
    
    # 可视化剪枝结果 | Visualize pruning results
    pruning_manager.visualize_sparsity()
    
    # 比较模型性能 | Compare model performance
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    evaluator = PrunedModelEvaluator(original_model, model, device)
    
    print("\n4. 性能对比 | Performance Comparison")
    performance_results = evaluator.compare_performance(test_loader)
    
    print("\n5. 推理时间对比 | Inference Time Comparison")
    timing_results = evaluator.measure_inference_time()
    
    # 移除剪枝掩码 | Remove pruning masks
    pruning_manager.remove_pruning()
    
    return pruning_manager, evaluator, performance_results

if __name__ == "__main__":
    model_pruning_demo()
```

---

**🎯 项目完成检查清单 | Project Completion Checklist:**

### 模型压缩理解 | Model Compression Understanding
- [ ] 深入理解知识蒸馏的原理和温度参数的作用
- [ ] 掌握不同剪枝策略的特点和适用场景
- [ ] 理解压缩率与性能保持之间的权衡
- [ ] 能够根据应用需求选择合适的压缩方法

### 技术实现能力 | Technical Implementation Capability
- [ ] 从零实现知识蒸馏训练框架
- [ ] 掌握结构化和非结构化剪枝技术
- [ ] 实现渐进式剪枝和自适应剪枝算法
- [ ] 能够评估和对比不同压缩方法的效果

### 工程应用技能 | Engineering Application Skills
- [ ] 能够在实际模型上应用压缩技术
- [ ] 掌握压缩模型的部署和优化
- [ ] 理解不同硬件平台的性能特点
- [ ] 具备端到端的模型优化能力

**记住**: 模型压缩是AI走向实际应用的关键桥梁。通过这个项目，你将掌握让大模型在资源受限环境中高效运行的核心技术！

**Remember**: Model compression is the key bridge for AI to reach practical applications. Through this project, you will master the core technologies to make large models run efficiently in resource-constrained environments! 