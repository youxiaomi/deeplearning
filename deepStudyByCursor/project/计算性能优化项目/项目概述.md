# è®¡ç®—æ€§èƒ½ä¼˜åŒ–é¡¹ç›®æ¦‚è¿°
# Computational Performance Optimization Project Overview

**è®©AIæ›´è½»é‡æ›´å¿«é€Ÿ - ä»ç ”ç©¶åˆ°ç”Ÿäº§çš„æ€§èƒ½è·¨è¶Š**
**Make AI Lighter and Faster - Performance Leap from Research to Production**

---

## ğŸ¯ é¡¹ç›®ç›®æ ‡ | Project Goals

æ€§èƒ½ä¼˜åŒ–æ˜¯AIèµ°å‘å®é™…åº”ç”¨çš„å…³é”®æ¡¥æ¢ï¼é€šè¿‡ä¼˜åŒ–æŠ€æœ¯ï¼Œæˆ‘ä»¬èƒ½è®©å¤§æ¨¡å‹åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šè¿è¡Œï¼Œè®©æ¨ç†é€Ÿåº¦æå‡æ•°å€ï¼Œè®©å†…å­˜å ç”¨å¤§å¹…é™ä½ã€‚é€šè¿‡è¿™ä¸ªé¡¹ç›®ï¼Œä½ å°†æŒæ¡ï¼š
Performance optimization is the key bridge for AI to reach practical applications! Through optimization techniques, we can make large models run on mobile devices, increase inference speed several times, and significantly reduce memory usage. Through this project, you will master:

- **æ¨¡å‹å‹ç¼©æŠ€æœ¯** | **Model Compression Techniques**: çŸ¥è¯†è’¸é¦ã€æ¨¡å‹å‰ªæç­‰è½»é‡åŒ–æ–¹æ³•
- **æ¨ç†åŠ é€ŸæŠ€æœ¯** | **Inference Acceleration**: é‡åŒ–ã€TensorRTç­‰é«˜æ€§èƒ½æ¨ç†æŠ€æœ¯
- **å†…å­˜ä¼˜åŒ–ç­–ç•¥** | **Memory Optimization**: æ¢¯åº¦ç´¯ç§¯ã€æ··åˆç²¾åº¦ç­‰å†…å­˜ç®¡ç†æŠ€æœ¯
- **éƒ¨ç½²ä¼˜åŒ–æ–¹æ¡ˆ** | **Deployment Optimization**: ç”Ÿäº§ç¯å¢ƒçš„æ€§èƒ½è°ƒä¼˜å’Œç³»ç»Ÿè®¾è®¡

## ğŸ”¬ ä¸ºä»€ä¹ˆæ€§èƒ½ä¼˜åŒ–å¦‚æ­¤é‡è¦ï¼Ÿ| Why is Performance Optimization So Important?

**æ€§èƒ½ä¼˜åŒ–å†³å®šäº†AIæŠ€æœ¯çš„æ™®åŠç¨‹åº¦ï¼**
**Performance optimization determines the adoption level of AI technology!**

ä»ChatGPTçš„åƒäº¿å‚æ•°æ¨¡å‹åˆ°æ‰‹æœºä¸Šçš„è¯­éŸ³åŠ©æ‰‹ï¼Œä»äº‘ç«¯çš„å›¾åƒè¯†åˆ«åˆ°è¾¹ç¼˜è®¾å¤‡çš„å®æ—¶æ£€æµ‹ï¼Œæ€§èƒ½ä¼˜åŒ–è®©AIä»å®éªŒå®¤èµ°å‘åƒå®¶ä¸‡æˆ·ã€‚æŒæ¡æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯ï¼Œå°±æŒæ¡äº†è®©AIçœŸæ­£æœåŠ¡äºäººç±»çš„å…³é”®èƒ½åŠ›ã€‚

From ChatGPT's hundred-billion parameter models to voice assistants on phones, from cloud image recognition to real-time detection on edge devices, performance optimization brings AI from laboratories to every household. Mastering performance optimization techniques means mastering the key ability to make AI truly serve humanity.

### æ€§èƒ½ä¼˜åŒ–çš„å‘å±•å†ç¨‹ | Evolution of Performance Optimization
```
2012: AlexNet GPUåŠ é€Ÿ | AlexNet GPU Acceleration
2015: æ¨¡å‹å‹ç¼©èµ·æ­¥ | Model Compression Beginnings
2017: çŸ¥è¯†è’¸é¦å…´èµ· | Rise of Knowledge Distillation
2018: é‡åŒ–æŠ€æœ¯æ™®åŠ | Quantization Technology Popularization
2020: æ··åˆç²¾åº¦è®­ç»ƒ | Mixed Precision Training
2021: å¤§æ¨¡å‹æ¨ç†ä¼˜åŒ– | Large Model Inference Optimization
2023: ç«¯ä¾§éƒ¨ç½²çªç ´ | Edge Deployment Breakthroughs
```

## ğŸ“š é¡¹ç›®ç»“æ„æ·±åº¦è§£æ | Deep Project Structure Analysis

### 01_æ¨¡å‹å‹ç¼©æŠ€æœ¯ | Model Compression Techniques

**è®©å¤§æ¨¡å‹å˜å°è€Œä¸å¤±æ€§èƒ½ï¼**
**Make large models smaller without losing performance!**

#### çŸ¥è¯†è’¸é¦ | Knowledge Distillation

**é¡¹ç›®æ ¸å¿ƒ | Project Core:**
çŸ¥è¯†è’¸é¦é€šè¿‡æ•™å¸ˆ-å­¦ç”Ÿæ¨¡å‹çš„è®­ç»ƒèŒƒå¼ï¼Œå°†å¤§æ¨¡å‹çš„çŸ¥è¯†è½¬ç§»åˆ°å°æ¨¡å‹ä¸­ï¼Œå®ç°æ€§èƒ½ä¸æ•ˆç‡çš„å¹³è¡¡ã€‚

Knowledge distillation transfers knowledge from large models to small models through teacher-student training paradigm, achieving a balance between performance and efficiency.

**æ•°å­¦åŸç† | Mathematical Principles:**

**è’¸é¦æŸå¤±å‡½æ•° | Distillation Loss Function:**
```
L_total = Î± * L_hard + (1-Î±) * L_soft

L_hard = CrossEntropy(y_true, y_student)
L_soft = KLDivergence(softmax(z_teacher/T), softmax(z_student/T))
```

å…¶ä¸­ï¼š
- `T`: æ¸©åº¦å‚æ•° | temperature parameter
- `Î±`: ç¡¬ç›®æ ‡å’Œè½¯ç›®æ ‡çš„æƒé‡ | weight between hard and soft targets
- `z_teacher`, `z_student`: æ•™å¸ˆå’Œå­¦ç”Ÿæ¨¡å‹çš„logits

**å®Œæ•´å®ç° | Complete Implementation:**
```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import matplotlib.pyplot as plt
from typing import Tuple, Dict, List

class KnowledgeDistillationLoss(nn.Module):
    """
    çŸ¥è¯†è’¸é¦æŸå¤±å‡½æ•°
    Knowledge Distillation Loss Function
    """
    def __init__(self, temperature=4.0, alpha=0.3):
        """
        Args:
            temperature: è½¯åŒ–æ¸©åº¦å‚æ•° | Softening temperature parameter
            alpha: ç¡¬ç›®æ ‡æŸå¤±æƒé‡ | Hard target loss weight
        """
        super(KnowledgeDistillationLoss, self).__init__()
        self.temperature = temperature
        self.alpha = alpha
        self.hard_loss = nn.CrossEntropyLoss()
        self.soft_loss = nn.KLDivLoss(reduction='batchmean')
    
    def forward(self, student_outputs, teacher_outputs, true_labels):
        """
        è®¡ç®—çŸ¥è¯†è’¸é¦æŸå¤±
        Compute knowledge distillation loss
        """
        # ç¡¬ç›®æ ‡æŸå¤±ï¼šå­¦ç”Ÿé¢„æµ‹ vs çœŸå®æ ‡ç­¾ | Hard target loss: student prediction vs true labels
        hard_loss = self.hard_loss(student_outputs, true_labels)
        
        # è½¯ç›®æ ‡æŸå¤±ï¼šå­¦ç”Ÿé¢„æµ‹ vs æ•™å¸ˆé¢„æµ‹ | Soft target loss: student prediction vs teacher prediction
        student_soft = F.log_softmax(student_outputs / self.temperature, dim=1)
        teacher_soft = F.softmax(teacher_outputs / self.temperature, dim=1)
        soft_loss = self.soft_loss(student_soft, teacher_soft) * (self.temperature ** 2)
        
        # æ€»æŸå¤± | Total loss
        total_loss = self.alpha * hard_loss + (1 - self.alpha) * soft_loss
        
        return total_loss, hard_loss, soft_loss

class TeacherModel(nn.Module):
    """
    æ•™å¸ˆæ¨¡å‹ï¼šå¤§è€Œå¼ºçš„æ¨¡å‹
    Teacher Model: Large and powerful model
    """
    def __init__(self, num_classes=10):
        super(TeacherModel, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            
            nn.Conv2d(64, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            
            nn.Conv2d(128, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
        )
        
        self.classifier = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
            nn.Linear(256, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(512, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(256, num_classes)
        )
    
    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return x
    
    def get_param_count(self):
        """è®¡ç®—å‚æ•°æ•°é‡ | Count parameters"""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)

class StudentModel(nn.Module):
    """
    å­¦ç”Ÿæ¨¡å‹ï¼šå°è€Œé«˜æ•ˆçš„æ¨¡å‹
    Student Model: Small and efficient model
    """
    def __init__(self, num_classes=10):
        super(StudentModel, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 32, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 32, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            
            nn.Conv2d(32, 64, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            
            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
        )
        
        self.classifier = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
            nn.Linear(128, 64),
            nn.ReLU(inplace=True),
            nn.Linear(64, num_classes)
        )
    
    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return x
    
    def get_param_count(self):
        """è®¡ç®—å‚æ•°æ•°é‡ | Count parameters"""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)

class KnowledgeDistillationTrainer:
    """
    çŸ¥è¯†è’¸é¦è®­ç»ƒå™¨
    Knowledge Distillation Trainer
    """
    def __init__(self, teacher_model, student_model, device='cuda'):
        self.teacher_model = teacher_model.to(device)
        self.student_model = student_model.to(device)
        self.device = device
        
        # å†»ç»“æ•™å¸ˆæ¨¡å‹å‚æ•° | Freeze teacher model parameters
        for param in self.teacher_model.parameters():
            param.requires_grad = False
        self.teacher_model.eval()
        
        # è®­ç»ƒå†å² | Training history
        self.training_history = {
            'total_loss': [],
            'hard_loss': [],
            'soft_loss': [],
            'accuracy': []
        }
    
    def train_student(self, train_loader, val_loader, num_epochs=50, 
                     learning_rate=0.001, temperature=4.0, alpha=0.3):
        """
        è®­ç»ƒå­¦ç”Ÿæ¨¡å‹
        Train student model
        """
        # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ | Loss function and optimizer
        kd_loss = KnowledgeDistillationLoss(temperature=temperature, alpha=alpha)
        optimizer = optim.Adam(self.student_model.parameters(), lr=learning_rate)
        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)
        
        print(f"å¼€å§‹çŸ¥è¯†è’¸é¦è®­ç»ƒï¼Œå…±{num_epochs}è½®...")
        print(f"Starting knowledge distillation training for {num_epochs} epochs...")
        
        for epoch in range(num_epochs):
            # è®­ç»ƒé˜¶æ®µ | Training phase
            self.student_model.train()
            epoch_loss = 0
            epoch_hard_loss = 0
            epoch_soft_loss = 0
            num_batches = 0
            
            for batch_idx, (data, targets) in enumerate(train_loader):
                data, targets = data.to(self.device), targets.to(self.device)
                
                # è·å–æ•™å¸ˆæ¨¡å‹è¾“å‡º | Get teacher model outputs
                with torch.no_grad():
                    teacher_outputs = self.teacher_model(data)
                
                # è·å–å­¦ç”Ÿæ¨¡å‹è¾“å‡º | Get student model outputs
                student_outputs = self.student_model(data)
                
                # è®¡ç®—æŸå¤± | Compute loss
                total_loss, hard_loss, soft_loss = kd_loss(
                    student_outputs, teacher_outputs, targets
                )
                
                # åå‘ä¼ æ’­ | Backpropagation
                optimizer.zero_grad()
                total_loss.backward()
                optimizer.step()
                
                # ç´¯è®¡æŸå¤± | Accumulate loss
                epoch_loss += total_loss.item()
                epoch_hard_loss += hard_loss.item()
                epoch_soft_loss += soft_loss.item()
                num_batches += 1
            
            # éªŒè¯é˜¶æ®µ | Validation phase
            val_accuracy = self.evaluate_student(val_loader)
            
            # æ›´æ–°å­¦ä¹ ç‡ | Update learning rate
            scheduler.step()
            
            # è®°å½•å†å² | Record history
            avg_loss = epoch_loss / num_batches
            avg_hard_loss = epoch_hard_loss / num_batches
            avg_soft_loss = epoch_soft_loss / num_batches
            
            self.training_history['total_loss'].append(avg_loss)
            self.training_history['hard_loss'].append(avg_hard_loss)
            self.training_history['soft_loss'].append(avg_soft_loss)
            self.training_history['accuracy'].append(val_accuracy)
            
            # æ‰“å°è¿›åº¦ | Print progress
            if (epoch + 1) % 10 == 0:
                print(f"Epoch {epoch+1}/{num_epochs}")
                print(f"  Total Loss: {avg_loss:.4f}")
                print(f"  Hard Loss: {avg_hard_loss:.4f}")
                print(f"  Soft Loss: {avg_soft_loss:.4f}")
                print(f"  Val Accuracy: {val_accuracy:.2f}%")
        
        print("çŸ¥è¯†è’¸é¦è®­ç»ƒå®Œæˆï¼")
        print("Knowledge distillation training completed!")
    
    def evaluate_student(self, data_loader):
        """
        è¯„ä¼°å­¦ç”Ÿæ¨¡å‹
        Evaluate student model
        """
        self.student_model.eval()
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, targets in data_loader:
                data, targets = data.to(self.device), targets.to(self.device)
                outputs = self.student_model(data)
                _, predicted = torch.max(outputs.data, 1)
                total += targets.size(0)
                correct += (predicted == targets).sum().item()
        
        accuracy = 100 * correct / total
        return accuracy
    
    def compare_models(self, test_loader):
        """
        æ¯”è¾ƒæ•™å¸ˆå’Œå­¦ç”Ÿæ¨¡å‹çš„æ€§èƒ½
        Compare performance of teacher and student models
        """
        print("=== æ¨¡å‹æ€§èƒ½å¯¹æ¯” | Model Performance Comparison ===")
        
        # å‚æ•°æ•°é‡å¯¹æ¯” | Parameter count comparison
        teacher_params = self.teacher_model.get_param_count()
        student_params = self.student_model.get_param_count()
        compression_ratio = teacher_params / student_params
        
        print(f"æ•™å¸ˆæ¨¡å‹å‚æ•°æ•°é‡: {teacher_params:,}")
        print(f"Teacher model parameters: {teacher_params:,}")
        print(f"å­¦ç”Ÿæ¨¡å‹å‚æ•°æ•°é‡: {student_params:,}")
        print(f"Student model parameters: {student_params:,}")
        print(f"å‹ç¼©æ¯”: {compression_ratio:.2f}x")
        print(f"Compression ratio: {compression_ratio:.2f}x")
        
        # å‡†ç¡®ç‡å¯¹æ¯” | Accuracy comparison
        teacher_accuracy = self.evaluate_teacher(test_loader)
        student_accuracy = self.evaluate_student(test_loader)
        
        print(f"æ•™å¸ˆæ¨¡å‹å‡†ç¡®ç‡: {teacher_accuracy:.2f}%")
        print(f"Teacher model accuracy: {teacher_accuracy:.2f}%")
        print(f"å­¦ç”Ÿæ¨¡å‹å‡†ç¡®ç‡: {student_accuracy:.2f}%")
        print(f"Student model accuracy: {student_accuracy:.2f}%")
        print(f"å‡†ç¡®ç‡ä¿æŒç‡: {student_accuracy/teacher_accuracy*100:.1f}%")
        print(f"Accuracy retention: {student_accuracy/teacher_accuracy*100:.1f}%")
        
        return {
            'teacher_params': teacher_params,
            'student_params': student_params,
            'compression_ratio': compression_ratio,
            'teacher_accuracy': teacher_accuracy,
            'student_accuracy': student_accuracy
        }
    
    def evaluate_teacher(self, data_loader):
        """
        è¯„ä¼°æ•™å¸ˆæ¨¡å‹
        Evaluate teacher model
        """
        self.teacher_model.eval()
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, targets in data_loader:
                data, targets = data.to(self.device), targets.to(self.device)
                outputs = self.teacher_model(data)
                _, predicted = torch.max(outputs.data, 1)
                total += targets.size(0)
                correct += (predicted == targets).sum().item()
        
        accuracy = 100 * correct / total
        return accuracy
    
    def visualize_training(self):
        """
        å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹
        Visualize training process
        """
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        
        epochs = range(1, len(self.training_history['total_loss']) + 1)
        
        # æ€»æŸå¤± | Total loss
        ax1.plot(epochs, self.training_history['total_loss'], 'b-', linewidth=2)
        ax1.set_title('Total Distillation Loss')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.grid(True, alpha=0.3)
        
        # ç¡¬æŸå¤±å’Œè½¯æŸå¤± | Hard loss and soft loss
        ax2.plot(epochs, self.training_history['hard_loss'], 'r-', linewidth=2, label='Hard Loss')
        ax2.plot(epochs, self.training_history['soft_loss'], 'g-', linewidth=2, label='Soft Loss')
        ax2.set_title('Hard vs Soft Loss')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Loss')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # å‡†ç¡®ç‡ | Accuracy
        ax3.plot(epochs, self.training_history['accuracy'], 'purple', linewidth=2)
        ax3.set_title('Validation Accuracy')
        ax3.set_xlabel('Epoch')
        ax3.set_ylabel('Accuracy (%)')
        ax3.grid(True, alpha=0.3)
        
        # æŸå¤±ç»„ä»¶æ¯”ä¾‹ | Loss component ratio
        hard_ratio = np.array(self.training_history['hard_loss']) / np.array(self.training_history['total_loss'])
        soft_ratio = np.array(self.training_history['soft_loss']) / np.array(self.training_history['total_loss'])
        
        ax4.plot(epochs, hard_ratio, 'r-', linewidth=2, label='Hard Loss Ratio')
        ax4.plot(epochs, soft_ratio, 'g-', linewidth=2, label='Soft Loss Ratio')
        ax4.set_title('Loss Component Ratios')
        ax4.set_xlabel('Epoch')
        ax4.set_ylabel('Ratio')
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()

def knowledge_distillation_demo():
    """
    çŸ¥è¯†è’¸é¦æ¼”ç¤º
    Knowledge distillation demo
    """
    # åˆ›å»ºè™šæ‹Ÿæ•°æ®é›† | Create dummy dataset
    def create_dummy_data(num_samples=1000, img_size=32):
        """åˆ›å»ºè™šæ‹ŸCIFAR-likeæ•°æ®"""
        data = torch.randn(num_samples, 3, img_size, img_size)
        targets = torch.randint(0, 10, (num_samples,))
        return TensorDataset(data, targets)
    
    # åˆ›å»ºæ•°æ®é›† | Create datasets
    train_dataset = create_dummy_data(2000)
    val_dataset = create_dummy_data(500)
    test_dataset = create_dummy_data(500)
    
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
    
    # åˆ›å»ºæ¨¡å‹ | Create models
    teacher_model = TeacherModel(num_classes=10)
    student_model = StudentModel(num_classes=10)
    
    print("=== æ¨¡å‹æ¶æ„å¯¹æ¯” | Model Architecture Comparison ===")
    print(f"æ•™å¸ˆæ¨¡å‹å‚æ•°æ•°é‡: {teacher_model.get_param_count():,}")
    print(f"Teacher model parameters: {teacher_model.get_param_count():,}")
    print(f"å­¦ç”Ÿæ¨¡å‹å‚æ•°æ•°é‡: {student_model.get_param_count():,}")
    print(f"Student model parameters: {student_model.get_param_count():,}")
    
    # æ£€æŸ¥è®¾å¤‡ | Check device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    print(f"Using device: {device}")
    
    # é¢„è®­ç»ƒæ•™å¸ˆæ¨¡å‹ï¼ˆç®€åŒ–ç‰ˆï¼‰| Pre-train teacher model (simplified)
    print("\né¢„è®­ç»ƒæ•™å¸ˆæ¨¡å‹...")
    print("Pre-training teacher model...")
    
    teacher_model = teacher_model.to(device)
    teacher_optimizer = optim.Adam(teacher_model.parameters(), lr=0.001)
    teacher_criterion = nn.CrossEntropyLoss()
    
    # ç®€å•è®­ç»ƒå‡ è½®æ•™å¸ˆæ¨¡å‹ | Simple training for teacher model
    teacher_model.train()
    for epoch in range(5):
        for batch_idx, (data, targets) in enumerate(train_loader):
            data, targets = data.to(device), targets.to(device)
            teacher_optimizer.zero_grad()
            outputs = teacher_model(data)
            loss = teacher_criterion(outputs, targets)
            loss.backward()
            teacher_optimizer.step()
        print(f"Teacher training epoch {epoch + 1}/5 completed")
    
    # åˆ›å»ºçŸ¥è¯†è’¸é¦è®­ç»ƒå™¨ | Create knowledge distillation trainer
    kd_trainer = KnowledgeDistillationTrainer(teacher_model, student_model, device)
    
    # è®­ç»ƒå­¦ç”Ÿæ¨¡å‹ | Train student model
    kd_trainer.train_student(
        train_loader, val_loader, 
        num_epochs=30, 
        learning_rate=0.001, 
        temperature=4.0, 
        alpha=0.3
    )
    
    # æ¯”è¾ƒæ¨¡å‹æ€§èƒ½ | Compare model performance
    comparison_results = kd_trainer.compare_models(test_loader)
    
    # å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹ | Visualize training process
    kd_trainer.visualize_training()
    
    return kd_trainer, comparison_results

if __name__ == "__main__":
    knowledge_distillation_demo()
```

#### æ¨¡å‹å‰ªæ | Model Pruning

**é¡¹ç›®ç‰¹è‰² | Project Features:**
æ¨¡å‹å‰ªæé€šè¿‡ç§»é™¤ä¸é‡è¦çš„ç¥ç»ç½‘ç»œè¿æ¥æˆ–ç¥ç»å…ƒï¼Œåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶å¤§å¹…å‡å°‘æ¨¡å‹å¤§å°å’Œè®¡ç®—é‡ã€‚

Model pruning removes unimportant neural network connections or neurons, significantly reducing model size and computation while maintaining performance.

**å‰ªæç­–ç•¥å®ç° | Pruning Strategy Implementation:**
```python
import torch
import torch.nn as nn
import torch.nn.utils.prune as prune
import numpy as np
import matplotlib.pyplot as plt
from collections import OrderedDict

class PruningManager:
    """
    æ¨¡å‹å‰ªæç®¡ç†å™¨
    Model Pruning Manager
    """
    def __init__(self, model):
        self.model = model
        self.original_params = self._count_parameters()
        self.pruning_history = []
        
    def _count_parameters(self):
        """è®¡ç®—æ¨¡å‹å‚æ•°æ•°é‡ | Count model parameters"""
        return sum(p.numel() for p in self.model.parameters())
    
    def _count_nonzero_parameters(self):
        """è®¡ç®—éé›¶å‚æ•°æ•°é‡ | Count non-zero parameters"""
        return sum((p != 0).sum().item() for p in self.model.parameters())
    
    def magnitude_pruning(self, pruning_ratio=0.2, structured=False):
        """
        åŸºäºæƒé‡å¤§å°çš„å‰ªæ
        Magnitude-based pruning
        
        Args:
            pruning_ratio: å‰ªææ¯”ä¾‹ | Pruning ratio
            structured: æ˜¯å¦ç»“æ„åŒ–å‰ªæ | Whether to use structured pruning
        """
        print(f"æ‰§è¡Œ{'ç»“æ„åŒ–' if structured else 'éç»“æ„åŒ–'}å¹…åº¦å‰ªæï¼Œå‰ªææ¯”ä¾‹: {pruning_ratio}")
        print(f"Performing {'structured' if structured else 'unstructured'} magnitude pruning, ratio: {pruning_ratio}")
        
        parameters_to_prune = []
        
        for name, module in self.model.named_modules():
            if isinstance(module, (nn.Conv2d, nn.Linear)):
                if structured:
                    # ç»“æ„åŒ–å‰ªæï¼šæ•´ä¸ªæ»¤æ³¢å™¨æˆ–ç¥ç»å…ƒ | Structured pruning: entire filters or neurons
                    if isinstance(module, nn.Conv2d):
                        prune.ln_structured(module, name='weight', amount=pruning_ratio, n=2, dim=0)
                    else:
                        prune.ln_structured(module, name='weight', amount=pruning_ratio, n=2, dim=0)
                else:
                    # éç»“æ„åŒ–å‰ªæï¼šéšæœºæƒé‡ | Unstructured pruning: random weights
                    prune.l1_unstructured(module, name='weight', amount=pruning_ratio)
                
                parameters_to_prune.append((module, 'weight'))
        
        # è®°å½•å‰ªæç»“æœ | Record pruning results
        current_params = self._count_nonzero_parameters()
        sparsity = 1 - (current_params / self.original_params)
        
        self.pruning_history.append({
            'method': 'magnitude',
            'ratio': pruning_ratio,
            'structured': structured,
            'remaining_params': current_params,
            'sparsity': sparsity
        })
        
        print(f"å‰ªæå®Œæˆï¼Œå½“å‰ç¨€ç–åº¦: {sparsity:.2%}")
        print(f"Pruning completed, current sparsity: {sparsity:.2%}")
        
        return sparsity
    
    def random_pruning(self, pruning_ratio=0.2):
        """
        éšæœºå‰ªæ
        Random pruning
        """
        print(f"æ‰§è¡Œéšæœºå‰ªæï¼Œå‰ªææ¯”ä¾‹: {pruning_ratio}")
        print(f"Performing random pruning, ratio: {pruning_ratio}")
        
        for name, module in self.model.named_modules():
            if isinstance(module, (nn.Conv2d, nn.Linear)):
                prune.random_unstructured(module, name='weight', amount=pruning_ratio)
        
        # è®°å½•å‰ªæç»“æœ | Record pruning results
        current_params = self._count_nonzero_parameters()
        sparsity = 1 - (current_params / self.original_params)
        
        self.pruning_history.append({
            'method': 'random',
            'ratio': pruning_ratio,
            'structured': False,
            'remaining_params': current_params,
            'sparsity': sparsity
        })
        
        return sparsity
    
    def gradual_pruning(self, initial_sparsity=0.0, final_sparsity=0.8, num_steps=10):
        """
        æ¸è¿›å¼å‰ªæ
        Gradual pruning
        """
        print(f"æ‰§è¡Œæ¸è¿›å¼å‰ªæï¼Œä» {initial_sparsity:.1%} åˆ° {final_sparsity:.1%}ï¼Œ{num_steps} æ­¥")
        print(f"Performing gradual pruning from {initial_sparsity:.1%} to {final_sparsity:.1%}, {num_steps} steps")
        
        sparsity_schedule = np.linspace(initial_sparsity, final_sparsity, num_steps)
        
        for step, target_sparsity in enumerate(sparsity_schedule):
            current_sparsity = 1 - (self._count_nonzero_parameters() / self.original_params)
            
            if target_sparsity > current_sparsity:
                # è®¡ç®—è¿™ä¸€æ­¥éœ€è¦å‰ªæçš„æ¯”ä¾‹ | Calculate pruning ratio for this step
                step_ratio = (target_sparsity - current_sparsity) / (1 - current_sparsity)
                
                for name, module in self.model.named_modules():
                    if isinstance(module, (nn.Conv2d, nn.Linear)):
                        prune.l1_unstructured(module, name='weight', amount=step_ratio)
                
                print(f"Step {step + 1}/{num_steps}, Target sparsity: {target_sparsity:.1%}")
        
        final_sparsity = 1 - (self._count_nonzero_parameters() / self.original_params)
        
        self.pruning_history.append({
            'method': 'gradual',
            'steps': num_steps,
            'final_sparsity': final_sparsity,
            'remaining_params': self._count_nonzero_parameters()
        })
        
        return final_sparsity
    
    def remove_pruning(self):
        """
        ç§»é™¤å‰ªææ©ç ï¼Œä½¿å‰ªææ°¸ä¹…åŒ–
        Remove pruning masks to make pruning permanent
        """
        for name, module in self.model.named_modules():
            if isinstance(module, (nn.Conv2d, nn.Linear)):
                try:
                    prune.remove(module, 'weight')
                except ValueError:
                    continue  # è¯¥å±‚æ²¡æœ‰è¢«å‰ªæ
        
        print("å‰ªææ©ç å·²ç§»é™¤ï¼Œå‰ªææ°¸ä¹…åŒ–å®Œæˆ")
        print("Pruning masks removed, pruning made permanent")
    
    def analyze_sparsity(self):
        """
        åˆ†ææ¨¡å‹ç¨€ç–åº¦
        Analyze model sparsity
        """
        layer_sparsity = {}
        
        for name, module in self.model.named_modules():
            if isinstance(module, (nn.Conv2d, nn.Linear)):
                if hasattr(module, 'weight_mask'):
                    # ä½¿ç”¨å‰ªææ©ç è®¡ç®—ç¨€ç–åº¦ | Use pruning mask to calculate sparsity
                    mask = module.weight_mask
                    sparsity = 1 - (mask.sum().item() / mask.numel())
                else:
                    # ç›´æ¥è®¡ç®—é›¶æƒé‡æ¯”ä¾‹ | Directly calculate zero weight ratio
                    weight = module.weight.data
                    sparsity = (weight == 0).sum().item() / weight.numel()
                
                layer_sparsity[name] = sparsity
        
        return layer_sparsity
    
    def visualize_sparsity(self):
        """
        å¯è§†åŒ–æ¨¡å‹ç¨€ç–åº¦
        Visualize model sparsity
        """
        layer_sparsity = self.analyze_sparsity()
        
        if not layer_sparsity:
            print("æ²¡æœ‰æ£€æµ‹åˆ°å‰ªæå±‚")
            print("No pruned layers detected")
            return
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # å„å±‚ç¨€ç–åº¦ | Layer-wise sparsity
        layers = list(layer_sparsity.keys())
        sparsities = list(layer_sparsity.values())
        
        ax1.bar(range(len(layers)), sparsities, alpha=0.7)
        ax1.set_title('Sparsity by Layer')
        ax1.set_xlabel('Layer')
        ax1.set_ylabel('Sparsity')
        ax1.set_xticks(range(len(layers)))
        ax1.set_xticklabels([f'L{i}' for i in range(len(layers))], rotation=45)
        ax1.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾ | Add value labels
        for i, v in enumerate(sparsities):
            ax1.text(i, v + 0.01, f'{v:.1%}', ha='center', va='bottom')
        
        # å‰ªæå†å² | Pruning history
        if self.pruning_history:
            steps = range(len(self.pruning_history))
            sparsity_history = [entry.get('sparsity', 0) for entry in self.pruning_history]
            
            ax2.plot(steps, sparsity_history, 'o-', linewidth=2, markersize=8)
            ax2.set_title('Pruning Progress')
            ax2.set_xlabel('Pruning Step')
            ax2.set_ylabel('Overall Sparsity')
            ax2.grid(True, alpha=0.3)
            
            # æ·»åŠ æ–¹æ³•æ ‡ç­¾ | Add method labels
            for i, entry in enumerate(self.pruning_history):
                method = entry.get('method', 'unknown')
                ax2.annotate(method, (i, sparsity_history[i]), 
                           textcoords="offset points", xytext=(0,10), ha='center')
        
        plt.tight_layout()
        plt.show()

class PrunedModelEvaluator:
    """
    å‰ªææ¨¡å‹è¯„ä¼°å™¨
    Pruned Model Evaluator
    """
    def __init__(self, original_model, pruned_model, device='cuda'):
        self.original_model = original_model.to(device)
        self.pruned_model = pruned_model.to(device)
        self.device = device
    
    def compare_performance(self, test_loader, criterion=nn.CrossEntropyLoss()):
        """
        æ¯”è¾ƒåŸå§‹æ¨¡å‹å’Œå‰ªææ¨¡å‹çš„æ€§èƒ½
        Compare performance of original and pruned models
        """
        print("=== æ¨¡å‹æ€§èƒ½å¯¹æ¯” | Model Performance Comparison ===")
        
        # è¯„ä¼°åŸå§‹æ¨¡å‹ | Evaluate original model
        original_accuracy, original_loss = self._evaluate_model(
            self.original_model, test_loader, criterion
        )
        
        # è¯„ä¼°å‰ªææ¨¡å‹ | Evaluate pruned model
        pruned_accuracy, pruned_loss = self._evaluate_model(
            self.pruned_model, test_loader, criterion
        )
        
        # è®¡ç®—å‚æ•°æ•°é‡ | Calculate parameter counts
        original_params = sum(p.numel() for p in self.original_model.parameters())
        pruned_params = sum((p != 0).sum().item() for p in self.pruned_model.parameters())
        
        compression_ratio = original_params / pruned_params if pruned_params > 0 else float('inf')
        accuracy_retention = pruned_accuracy / original_accuracy if original_accuracy > 0 else 0
        
        print(f"åŸå§‹æ¨¡å‹ | Original Model:")
        print(f"  å‚æ•°æ•°é‡: {original_params:,}")
        print(f"  å‡†ç¡®ç‡: {original_accuracy:.2f}%")
        print(f"  æŸå¤±: {original_loss:.4f}")
        
        print(f"å‰ªææ¨¡å‹ | Pruned Model:")
        print(f"  å‚æ•°æ•°é‡: {pruned_params:,}")
        print(f"  å‡†ç¡®ç‡: {pruned_accuracy:.2f}%")
        print(f"  æŸå¤±: {pruned_loss:.4f}")
        
        print(f"å‹ç¼©æ¯”: {compression_ratio:.2f}x")
        print(f"å‡†ç¡®ç‡ä¿æŒç‡: {accuracy_retention:.1%}")
        
        return {
            'original_accuracy': original_accuracy,
            'pruned_accuracy': pruned_accuracy,
            'compression_ratio': compression_ratio,
            'accuracy_retention': accuracy_retention
        }
    
    def _evaluate_model(self, model, data_loader, criterion):
        """
        è¯„ä¼°å•ä¸ªæ¨¡å‹
        Evaluate single model
        """
        model.eval()
        correct = 0
        total = 0
        total_loss = 0
        num_batches = 0
        
        with torch.no_grad():
            for data, targets in data_loader:
                data, targets = data.to(self.device), targets.to(self.device)
                outputs = model(data)
                loss = criterion(outputs, targets)
                
                _, predicted = torch.max(outputs.data, 1)
                total += targets.size(0)
                correct += (predicted == targets).sum().item()
                total_loss += loss.item()
                num_batches += 1
        
        accuracy = 100 * correct / total
        avg_loss = total_loss / num_batches
        
        return accuracy, avg_loss
    
    def measure_inference_time(self, input_shape=(1, 3, 32, 32), num_runs=100):
        """
        æµ‹é‡æ¨ç†æ—¶é—´
        Measure inference time
        """
        dummy_input = torch.randn(input_shape).to(self.device)
        
        # é¢„çƒ­ | Warmup
        with torch.no_grad():
            for _ in range(10):
                _ = self.original_model(dummy_input)
                _ = self.pruned_model(dummy_input)
        
        torch.cuda.synchronize() if self.device.type == 'cuda' else None
        
        # æµ‹é‡åŸå§‹æ¨¡å‹ | Measure original model
        start_time = torch.cuda.Event(enable_timing=True) if self.device.type == 'cuda' else None
        end_time = torch.cuda.Event(enable_timing=True) if self.device.type == 'cuda' else None
        
        if self.device.type == 'cuda':
            start_time.record()
            
        import time
        cpu_start = time.time()
        
        with torch.no_grad():
            for _ in range(num_runs):
                _ = self.original_model(dummy_input)
        
        cpu_end = time.time()
        
        if self.device.type == 'cuda':
            end_time.record()
            torch.cuda.synchronize()
            original_time = start_time.elapsed_time(end_time) / num_runs
        else:
            original_time = (cpu_end - cpu_start) * 1000 / num_runs
        
        # æµ‹é‡å‰ªææ¨¡å‹ | Measure pruned model
        if self.device.type == 'cuda':
            start_time.record()
            
        cpu_start = time.time()
        
        with torch.no_grad():
            for _ in range(num_runs):
                _ = self.pruned_model(dummy_input)
        
        cpu_end = time.time()
        
        if self.device.type == 'cuda':
            end_time.record()
            torch.cuda.synchronize()
            pruned_time = start_time.elapsed_time(end_time) / num_runs
        else:
            pruned_time = (cpu_end - cpu_start) * 1000 / num_runs
        
        speedup = original_time / pruned_time if pruned_time > 0 else float('inf')
        
        print(f"æ¨ç†æ—¶é—´å¯¹æ¯” | Inference Time Comparison:")
        print(f"  åŸå§‹æ¨¡å‹: {original_time:.2f} ms")
        print(f"  å‰ªææ¨¡å‹: {pruned_time:.2f} ms")
        print(f"  åŠ é€Ÿæ¯”: {speedup:.2f}x")
        
        return original_time, pruned_time, speedup

def model_pruning_demo():
    """
    æ¨¡å‹å‰ªææ¼”ç¤º
    Model pruning demo
    """
    # åˆ›å»ºç®€å•çš„CNNæ¨¡å‹ | Create simple CNN model
    class SimpleCNN(nn.Module):
        def __init__(self, num_classes=10):
            super(SimpleCNN, self).__init__()
            self.features = nn.Sequential(
                nn.Conv2d(3, 32, 3, padding=1),
                nn.ReLU(inplace=True),
                nn.Conv2d(32, 64, 3, padding=1),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(2, 2),
                nn.Conv2d(64, 128, 3, padding=1),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(2, 2),
            )
            self.classifier = nn.Sequential(
                nn.AdaptiveAvgPool2d((1, 1)),
                nn.Flatten(),
                nn.Linear(128, 64),
                nn.ReLU(inplace=True),
                nn.Linear(64, num_classes)
            )
        
        def forward(self, x):
            x = self.features(x)
            x = self.classifier(x)
            return x
    
    # åˆ›å»ºæ¨¡å‹å’Œæ•°æ® | Create model and data
    model = SimpleCNN()
    
    # åˆ›å»ºè™šæ‹Ÿæ•°æ® | Create dummy data
    from torch.utils.data import DataLoader, TensorDataset
    
    test_data = torch.randn(500, 3, 32, 32)
    test_targets = torch.randint(0, 10, (500,))
    test_dataset = TensorDataset(test_data, test_targets)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
    
    # å¤åˆ¶æ¨¡å‹ç”¨äºå¯¹æ¯” | Copy model for comparison
    import copy
    original_model = copy.deepcopy(model)
    
    print("=== æ¨¡å‹å‰ªææ¼”ç¤º | Model Pruning Demo ===")
    print(f"åŸå§‹æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # åˆ›å»ºå‰ªæç®¡ç†å™¨ | Create pruning manager
    pruning_manager = PruningManager(model)
    
    # æ‰§è¡Œä¸åŒçš„å‰ªæç­–ç•¥ | Perform different pruning strategies
    print("\n1. å¹…åº¦å‰ªæ | Magnitude Pruning")
    pruning_manager.magnitude_pruning(pruning_ratio=0.3, structured=False)
    
    print("\n2. æ¸è¿›å¼å‰ªæ | Gradual Pruning")
    pruning_manager.gradual_pruning(initial_sparsity=0.3, final_sparsity=0.7, num_steps=5)
    
    # åˆ†æç¨€ç–åº¦ | Analyze sparsity
    print("\n3. ç¨€ç–åº¦åˆ†æ | Sparsity Analysis")
    layer_sparsity = pruning_manager.analyze_sparsity()
    for layer, sparsity in layer_sparsity.items():
        print(f"  {layer}: {sparsity:.1%}")
    
    # å¯è§†åŒ–å‰ªæç»“æœ | Visualize pruning results
    pruning_manager.visualize_sparsity()
    
    # æ¯”è¾ƒæ¨¡å‹æ€§èƒ½ | Compare model performance
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    evaluator = PrunedModelEvaluator(original_model, model, device)
    
    print("\n4. æ€§èƒ½å¯¹æ¯” | Performance Comparison")
    performance_results = evaluator.compare_performance(test_loader)
    
    print("\n5. æ¨ç†æ—¶é—´å¯¹æ¯” | Inference Time Comparison")
    timing_results = evaluator.measure_inference_time()
    
    # ç§»é™¤å‰ªææ©ç  | Remove pruning masks
    pruning_manager.remove_pruning()
    
    return pruning_manager, evaluator, performance_results

if __name__ == "__main__":
    model_pruning_demo()
```

---

**ğŸ¯ é¡¹ç›®å®Œæˆæ£€æŸ¥æ¸…å• | Project Completion Checklist:**

### æ¨¡å‹å‹ç¼©ç†è§£ | Model Compression Understanding
- [ ] æ·±å…¥ç†è§£çŸ¥è¯†è’¸é¦çš„åŸç†å’Œæ¸©åº¦å‚æ•°çš„ä½œç”¨
- [ ] æŒæ¡ä¸åŒå‰ªæç­–ç•¥çš„ç‰¹ç‚¹å’Œé€‚ç”¨åœºæ™¯
- [ ] ç†è§£å‹ç¼©ç‡ä¸æ€§èƒ½ä¿æŒä¹‹é—´çš„æƒè¡¡
- [ ] èƒ½å¤Ÿæ ¹æ®åº”ç”¨éœ€æ±‚é€‰æ‹©åˆé€‚çš„å‹ç¼©æ–¹æ³•

### æŠ€æœ¯å®ç°èƒ½åŠ› | Technical Implementation Capability
- [ ] ä»é›¶å®ç°çŸ¥è¯†è’¸é¦è®­ç»ƒæ¡†æ¶
- [ ] æŒæ¡ç»“æ„åŒ–å’Œéç»“æ„åŒ–å‰ªææŠ€æœ¯
- [ ] å®ç°æ¸è¿›å¼å‰ªæå’Œè‡ªé€‚åº”å‰ªæç®—æ³•
- [ ] èƒ½å¤Ÿè¯„ä¼°å’Œå¯¹æ¯”ä¸åŒå‹ç¼©æ–¹æ³•çš„æ•ˆæœ

### å·¥ç¨‹åº”ç”¨æŠ€èƒ½ | Engineering Application Skills
- [ ] èƒ½å¤Ÿåœ¨å®é™…æ¨¡å‹ä¸Šåº”ç”¨å‹ç¼©æŠ€æœ¯
- [ ] æŒæ¡å‹ç¼©æ¨¡å‹çš„éƒ¨ç½²å’Œä¼˜åŒ–
- [ ] ç†è§£ä¸åŒç¡¬ä»¶å¹³å°çš„æ€§èƒ½ç‰¹ç‚¹
- [ ] å…·å¤‡ç«¯åˆ°ç«¯çš„æ¨¡å‹ä¼˜åŒ–èƒ½åŠ›

**è®°ä½**: æ¨¡å‹å‹ç¼©æ˜¯AIèµ°å‘å®é™…åº”ç”¨çš„å…³é”®æ¡¥æ¢ã€‚é€šè¿‡è¿™ä¸ªé¡¹ç›®ï¼Œä½ å°†æŒæ¡è®©å¤§æ¨¡å‹åœ¨èµ„æºå—é™ç¯å¢ƒä¸­é«˜æ•ˆè¿è¡Œçš„æ ¸å¿ƒæŠ€æœ¯ï¼

**Remember**: Model compression is the key bridge for AI to reach practical applications. Through this project, you will master the core technologies to make large models run efficiently in resource-constrained environments! 