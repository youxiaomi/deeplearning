"""
ä½¿ç”¨LSTMè¿›è¡Œè‚¡ç¥¨ä»·æ ¼é¢„æµ‹ - æ—¶é—´åºåˆ—é¢„æµ‹çš„å®é™…åº”ç”¨
Stock Price Prediction using LSTM - Real-world Application of Time Series Forecasting

è¿™ä¸ªé¡¹ç›®æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨LSTMé¢„æµ‹è‚¡ç¥¨ä»·æ ¼ï¼Œè™½ç„¶å®é™…æŠ•èµ„éœ€è¦è€ƒè™‘æ›´å¤šå› ç´ ï¼Œ
ä½†è¿™æ˜¯å­¦ä¹ æ—¶é—´åºåˆ—é¢„æµ‹çš„å¥½ä¾‹å­ã€‚

This project demonstrates how to use LSTM for stock price prediction. 
Although actual investment requires considering more factors, 
this is a good example for learning time series forecasting.
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import yfinance as yf
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
import warnings
import sys
import os

warnings.filterwarnings('ignore')

# æ·»åŠ ä¸Šçº§ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils.data_utils import set_random_seed, create_sequences, normalize_data


class StockLSTM(nn.Module):
    """
    è‚¡ç¥¨ä»·æ ¼é¢„æµ‹LSTMæ¨¡å‹
    Stock Price Prediction LSTM Model
    
    è¿™ä¸ªæ¨¡å‹å°±åƒä¸€ä¸ªç»éªŒä¸°å¯Œçš„åˆ†æå¸ˆï¼Œé€šè¿‡è§‚å¯Ÿå†å²ä»·æ ¼æ¨¡å¼æ¥é¢„æµ‹æœªæ¥èµ°åŠ¿ã€‚
    This model is like an experienced analyst who predicts future trends by observing historical price patterns.
    """
    
    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):
        """
        åˆå§‹åŒ–è‚¡ç¥¨é¢„æµ‹LSTMæ¨¡å‹
        Initialize stock prediction LSTM model
        
        Args:
            input_size: è¾“å…¥ç‰¹å¾æ•°ï¼ˆå¦‚å¼€ç›˜ä»·ã€æ”¶ç›˜ä»·ã€æˆäº¤é‡ç­‰ï¼‰| Number of input features
            hidden_size: éšè—å±‚å¤§å° | Hidden layer size
            num_layers: LSTMå±‚æ•° | Number of LSTM layers
            output_size: è¾“å‡ºå¤§å°ï¼ˆé€šå¸¸ä¸º1ï¼Œé¢„æµ‹æ”¶ç›˜ä»·ï¼‰| Output size (usually 1, predicting close price)
            dropout: Dropoutæ¯”ç‡ | Dropout ratio
        """
        super(StockLSTM, self).__init__()
        
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # LSTMå±‚ - æ ¸å¿ƒçš„æ—¶é—´åºåˆ—å¤„ç†å±‚
        # LSTM layer - core time series processing layer
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            dropout=dropout if num_layers > 1 else 0,
            batch_first=True
        )
        
        # æ³¨æ„åŠ›æœºåˆ¶ï¼ˆç®€åŒ–ç‰ˆï¼‰- è®©æ¨¡å‹å…³æ³¨é‡è¦çš„æ—¶é—´ç‚¹
        # Attention mechanism (simplified) - let model focus on important time points
        self.attention = nn.Linear(hidden_size, 1)
        
        # å…¨è¿æ¥å±‚ - å°†LSTMè¾“å‡ºè½¬æ¢ä¸ºä»·æ ¼é¢„æµ‹
        # Fully connected layer - convert LSTM output to price prediction
        self.fc = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size // 2, output_size)
        )
        
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for name, param in self.named_parameters():
            if 'weight_ih' in name:
                nn.init.xavier_uniform_(param.data)
            elif 'weight_hh' in name:
                nn.init.orthogonal_(param.data)
            elif 'bias' in name:
                param.data.fill_(0)
    
    def forward(self, x):
        """
        å‰å‘ä¼ æ’­
        Forward propagation
        
        Args:
            x: è¾“å…¥åºåˆ— [batch_size, seq_length, features] | Input sequence
            
        Returns:
            é¢„æµ‹çš„è‚¡ä»· | Predicted stock price
        """
        batch_size = x.size(0)
        
        # åˆå§‹åŒ–éšè—çŠ¶æ€
        # Initialize hidden states
        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)
        
        # LSTMå‰å‘ä¼ æ’­
        # LSTM forward propagation
        lstm_out, (hn, cn) = self.lstm(x, (h0, c0))
        
        # åº”ç”¨æ³¨æ„åŠ›æœºåˆ¶
        # Apply attention mechanism
        attention_weights = torch.softmax(self.attention(lstm_out), dim=1)
        attended_output = torch.sum(attention_weights * lstm_out, dim=1)
        
        # æœ€ç»ˆé¢„æµ‹
        # Final prediction
        output = self.fc(attended_output)
        
        return output


def download_stock_data(symbol='AAPL', period='2y'):
    """
    ä¸‹è½½è‚¡ç¥¨æ•°æ®
    Download stock data
    
    Args:
        symbol: è‚¡ç¥¨ä»£ç  | Stock symbol
        period: æ—¶é—´å‘¨æœŸ | Time period
        
    Returns:
        DataFrame: è‚¡ç¥¨æ•°æ® | Stock data
    """
    print(f"ğŸ“ˆ ä¸‹è½½è‚¡ç¥¨æ•°æ®: {symbol} | Downloading stock data: {symbol}")
    
    try:
        # ä½¿ç”¨yfinanceä¸‹è½½æ•°æ®
        # Download data using yfinance
        stock = yf.Ticker(symbol)
        data = stock.history(period=period)
        
        if data.empty:
            print("âš ï¸ æ— æ³•è·å–è‚¡ç¥¨æ•°æ®ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
            print("âš ï¸ Unable to fetch stock data, using simulated data")
            return create_simulated_stock_data()
        
        print(f"âœ… æˆåŠŸä¸‹è½½ {len(data)} å¤©çš„æ•°æ®")
        print(f"âœ… Successfully downloaded {len(data)} days of data")
        
        return data
        
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {str(e)}")
        print(f"âŒ Download failed: {str(e)}")
        print("ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ä»£æ›¿ | Using simulated data instead")
        return create_simulated_stock_data()


def create_simulated_stock_data():
    """
    åˆ›å»ºæ¨¡æ‹Ÿè‚¡ç¥¨æ•°æ®
    Create simulated stock data
    """
    print("ğŸ­ åˆ›å»ºæ¨¡æ‹Ÿè‚¡ç¥¨æ•°æ® | Creating simulated stock data")
    
    # ç”Ÿæˆ500å¤©çš„æ¨¡æ‹Ÿæ•°æ®
    # Generate 500 days of simulated data
    np.random.seed(42)
    days = 500
    
    # åŸºç¡€ä»·æ ¼è¶‹åŠ¿
    # Base price trend
    base_price = 100
    trend = np.cumsum(np.random.randn(days) * 0.01)
    
    # å­£èŠ‚æ€§æ¨¡å¼
    # Seasonal pattern
    seasonal = 5 * np.sin(np.arange(days) * 2 * np.pi / 252)  # å¹´åº¦å­£èŠ‚æ€§
    
    # ä»·æ ¼è®¡ç®—
    # Price calculation
    prices = base_price + trend + seasonal + np.random.randn(days) * 2
    
    # ç¡®ä¿ä»·æ ¼ä¸ºæ­£
    # Ensure positive prices
    prices = np.maximum(prices, 10)
    
    # åˆ›å»ºDataFrame
    # Create DataFrame
    dates = pd.date_range(start='2022-01-01', periods=days, freq='D')
    
    data = pd.DataFrame({
        'Open': prices + np.random.randn(days) * 0.5,
        'High': prices + np.abs(np.random.randn(days)) * 2,
        'Low': prices - np.abs(np.random.randn(days)) * 2,
        'Close': prices,
        'Volume': np.random.randint(1000000, 10000000, days)
    }, index=dates)
    
    # ç¡®ä¿High >= Low
    # Ensure High >= Low
    data['High'] = np.maximum(data['High'], data['Low'] + 0.01)
    
    return data


def prepare_stock_data(data, sequence_length=60, features=['Close', 'Volume', 'High', 'Low']):
    """
    å‡†å¤‡è‚¡ç¥¨æ•°æ®ç”¨äºLSTMè®­ç»ƒ
    Prepare stock data for LSTM training
    
    Args:
        data: è‚¡ç¥¨æ•°æ® | Stock data
        sequence_length: åºåˆ—é•¿åº¦ | Sequence length
        features: ä½¿ç”¨çš„ç‰¹å¾ | Features to use
        
    Returns:
        è®­ç»ƒå’Œæµ‹è¯•æ•°æ® | Training and test data
    """
    print(f"ğŸ”§ å‡†å¤‡è‚¡ç¥¨æ•°æ® | Preparing stock data")
    print(f"ä½¿ç”¨ç‰¹å¾: {features} | Using features: {features}")
    
    # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
    # Calculate technical indicators
    data = data.copy()
    
    # ç§»åŠ¨å¹³å‡çº¿
    # Moving averages
    data['MA_5'] = data['Close'].rolling(window=5).mean()
    data['MA_20'] = data['Close'].rolling(window=20).mean()
    
    # ç›¸å¯¹å¼ºå¼±æŒ‡æ•° (RSI)
    # Relative Strength Index (RSI)
    delta = data['Close'].diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
    rs = gain / loss
    data['RSI'] = 100 - (100 / (1 + rs))
    
    # å¸ƒæ—å¸¦
    # Bollinger Bands
    rolling_mean = data['Close'].rolling(window=20).mean()
    rolling_std = data['Close'].rolling(window=20).std()
    data['Bollinger_Upper'] = rolling_mean + (rolling_std * 2)
    data['Bollinger_Lower'] = rolling_mean - (rolling_std * 2)
    
    # ä»·æ ¼å˜åŒ–ç‡
    # Price change rate
    data['Price_Change'] = data['Close'].pct_change()
    
    # æˆäº¤é‡å˜åŒ–ç‡
    # Volume change rate
    data['Volume_Change'] = data['Volume'].pct_change()
    
    # é€‰æ‹©ç‰¹å¾
    # Select features
    extended_features = features + ['MA_5', 'MA_20', 'RSI', 'Price_Change', 'Volume_Change']
    
    # ç§»é™¤NaNå€¼
    # Remove NaN values
    data = data.dropna()
    
    # æå–ç‰¹å¾æ•°æ®
    # Extract feature data
    feature_data = data[extended_features].values
    target_data = data['Close'].values
    
    print(f"æ•°æ®å½¢çŠ¶: {feature_data.shape} | Data shape: {feature_data.shape}")
    print(f"æœ‰æ•ˆæ•°æ®ç‚¹: {len(feature_data)} | Valid data points: {len(feature_data)}")
    
    # æ•°æ®æ ‡å‡†åŒ–
    # Data normalization
    feature_scaler = MinMaxScaler()
    target_scaler = MinMaxScaler()
    
    scaled_features = feature_scaler.fit_transform(feature_data)
    scaled_targets = target_scaler.fit_transform(target_data.reshape(-1, 1)).flatten()
    
    # åˆ›å»ºåºåˆ—æ•°æ®
    # Create sequence data
    X, y = [], []
    
    for i in range(sequence_length, len(scaled_features)):
        X.append(scaled_features[i-sequence_length:i])
        y.append(scaled_targets[i])
    
    X = np.array(X)
    y = np.array(y)
    
    print(f"åºåˆ—æ•°æ®å½¢çŠ¶: X={X.shape}, y={y.shape}")
    print(f"Sequence data shape: X={X.shape}, y={y.shape}")
    
    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    # Split training and test sets
    train_size = int(len(X) * 0.8)
    
    X_train, X_test = X[:train_size], X[train_size:]
    y_train, y_test = y[:train_size], y[train_size:]
    
    return (X_train, X_test, y_train, y_test, 
            feature_scaler, target_scaler, data.index[sequence_length:])


def train_stock_model(X_train, y_train, X_test, y_test, input_size):
    """
    è®­ç»ƒè‚¡ç¥¨é¢„æµ‹æ¨¡å‹
    Train stock prediction model
    """
    print("\nğŸš€ å¼€å§‹è®­ç»ƒè‚¡ç¥¨é¢„æµ‹æ¨¡å‹ | Starting stock prediction model training")
    print("=" * 70)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device} | Using device: {device}")
    
    # è½¬æ¢ä¸ºPyTorchå¼ é‡
    # Convert to PyTorch tensors
    X_train = torch.FloatTensor(X_train).to(device)
    y_train = torch.FloatTensor(y_train).to(device)
    X_test = torch.FloatTensor(X_test).to(device)
    y_test = torch.FloatTensor(y_test).to(device)
    
    # åˆ›å»ºæ¨¡å‹
    # Create model
    model = StockLSTM(
        input_size=input_size,
        hidden_size=128,
        num_layers=3,
        output_size=1,
        dropout=0.2
    ).to(device)
    
    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    # Loss function and optimizer
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', 
                                                   factor=0.5, patience=10, verbose=True)
    
    # è®­ç»ƒå‚æ•°
    # Training parameters
    num_epochs = 100
    batch_size = 32
    best_loss = float('inf')
    patience = 20
    patience_counter = 0
    
    train_losses = []
    test_losses = []
    
    print("å¼€å§‹è®­ç»ƒ... | Starting training...")
    
    for epoch in range(num_epochs):
        model.train()
        total_train_loss = 0
        
        # æ‰¹é‡è®­ç»ƒ
        # Batch training
        for i in range(0, len(X_train), batch_size):
            batch_X = X_train[i:i+batch_size]
            batch_y = y_train[i:i+batch_size]
            
            optimizer.zero_grad()
            predictions = model(batch_X).squeeze()
            loss = criterion(predictions, batch_y)
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            total_train_loss += loss.item()
        
        # éªŒè¯
        # Validation
        model.eval()
        with torch.no_grad():
            test_predictions = model(X_test).squeeze()
            test_loss = criterion(test_predictions, y_test).item()
        
        avg_train_loss = total_train_loss / (len(X_train) // batch_size + 1)
        
        train_losses.append(avg_train_loss)
        test_losses.append(test_loss)
        
        # å­¦ä¹ ç‡è°ƒåº¦
        # Learning rate scheduling
        scheduler.step(test_loss)
        
        # æ—©åœæœºåˆ¶
        # Early stopping
        if test_loss < best_loss:
            best_loss = test_loss
            patience_counter = 0
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            # Save best model
            torch.save(model.state_dict(), 'best_stock_model.pth')
        else:
            patience_counter += 1
        
        if (epoch + 1) % 10 == 0:
            print(f'è½®æ¬¡ {epoch+1}/{num_epochs}:')
            print(f'  è®­ç»ƒæŸå¤±: {avg_train_loss:.6f}')
            print(f'  æµ‹è¯•æŸå¤±: {test_loss:.6f}')
            print(f'  å­¦ä¹ ç‡: {optimizer.param_groups[0]["lr"]:.8f}')
        
        if patience_counter >= patience:
            print(f"æ—©åœï¼š{patience}è½®æ¬¡æ— æ”¹å–„")
            break
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    # Load best model
    model.load_state_dict(torch.load('best_stock_model.pth'))
    
    return model, train_losses, test_losses


def evaluate_model(model, X_test, y_test, target_scaler, dates):
    """
    è¯„ä¼°æ¨¡å‹æ€§èƒ½
    Evaluate model performance
    """
    print("\nğŸ“Š è¯„ä¼°æ¨¡å‹æ€§èƒ½ | Evaluating model performance")
    print("=" * 50)
    
    device = next(model.parameters()).device
    
    model.eval()
    with torch.no_grad():
        predictions = model(X_test).squeeze().cpu().numpy()
        actual = y_test.cpu().numpy()
    
    # åæ ‡å‡†åŒ–
    # Denormalize
    predictions_denorm = target_scaler.inverse_transform(predictions.reshape(-1, 1)).flatten()
    actual_denorm = target_scaler.inverse_transform(actual.reshape(-1, 1)).flatten()
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    # Calculate evaluation metrics
    mse = mean_squared_error(actual_denorm, predictions_denorm)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(actual_denorm, predictions_denorm)
    mape = np.mean(np.abs((actual_denorm - predictions_denorm) / actual_denorm)) * 100
    
    print(f"å‡æ–¹è¯¯å·® (MSE): {mse:.2f}")
    print(f"å‡æ–¹æ ¹è¯¯å·® (RMSE): {rmse:.2f}")
    print(f"å¹³å‡ç»å¯¹è¯¯å·® (MAE): {mae:.2f}")
    print(f"å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·® (MAPE): {mape:.2f}%")
    
    # æ–¹å‘å‡†ç¡®ç‡
    # Direction accuracy
    actual_direction = np.diff(actual_denorm) > 0
    pred_direction = np.diff(predictions_denorm) > 0
    direction_accuracy = np.mean(actual_direction == pred_direction) * 100
    
    print(f"æ–¹å‘é¢„æµ‹å‡†ç¡®ç‡: {direction_accuracy:.2f}%")
    
    return predictions_denorm, actual_denorm, {
        'MSE': mse, 'RMSE': rmse, 'MAE': mae, 'MAPE': mape,
        'Direction_Accuracy': direction_accuracy
    }


def visualize_stock_predictions(predictions, actual, dates, train_losses, test_losses, metrics):
    """
    å¯è§†åŒ–è‚¡ç¥¨é¢„æµ‹ç»“æœ
    Visualize stock prediction results
    """
    plt.figure(figsize=(20, 15))
    
    # é¢„æµ‹vså®é™…ä»·æ ¼
    # Predictions vs actual prices
    plt.subplot(3, 3, 1)
    test_dates = dates[-len(predictions):]
    plt.plot(test_dates, actual, label='å®é™…ä»·æ ¼ | Actual Price', color='blue', alpha=0.7)
    plt.plot(test_dates, predictions, label='é¢„æµ‹ä»·æ ¼ | Predicted Price', color='red', alpha=0.7)
    plt.title('è‚¡ç¥¨ä»·æ ¼é¢„æµ‹ç»“æœ | Stock Price Prediction Results')
    plt.xlabel('æ—¥æœŸ | Date')
    plt.ylabel('ä»·æ ¼ | Price')
    plt.legend()
    plt.xticks(rotation=45)
    plt.grid(True)
    
    # è®­ç»ƒæŸå¤±æ›²çº¿
    # Training loss curve
    plt.subplot(3, 3, 2)
    plt.plot(train_losses, label='è®­ç»ƒæŸå¤± | Training Loss')
    plt.plot(test_losses, label='æµ‹è¯•æŸå¤± | Test Loss')
    plt.title('æŸå¤±æ›²çº¿ | Loss Curves')
    plt.xlabel('è½®æ¬¡ | Epoch')
    plt.ylabel('æŸå¤± | Loss')
    plt.legend()
    plt.grid(True)
    
    # é¢„æµ‹è¯¯å·®åˆ†å¸ƒ
    # Prediction error distribution
    plt.subplot(3, 3, 3)
    errors = predictions - actual
    plt.hist(errors, bins=50, alpha=0.7, color='purple')
    plt.title('é¢„æµ‹è¯¯å·®åˆ†å¸ƒ | Prediction Error Distribution')
    plt.xlabel('è¯¯å·® | Error')
    plt.ylabel('é¢‘æ•° | Frequency')
    plt.axvline(x=0, color='red', linestyle='--', alpha=0.7)
    plt.grid(True)
    
    # æ•£ç‚¹å›¾ï¼šé¢„æµ‹vså®é™…
    # Scatter plot: predictions vs actual
    plt.subplot(3, 3, 4)
    plt.scatter(actual, predictions, alpha=0.5, color='green')
    plt.plot([actual.min(), actual.max()], [actual.min(), actual.max()], 'r--', lw=2)
    plt.title('é¢„æµ‹vså®é™…æ•£ç‚¹å›¾ | Predictions vs Actual Scatter Plot')
    plt.xlabel('å®é™…ä»·æ ¼ | Actual Price')
    plt.ylabel('é¢„æµ‹ä»·æ ¼ | Predicted Price')
    plt.grid(True)
    
    # æ®‹å·®å›¾
    # Residual plot
    plt.subplot(3, 3, 5)
    residuals = actual - predictions
    plt.scatter(range(len(residuals)), residuals, alpha=0.5, color='orange')
    plt.axhline(y=0, color='red', linestyle='--')
    plt.title('æ®‹å·®å›¾ | Residual Plot')
    plt.xlabel('æ ·æœ¬ | Sample')
    plt.ylabel('æ®‹å·® | Residual')
    plt.grid(True)
    
    # æ€§èƒ½æŒ‡æ ‡æ¡å½¢å›¾
    # Performance metrics bar chart
    plt.subplot(3, 3, 6)
    metric_names = ['RMSE', 'MAE', 'MAPE', 'Direction_Acc']
    metric_values = [metrics['RMSE'], metrics['MAE'], metrics['MAPE'], metrics['Direction_Accuracy']]
    
    bars = plt.bar(metric_names, metric_values, color=['skyblue', 'lightgreen', 'lightcoral', 'lightyellow'])
    plt.title('æ€§èƒ½æŒ‡æ ‡ | Performance Metrics')
    plt.ylabel('æ•°å€¼ | Value')
    
    # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°å€¼
    for bar, value in zip(bars, metric_values):
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                f'{value:.2f}', ha='center', va='bottom')
    
    plt.xticks(rotation=45)
    
    # è¿‘æœŸé¢„æµ‹æ”¾å¤§å›¾
    # Recent predictions zoom-in
    plt.subplot(3, 3, 7)
    recent_days = 30
    recent_actual = actual[-recent_days:]
    recent_pred = predictions[-recent_days:]
    recent_dates = test_dates[-recent_days:]
    
    plt.plot(recent_dates, recent_actual, label='å®é™…', marker='o', markersize=3)
    plt.plot(recent_dates, recent_pred, label='é¢„æµ‹', marker='s', markersize=3)
    plt.title(f'æœ€è¿‘{recent_days}å¤©é¢„æµ‹ | Recent {recent_days} Days Prediction')
    plt.xlabel('æ—¥æœŸ | Date')
    plt.ylabel('ä»·æ ¼ | Price')
    plt.legend()
    plt.xticks(rotation=45)
    plt.grid(True)
    
    # ç´¯ç§¯è¯¯å·®
    # Cumulative error
    plt.subplot(3, 3, 8)
    cumulative_error = np.cumsum(np.abs(errors))
    plt.plot(cumulative_error, color='purple')
    plt.title('ç´¯ç§¯ç»å¯¹è¯¯å·® | Cumulative Absolute Error')
    plt.xlabel('æ ·æœ¬ | Sample')
    plt.ylabel('ç´¯ç§¯è¯¯å·® | Cumulative Error')
    plt.grid(True)
    
    # é¢„æµ‹å‡†ç¡®ç‡è¶‹åŠ¿
    # Prediction accuracy trend
    plt.subplot(3, 3, 9)
    window_size = 20
    rolling_mape = []
    for i in range(window_size, len(predictions)):
        window_actual = actual[i-window_size:i]
        window_pred = predictions[i-window_size:i]
        window_mape = np.mean(np.abs((window_actual - window_pred) / window_actual)) * 100
        rolling_mape.append(window_mape)
    
    plt.plot(rolling_mape, color='brown')
    plt.title(f'æ»šåŠ¨MAPE ({window_size}å¤©) | Rolling MAPE ({window_size} days)')
    plt.xlabel('æ ·æœ¬ | Sample')
    plt.ylabel('MAPE (%)')
    plt.grid(True)
    
    plt.tight_layout()
    plt.savefig('stock_prediction_results.png', dpi=300, bbox_inches='tight')
    plt.show()


if __name__ == "__main__":
    print("ğŸ“ˆ LSTMè‚¡ç¥¨ä»·æ ¼é¢„æµ‹é¡¹ç›® | LSTM Stock Price Prediction Project")
    print("=" * 70)
    
    set_random_seed(42)
    
    # ä¸‹è½½è‚¡ç¥¨æ•°æ®
    # Download stock data
    stock_data = download_stock_data('AAPL', '2y')
    
    # å‡†å¤‡æ•°æ®
    # Prepare data
    (X_train, X_test, y_train, y_test, 
     feature_scaler, target_scaler, dates) = prepare_stock_data(stock_data)
    
    # è®­ç»ƒæ¨¡å‹
    # Train model
    model, train_losses, test_losses = train_stock_model(
        X_train, y_train, X_test, y_test, X_train.shape[2]
    )
    
    # è¯„ä¼°æ¨¡å‹
    # Evaluate model
    predictions, actual, metrics = evaluate_model(
        model, X_test, y_test, target_scaler, dates
    )
    
    # å¯è§†åŒ–ç»“æœ
    # Visualize results
    visualize_stock_predictions(predictions, actual, dates, train_losses, test_losses, metrics)
    
    print("\nğŸ‰ è‚¡ç¥¨é¢„æµ‹é¡¹ç›®å®Œæˆï¼| Stock Prediction Project Completed!")
    print("ğŸ“š é€šè¿‡è¿™ä¸ªé¡¹ç›®ï¼Œä½ å­¦ä¼šäº†ï¼š")
    print("ğŸ“š Through this project, you learned:")
    print("1. å¦‚ä½•å¤„ç†çœŸå®çš„æ—¶é—´åºåˆ—æ•°æ®")
    print("   How to handle real time series data")
    print("2. æŠ€æœ¯æŒ‡æ ‡çš„è®¡ç®—å’Œåº”ç”¨")
    print("   Calculation and application of technical indicators")
    print("3. LSTMåœ¨é‡‘èé¢„æµ‹ä¸­çš„åº”ç”¨")
    print("   Application of LSTM in financial prediction")
    print("4. æ¨¡å‹è¯„ä¼°å’Œå¯è§†åŒ–çš„å®Œæ•´æµç¨‹")
    print("   Complete process of model evaluation and visualization")
    
    print("\nâš ï¸ é‡è¦æé†’ï¼š")
    print("âš ï¸ Important reminder:")
    print("è¿™ä¸ªæ¨¡å‹ä»…ç”¨äºå­¦ä¹ ç›®çš„ï¼Œä¸åº”ç”¨äºå®é™…æŠ•èµ„å†³ç­–ï¼")
    print("This model is for learning purposes only and should not be used for actual investment decisions!") 