"""
LSTM vs GRU ç»¼åˆå¯¹æ¯”åˆ†æ
LSTM vs GRU Comprehensive Comparison Analysis

è¿™ä¸ªæ–‡ä»¶å¯¹LSTMå’ŒGRUè¿›è¡Œå…¨é¢çš„å¯¹æ¯”åˆ†æï¼ŒåŒ…æ‹¬ï¼š
- ç†è®ºå·®å¼‚å¯¹æ¯”
- æ€§èƒ½åŸºå‡†æµ‹è¯•
- ä¸åŒä»»åŠ¡åœºæ™¯ä¸‹çš„è¡¨ç°
- è®­ç»ƒæ•ˆç‡å¯¹æ¯”
- å†…å­˜ä½¿ç”¨å¯¹æ¯”

This file provides comprehensive comparison analysis of LSTM and GRU, including:
- Theoretical differences comparison
- Performance benchmarking
- Performance in different task scenarios
- Training efficiency comparison  
- Memory usage comparison
"""

import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import time
import psutil
import os
from typing import Dict, List, Tuple
import warnings
import sys

warnings.filterwarnings('ignore')

# æ·»åŠ ä¸Šçº§ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils.data_utils import set_random_seed


class LSTMModel(nn.Module):
    """æ ‡å‡†LSTMæ¨¡å‹"""
    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.1):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, 
                           dropout=dropout, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x):
        lstm_out, (h_n, c_n) = self.lstm(x)
        # ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        output = self.dropout(lstm_out[:, -1, :])
        output = self.fc(output)
        return output


class GRUModel(nn.Module):
    """æ ‡å‡†GRUæ¨¡å‹"""
    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.1):
        super(GRUModel, self).__init__()
        self.gru = nn.GRU(input_size, hidden_size, num_layers, 
                         dropout=dropout, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x):
        gru_out, h_n = self.gru(x)
        # ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        output = self.dropout(gru_out[:, -1, :])
        output = self.fc(output)
        return output


class ModelComparator:
    """
    æ¨¡å‹å¯¹æ¯”å™¨
    Model Comparator
    
    è¿™ä¸ªç±»å°±åƒä¸€ä¸ªå…¬æ­£çš„è£åˆ¤ï¼Œå…¨é¢æµ‹è¯•å’Œæ¯”è¾ƒLSTMå’ŒGRUçš„å„é¡¹æŒ‡æ ‡ã€‚
    This class is like an impartial referee that comprehensively tests and compares various metrics of LSTM and GRU.
    """
    
    def __init__(self):
        self.results = {}
        
    def create_synthetic_data(self, task_type: str, num_samples: int = 1000, 
                            seq_length: int = 50, input_size: int = 1) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        åˆ›å»ºä¸åŒç±»å‹çš„åˆæˆæ•°æ®
        Create different types of synthetic data
        
        Args:
            task_type: ä»»åŠ¡ç±»å‹ ('regression', 'classification', 'long_sequence') | Task type
            num_samples: æ ·æœ¬æ•°é‡ | Number of samples
            seq_length: åºåˆ—é•¿åº¦ | Sequence length
            input_size: è¾“å…¥ç»´åº¦ | Input dimension
            
        Returns:
            è¾“å…¥å’Œæ ‡ç­¾æ•°æ® | Input and label data
        """
        np.random.seed(42)
        
        if task_type == 'regression':
            # å›å½’ä»»åŠ¡ï¼šé¢„æµ‹æ­£å¼¦æ³¢çš„ä¸‹ä¸€ä¸ªå€¼
            # Regression task: predict next value of sine wave
            t = np.linspace(0, 4*np.pi, num_samples + seq_length)
            data = np.sin(t) + 0.1 * np.sin(10*t) + 0.05 * np.random.randn(len(t))
            
            X, y = [], []
            for i in range(num_samples):
                X.append(data[i:i+seq_length])
                y.append(data[i+seq_length])
                
            X = np.array(X).reshape(num_samples, seq_length, input_size)
            y = np.array(y).reshape(num_samples, 1)
            
        elif task_type == 'classification':
            # åˆ†ç±»ä»»åŠ¡ï¼šåŸºäºåºåˆ—æ¨¡å¼åˆ†ç±»
            # Classification task: classify based on sequence patterns
            X = np.random.randn(num_samples, seq_length, input_size)
            
            # åˆ›å»ºä¸¤ç±»æ¨¡å¼ï¼šä¸Šå‡è¶‹åŠ¿ vs ä¸‹é™è¶‹åŠ¿
            # Create two class patterns: upward trend vs downward trend
            y = []
            for i in range(num_samples):
                trend = np.mean(np.diff(X[i, :, 0]))
                y.append(1 if trend > 0 else 0)
            
            y = np.array(y)
            
        elif task_type == 'long_sequence':
            # é•¿åºåˆ—ä»»åŠ¡ï¼šæµ‹è¯•é•¿æœŸä¾èµ–èƒ½åŠ›
            # Long sequence task: test long-term dependency capability
            seq_length = 200  # æ›´é•¿çš„åºåˆ—
            X = np.random.randn(num_samples, seq_length, input_size)
            
            # åˆ›å»ºé•¿æœŸä¾èµ–ï¼šåºåˆ—å¼€å§‹çš„ä¿¡å·å½±å“æœ€ç»ˆç»“æœ
            # Create long-term dependency: signal at sequence start affects final result
            y = []
            for i in range(num_samples):
                # å¦‚æœåºåˆ—å‰10ä¸ªå€¼çš„å¹³å‡å€¼å¤§äº0ï¼Œåˆ™æ ‡ç­¾ä¸º1
                # If average of first 10 values > 0, label is 1
                early_signal = np.mean(X[i, :10, 0])
                y.append(1 if early_signal > 0 else 0)
            
            y = np.array(y)
        
        return torch.FloatTensor(X), torch.LongTensor(y) if task_type == 'classification' else torch.FloatTensor(y)
    
    def measure_training_time(self, model: nn.Module, X: torch.Tensor, y: torch.Tensor, 
                            num_epochs: int = 50) -> float:
        """
        æµ‹é‡è®­ç»ƒæ—¶é—´
        Measure training time
        """
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model = model.to(device)
        X, y = X.to(device), y.to(device)
        
        criterion = nn.MSELoss() if len(y.shape) > 1 else nn.CrossEntropyLoss()
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
        
        start_time = time.time()
        
        model.train()
        for epoch in range(num_epochs):
            optimizer.zero_grad()
            outputs = model(X)
            
            if len(y.shape) > 1:  # å›å½’ä»»åŠ¡
                loss = criterion(outputs, y)
            else:  # åˆ†ç±»ä»»åŠ¡
                loss = criterion(outputs, y.long())
                
            loss.backward()
            optimizer.step()
        
        end_time = time.time()
        return end_time - start_time
    
    def measure_memory_usage(self, model: nn.Module, X: torch.Tensor) -> Dict[str, float]:
        """
        æµ‹é‡å†…å­˜ä½¿ç”¨é‡
        Measure memory usage
        """
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model = model.to(device)
        X = X.to(device)
        
        # è·å–åˆå§‹å†…å­˜
        # Get initial memory
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            initial_memory = torch.cuda.memory_allocated() / 1024**2  # MB
        else:
            initial_memory = 0
        
        process = psutil.Process(os.getpid())
        initial_ram = process.memory_info().rss / 1024**2  # MB
        
        # å‰å‘ä¼ æ’­
        # Forward pass
        model.eval()
        with torch.no_grad():
            _ = model(X)
        
        # è·å–å³°å€¼å†…å­˜
        # Get peak memory
        if torch.cuda.is_available():
            peak_gpu_memory = torch.cuda.max_memory_allocated() / 1024**2  # MB
            gpu_usage = peak_gpu_memory - initial_memory
        else:
            gpu_usage = 0
        
        peak_ram = process.memory_info().rss / 1024**2  # MB
        ram_usage = peak_ram - initial_ram
        
        return {
            'gpu_memory_mb': gpu_usage,
            'ram_memory_mb': ram_usage,
            'parameters': sum(p.numel() for p in model.parameters()),
            'model_size_mb': sum(p.numel() * p.element_size() for p in model.parameters()) / 1024**2
        }
    
    def run_comprehensive_comparison(self):
        """
        è¿è¡Œç»¼åˆå¯¹æ¯”åˆ†æ
        Run comprehensive comparison analysis
        """
        print("ğŸ”¬ å¼€å§‹LSTM vs GRUç»¼åˆå¯¹æ¯”åˆ†æ")
        print("ğŸ”¬ Starting LSTM vs GRU Comprehensive Comparison Analysis")
        print("=" * 80)
        
        set_random_seed(42)
        
        # æµ‹è¯•é…ç½®
        # Test configurations
        configs = [
            {'hidden_size': 64, 'num_layers': 1, 'name': 'å•å±‚64å•å…ƒ | Single Layer 64 Units'},
            {'hidden_size': 128, 'num_layers': 2, 'name': 'åŒå±‚128å•å…ƒ | Double Layer 128 Units'},
            {'hidden_size': 256, 'num_layers': 3, 'name': 'ä¸‰å±‚256å•å…ƒ | Triple Layer 256 Units'},
        ]
        
        task_types = ['regression', 'classification', 'long_sequence']
        
        comparison_results = {
            'training_time': {},
            'memory_usage': {},
            'model_parameters': {},
            'accuracy': {}
        }
        
        for task_type in task_types:
            print(f"\nğŸ“Š æµ‹è¯•ä»»åŠ¡ç±»å‹: {task_type}")
            print(f"ğŸ“Š Testing task type: {task_type}")
            
            # åˆ›å»ºæ•°æ®
            # Create data
            if task_type == 'long_sequence':
                X, y = self.create_synthetic_data(task_type, num_samples=500, seq_length=200)
            else:
                X, y = self.create_synthetic_data(task_type, num_samples=1000, seq_length=50)
            
            output_size = 2 if task_type == 'classification' else 1
            
            for config in configs:
                config_name = config['name']
                print(f"\n  é…ç½®: {config_name}")
                print(f"  Configuration: {config_name}")
                
                # åˆ›å»ºæ¨¡å‹
                # Create models
                lstm_model = LSTMModel(
                    input_size=X.shape[2],
                    hidden_size=config['hidden_size'],
                    num_layers=config['num_layers'],
                    output_size=output_size
                )
                
                gru_model = GRUModel(
                    input_size=X.shape[2],
                    hidden_size=config['hidden_size'],
                    num_layers=config['num_layers'],
                    output_size=output_size
                )
                
                # æµ‹è¯•è®­ç»ƒæ—¶é—´
                # Test training time
                lstm_time = self.measure_training_time(lstm_model, X, y, num_epochs=30)
                gru_time = self.measure_training_time(gru_model, X, y, num_epochs=30)
                
                # æµ‹è¯•å†…å­˜ä½¿ç”¨
                # Test memory usage
                lstm_memory = self.measure_memory_usage(lstm_model, X)
                gru_memory = self.measure_memory_usage(gru_model, X)
                
                # å­˜å‚¨ç»“æœ
                # Store results
                key = f"{task_type}_{config_name}"
                
                comparison_results['training_time'][key] = {
                    'LSTM': lstm_time,
                    'GRU': gru_time,
                    'GRU_speedup': lstm_time / gru_time
                }
                
                comparison_results['memory_usage'][key] = {
                    'LSTM': lstm_memory,
                    'GRU': gru_memory
                }
                
                comparison_results['model_parameters'][key] = {
                    'LSTM': lstm_memory['parameters'],
                    'GRU': gru_memory['parameters'],
                    'reduction': (lstm_memory['parameters'] - gru_memory['parameters']) / lstm_memory['parameters'] * 100
                }
                
                print(f"    LSTMè®­ç»ƒæ—¶é—´: {lstm_time:.2f}s | LSTM training time: {lstm_time:.2f}s")
                print(f"    GRUè®­ç»ƒæ—¶é—´: {gru_time:.2f}s | GRU training time: {gru_time:.2f}s")
                print(f"    GRUåŠ é€Ÿæ¯”: {lstm_time/gru_time:.2f}x | GRU speedup: {lstm_time/gru_time:.2f}x")
                print(f"    LSTMå‚æ•°é‡: {lstm_memory['parameters']:,} | LSTM parameters: {lstm_memory['parameters']:,}")
                print(f"    GRUå‚æ•°é‡: {gru_memory['parameters']:,} | GRU parameters: {gru_memory['parameters']:,}")
                print(f"    å‚æ•°å‡å°‘: {(lstm_memory['parameters'] - gru_memory['parameters']) / lstm_memory['parameters'] * 100:.1f}%")
        
        self.results = comparison_results
        return comparison_results
    
    def visualize_comparison_results(self):
        """
        å¯è§†åŒ–å¯¹æ¯”ç»“æœ
        Visualize comparison results
        """
        if not self.results:
            print("âŒ æ²¡æœ‰ç»“æœå¯è§†åŒ–ï¼Œè¯·å…ˆè¿è¡Œå¯¹æ¯”åˆ†æ")
            print("âŒ No results to visualize, please run comparison analysis first")
            return
        
        fig, axes = plt.subplots(2, 3, figsize=(20, 12))
        fig.suptitle('LSTM vs GRU ç»¼åˆå¯¹æ¯”åˆ†æ | LSTM vs GRU Comprehensive Comparison', fontsize=16)
        
        # 1. è®­ç»ƒæ—¶é—´å¯¹æ¯”
        # Training time comparison
        ax = axes[0, 0]
        tasks = []
        lstm_times = []
        gru_times = []
        
        for key, data in self.results['training_time'].items():
            tasks.append(key.replace('_', '\n'))
            lstm_times.append(data['LSTM'])
            gru_times.append(data['GRU'])
        
        x = np.arange(len(tasks))
        width = 0.35
        
        ax.bar(x - width/2, lstm_times, width, label='LSTM', alpha=0.8, color='skyblue')
        ax.bar(x + width/2, gru_times, width, label='GRU', alpha=0.8, color='lightcoral')
        
        ax.set_xlabel('ä»»åŠ¡é…ç½® | Task Configuration')
        ax.set_ylabel('è®­ç»ƒæ—¶é—´ (ç§’) | Training Time (seconds)')
        ax.set_title('è®­ç»ƒæ—¶é—´å¯¹æ¯” | Training Time Comparison')
        ax.set_xticks(x)
        ax.set_xticklabels(tasks, rotation=45, ha='right')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # 2. å‚æ•°æ•°é‡å¯¹æ¯”
        # Parameter count comparison
        ax = axes[0, 1]
        lstm_params = []
        gru_params = []
        
        for key, data in self.results['model_parameters'].items():
            lstm_params.append(data['LSTM'])
            gru_params.append(data['GRU'])
        
        ax.bar(x - width/2, lstm_params, width, label='LSTM', alpha=0.8, color='skyblue')
        ax.bar(x + width/2, gru_params, width, label='GRU', alpha=0.8, color='lightcoral')
        
        ax.set_xlabel('ä»»åŠ¡é…ç½® | Task Configuration')
        ax.set_ylabel('å‚æ•°æ•°é‡ | Parameter Count')
        ax.set_title('æ¨¡å‹å‚æ•°æ•°é‡å¯¹æ¯” | Model Parameter Count Comparison')
        ax.set_xticks(x)
        ax.set_xticklabels(tasks, rotation=45, ha='right')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # 3. å†…å­˜ä½¿ç”¨å¯¹æ¯” (GPU)
        # Memory usage comparison (GPU)
        ax = axes[0, 2]
        lstm_gpu_mem = []
        gru_gpu_mem = []
        
        for key, data in self.results['memory_usage'].items():
            lstm_gpu_mem.append(data['LSTM']['gpu_memory_mb'])
            gru_gpu_mem.append(data['GRU']['gpu_memory_mb'])
        
        ax.bar(x - width/2, lstm_gpu_mem, width, label='LSTM', alpha=0.8, color='skyblue')
        ax.bar(x + width/2, gru_gpu_mem, width, label='GRU', alpha=0.8, color='lightcoral')
        
        ax.set_xlabel('ä»»åŠ¡é…ç½® | Task Configuration')
        ax.set_ylabel('GPUå†…å­˜ä½¿ç”¨ (MB) | GPU Memory Usage (MB)')
        ax.set_title('GPUå†…å­˜ä½¿ç”¨å¯¹æ¯” | GPU Memory Usage Comparison')
        ax.set_xticks(x)
        ax.set_xticklabels(tasks, rotation=45, ha='right')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # 4. è®­ç»ƒåŠ é€Ÿæ¯”
        # Training speedup
        ax = axes[1, 0]
        speedups = [data['GRU_speedup'] for data in self.results['training_time'].values()]
        
        bars = ax.bar(tasks, speedups, color='lightgreen', alpha=0.8)
        ax.axhline(y=1, color='red', linestyle='--', alpha=0.7, label='æ— åŠ é€Ÿçº¿ | No speedup line')
        
        # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°å€¼
        # Add values on bars
        for bar, speedup in zip(bars, speedups):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                   f'{speedup:.2f}x', ha='center', va='bottom')
        
        ax.set_xlabel('ä»»åŠ¡é…ç½® | Task Configuration')
        ax.set_ylabel('åŠ é€Ÿæ¯” | Speedup Ratio')
        ax.set_title('GRUç›¸å¯¹LSTMçš„è®­ç»ƒåŠ é€Ÿæ¯” | GRU Training Speedup vs LSTM')
        ax.set_xticklabels(tasks, rotation=45, ha='right')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # 5. å‚æ•°å‡å°‘ç™¾åˆ†æ¯”
        # Parameter reduction percentage
        ax = axes[1, 1]
        reductions = [data['reduction'] for data in self.results['model_parameters'].values()]
        
        bars = ax.bar(tasks, reductions, color='orange', alpha=0.8)
        
        # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°å€¼
        for bar, reduction in zip(bars, reductions):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                   f'{reduction:.1f}%', ha='center', va='bottom')
        
        ax.set_xlabel('ä»»åŠ¡é…ç½® | Task Configuration')
        ax.set_ylabel('å‚æ•°å‡å°‘ç™¾åˆ†æ¯” | Parameter Reduction (%)')
        ax.set_title('GRUç›¸å¯¹LSTMçš„å‚æ•°å‡å°‘ | GRU Parameter Reduction vs LSTM')
        ax.set_xticklabels(tasks, rotation=45, ha='right')
        ax.grid(True, alpha=0.3)
        
        # 6. ç†è®ºå¯¹æ¯”è¡¨æ ¼
        # Theoretical comparison table
        ax = axes[1, 2]
        ax.axis('tight')
        ax.axis('off')
        
        comparison_data = [
            ['ç‰¹æ€§ | Feature', 'LSTM', 'GRU'],
            ['é—¨çš„æ•°é‡ | Number of Gates', '3ä¸ªé—¨ | 3 gates', '2ä¸ªé—¨ | 2 gates'],
            ['ç»†èƒçŠ¶æ€ | Cell State', 'æœ‰ | Yes', 'æ—  | No'],
            ['å‚æ•°æ•°é‡ | Parameters', 'æ›´å¤š | More', 'æ›´å°‘ | Fewer'],
            ['è®­ç»ƒé€Ÿåº¦ | Training Speed', 'è¾ƒæ…¢ | Slower', 'è¾ƒå¿« | Faster'],
            ['é•¿æœŸè®°å¿† | Long-term Memory', 'ä¼˜ç§€ | Excellent', 'è‰¯å¥½ | Good'],
            ['æ¢¯åº¦æµ | Gradient Flow', 'å¾ˆç¨³å®š | Very Stable', 'ç¨³å®š | Stable'],
            ['é€‚ç”¨åœºæ™¯ | Use Cases', 'å¤æ‚åºåˆ— | Complex Seq', 'ä¸€èˆ¬åºåˆ— | General Seq']
        ]
        
        table = ax.table(cellText=comparison_data[1:], colLabels=comparison_data[0],
                        cellLoc='center', loc='center', bbox=[0, 0, 1, 1])
        table.auto_set_font_size(False)
        table.set_fontsize(9)
        table.scale(1, 2)
        
        # è®¾ç½®è¡¨æ ¼æ ·å¼
        # Set table style
        for i in range(len(comparison_data)):
            for j in range(len(comparison_data[0])):
                cell = table[(i, j)]
                if i == 0:  # è¡¨å¤´
                    cell.set_facecolor('#4472C4')
                    cell.set_text_props(weight='bold', color='white')
                elif j == 0:  # ç¬¬ä¸€åˆ—
                    cell.set_facecolor('#D9E2F3')
                    cell.set_text_props(weight='bold')
                else:
                    cell.set_facecolor('#F2F2F2')
        
        ax.set_title('LSTM vs GRU ç†è®ºå¯¹æ¯” | LSTM vs GRU Theoretical Comparison', 
                    fontsize=12, fontweight='bold', pad=20)
        
        plt.tight_layout()
        plt.savefig('lstm_gru_comprehensive_comparison.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def generate_recommendations(self):
        """
        ç”Ÿæˆä½¿ç”¨å»ºè®®
        Generate usage recommendations
        """
        print("\nğŸ¯ ä½¿ç”¨å»ºè®®ä¸ç»“è®º | Usage Recommendations and Conclusions")
        print("=" * 70)
        
        print("ğŸ“Š åŸºäºå¯¹æ¯”åˆ†æçš„å»ºè®®ï¼š")
        print("ğŸ“Š Recommendations based on comparison analysis:")
        print()
        
        print("ğŸ† é€‰æ‹©LSTMçš„æƒ…å†µ | Choose LSTM when:")
        print("  1. å¤„ç†éå¸¸å¤æ‚çš„é•¿åºåˆ—ä»»åŠ¡")
        print("     Processing very complex long sequence tasks")
        print("  2. éœ€è¦ç²¾ç¡®çš„é•¿æœŸä¾èµ–å»ºæ¨¡")
        print("     Requiring precise long-term dependency modeling")
        print("  3. æ•°æ®é›†è¾ƒå¤§ï¼Œè®­ç»ƒæ—¶é—´ä¸æ˜¯ä¸»è¦è€ƒè™‘å› ç´ ")
        print("     Large dataset where training time is not a primary concern")
        print("  4. éœ€è¦æœ€ä½³çš„åºåˆ—å»ºæ¨¡æ€§èƒ½")
        print("     Requiring optimal sequence modeling performance")
        print()
        
        print("âš¡ é€‰æ‹©GRUçš„æƒ…å†µ | Choose GRU when:")
        print("  1. éœ€è¦å¿«é€Ÿè®­ç»ƒå’Œéƒ¨ç½²")
        print("     Requiring fast training and deployment")
        print("  2. è®¡ç®—èµ„æºæœ‰é™")
        print("     Limited computational resources")
        print("  3. å¤„ç†ä¸­ç­‰å¤æ‚åº¦çš„åºåˆ—ä»»åŠ¡")
        print("     Processing moderately complex sequence tasks")
        print("  4. éœ€è¦åœ¨æ€§èƒ½å’Œæ•ˆç‡ä¹‹é—´å¹³è¡¡")
        print("     Needing balance between performance and efficiency")
        print()
        
        print("ğŸ“ˆ æ€§èƒ½æ€»ç»“ | Performance Summary:")
        if self.results:
            avg_speedup = np.mean([data['GRU_speedup'] for data in self.results['training_time'].values()])
            avg_param_reduction = np.mean([data['reduction'] for data in self.results['model_parameters'].values()])
            
            print(f"  â€¢ GRUå¹³å‡è®­ç»ƒåŠ é€Ÿ: {avg_speedup:.2f}å€")
            print(f"    GRU average training speedup: {avg_speedup:.2f}x")
            print(f"  â€¢ GRUå¹³å‡å‚æ•°å‡å°‘: {avg_param_reduction:.1f}%")
            print(f"    GRU average parameter reduction: {avg_param_reduction:.1f}%")
        
        print()
        print("ğŸ”„ å®é™…é¡¹ç›®ä¸­çš„é€‰æ‹©ç­–ç•¥ | Selection Strategy in Real Projects:")
        print("  1. å…ˆç”¨GRUè¿›è¡Œå¿«é€ŸåŸå‹å¼€å‘å’ŒéªŒè¯")
        print("     Start with GRU for rapid prototyping and validation")
        print("  2. å¦‚æœGRUæ€§èƒ½ä¸è¶³ï¼Œå†è€ƒè™‘ä½¿ç”¨LSTM")
        print("     Consider LSTM if GRU performance is insufficient")
        print("  3. å¯¹äºç”Ÿäº§ç¯å¢ƒï¼Œæƒè¡¡ç²¾åº¦å’Œæ¨ç†é€Ÿåº¦")
        print("     For production, balance accuracy and inference speed")
        print("  4. å¯ä»¥å°è¯•é›†æˆä¸¤ç§æ¨¡å‹è·å¾—æ›´å¥½æ•ˆæœ")
        print("     Consider ensemble of both models for better performance")


def run_detailed_architecture_analysis():
    """
    è¿è¡Œè¯¦ç»†çš„æ¶æ„åˆ†æ
    Run detailed architecture analysis
    """
    print("\nğŸ—ï¸ è¯¦ç»†æ¶æ„åˆ†æ | Detailed Architecture Analysis")
    print("=" * 60)
    
    # åˆ›å»ºæ¶æ„å¯¹æ¯”å›¾
    # Create architecture comparison diagram
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))
    
    # LSTMæ¶æ„å›¾
    # LSTM architecture diagram
    ax1.text(0.5, 0.9, 'LSTMæ¶æ„ | LSTM Architecture', ha='center', va='center', 
             fontsize=14, fontweight='bold', transform=ax1.transAxes)
    
    # ç»˜åˆ¶LSTMçš„ä¸‰ä¸ªé—¨
    # Draw LSTM's three gates
    gates = ['é—å¿˜é—¨\nForget Gate', 'è¾“å…¥é—¨\nInput Gate', 'è¾“å‡ºé—¨\nOutput Gate']
    colors = ['lightcoral', 'lightblue', 'lightgreen']
    
    for i, (gate, color) in enumerate(zip(gates, colors)):
        rect = plt.Rectangle((0.1, 0.7 - i*0.2), 0.8, 0.15, 
                           facecolor=color, alpha=0.7, transform=ax1.transAxes)
        ax1.add_patch(rect)
        ax1.text(0.5, 0.775 - i*0.2, gate, ha='center', va='center', 
                fontweight='bold', transform=ax1.transAxes)
    
    # ç»†èƒçŠ¶æ€
    # Cell state
    rect = plt.Rectangle((0.1, 0.05), 0.8, 0.1, 
                        facecolor='gold', alpha=0.7, transform=ax1.transAxes)
    ax1.add_patch(rect)
    ax1.text(0.5, 0.1, 'ç»†èƒçŠ¶æ€ Cell State', ha='center', va='center', 
            fontweight='bold', transform=ax1.transAxes)
    
    ax1.set_xlim(0, 1)
    ax1.set_ylim(0, 1)
    ax1.axis('off')
    
    # GRUæ¶æ„å›¾
    # GRU architecture diagram
    ax2.text(0.5, 0.9, 'GRUæ¶æ„ | GRU Architecture', ha='center', va='center', 
             fontsize=14, fontweight='bold', transform=ax2.transAxes)
    
    # ç»˜åˆ¶GRUçš„ä¸¤ä¸ªé—¨
    # Draw GRU's two gates
    gates = ['é‡ç½®é—¨\nReset Gate', 'æ›´æ–°é—¨\nUpdate Gate']
    colors = ['lightcoral', 'lightblue']
    
    for i, (gate, color) in enumerate(zip(gates, colors)):
        rect = plt.Rectangle((0.1, 0.6 - i*0.25), 0.8, 0.2, 
                           facecolor=color, alpha=0.7, transform=ax2.transAxes)
        ax2.add_patch(rect)
        ax2.text(0.5, 0.7 - i*0.25, gate, ha='center', va='center', 
                fontweight='bold', transform=ax2.transAxes)
    
    # éšè—çŠ¶æ€
    # Hidden state
    rect = plt.Rectangle((0.1, 0.05), 0.8, 0.15, 
                        facecolor='lightgreen', alpha=0.7, transform=ax2.transAxes)
    ax2.add_patch(rect)
    ax2.text(0.5, 0.125, 'éšè—çŠ¶æ€\nHidden State', ha='center', va='center', 
            fontweight='bold', transform=ax2.transAxes)
    
    ax2.set_xlim(0, 1)
    ax2.set_ylim(0, 1)
    ax2.axis('off')
    
    plt.suptitle('LSTM vs GRU æ¶æ„å¯¹æ¯” | LSTM vs GRU Architecture Comparison', 
                fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.savefig('lstm_gru_architecture_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()


if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹LSTM vs GRUç»¼åˆå¯¹æ¯”åˆ†æé¡¹ç›®")
    print("ğŸš€ Starting LSTM vs GRU Comprehensive Comparison Analysis Project")
    print("=" * 80)
    
    # åˆ›å»ºå¯¹æ¯”å™¨å®ä¾‹
    # Create comparator instance
    comparator = ModelComparator()
    
    # è¿è¡Œç»¼åˆå¯¹æ¯”
    # Run comprehensive comparison
    results = comparator.run_comprehensive_comparison()
    
    # å¯è§†åŒ–ç»“æœ
    # Visualize results
    comparator.visualize_comparison_results()
    
    # è¯¦ç»†æ¶æ„åˆ†æ
    # Detailed architecture analysis
    run_detailed_architecture_analysis()
    
    # ç”Ÿæˆå»ºè®®
    # Generate recommendations
    comparator.generate_recommendations()
    
    print("\nğŸ‰ ç»¼åˆå¯¹æ¯”åˆ†æå®Œæˆï¼| Comprehensive Comparison Analysis Completed!")
    print("ğŸ“š ä¸»è¦å‘ç° | Key Findings:")
    print("1. GRUåœ¨è®­ç»ƒé€Ÿåº¦ä¸Šé€šå¸¸æ¯”LSTMå¿«20-30%")
    print("   GRU is typically 20-30% faster than LSTM in training")
    print("2. GRUçš„å‚æ•°æ•°é‡æ¯”LSTMå°‘çº¦25%")
    print("   GRU has about 25% fewer parameters than LSTM")
    print("3. åœ¨å¤§å¤šæ•°ä»»åŠ¡ä¸Šï¼ŒGRUå’ŒLSTMçš„æ€§èƒ½æ¥è¿‘")
    print("   GRU and LSTM perform similarly on most tasks")
    print("4. LSTMåœ¨å¤æ‚é•¿åºåˆ—ä»»åŠ¡ä¸Šå¯èƒ½æœ‰è½»å¾®ä¼˜åŠ¿")
    print("   LSTM may have slight advantages on complex long sequence tasks")
    print("5. é€‰æ‹©å“ªä¸ªæ¨¡å‹ä¸»è¦å–å†³äºå…·ä½“åº”ç”¨éœ€æ±‚")
    print("   Model choice mainly depends on specific application requirements") 