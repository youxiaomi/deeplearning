"""
ä»é›¶å®ç°LSTM - æ·±å…¥ç†è§£é•¿çŸ­æœŸè®°å¿†ç½‘ç»œ
LSTM Implementation from Scratch - Deep Understanding of Long Short-Term Memory Networks

è¿™ä¸ªæ–‡ä»¶æ¼”ç¤ºäº†å¦‚ä½•ä»æ•°å­¦å…¬å¼å¼€å§‹ï¼Œæ‰‹åŠ¨å®ç°LSTMçš„å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ã€‚
This file demonstrates how to manually implement LSTM forward and backward propagation from mathematical formulas.

å°±åƒå­¦ä¹ åšèœä¸€æ ·ï¼Œæˆ‘ä»¬å…ˆä»æœ€åŸºç¡€çš„åŸæ–™å’Œæ­¥éª¤å¼€å§‹ï¼Œè€Œä¸æ˜¯ç›´æ¥ç”¨ç°æˆçš„è°ƒæ–™åŒ…ã€‚
Like learning to cook, we start with the most basic ingredients and steps, rather than using ready-made seasoning packets.
"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from typing import Tuple, Optional
import sys
import os

# æ·»åŠ ä¸Šçº§ç›®å½•åˆ°è·¯å¾„ä»¥å¯¼å…¥å·¥å…·å‡½æ•°
# Add parent directory to path to import utility functions
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils.data_utils import set_random_seed
from utils.visualization import plot_loss_curve, plot_predictions


class LSTMCellFromScratch:
    """
    ä»é›¶å®ç°çš„LSTMå•å…ƒ
    LSTM Cell implemented from scratch
    
    è¿™å°±åƒä¸€ä¸ªæœ‰ä¸‰ä¸ªé—¨çš„è®°å¿†ç›’å­ï¼š
    - é—å¿˜é—¨ï¼šå†³å®šä¸¢å¼ƒä»€ä¹ˆæ—§è®°å¿†
    - è¾“å…¥é—¨ï¼šå†³å®šä¿å­˜ä»€ä¹ˆæ–°ä¿¡æ¯
    - è¾“å‡ºé—¨ï¼šå†³å®šè¾“å‡ºä»€ä¹ˆä¿¡æ¯
    
    It's like a memory box with three gates:
    - Forget gate: decides what old memories to discard
    - Input gate: decides what new information to store
    - Output gate: decides what information to output
    """
    
    def __init__(self, input_size: int, hidden_size: int, device: str = 'cpu'):
        """
        åˆå§‹åŒ–LSTMå•å…ƒå‚æ•°
        Initialize LSTM cell parameters
        
        Args:
            input_size: è¾“å…¥ç‰¹å¾ç»´åº¦ | Input feature dimension
            hidden_size: éšè—çŠ¶æ€ç»´åº¦ | Hidden state dimension
            device: è®¡ç®—è®¾å¤‡ | Computing device
        """
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.device = device
        
        # åˆå§‹åŒ–æƒé‡çŸ©é˜µ - ä½¿ç”¨Xavieråˆå§‹åŒ–
        # Initialize weight matrices - using Xavier initialization
        self._init_weights()
        
    def _init_weights(self):
        """
        åˆå§‹åŒ–æƒé‡å’Œåç½®
        Initialize weights and biases
        
        LSTMæœ‰4ä¸ªé—¨/çŠ¶æ€æ›´æ–°ï¼Œæ¯ä¸ªéƒ½éœ€è¦æƒé‡çŸ©é˜µï¼š
        LSTM has 4 gates/state updates, each needs weight matrices:
        - é—å¿˜é—¨ | Forget gate
        - è¾“å…¥é—¨ | Input gate  
        - å€™é€‰å€¼ | Candidate values
        - è¾“å‡ºé—¨ | Output gate
        """
        # Xavieråˆå§‹åŒ–çš„æ ‡å‡†å·®
        # Standard deviation for Xavier initialization
        std = 1.0 / np.sqrt(self.hidden_size)
        
        # é—å¿˜é—¨æƒé‡ | Forget gate weights
        self.W_f = torch.randn(self.hidden_size, self.input_size + self.hidden_size, 
                              dtype=torch.float32, device=self.device) * std
        self.b_f = torch.zeros(self.hidden_size, dtype=torch.float32, device=self.device)
        
        # è¾“å…¥é—¨æƒé‡ | Input gate weights
        self.W_i = torch.randn(self.hidden_size, self.input_size + self.hidden_size,
                              dtype=torch.float32, device=self.device) * std
        self.b_i = torch.zeros(self.hidden_size, dtype=torch.float32, device=self.device)
        
        # å€™é€‰å€¼æƒé‡ | Candidate values weights
        self.W_C = torch.randn(self.hidden_size, self.input_size + self.hidden_size,
                              dtype=torch.float32, device=self.device) * std
        self.b_C = torch.zeros(self.hidden_size, dtype=torch.float32, device=self.device)
        
        # è¾“å‡ºé—¨æƒé‡ | Output gate weights
        self.W_o = torch.randn(self.hidden_size, self.input_size + self.hidden_size,
                              dtype=torch.float32, device=self.device) * std
        self.b_o = torch.zeros(self.hidden_size, dtype=torch.float32, device=self.device)
        
        # å­˜å‚¨æ‰€æœ‰å‚æ•°ä»¥ä¾¿è®­ç»ƒ
        # Store all parameters for training
        self.parameters = [self.W_f, self.b_f, self.W_i, self.b_i, 
                          self.W_C, self.b_C, self.W_o, self.b_o]
        
        # ä¸ºå‚æ•°å¯ç”¨æ¢¯åº¦è®¡ç®—
        # Enable gradient computation for parameters
        for param in self.parameters:
            param.requires_grad_(True)
    
    def forward(self, x: torch.Tensor, hidden_state: Tuple[torch.Tensor, torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        LSTMå‰å‘ä¼ æ’­
        LSTM forward propagation
        
        è¿™æ˜¯LSTMçš„æ ¸å¿ƒè®¡ç®—è¿‡ç¨‹ï¼Œå°±åƒä¸€ä¸ªå¤æ‚çš„è®°å¿†å¤„ç†å·¥å‚ï¼š
        This is the core computation process of LSTM, like a complex memory processing factory:
        
        1. æ£€æŸ¥è¾“å…¥ï¼Œå†³å®šé—å¿˜ä»€ä¹ˆæ—§ä¿¡æ¯
        2. æ£€æŸ¥è¾“å…¥ï¼Œå†³å®šå­¦ä¹ ä»€ä¹ˆæ–°ä¿¡æ¯
        3. æ›´æ–°è®°å¿†çŠ¶æ€
        4. å†³å®šè¾“å‡ºä»€ä¹ˆä¿¡æ¯
        
        1. Check input, decide what old information to forget
        2. Check input, decide what new information to learn
        3. Update memory state
        4. Decide what information to output
        
        Args:
            x: å½“å‰æ—¶é—´æ­¥çš„è¾“å…¥ [batch_size, input_size] | Current timestep input
            hidden_state: ä¸Šä¸€æ—¶é—´æ­¥çš„(éšè—çŠ¶æ€, ç»†èƒçŠ¶æ€) | Previous (hidden state, cell state)
            
        Returns:
            Tuple: æ–°çš„(éšè—çŠ¶æ€, ç»†èƒçŠ¶æ€) | New (hidden state, cell state)
        """
        h_prev, C_prev = hidden_state
        
        # å°†è¾“å…¥å’Œä¸Šä¸€ä¸ªéšè—çŠ¶æ€è¿æ¥
        # Concatenate input and previous hidden state
        # å°±åƒæŠŠæ–°ä¿¡æ¯å’Œæ—§è®°å¿†æ”¾åœ¨ä¸€èµ·è€ƒè™‘
        # Like considering new information together with old memories
        combined = torch.cat([x, h_prev], dim=1)  # [batch_size, input_size + hidden_size]
        
        # æ­¥éª¤1ï¼šé—å¿˜é—¨ - "æˆ‘åº”è¯¥å¿˜è®°ä»€ä¹ˆï¼Ÿ"
        # Step 1: Forget gate - "What should I forget?"
        # f_t = Ïƒ(W_f Â· [h_{t-1}, x_t] + b_f)
        f_t = torch.sigmoid(combined @ self.W_f.T + self.b_f)
        
        # æ­¥éª¤2ï¼šè¾“å…¥é—¨ - "æˆ‘åº”è¯¥å­¦ä¹ ä»€ä¹ˆæ–°ä¿¡æ¯ï¼Ÿ"
        # Step 2: Input gate - "What new information should I learn?"
        # i_t = Ïƒ(W_i Â· [h_{t-1}, x_t] + b_i)
        i_t = torch.sigmoid(combined @ self.W_i.T + self.b_i)
        
        # å€™é€‰å€¼ - "æ–°ä¿¡æ¯çš„å…·ä½“å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ"
        # Candidate values - "What is the specific content of new information?"
        # CÌƒ_t = tanh(W_C Â· [h_{t-1}, x_t] + b_C)
        C_tilde = torch.tanh(combined @ self.W_C.T + self.b_C)
        
        # æ­¥éª¤3ï¼šæ›´æ–°ç»†èƒçŠ¶æ€ - "æ›´æ–°æˆ‘çš„é•¿æœŸè®°å¿†"
        # Step 3: Update cell state - "Update my long-term memory"
        # C_t = f_t âŠ™ C_{t-1} + i_t âŠ™ CÌƒ_t
        # è¿™æ˜¯LSTMæœ€å…³é”®çš„æ–¹ç¨‹ï¼é€šè¿‡åŠ æ³•è€Œä¸æ˜¯ä¹˜æ³•æ¥é¿å…æ¢¯åº¦æ¶ˆå¤±
        # This is the most crucial equation in LSTM! Uses addition instead of multiplication to avoid gradient vanishing
        C_t = f_t * C_prev + i_t * C_tilde
        
        # æ­¥éª¤4ï¼šè¾“å‡ºé—¨ - "æˆ‘åº”è¯¥è¾“å‡ºä»€ä¹ˆä¿¡æ¯ï¼Ÿ"
        # Step 4: Output gate - "What information should I output?"
        # o_t = Ïƒ(W_o Â· [h_{t-1}, x_t] + b_o)
        o_t = torch.sigmoid(combined @ self.W_o.T + self.b_o)
        
        # æœ€ç»ˆéšè—çŠ¶æ€ - "è¿™æ˜¯æˆ‘å½“å‰çš„æƒ³æ³•"
        # Final hidden state - "This is my current thought"
        # h_t = o_t âŠ™ tanh(C_t)
        h_t = o_t * torch.tanh(C_t)
        
        return h_t, C_t
    
    def init_hidden(self, batch_size: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        åˆå§‹åŒ–éšè—çŠ¶æ€å’Œç»†èƒçŠ¶æ€
        Initialize hidden state and cell state
        
        å°±åƒç»™ä¸€ä¸ªäººä¸€ä¸ªç©ºç™½çš„å¤§è„‘å¼€å§‹æ€è€ƒ
        Like giving a person a blank brain to start thinking
        
        Args:
            batch_size: æ‰¹æ¬¡å¤§å° | Batch size
            
        Returns:
            Tuple: åˆå§‹(éšè—çŠ¶æ€, ç»†èƒçŠ¶æ€) | Initial (hidden state, cell state)
        """
        h_0 = torch.zeros(batch_size, self.hidden_size, device=self.device)
        C_0 = torch.zeros(batch_size, self.hidden_size, device=self.device)
        return h_0, C_0


class LSTMFromScratch(nn.Module):
    """
    å®Œæ•´çš„LSTMç½‘ç»œå®ç°
    Complete LSTM network implementation
    
    è¿™ä¸ªç±»å°†å¤šä¸ªLSTMå•å…ƒç»„åˆæˆä¸€ä¸ªå®Œæ•´çš„ç½‘ç»œï¼Œ
    å°±åƒæŠŠå¤šä¸ªè®°å¿†å¤„ç†å•å…ƒä¸²è”èµ·æ¥å¤„ç†åºåˆ—æ•°æ®ã€‚
    
    This class combines multiple LSTM cells into a complete network,
    like connecting multiple memory processing units in series to handle sequence data.
    """
    
    def __init__(self, input_size: int, hidden_size: int, num_layers: int = 1, 
                 output_size: int = 1, dropout: float = 0.0):
        """
        åˆå§‹åŒ–LSTMç½‘ç»œ
        Initialize LSTM network
        
        Args:
            input_size: è¾“å…¥ç‰¹å¾æ•° | Number of input features
            hidden_size: éšè—å±‚å¤§å° | Hidden layer size
            num_layers: LSTMå±‚æ•° | Number of LSTM layers
            output_size: è¾“å‡ºå¤§å° | Output size
            dropout: Dropoutæ¯”ç‡ | Dropout ratio
        """
        super(LSTMFromScratch, self).__init__()
        
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.output_size = output_size
        
        # åˆ›å»ºLSTMå•å…ƒå±‚
        # Create LSTM cell layers
        self.lstm_cells = nn.ModuleList()
        for i in range(num_layers):
            # ç¬¬ä¸€å±‚çš„è¾“å…¥å¤§å°æ˜¯input_sizeï¼Œå…¶ä»–å±‚çš„è¾“å…¥å¤§å°æ˜¯hidden_size
            # First layer input size is input_size, other layers input size is hidden_size
            layer_input_size = input_size if i == 0 else hidden_size
            self.lstm_cells.append(LSTMCellFromScratch(layer_input_size, hidden_size))
        
        # è¾“å‡ºå±‚
        # Output layer
        self.output_layer = nn.Linear(hidden_size, output_size)
        
        # Dropoutå±‚
        # Dropout layer
        self.dropout = nn.Dropout(dropout) if dropout > 0 else None
    
    def forward(self, x: torch.Tensor, hidden: Optional[Tuple] = None) -> Tuple[torch.Tensor, Tuple]:
        """
        å‰å‘ä¼ æ’­
        Forward propagation
        
        Args:
            x: è¾“å…¥åºåˆ— [seq_len, batch_size, input_size] | Input sequence
            hidden: åˆå§‹éšè—çŠ¶æ€ | Initial hidden state
            
        Returns:
            Tuple: (è¾“å‡ºåºåˆ—, æœ€ç»ˆéšè—çŠ¶æ€) | (Output sequence, final hidden state)
        """
        seq_len, batch_size, _ = x.size()
        
        # åˆå§‹åŒ–éšè—çŠ¶æ€
        # Initialize hidden states
        if hidden is None:
            hidden_states = []
            for layer in self.lstm_cells:
                h_0, C_0 = layer.init_hidden(batch_size)
                hidden_states.append((h_0, C_0))
        else:
            hidden_states = hidden
        
        outputs = []
        
        # å¯¹æ¯ä¸ªæ—¶é—´æ­¥è¿›è¡Œå¤„ç†
        # Process each time step
        for t in range(seq_len):
            x_t = x[t]  # å½“å‰æ—¶é—´æ­¥çš„è¾“å…¥ | Current timestep input
            
            # é€šè¿‡æ‰€æœ‰LSTMå±‚
            # Pass through all LSTM layers
            for layer_idx, lstm_cell in enumerate(self.lstm_cells):
                h_t, C_t = lstm_cell.forward(x_t, hidden_states[layer_idx])
                hidden_states[layer_idx] = (h_t, C_t)
                
                # ä¸ºä¸‹ä¸€å±‚å‡†å¤‡è¾“å…¥
                # Prepare input for next layer
                x_t = h_t
                
                # åº”ç”¨dropoutï¼ˆé™¤äº†æœ€åä¸€å±‚ï¼‰
                # Apply dropout (except last layer)
                if self.dropout is not None and layer_idx < len(self.lstm_cells) - 1:
                    x_t = self.dropout(x_t)
            
            # é€šè¿‡è¾“å‡ºå±‚
            # Pass through output layer
            output_t = self.output_layer(x_t)
            outputs.append(output_t)
        
        # å †å æ‰€æœ‰æ—¶é—´æ­¥çš„è¾“å‡º
        # Stack outputs from all timesteps
        outputs = torch.stack(outputs, dim=0)  # [seq_len, batch_size, output_size]
        
        return outputs, hidden_states


def demonstrate_lstm_gates():
    """
    æ¼”ç¤ºLSTMé—¨æ§æœºåˆ¶çš„å·¥ä½œåŸç†
    Demonstrate how LSTM gating mechanisms work
    
    é€šè¿‡å…·ä½“çš„æ•°å€¼ä¾‹å­æ¥ç†è§£æ¯ä¸ªé—¨çš„ä½œç”¨
    Understand the role of each gate through specific numerical examples
    """
    print("ğŸ§  LSTMé—¨æ§æœºåˆ¶æ¼”ç¤º | LSTM Gating Mechanism Demonstration")
    print("=" * 60)
    
    set_random_seed(42)
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„LSTMå•å…ƒ
    # Create a simple LSTM cell
    input_size = 3
    hidden_size = 4
    batch_size = 1
    
    lstm_cell = LSTMCellFromScratch(input_size, hidden_size)
    
    # åˆ›å»ºç¤ºä¾‹è¾“å…¥
    # Create example input
    x = torch.tensor([[1.0, 0.5, -0.3]], dtype=torch.float32)  # [batch_size, input_size]
    h_prev = torch.zeros(batch_size, hidden_size)
    C_prev = torch.zeros(batch_size, hidden_size)
    
    print(f"è¾“å…¥ x: {x.numpy()}")
    print(f"å‰ä¸€éšè—çŠ¶æ€ h_prev: {h_prev.numpy()}")
    print(f"å‰ä¸€ç»†èƒçŠ¶æ€ C_prev: {C_prev.numpy()}")
    print()
    
    # æ‰‹åŠ¨è®¡ç®—æ¯ä¸ªé—¨çš„å€¼
    # Manually calculate each gate value
    combined = torch.cat([x, h_prev], dim=1)
    
    # é—å¿˜é—¨
    # Forget gate
    f_t = torch.sigmoid(combined @ lstm_cell.W_f.T + lstm_cell.b_f)
    print(f"ğŸšª é—å¿˜é—¨ f_t: {f_t.detach().numpy()}")
    print("   -> å€¼è¶Šæ¥è¿‘1è¡¨ç¤ºè¶Šè¦ä¿ç•™æ—§è®°å¿†ï¼Œè¶Šæ¥è¿‘0è¡¨ç¤ºè¶Šè¦å¿˜è®°")
    print("   -> Values closer to 1 mean more retention of old memories, closer to 0 mean more forgetting")
    
    # è¾“å…¥é—¨
    # Input gate
    i_t = torch.sigmoid(combined @ lstm_cell.W_i.T + lstm_cell.b_i)
    print(f"ğŸšª è¾“å…¥é—¨ i_t: {i_t.detach().numpy()}")
    print("   -> å€¼è¶Šæ¥è¿‘1è¡¨ç¤ºè¶Šè¦æ¥å—æ–°ä¿¡æ¯ï¼Œè¶Šæ¥è¿‘0è¡¨ç¤ºè¶Šè¦æ‹’ç»")
    print("   -> Values closer to 1 mean more acceptance of new information, closer to 0 mean more rejection")
    
    # å€™é€‰å€¼
    # Candidate values
    C_tilde = torch.tanh(combined @ lstm_cell.W_C.T + lstm_cell.b_C)
    print(f"ğŸ”„ å€™é€‰å€¼ C_tilde: {C_tilde.detach().numpy()}")
    print("   -> è¿™æ˜¯å€™é€‰çš„æ–°ä¿¡æ¯å†…å®¹ï¼ŒèŒƒå›´åœ¨-1åˆ°1ä¹‹é—´")
    print("   -> This is the candidate new information content, ranging from -1 to 1")
    
    # æ›´æ–°ç»†èƒçŠ¶æ€
    # Update cell state
    C_t = f_t * C_prev + i_t * C_tilde
    print(f"ğŸ§  æ–°ç»†èƒçŠ¶æ€ C_t: {C_t.detach().numpy()}")
    print("   -> è¿™æ˜¯é€šè¿‡é—å¿˜é—¨å’Œè¾“å…¥é—¨æ›´æ–°åçš„é•¿æœŸè®°å¿†")
    print("   -> This is the long-term memory updated through forget and input gates")
    
    # è¾“å‡ºé—¨
    # Output gate
    o_t = torch.sigmoid(combined @ lstm_cell.W_o.T + lstm_cell.b_o)
    print(f"ğŸšª è¾“å‡ºé—¨ o_t: {o_t.detach().numpy()}")
    print("   -> å†³å®šè¾“å‡ºç»†èƒçŠ¶æ€çš„å“ªäº›éƒ¨åˆ†")
    print("   -> Decides which parts of cell state to output")
    
    # æœ€ç»ˆéšè—çŠ¶æ€
    # Final hidden state
    h_t = o_t * torch.tanh(C_t)
    print(f"ğŸ¯ æ–°éšè—çŠ¶æ€ h_t: {h_t.detach().numpy()}")
    print("   -> è¿™æ˜¯æœ€ç»ˆçš„è¾“å‡ºï¼Œç»“åˆäº†é•¿æœŸè®°å¿†å’Œå½“å‰éœ€è¦")
    print("   -> This is the final output, combining long-term memory and current needs")
    
    print("\n" + "=" * 60)
    print("ğŸ” å…³é”®æ´å¯Ÿ | Key Insights:")
    print("1. é—å¿˜é—¨å’Œè¾“å…¥é—¨å…±åŒæ§åˆ¶ä¿¡æ¯æµ")
    print("   Forget and input gates jointly control information flow")
    print("2. ç»†èƒçŠ¶æ€é€šè¿‡åŠ æ³•æ›´æ–°ï¼Œé¿å…äº†æ¢¯åº¦æ¶ˆå¤±")
    print("   Cell state updates through addition, avoiding gradient vanishing")
    print("3. è¾“å‡ºé—¨å†³å®šå“ªäº›è®°å¿†ä¿¡æ¯è¢«æ¿€æ´»è¾“å‡º")
    print("   Output gate decides which memory information is activated for output")


def train_simple_sequence():
    """
    è®­ç»ƒä¸€ä¸ªç®€å•çš„åºåˆ—é¢„æµ‹ä»»åŠ¡
    Train a simple sequence prediction task
    
    ä»»åŠ¡ï¼šå­¦ä¹ é¢„æµ‹æ­£å¼¦æ³¢çš„ä¸‹ä¸€ä¸ªå€¼
    Task: Learn to predict the next value of a sine wave
    """
    print("\nğŸ“ˆ LSTMåºåˆ—é¢„æµ‹è®­ç»ƒæ¼”ç¤º | LSTM Sequence Prediction Training Demo")
    print("=" * 60)
    
    set_random_seed(42)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device} | Using device: {device}")
    
    # ç”Ÿæˆæ­£å¼¦æ³¢æ•°æ®
    # Generate sine wave data
    seq_length = 50
    num_samples = 1000
    
    # åˆ›å»ºæ­£å¼¦æ³¢åºåˆ—
    # Create sine wave sequence
    t = np.linspace(0, 4*np.pi, num_samples)
    data = np.sin(t) + 0.1 * np.random.randn(num_samples)  # æ·»åŠ å™ªå£° | Add noise
    
    print(f"æ•°æ®é•¿åº¦: {len(data)} | Data length: {len(data)}")
    print(f"åºåˆ—é•¿åº¦: {seq_length} | Sequence length: {seq_length}")
    
    # åˆ›å»ºåºåˆ—æ•°æ®
    # Create sequence data
    X, y = [], []
    for i in range(len(data) - seq_length):
        X.append(data[i:i+seq_length])
        y.append(data[i+seq_length])
    
    X = np.array(X)
    y = np.array(y)
    
    # è½¬æ¢ä¸ºå¼ é‡
    # Convert to tensors
    X = torch.FloatTensor(X).unsqueeze(-1).to(device)  # [num_samples, seq_length, 1]
    y = torch.FloatTensor(y).unsqueeze(-1).to(device)  # [num_samples, 1]
    
    # è°ƒæ•´ç»´åº¦ä¸º [seq_length, batch_size, input_size]
    # Adjust dimensions to [seq_length, batch_size, input_size]
    X = X.transpose(0, 1)  # [seq_length, num_samples, 1]
    
    # åˆ›å»ºLSTMæ¨¡å‹
    # Create LSTM model
    model = LSTMFromScratch(
        input_size=1,
        hidden_size=32,
        num_layers=2,
        output_size=1,
        dropout=0.1
    ).to(device)
    
    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    # Loss function and optimizer
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    # è®­ç»ƒå¾ªç¯
    # Training loop
    num_epochs = 100
    losses = []
    
    print("å¼€å§‹è®­ç»ƒ... | Starting training...")
    
    for epoch in range(num_epochs):
        model.train()
        
        # å‰å‘ä¼ æ’­
        # Forward propagation
        outputs, _ = model(X)
        
        # åªä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        # Only use the output from the last time step
        predictions = outputs[-1]  # [num_samples, 1]
        
        # è®¡ç®—æŸå¤±
        # Calculate loss
        loss = criterion(predictions, y)
        
        # åå‘ä¼ æ’­
        # Backward propagation
        optimizer.zero_grad()
        loss.backward()
        
        # æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
        # Gradient clipping to prevent gradient explosion
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        optimizer.step()
        
        losses.append(loss.item())
        
        if (epoch + 1) % 20 == 0:
            print(f'è½®æ¬¡ {epoch+1}/{num_epochs}, æŸå¤±: {loss.item():.6f}')
            print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.6f}')
    
    print("è®­ç»ƒå®Œæˆï¼| Training completed!")
    
    # æµ‹è¯•æ¨¡å‹
    # Test model
    model.eval()
    with torch.no_grad():
        test_outputs, _ = model(X)
        test_predictions = test_outputs[-1].cpu().numpy()
        actual_values = y.cpu().numpy()
    
    # å¯è§†åŒ–ç»“æœ
    # Visualize results
    plt.figure(figsize=(15, 10))
    
    # æŸå¤±æ›²çº¿
    # Loss curve
    plt.subplot(2, 1, 1)
    plt.plot(losses)
    plt.title('è®­ç»ƒæŸå¤±æ›²çº¿ | Training Loss Curve')
    plt.xlabel('è½®æ¬¡ | Epoch')
    plt.ylabel('æŸå¤± | Loss')
    plt.grid(True)
    
    # é¢„æµ‹ç»“æœå¯¹æ¯”
    # Prediction comparison
    plt.subplot(2, 1, 2)
    indices = range(min(200, len(test_predictions)))  # åªæ˜¾ç¤ºå‰200ä¸ªç‚¹ | Only show first 200 points
    plt.plot(indices, actual_values[:len(indices)], 'b-', label='å®é™…å€¼ | Actual', alpha=0.7)
    plt.plot(indices, test_predictions[:len(indices)], 'r-', label='é¢„æµ‹å€¼ | Predicted', alpha=0.7)
    plt.title('LSTMåºåˆ—é¢„æµ‹ç»“æœ | LSTM Sequence Prediction Results')
    plt.xlabel('æ—¶é—´æ­¥ | Time Step')
    plt.ylabel('å€¼ | Value')
    plt.legend()
    plt.grid(True)
    
    plt.tight_layout()
    plt.savefig('lstm_training_results.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    # Calculate evaluation metrics
    mse = np.mean((test_predictions - actual_values) ** 2)
    rmse = np.sqrt(mse)
    mae = np.mean(np.abs(test_predictions - actual_values))
    
    print(f"\nğŸ“Š è¯„ä¼°æŒ‡æ ‡ | Evaluation Metrics:")
    print(f"MSE (å‡æ–¹è¯¯å·®): {mse:.6f}")
    print(f"RMSE (å‡æ–¹æ ¹è¯¯å·®): {rmse:.6f}")
    print(f"MAE (å¹³å‡ç»å¯¹è¯¯å·®): {mae:.6f}")


def compare_with_pytorch_lstm():
    """
    ä¸PyTorchå†…ç½®LSTMè¿›è¡Œå¯¹æ¯”
    Compare with PyTorch built-in LSTM
    
    éªŒè¯æˆ‘ä»¬çš„å®ç°æ˜¯å¦æ­£ç¡®
    Verify if our implementation is correct
    """
    print("\nğŸ” ä¸PyTorch LSTMå¯¹æ¯” | Comparison with PyTorch LSTM")
    print("=" * 60)
    
    set_random_seed(42)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    # Create test data
    seq_len, batch_size, input_size = 10, 5, 3
    hidden_size = 8
    
    x = torch.randn(seq_len, batch_size, input_size, device=device)
    
    # æˆ‘ä»¬çš„LSTM
    # Our LSTM
    our_lstm = LSTMFromScratch(input_size, hidden_size, num_layers=1, output_size=hidden_size).to(device)
    
    # PyTorchçš„LSTM
    # PyTorch's LSTM
    pytorch_lstm = nn.LSTM(input_size, hidden_size, batch_first=False).to(device)
    
    # ä¸ºäº†å…¬å¹³æ¯”è¾ƒï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨ç›¸åŒçš„æƒé‡
    # For fair comparison, we need to use the same weights
    # è¿™é‡Œæˆ‘ä»¬åªæ¯”è¾ƒè¾“å‡ºçš„å½¢çŠ¶å’Œè®¡ç®—å›¾çš„æ­£ç¡®æ€§
    # Here we only compare output shapes and computational graph correctness
    
    print("æ¯”è¾ƒè¾“å‡ºå½¢çŠ¶å’Œæ¢¯åº¦æµ...")
    print("Comparing output shapes and gradient flow...")
    
    # æˆ‘ä»¬çš„LSTMå‰å‘ä¼ æ’­
    # Our LSTM forward propagation
    our_output, our_hidden = our_lstm(x)
    our_loss = our_output.mean()
    
    print(f"æˆ‘ä»¬çš„LSTMè¾“å‡ºå½¢çŠ¶: {our_output.shape}")
    print(f"Our LSTM output shape: {our_output.shape}")
    
    # PyTorch LSTMå‰å‘ä¼ æ’­
    # PyTorch LSTM forward propagation
    pytorch_output, pytorch_hidden = pytorch_lstm(x)
    pytorch_loss = pytorch_output.mean()
    
    print(f"PyTorch LSTMè¾“å‡ºå½¢çŠ¶: {pytorch_output.shape}")
    print(f"PyTorch LSTM output shape: {pytorch_output.shape}")
    
    # æµ‹è¯•æ¢¯åº¦è®¡ç®—
    # Test gradient computation
    print("\næµ‹è¯•æ¢¯åº¦è®¡ç®—...")
    print("Testing gradient computation...")
    
    our_loss.backward()
    pytorch_loss.backward()
    
    # æ£€æŸ¥æˆ‘ä»¬çš„æ¨¡å‹æ˜¯å¦æœ‰æ¢¯åº¦
    # Check if our model has gradients
    our_has_grad = any(p.grad is not None for p in our_lstm.parameters())
    pytorch_has_grad = any(p.grad is not None for p in pytorch_lstm.parameters())
    
    print(f"æˆ‘ä»¬çš„LSTMæ¢¯åº¦è®¡ç®—: {'âœ… æˆåŠŸ' if our_has_grad else 'âŒ å¤±è´¥'}")
    print(f"Our LSTM gradient computation: {'âœ… Success' if our_has_grad else 'âŒ Failed'}")
    print(f"PyTorch LSTMæ¢¯åº¦è®¡ç®—: {'âœ… æˆåŠŸ' if pytorch_has_grad else 'âŒ å¤±è´¥'}")
    print(f"PyTorch LSTM gradient computation: {'âœ… Success' if pytorch_has_grad else 'âŒ Failed'}")
    
    print("\nâœ… å¯¹æ¯”å®Œæˆï¼æˆ‘ä»¬çš„å®ç°åœ¨ç»“æ„ä¸Šæ˜¯æ­£ç¡®çš„ã€‚")
    print("âœ… Comparison completed! Our implementation is structurally correct.")


if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹LSTMä»é›¶å®ç°æ¼”ç¤º | Starting LSTM From Scratch Demonstration")
    print("=" * 80)
    
    # æ¼”ç¤ºLSTMé—¨æ§æœºåˆ¶
    # Demonstrate LSTM gating mechanism
    demonstrate_lstm_gates()
    
    # è®­ç»ƒç®€å•åºåˆ—é¢„æµ‹ä»»åŠ¡
    # Train simple sequence prediction task
    train_simple_sequence()
    
    # ä¸PyTorch LSTMå¯¹æ¯”
    # Compare with PyTorch LSTM
    compare_with_pytorch_lstm()
    
    print("\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼é€šè¿‡è¿™ä¸ªå®ä¾‹ï¼Œä½ åº”è¯¥èƒ½å¤Ÿæ·±å…¥ç†è§£ï¼š")
    print("ğŸ‰ Demonstration completed! Through this example, you should deeply understand:")
    print("1. LSTMçš„ä¸‰ä¸ªé—¨å¦‚ä½•æ§åˆ¶ä¿¡æ¯æµ")
    print("   How LSTM's three gates control information flow")
    print("2. ä¸ºä»€ä¹ˆåŠ æ³•æ›´æ–°èƒ½é¿å…æ¢¯åº¦æ¶ˆå¤±")
    print("   Why additive updates can avoid gradient vanishing")
    print("3. å¦‚ä½•ä»æ•°å­¦å…¬å¼å®ç°å®Œæ•´çš„LSTM")
    print("   How to implement complete LSTM from mathematical formulas")
    print("4. LSTMåœ¨åºåˆ—é¢„æµ‹ä»»åŠ¡ä¸­çš„å®é™…åº”ç”¨")
    print("   Practical application of LSTM in sequence prediction tasks")
    
    print("\nğŸ“š æ¥ä¸‹æ¥å¯ä»¥å­¦ä¹ ï¼š")
    print("ğŸ“š Next you can learn:")
    print("- GRUçš„å®ç°å’Œå¯¹æ¯”")
    print("  GRU implementation and comparison")
    print("- æ›´å¤æ‚çš„åºåˆ—ä»»åŠ¡ï¼ˆæ–‡æœ¬ç”Ÿæˆã€æƒ…æ„Ÿåˆ†æç­‰ï¼‰")
    print("  More complex sequence tasks (text generation, sentiment analysis, etc.)")
    print("- æ³¨æ„åŠ›æœºåˆ¶å’ŒTransformer")
    print("  Attention mechanism and Transformer") 