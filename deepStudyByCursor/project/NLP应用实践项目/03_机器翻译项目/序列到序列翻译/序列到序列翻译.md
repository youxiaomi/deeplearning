# 序列到序列机器翻译
# Sequence-to-Sequence Machine Translation

**让机器跨越语言障碍 - 构建智能翻译系统**
**Making machines cross language barriers - Building intelligent translation systems**

---

## 🌍 什么是序列到序列翻译？| What is Sequence-to-Sequence Translation?

序列到序列(Seq2Seq)翻译就像给机器配备了"语言转换器"。它接收一种语言的句子作为输入，然后生成另一种语言的对应翻译。这不是简单的词汇替换，而是需要理解源语言的语义，然后用目标语言重新表达。

Sequence-to-sequence (Seq2Seq) translation is like equipping machines with a "language converter". It takes a sentence in one language as input, then generates the corresponding translation in another language. This is not simple word substitution, but requires understanding the semantics of the source language and then re-expressing it in the target language.

### 机器翻译的演进历程 | Evolution of Machine Translation

```
1940s-1980s: 基于规则的翻译 | Rule-based Translation
- 手工编写语法规则和词典
- Manually written grammar rules and dictionaries

1990s-2000s: 统计机器翻译 | Statistical Machine Translation  
- 基于大量平行语料的统计模型
- Statistical models based on large parallel corpora

2010s: 神经机器翻译 | Neural Machine Translation
- 端到端的深度学习方法
- End-to-end deep learning approaches

2020s+: 大规模预训练翻译模型 | Large-scale Pretrained Translation Models
- Transformer架构 + 海量数据
- Transformer architecture + massive data
```

### Seq2Seq的核心思想 | Core Ideas of Seq2Seq

**编码器-解码器架构 (Encoder-Decoder Architecture)**

```python
class Seq2SeqFramework:
    """
    序列到序列框架的概念模型
    Conceptual model of Seq2Seq framework
    """
    def __init__(self):
        # 编码器：将源语言序列编码为向量表示
        # Encoder: encode source language sequence to vector representation
        self.encoder = LanguageEncoder()
        
        # 解码器：从向量表示生成目标语言序列
        # Decoder: generate target language sequence from vector representation
        self.decoder = LanguageDecoder()
    
    def translate(self, source_sentence):
        """
        翻译过程 | Translation process
        """
        # 步骤1: 编码源语言
        # Step 1: Encode source language
        context_vector = self.encoder.encode(source_sentence)
        
        # 步骤2: 解码到目标语言
        # Step 2: Decode to target language
        target_sentence = self.decoder.decode(context_vector)
        
        return target_sentence
```

## 🔬 Seq2Seq模型技术架构 | Seq2Seq Model Technical Architecture

### 1. 经典RNN-based Seq2Seq

#### 基础编码器-解码器模型 | Basic Encoder-Decoder Model

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import random

class Encoder(nn.Module):
    """
    RNN编码器
    RNN Encoder
    
    将源语言序列编码为固定大小的上下文向量
    Encode source language sequence to fixed-size context vector
    """
    def __init__(self, input_size, hidden_size, num_layers=2, dropout=0.1):
        super(Encoder, self).__init__()
        
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # 词嵌入层 | Word embedding layer
        self.embedding = nn.Embedding(input_size, hidden_size)
        
        # LSTM编码器 | LSTM encoder
        self.lstm = nn.LSTM(
            hidden_size, 
            hidden_size, 
            num_layers, 
            dropout=dropout,
            batch_first=True,
            bidirectional=True
        )
        
        self.dropout = nn.Dropout(dropout)
        
        # 双向LSTM的输出需要压缩 | Bidirectional LSTM output needs compression
        self.hidden_projection = nn.Linear(hidden_size * 2, hidden_size)
        self.cell_projection = nn.Linear(hidden_size * 2, hidden_size)
    
    def forward(self, source_sequence, source_lengths):
        """
        编码源序列
        Encode source sequence
        
        Args:
            source_sequence: [batch_size, max_src_len]
            source_lengths: [batch_size] 每个序列的实际长度
        """
        batch_size = source_sequence.size(0)
        
        # 词嵌入 | Word embedding
        embedded = self.embedding(source_sequence)
        embedded = self.dropout(embedded)
        
        # 打包序列以处理不同长度 | Pack sequences to handle different lengths
        packed_embedded = nn.utils.rnn.pack_padded_sequence(
            embedded, source_lengths, batch_first=True, enforce_sorted=False
        )
        
        # LSTM编码 | LSTM encoding
        packed_outputs, (hidden, cell) = self.lstm(packed_embedded)
        
        # 解包序列 | Unpack sequences
        encoder_outputs, _ = nn.utils.rnn.pad_packed_sequence(
            packed_outputs, batch_first=True
        )
        
        # 处理双向LSTM的隐状态 | Process bidirectional LSTM hidden states
        # hidden: [num_layers * 2, batch_size, hidden_size]
        # 重新组织为 [num_layers, batch_size, hidden_size * 2]
        hidden = hidden.view(self.num_layers, 2, batch_size, self.hidden_size)
        hidden = torch.cat([hidden[:, 0, :, :], hidden[:, 1, :, :]], dim=2)
        
        cell = cell.view(self.num_layers, 2, batch_size, self.hidden_size)
        cell = torch.cat([cell[:, 0, :, :], cell[:, 1, :, :]], dim=2)
        
        # 投影到解码器的隐状态大小 | Project to decoder hidden state size
        hidden = self.hidden_projection(hidden)
        cell = self.cell_projection(cell)
        
        return encoder_outputs, (hidden, cell)


class Decoder(nn.Module):
    """
    RNN解码器（无注意力机制）
    RNN Decoder (without attention mechanism)
    """
    def __init__(self, output_size, hidden_size, num_layers=2, dropout=0.1):
        super(Decoder, self).__init__()
        
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.num_layers = num_layers
        
        # 词嵌入层 | Word embedding layer
        self.embedding = nn.Embedding(output_size, hidden_size)
        
        # LSTM解码器 | LSTM decoder
        self.lstm = nn.LSTM(
            hidden_size,
            hidden_size,
            num_layers,
            dropout=dropout,
            batch_first=True
        )
        
        # 输出投影层 | Output projection layer
        self.out = nn.Linear(hidden_size, output_size)
        
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, input_token, hidden_state, cell_state):
        """
        解码单个时间步
        Decode single time step
        
        Args:
            input_token: [batch_size, 1] 当前输入token
            hidden_state: [num_layers, batch_size, hidden_size]
            cell_state: [num_layers, batch_size, hidden_size]
        """
        # 词嵌入 | Word embedding
        embedded = self.embedding(input_token)
        embedded = self.dropout(embedded)
        
        # LSTM前向传播 | LSTM forward pass
        output, (hidden, cell) = self.lstm(embedded, (hidden_state, cell_state))
        
        # 输出预测 | Output prediction
        prediction = self.out(output)
        
        return prediction, hidden, cell


class BasicSeq2Seq(nn.Module):
    """
    基础序列到序列模型
    Basic Sequence-to-Sequence Model
    """
    def __init__(self, encoder, decoder, device):
        super(BasicSeq2Seq, self).__init__()
        
        self.encoder = encoder
        self.decoder = decoder
        self.device = device
    
    def forward(self, source, target, source_lengths, teacher_forcing_ratio=0.5):
        """
        前向传播
        Forward pass
        
        Args:
            source: [batch_size, src_len] 源序列
            target: [batch_size, trg_len] 目标序列 
            source_lengths: [batch_size] 源序列长度
            teacher_forcing_ratio: 教师强制比率
        """
        batch_size = source.shape[0]
        target_len = target.shape[1]
        target_vocab_size = self.decoder.output_size
        
        # 存储解码器输出 | Store decoder outputs
        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(self.device)
        
        # 编码源序列 | Encode source sequence
        encoder_outputs, (hidden, cell) = self.encoder(source, source_lengths)
        
        # 解码器的第一个输入是SOS token
        # First input to decoder is SOS token
        input_token = target[:, 0].unsqueeze(1)  # [batch_size, 1]
        
        for t in range(1, target_len):
            # 解码一步
            # Decode one step
            output, hidden, cell = self.decoder(input_token, hidden, cell)
            outputs[:, t] = output.squeeze(1)
            
            # 决定下一个输入：教师强制 vs 模型预测
            # Decide next input: teacher forcing vs model prediction
            teacher_force = random.random() < teacher_forcing_ratio
            top1 = output.argmax(2)
            input_token = target[:, t].unsqueeze(1) if teacher_force else top1
        
        return outputs
    
    def translate(self, source, source_length, max_length=50, sos_token=1, eos_token=2):
        """
        翻译单个句子（推理模式）
        Translate single sentence (inference mode)
        """
        self.eval()
        
        with torch.no_grad():
            # 编码
            # Encoding
            encoder_outputs, (hidden, cell) = self.encoder(source, source_length)
            
            # 初始化解码
            # Initialize decoding
            input_token = torch.tensor([[sos_token]]).to(self.device)
            outputs = [sos_token]
            
            for _ in range(max_length):
                output, hidden, cell = self.decoder(input_token, hidden, cell)
                predicted = output.argmax(2).item()
                outputs.append(predicted)
                
                if predicted == eos_token:
                    break
                
                input_token = torch.tensor([[predicted]]).to(self.device)
        
        return outputs
```

### 2. 注意力机制增强的Seq2Seq | Attention-Enhanced Seq2Seq

**解决信息瓶颈问题 | Solving Information Bottleneck Problem**

基础Seq2Seq模型的问题是将整个源序列压缩到一个固定大小的向量中，这会导致信息丢失。注意力机制允许解码器在每个时间步访问编码器的所有隐状态。

The problem with basic Seq2Seq models is compressing the entire source sequence into a fixed-size vector, which leads to information loss. Attention mechanism allows the decoder to access all encoder hidden states at each time step.

```python
class AttentionMechanism(nn.Module):
    """
    注意力机制
    Attention Mechanism
    
    计算解码器当前状态与编码器所有状态的注意力权重
    Compute attention weights between decoder current state and all encoder states
    """
    def __init__(self, hidden_size, method='general'):
        super(AttentionMechanism, self).__init__()
        
        self.method = method
        self.hidden_size = hidden_size
        
        if method == 'general':
            self.attn = nn.Linear(hidden_size, hidden_size)
        elif method == 'concat':
            self.attn = nn.Linear(hidden_size * 2, hidden_size)
            self.v = nn.Parameter(torch.FloatTensor(hidden_size))
    
    def forward(self, hidden, encoder_outputs, encoder_mask=None):
        """
        计算注意力权重和上下文向量
        Compute attention weights and context vector
        
        Args:
            hidden: [batch_size, hidden_size] 解码器当前隐状态
            encoder_outputs: [batch_size, src_len, hidden_size] 编码器输出
            encoder_mask: [batch_size, src_len] 编码器掩码
        """
        batch_size = encoder_outputs.size(0)
        src_len = encoder_outputs.size(1)
        
        # 计算注意力分数
        # Compute attention scores
        if self.method == 'general':
            # hidden: [batch_size, hidden_size] -> [batch_size, 1, hidden_size]
            hidden = hidden.unsqueeze(1)
            # 计算与所有编码器状态的相似度
            # Compute similarity with all encoder states
            energy = torch.bmm(hidden, self.attn(encoder_outputs).transpose(1, 2))
            energy = energy.squeeze(1)  # [batch_size, src_len]
        
        elif self.method == 'concat':
            # 拼接隐状态和编码器输出
            # Concatenate hidden state and encoder outputs
            hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)
            energy = torch.cat([hidden, encoder_outputs], dim=2)
            energy = torch.tanh(self.attn(energy))
            energy = torch.sum(self.v * energy, dim=2)  # [batch_size, src_len]
        
        # 应用掩码
        # Apply mask
        if encoder_mask is not None:
            energy = energy.masked_fill(encoder_mask == 0, -1e10)
        
        # 计算注意力权重
        # Compute attention weights
        attention_weights = F.softmax(energy, dim=1)  # [batch_size, src_len]
        
        # 计算上下文向量
        # Compute context vector
        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)
        context = context.squeeze(1)  # [batch_size, hidden_size]
        
        return context, attention_weights


class AttentionDecoder(nn.Module):
    """
    带注意力机制的解码器
    Decoder with Attention Mechanism
    """
    def __init__(self, output_size, hidden_size, num_layers=2, dropout=0.1, attention_method='general'):
        super(AttentionDecoder, self).__init__()
        
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.num_layers = num_layers
        
        # 词嵌入层 | Word embedding layer
        self.embedding = nn.Embedding(output_size, hidden_size)
        
        # 注意力机制 | Attention mechanism
        self.attention = AttentionMechanism(hidden_size, attention_method)
        
        # LSTM解码器 | LSTM decoder
        self.lstm = nn.LSTM(
            hidden_size * 2,  # 嵌入 + 上下文向量
            hidden_size,
            num_layers,
            dropout=dropout,
            batch_first=True
        )
        
        # 输出层 | Output layer
        self.concat = nn.Linear(hidden_size * 2, hidden_size)
        self.out = nn.Linear(hidden_size, output_size)
        
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, input_token, hidden_state, cell_state, encoder_outputs, encoder_mask=None):
        """
        带注意力的解码步骤
        Decoding step with attention
        """
        # 词嵌入 | Word embedding
        embedded = self.embedding(input_token)
        embedded = self.dropout(embedded)
        
        # 计算注意力上下文
        # Compute attention context
        # 使用上一时刻的隐状态计算注意力
        # Use previous hidden state to compute attention
        context, attention_weights = self.attention(
            hidden_state[-1], encoder_outputs, encoder_mask
        )
        
        # 拼接嵌入和上下文
        # Concatenate embedding and context
        lstm_input = torch.cat([embedded, context.unsqueeze(1)], dim=2)
        
        # LSTM前向传播 | LSTM forward pass
        output, (hidden, cell) = self.lstm(lstm_input, (hidden_state, cell_state))
        
        # 输出层计算 | Output layer computation
        concat_input = torch.cat([output.squeeze(1), context], dim=1)
        concat_output = torch.tanh(self.concat(concat_input))
        prediction = self.out(concat_output).unsqueeze(1)
        
        return prediction, hidden, cell, attention_weights


class AttentionSeq2Seq(nn.Module):
    """
    带注意力机制的序列到序列模型
    Sequence-to-Sequence Model with Attention
    """
    def __init__(self, encoder, decoder, device):
        super(AttentionSeq2Seq, self).__init__()
        
        self.encoder = encoder
        self.decoder = decoder
        self.device = device
    
    def create_mask(self, src, src_lengths):
        """
        创建源序列掩码
        Create source sequence mask
        """
        batch_size, max_len = src.size()
        mask = torch.zeros(batch_size, max_len).to(self.device)
        
        for i, length in enumerate(src_lengths):
            mask[i, :length] = 1
        
        return mask.bool()
    
    def forward(self, source, target, source_lengths, teacher_forcing_ratio=0.5):
        """
        前向传播
        Forward pass
        """
        batch_size = source.shape[0]
        target_len = target.shape[1]
        target_vocab_size = self.decoder.output_size
        
        # 存储输出和注意力权重
        # Store outputs and attention weights
        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(self.device)
        attentions = torch.zeros(batch_size, target_len, source.size(1)).to(self.device)
        
        # 编码 | Encoding
        encoder_outputs, (hidden, cell) = self.encoder(source, source_lengths)
        
        # 创建源序列掩码 | Create source sequence mask
        encoder_mask = self.create_mask(source, source_lengths)
        
        # 解码 | Decoding
        input_token = target[:, 0].unsqueeze(1)
        
        for t in range(1, target_len):
            output, hidden, cell, attention = self.decoder(
                input_token, hidden, cell, encoder_outputs, encoder_mask
            )
            
            outputs[:, t] = output.squeeze(1)
            attentions[:, t] = attention
            
            # 教师强制 | Teacher forcing
            teacher_force = random.random() < teacher_forcing_ratio
            top1 = output.argmax(2)
            input_token = target[:, t].unsqueeze(1) if teacher_force else top1
        
        return outputs, attentions
```

### 3. 高级技术：束搜索解码 | Advanced Technique: Beam Search Decoding

**解决贪心解码的局限性 | Solving Limitations of Greedy Decoding**

贪心解码在每个时间步选择概率最高的词，但这可能导致次优的全局解。束搜索保持多个候选序列，选择全局最优解。

Greedy decoding chooses the highest probability word at each time step, but this may lead to suboptimal global solutions. Beam search maintains multiple candidate sequences to find globally optimal solutions.

```python
class BeamSearchDecoder:
    """
    束搜索解码器
    Beam Search Decoder
    
    在解码过程中维护top-k个候选序列
    Maintain top-k candidate sequences during decoding
    """
    def __init__(self, model, beam_size=5, max_length=50, sos_token=1, eos_token=2):
        self.model = model
        self.beam_size = beam_size
        self.max_length = max_length
        self.sos_token = sos_token
        self.eos_token = eos_token
    
    def search(self, source, source_length):
        """
        执行束搜索
        Execute beam search
        """
        device = source.device
        
        # 编码源序列 | Encode source sequence
        with torch.no_grad():
            encoder_outputs, (hidden, cell) = self.model.encoder(source, source_length)
            encoder_mask = self.model.create_mask(source, source_length)
        
        # 初始化束 | Initialize beam
        # 每个候选包含：(序列, 累积分数, 隐状态, 细胞状态)
        # Each candidate contains: (sequence, cumulative score, hidden state, cell state)
        initial_hidden = hidden
        initial_cell = cell
        
        candidates = [(
            [self.sos_token],  # 序列
            0.0,               # 累积log概率
            initial_hidden,    # 隐状态
            initial_cell       # 细胞状态
        )]
        
        finished_sequences = []
        
        for step in range(self.max_length):
            if not candidates:
                break
            
            # 为当前所有候选生成下一个词的分布
            # Generate next word distributions for all current candidates
            all_candidates = []
            
            for sequence, score, hidden, cell in candidates:
                if sequence[-1] == self.eos_token:
                    # 已完成的序列直接加入结果
                    # Add completed sequences directly to results
                    finished_sequences.append((sequence, score))
                    continue
                
                # 准备解码器输入
                # Prepare decoder input
                input_token = torch.tensor([[sequence[-1]]]).to(device)
                
                with torch.no_grad():
                    if hasattr(self.model.decoder, 'attention'):
                        # 带注意力的解码器
                        # Decoder with attention
                        output, new_hidden, new_cell, _ = self.model.decoder(
                            input_token, hidden, cell, encoder_outputs, encoder_mask
                        )
                    else:
                        # 基础解码器
                        # Basic decoder
                        output, new_hidden, new_cell = self.model.decoder(
                            input_token, hidden, cell
                        )
                
                # 获取词汇表上的概率分布
                # Get probability distribution over vocabulary
                probs = F.log_softmax(output.squeeze(), dim=-1)
                
                # 获取top-k个词
                # Get top-k words
                top_k_probs, top_k_indices = torch.topk(probs, self.beam_size)
                
                # 为每个可能的下一个词创建新候选
                # Create new candidates for each possible next word
                for i in range(self.beam_size):
                    next_token = top_k_indices[i].item()
                    token_prob = top_k_probs[i].item()
                    
                    new_sequence = sequence + [next_token]
                    new_score = score + token_prob
                    
                    all_candidates.append((
                        new_sequence,
                        new_score,
                        new_hidden,
                        new_cell
                    ))
            
            # 选择top-k个候选继续
            # Select top-k candidates to continue
            all_candidates.sort(key=lambda x: x[1] / len(x[0]), reverse=True)  # 按平均分数排序
            candidates = all_candidates[:self.beam_size]
        
        # 将未完成的候选也加入结果
        # Add unfinished candidates to results
        for sequence, score, _, _ in candidates:
            finished_sequences.append((sequence, score))
        
        # 按分数排序并返回最佳序列
        # Sort by score and return best sequence
        if finished_sequences:
            finished_sequences.sort(key=lambda x: x[1] / len(x[0]), reverse=True)
            return finished_sequences[0][0]  # 返回最佳序列
        else:
            return [self.sos_token, self.eos_token]  # 默认返回
    
    def translate_batch(self, sources, source_lengths):
        """
        批量翻译
        Batch translation
        """
        translations = []
        
        for i in range(len(sources)):
            source = sources[i:i+1]  # 单个样本
            source_length = source_lengths[i:i+1]
            
            translation = self.search(source, source_length)
            translations.append(translation)
        
        return translations
```

### 4. 训练策略优化 | Training Strategy Optimization

#### 课程学习 (Curriculum Learning)

```python
class CurriculumLearningTrainer:
    """
    课程学习训练器
    Curriculum Learning Trainer
    
    从简单到复杂逐步训练模型
    Gradually train model from simple to complex
    """
    def __init__(self, model, optimizer, criterion, device):
        self.model = model
        self.optimizer = optimizer
        self.criterion = criterion
        self.device = device
    
    def sort_by_difficulty(self, dataset):
        """
        按难度排序数据
        Sort data by difficulty
        """
        # 按句子长度排序（简单的难度度量）
        # Sort by sentence length (simple difficulty measure)
        dataset.sort(key=lambda x: len(x['source']) + len(x['target']))
        return dataset
    
    def create_curriculum_batches(self, dataset, batch_size, num_stages=5):
        """
        创建课程学习批次
        Create curriculum learning batches
        """
        # 将数据分为不同难度阶段
        # Divide data into different difficulty stages
        data_per_stage = len(dataset) // num_stages
        stages = []
        
        for i in range(num_stages):
            start_idx = i * data_per_stage
            end_idx = (i + 1) * data_per_stage if i < num_stages - 1 else len(dataset)
            
            stage_data = dataset[start_idx:end_idx]
            stages.append(stage_data)
        
        return stages
    
    def train_with_curriculum(self, dataset, batch_size, epochs_per_stage=2):
        """
        使用课程学习训练
        Train with curriculum learning
        """
        # 按难度排序并分阶段
        # Sort by difficulty and divide into stages
        sorted_dataset = self.sort_by_difficulty(dataset)
        curriculum_stages = self.create_curriculum_batches(sorted_dataset, batch_size)
        
        for stage_idx, stage_data in enumerate(curriculum_stages):
            print(f"训练阶段 {stage_idx + 1}/{len(curriculum_stages)}")
            print(f"Training stage {stage_idx + 1}/{len(curriculum_stages)}")
            
            # 在当前阶段训练
            # Train on current stage
            for epoch in range(epochs_per_stage):
                self.train_epoch(stage_data, batch_size)
                
            # 评估当前阶段
            # Evaluate current stage
            self.evaluate_stage(stage_data, batch_size)
    
    def train_epoch(self, data, batch_size):
        """
        训练一个epoch
        Train one epoch
        """
        self.model.train()
        total_loss = 0
        
        # 创建批次
        # Create batches
        for i in range(0, len(data), batch_size):
            batch = data[i:i + batch_size]
            
            # 准备批次数据
            # Prepare batch data
            sources, targets, src_lengths = self.prepare_batch(batch)
            
            # 前向传播
            # Forward pass
            self.optimizer.zero_grad()
            outputs = self.model(sources, targets, src_lengths)
            
            # 计算损失
            # Compute loss
            loss = self.compute_loss(outputs, targets)
            
            # 反向传播
            # Backward pass
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            self.optimizer.step()
            
            total_loss += loss.item()
        
        avg_loss = total_loss / (len(data) // batch_size)
        print(f"平均损失 | Average loss: {avg_loss:.4f}")
    
    def prepare_batch(self, batch):
        """
        准备批次数据
        Prepare batch data
        """
        # 实现数据预处理逻辑
        # Implement data preprocessing logic
        sources = []
        targets = []
        src_lengths = []
        
        for item in batch:
            sources.append(item['source'])
            targets.append(item['target'])
            src_lengths.append(len(item['source']))
        
        # 转换为tensor并padding
        # Convert to tensors and pad
        sources = self.pad_sequences(sources)
        targets = self.pad_sequences(targets)
        
        return sources.to(self.device), targets.to(self.device), src_lengths
    
    def pad_sequences(self, sequences):
        """
        序列padding
        Sequence padding
        """
        max_len = max(len(seq) for seq in sequences)
        padded = torch.zeros(len(sequences), max_len, dtype=torch.long)
        
        for i, seq in enumerate(sequences):
            length = len(seq)
            padded[i, :length] = torch.tensor(seq, dtype=torch.long)
        
        return padded
    
    def compute_loss(self, outputs, targets):
        """
        计算损失
        Compute loss
        """
        # 重塑输出和目标以计算损失
        # Reshape outputs and targets to compute loss
        outputs = outputs[:, 1:].contiguous().view(-1, outputs.size(-1))
        targets = targets[:, 1:].contiguous().view(-1)
        
        return self.criterion(outputs, targets)
```

## 📊 评估指标与质量分析 | Evaluation Metrics and Quality Analysis

### BLEU分数计算 | BLEU Score Calculation

```python
from collections import Counter
import math

class BLEUEvaluator:
    """
    BLEU评估器
    BLEU Evaluator
    
    计算机器翻译的BLEU分数
    Calculate BLEU scores for machine translation
    """
    
    def __init__(self, max_order=4):
        self.max_order = max_order
    
    def compute_bleu(self, reference_corpus, translation_corpus):
        """
        计算语料库级别的BLEU分数
        Compute corpus-level BLEU score
        """
        matches_by_order = [0] * self.max_order
        possible_matches_by_order = [0] * self.max_order
        reference_length = 0
        translation_length = 0
        
        for references, translation in zip(reference_corpus, translation_corpus):
            reference_length += min(len(r) for r in references)
            translation_length += len(translation)
            
            # 计算各阶n-gram匹配
            # Compute n-gram matches for each order
            for order in range(1, self.max_order + 1):
                matches, possible_matches = self._compute_ngram_matches(
                    references, translation, order
                )
                matches_by_order[order - 1] += matches
                possible_matches_by_order[order - 1] += possible_matches
        
        # 计算精确率
        # Compute precisions
        precisions = []
        for i in range(self.max_order):
            if possible_matches_by_order[i] > 0:
                precisions.append(matches_by_order[i] / possible_matches_by_order[i])
            else:
                precisions.append(0.0)
        
        # 计算简短惩罚
        # Compute brevity penalty
        if translation_length > reference_length:
            bp = 1.0
        else:
            bp = math.exp(1 - reference_length / translation_length)
        
        # 计算BLEU分数
        # Compute BLEU score
        if min(precisions) > 0:
            p_log_sum = sum(math.log(p) for p in precisions) / self.max_order
            geo_mean = math.exp(p_log_sum)
            bleu = bp * geo_mean
        else:
            bleu = 0.0
        
        return {
            'bleu': bleu,
            'precisions': precisions,
            'brevity_penalty': bp,
            'length_ratio': translation_length / reference_length,
            'translation_length': translation_length,
            'reference_length': reference_length
        }
    
    def _compute_ngram_matches(self, references, translation, order):
        """
        计算n-gram匹配数
        Compute n-gram matches
        """
        translation_ngrams = self._get_ngrams(translation, order)
        
        # 合并所有参考翻译的n-gram
        # Merge n-grams from all reference translations
        reference_ngrams = Counter()
        for reference in references:
            ref_ngrams = self._get_ngrams(reference, order)
            for ngram in ref_ngrams:
                reference_ngrams[ngram] = max(reference_ngrams[ngram], ref_ngrams[ngram])
        
        # 计算匹配数
        # Compute matches
        matches = 0
        for ngram in translation_ngrams:
            matches += min(translation_ngrams[ngram], reference_ngrams[ngram])
        
        possible_matches = sum(translation_ngrams.values())
        
        return matches, possible_matches
    
    def _get_ngrams(self, segment, order):
        """
        提取n-gram
        Extract n-grams
        """
        ngram_counts = Counter()
        for i in range(0, len(segment) - order + 1):
            ngram = tuple(segment[i:i + order])
            ngram_counts[ngram] += 1
        return ngram_counts


def calculate_meteor_score(reference, hypothesis):
    """
    计算METEOR分数（简化版）
    Calculate METEOR score (simplified version)
    """
    # 实际实现需要使用METEOR库或更复杂的算法
    # Actual implementation requires METEOR library or more complex algorithms
    
    ref_words = set(reference.split())
    hyp_words = set(hypothesis.split())
    
    # 计算精确率和召回率
    # Compute precision and recall
    if len(hyp_words) == 0:
        precision = 0
    else:
        precision = len(ref_words & hyp_words) / len(hyp_words)
    
    if len(ref_words) == 0:
        recall = 0
    else:
        recall = len(ref_words & hyp_words) / len(ref_words)
    
    # 计算F1分数
    # Compute F1 score
    if precision + recall == 0:
        f1 = 0
    else:
        f1 = 2 * precision * recall / (precision + recall)
    
    return f1
```

## 🎓 实践项目指导 | Practical Project Guidance

### 完整训练流程 | Complete Training Pipeline

```python
def main_training_pipeline():
    """
    完整的训练流程示例
    Complete training pipeline example
    """
    # 1. 数据准备 | Data preparation
    print("=== 数据准备阶段 | Data Preparation ===")
    
    # 加载和预处理数据
    # Load and preprocess data
    train_data = load_translation_data('train.txt')
    val_data = load_translation_data('val.txt')
    
    # 构建词汇表
    # Build vocabulary
    src_vocab = build_vocabulary([item['source'] for item in train_data])
    tgt_vocab = build_vocabulary([item['target'] for item in train_data])
    
    print(f"源语言词汇表大小: {len(src_vocab)}")
    print(f"目标语言词汇表大小: {len(tgt_vocab)}")
    
    # 2. 模型构建 | Model construction
    print("\n=== 模型构建阶段 | Model Construction ===")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # 创建编码器和解码器
    # Create encoder and decoder
    encoder = Encoder(
        input_size=len(src_vocab),
        hidden_size=512,
        num_layers=2,
        dropout=0.1
    )
    
    decoder = AttentionDecoder(
        output_size=len(tgt_vocab),
        hidden_size=512,
        num_layers=2,
        dropout=0.1,
        attention_method='general'
    )
    
    # 创建完整模型
    # Create complete model
    model = AttentionSeq2Seq(encoder, decoder, device).to(device)
    
    print(f"模型参数数量: {sum(p.numel() for p in model.parameters())}")
    
    # 3. 训练设置 | Training setup
    print("\n=== 训练设置阶段 | Training Setup ===")
    
    criterion = nn.CrossEntropyLoss(ignore_index=0)  # 忽略padding
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)
    
    # 4. 课程学习训练 | Curriculum learning training
    print("\n=== 训练阶段 | Training Phase ===")
    
    trainer = CurriculumLearningTrainer(model, optimizer, criterion, device)
    trainer.train_with_curriculum(train_data, batch_size=32, epochs_per_stage=3)
    
    # 5. 模型评估 | Model evaluation
    print("\n=== 评估阶段 | Evaluation Phase ===")
    
    # 创建束搜索解码器
    # Create beam search decoder
    beam_decoder = BeamSearchDecoder(model, beam_size=5)
    
    # 评估BLEU分数
    # Evaluate BLEU score
    evaluator = BLEUEvaluator()
    
    references = []
    translations = []
    
    for item in val_data[:100]:  # 评估前100个样本
        source = torch.tensor([item['source']]).to(device)
        source_length = torch.tensor([len(item['source'])])
        
        translation = beam_decoder.search(source, source_length)
        
        references.append([item['target']])
        translations.append(translation)
    
    bleu_result = evaluator.compute_bleu(references, translations)
    print(f"BLEU分数: {bleu_result['bleu']:.4f}")
    
    # 6. 模型保存 | Model saving
    print("\n=== 模型保存阶段 | Model Saving ===")
    
    torch.save({
        'model_state_dict': model.state_dict(),
        'src_vocab': src_vocab,
        'tgt_vocab': tgt_vocab,
        'model_config': {
            'hidden_size': 512,
            'num_layers': 2,
            'dropout': 0.1
        }
    }, 'seq2seq_translation_model.pt')
    
    print("训练完成！模型已保存。")
    print("Training completed! Model saved.")


def load_translation_data(file_path):
    """
    加载翻译数据
    Load translation data
    """
    # 简化的数据加载示例
    # Simplified data loading example
    data = []
    
    # 示例数据格式：源语言\t目标语言
    # Example data format: source_language\ttarget_language
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            parts = line.strip().split('\t')
            if len(parts) == 2:
                source = parts[0].split()
                target = parts[1].split()
                data.append({
                    'source': source,
                    'target': ['<sos>'] + target + ['<eos>']
                })
    
    return data


def build_vocabulary(sentences):
    """
    构建词汇表
    Build vocabulary
    """
    vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}
    
    for sentence in sentences:
        for word in sentence:
            if word not in vocab:
                vocab[word] = len(vocab)
    
    return vocab


if __name__ == "__main__":
    main_training_pipeline()
```

通过这个序列到序列翻译项目，你将掌握神经机器翻译的核心技术，学会构建从基础到高级的翻译模型，为开发实用的翻译系统奠定基础！

Through this sequence-to-sequence translation project, you will master the core technologies of neural machine translation, learn to build translation models from basic to advanced, and lay the foundation for developing practical translation systems! 