# æ³¨æ„åŠ›æœºåˆ¶æœºå™¨ç¿»è¯‘
# Attention Mechanism Machine Translation

**è®©æœºå™¨å­¦ä¼š"ä¸“æ³¨"ç¿»è¯‘ - æ·±å…¥å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶**
**Making machines learn to "focus" on translation - Deep dive into multi-head attention**

---

## ğŸ¯ æ³¨æ„åŠ›æœºåˆ¶çš„æ ¸å¿ƒæ€æƒ³ | Core Ideas of Attention Mechanism

æ³¨æ„åŠ›æœºåˆ¶è§£å†³äº†ä¼ ç»ŸSeq2Seqçš„"ä¿¡æ¯ç“¶é¢ˆ"é—®é¢˜ã€‚å°±åƒäººç±»ç¿»è¯‘æ—¶ä¼šé‡ç‚¹å…³æ³¨æºè¯­è¨€ä¸­çš„ç‰¹å®šéƒ¨åˆ†ï¼Œæ³¨æ„åŠ›æœºåˆ¶è®©è§£ç å™¨åœ¨ç”Ÿæˆæ¯ä¸ªè¯æ—¶éƒ½èƒ½"çœ‹åˆ°"å¹¶"ä¸“æ³¨äº"æºåºåˆ—çš„ä¸åŒéƒ¨åˆ†ã€‚

Attention mechanism solves the "information bottleneck" problem of traditional Seq2Seq. Just like humans focus on specific parts of the source language when translating, attention mechanism allows the decoder to "see" and "focus on" different parts of the source sequence when generating each word.

### æ³¨æ„åŠ›çš„ç›´è§‚ç†è§£ | Intuitive Understanding of Attention

**ç¿»è¯‘ç¤ºä¾‹ | Translation Example:**
```
è‹±æ–‡: "The cat sat on the mat"
ä¸­æ–‡: "çŒ«ååœ¨å«å­ä¸Š"

åœ¨ç¿»è¯‘"çŒ«"æ—¶ï¼Œæ³¨æ„åŠ›ä¸»è¦é›†ä¸­åœ¨"cat"
When translating "çŒ«", attention focuses mainly on "cat"

åœ¨ç¿»è¯‘"å"æ—¶ï¼Œæ³¨æ„åŠ›ä¸»è¦é›†ä¸­åœ¨"sat" 
When translating "å", attention focuses mainly on "sat"

åœ¨ç¿»è¯‘"å«å­"æ—¶ï¼Œæ³¨æ„åŠ›ä¸»è¦é›†ä¸­åœ¨"mat"
When translating "å«å­", attention focuses mainly on "mat"
```

## ğŸ”¬ å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ | Multi-Head Attention Mechanism

### 1. ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ› | Scaled Dot-Product Attention

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

def scaled_dot_product_attention(query, key, value, mask=None, dropout=None):
    """
    ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›
    Scaled Dot-Product Attention
    
    Attention(Q,K,V) = softmax(QK^T/âˆšd_k)V
    """
    d_k = query.size(-1)
    
    # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•° | Compute attention scores
    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)
    
    # åº”ç”¨æ©ç  | Apply mask
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)
    
    # è®¡ç®—æ³¨æ„åŠ›æƒé‡ | Compute attention weights
    attention_weights = F.softmax(scores, dim=-1)
    
    # åº”ç”¨dropout | Apply dropout
    if dropout is not None:
        attention_weights = dropout(attention_weights)
    
    # è®¡ç®—è¾“å‡º | Compute output
    output = torch.matmul(attention_weights, value)
    
    return output, attention_weights


class MultiHeadAttention(nn.Module):
    """
    å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
    Multi-Head Attention Mechanism
    """
    def __init__(self, d_model, num_heads, dropout=0.1):
        super(MultiHeadAttention, self).__init__()
        
        assert d_model % num_heads == 0
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        # çº¿æ€§å˜æ¢å±‚ | Linear transformation layers
        self.w_q = nn.Linear(d_model, d_model)
        self.w_k = nn.Linear(d_model, d_model)
        self.w_v = nn.Linear(d_model, d_model)
        self.w_o = nn.Linear(d_model, d_model)
        
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, query, key, value, mask=None):
        """
        å¤šå¤´æ³¨æ„åŠ›å‰å‘ä¼ æ’­
        Multi-head attention forward pass
        """
        batch_size = query.size(0)
        
        # 1. çº¿æ€§å˜æ¢ | Linear transformations
        Q = self.w_q(query)
        K = self.w_k(key)
        V = self.w_v(value)
        
        # 2. é‡å¡‘ä¸ºå¤šå¤´å½¢å¼ | Reshape for multi-head
        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        
        # 3. åº”ç”¨æ³¨æ„åŠ› | Apply attention
        attention_output, attention_weights = scaled_dot_product_attention(
            Q, K, V, mask, self.dropout
        )
        
        # 4. åˆå¹¶å¤šå¤´ | Concatenate heads
        attention_output = attention_output.transpose(1, 2).contiguous().view(
            batch_size, -1, self.d_model
        )
        
        # 5. æœ€ç»ˆçº¿æ€§å˜æ¢ | Final linear transformation
        output = self.w_o(attention_output)
        
        return output, attention_weights
```

### 2. Transformerç¼–ç å™¨ | Transformer Encoder

```python
class TransformerEncoder(nn.Module):
    """
    Transformerç¼–ç å™¨å±‚
    Transformer Encoder Layer
    """
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super(TransformerEncoder, self).__init__()
        
        # å¤šå¤´è‡ªæ³¨æ„åŠ› | Multi-head self-attention
        self.self_attention = MultiHeadAttention(d_model, num_heads, dropout)
        
        # å‰é¦ˆç½‘ç»œ | Feed-forward network
        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model)
        )
        
        # å±‚å½’ä¸€åŒ– | Layer normalization
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x, mask=None):
        """
        ç¼–ç å™¨å‰å‘ä¼ æ’­
        Encoder forward pass
        """
        # è‡ªæ³¨æ„åŠ› + æ®‹å·®è¿æ¥ | Self-attention + residual connection
        attn_output, _ = self.self_attention(x, x, x, mask)
        x = self.norm1(x + self.dropout(attn_output))
        
        # å‰é¦ˆç½‘ç»œ + æ®‹å·®è¿æ¥ | Feed-forward + residual connection
        ff_output = self.feed_forward(x)
        x = self.norm2(x + self.dropout(ff_output))
        
        return x


class TransformerDecoder(nn.Module):
    """
    Transformerè§£ç å™¨å±‚
    Transformer Decoder Layer
    """
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super(TransformerDecoder, self).__init__()
        
        # æ©ç è‡ªæ³¨æ„åŠ› | Masked self-attention
        self.self_attention = MultiHeadAttention(d_model, num_heads, dropout)
        
        # äº¤å‰æ³¨æ„åŠ› | Cross attention
        self.cross_attention = MultiHeadAttention(d_model, num_heads, dropout)
        
        # å‰é¦ˆç½‘ç»œ | Feed-forward network
        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model)
        )
        
        # å±‚å½’ä¸€åŒ– | Layer normalization
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)
        
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):
        """
        è§£ç å™¨å‰å‘ä¼ æ’­
        Decoder forward pass
        """
        # æ©ç è‡ªæ³¨æ„åŠ› | Masked self-attention
        attn_output, _ = self.self_attention(x, x, x, tgt_mask)
        x = self.norm1(x + self.dropout(attn_output))
        
        # äº¤å‰æ³¨æ„åŠ› | Cross attention
        cross_attn_output, attention_weights = self.cross_attention(
            x, encoder_output, encoder_output, src_mask
        )
        x = self.norm2(x + self.dropout(cross_attn_output))
        
        # å‰é¦ˆç½‘ç»œ | Feed-forward network
        ff_output = self.feed_forward(x)
        x = self.norm3(x + self.dropout(ff_output))
        
        return x, attention_weights
```

### 3. å®Œæ•´çš„Transformerç¿»è¯‘æ¨¡å‹ | Complete Transformer Translation Model

```python
class TransformerTranslator(nn.Module):
    """
    åŸºäºTransformerçš„å®Œæ•´ç¿»è¯‘æ¨¡å‹
    Complete Transformer-based Translation Model
    """
    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, 
                 num_heads=8, num_encoder_layers=6, num_decoder_layers=6, 
                 d_ff=2048, max_seq_length=5000, dropout=0.1):
        super(TransformerTranslator, self).__init__()
        
        self.d_model = d_model
        
        # è¯åµŒå…¥å’Œä½ç½®ç¼–ç  | Word embedding and positional encoding
        self.src_embedding = nn.Embedding(src_vocab_size, d_model)
        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)
        self.positional_encoding = PositionalEncoding(d_model, dropout, max_seq_length)
        
        # ç¼–ç å™¨å’Œè§£ç å™¨å±‚ | Encoder and decoder layers
        self.encoder_layers = nn.ModuleList([
            TransformerEncoder(d_model, num_heads, d_ff, dropout)
            for _ in range(num_encoder_layers)
        ])
        
        self.decoder_layers = nn.ModuleList([
            TransformerDecoder(d_model, num_heads, d_ff, dropout)
            for _ in range(num_decoder_layers)
        ])
        
        # è¾“å‡ºæŠ•å½±å±‚ | Output projection layer
        self.output_projection = nn.Linear(d_model, tgt_vocab_size)
        
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, src, tgt, src_mask=None, tgt_mask=None):
        """
        å‰å‘ä¼ æ’­
        Forward pass
        """
        # ç¼–ç å™¨ | Encoder
        src_embedded = self.src_embedding(src) * math.sqrt(self.d_model)
        src_embedded = self.positional_encoding(src_embedded)
        
        encoder_output = src_embedded
        for encoder_layer in self.encoder_layers:
            encoder_output = encoder_layer(encoder_output, src_mask)
        
        # è§£ç å™¨ | Decoder
        tgt_embedded = self.tgt_embedding(tgt) * math.sqrt(self.d_model)
        tgt_embedded = self.positional_encoding(tgt_embedded)
        
        decoder_output = tgt_embedded
        attention_weights = []
        
        for decoder_layer in self.decoder_layers:
            decoder_output, attn_weights = decoder_layer(
                decoder_output, encoder_output, src_mask, tgt_mask
            )
            attention_weights.append(attn_weights)
        
        # è¾“å‡ºæŠ•å½± | Output projection
        output = self.output_projection(decoder_output)
        
        return output, attention_weights


class PositionalEncoding(nn.Module):
    """
    ä½ç½®ç¼–ç 
    Positional Encoding
    """
    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super(PositionalEncoding, self).__init__()
        
        self.dropout = nn.Dropout(p=dropout)
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        x = x + self.pe[:x.size(0), :]
        return self.dropout(x)
```

## ğŸš€ é«˜çº§æŠ€æœ¯ï¼šé¢„è®­ç»ƒç¿»è¯‘æ¨¡å‹ | Advanced: Pre-trained Translation Models

### ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œç¿»è¯‘ | Using Pre-trained Models for Translation

```python
from transformers import MarianMTModel, MarianTokenizer

class PretrainedTranslator:
    """
    åŸºäºé¢„è®­ç»ƒæ¨¡å‹çš„ç¿»è¯‘å™¨
    Pre-trained Model-based Translator
    """
    def __init__(self, model_name='Helsinki-NLP/opus-mt-en-zh'):
        # åŠ è½½é¢„è®­ç»ƒçš„ç¿»è¯‘æ¨¡å‹
        # Load pre-trained translation model
        self.tokenizer = MarianTokenizer.from_pretrained(model_name)
        self.model = MarianMTModel.from_pretrained(model_name)
        
        print(f"å·²åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {model_name}")
    
    def translate(self, texts, max_length=512):
        """
        ç¿»è¯‘æ–‡æœ¬
        Translate texts
        """
        if isinstance(texts, str):
            texts = [texts]
        
        # ç¼–ç è¾“å…¥
        # Encode inputs
        inputs = self.tokenizer(texts, return_tensors="pt", padding=True, truncation=True, max_length=max_length)
        
        # ç”Ÿæˆç¿»è¯‘
        # Generate translations
        with torch.no_grad():
            outputs = self.model.generate(**inputs, max_length=max_length, num_beams=4, early_stopping=True)
        
        # è§£ç è¾“å‡º
        # Decode outputs
        translations = [self.tokenizer.decode(output, skip_special_tokens=True) for output in outputs]
        
        if len(translations) == 1:
            return translations[0]
        return translations
    
    def translate_with_scores(self, text, num_return_sequences=3):
        """
        ç¿»è¯‘å¹¶è¿”å›ç½®ä¿¡åº¦åˆ†æ•°
        Translate and return confidence scores
        """
        inputs = self.tokenizer(text, return_tensors="pt")
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_length=512,
                num_beams=5,
                num_return_sequences=num_return_sequences,
                return_dict_in_generate=True,
                output_scores=True
            )
        
        translations = []
        for i, sequence in enumerate(outputs.sequences):
            translation = self.tokenizer.decode(sequence, skip_special_tokens=True)
            score = outputs.sequences_scores[i].item()
            translations.append({
                'translation': translation,
                'score': score
            })
        
        return translations


def main_demo():
    """
    æ¼”ç¤ºä¸åŒç¿»è¯‘æ–¹æ³•
    Demonstrate different translation methods
    """
    print("=== æœºå™¨ç¿»è¯‘ç³»ç»Ÿæ¼”ç¤º | Machine Translation System Demo ===")
    
    # 1. é¢„è®­ç»ƒæ¨¡å‹ç¿»è¯‘
    print("\n1. é¢„è®­ç»ƒæ¨¡å‹ç¿»è¯‘ | Pre-trained Model Translation:")
    translator = PretrainedTranslator()
    
    test_sentences = [
        "Hello, how are you today?",
        "Machine learning is changing the world.",
        "The weather is beautiful today."
    ]
    
    for sentence in test_sentences:
        translation = translator.translate(sentence)
        print(f"EN: {sentence}")
        print(f"ZH: {translation}")
        print()
    
    # 2. å¸¦ç½®ä¿¡åº¦çš„ç¿»è¯‘
    print("\n2. å¸¦ç½®ä¿¡åº¦çš„ç¿»è¯‘ | Translation with Confidence:")
    text = "Artificial intelligence will transform our future."
    results = translator.translate_with_scores(text, num_return_sequences=3)
    
    print(f"åŸæ–‡: {text}")
    print("ç¿»è¯‘å€™é€‰:")
    for i, result in enumerate(results):
        print(f"{i+1}. {result['translation']} (åˆ†æ•°: {result['score']:.4f})")


if __name__ == "__main__":
    main_demo()
```

## ğŸ“Š ç¿»è¯‘è´¨é‡è¯„ä¼° | Translation Quality Evaluation

### è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡ | Automatic Evaluation Metrics

```python
def comprehensive_evaluation(references, hypotheses):
    """
    ç»¼åˆè¯„ä¼°ç¿»è¯‘è´¨é‡
    Comprehensive translation quality evaluation
    """
    from sacrebleu import corpus_bleu
    from rouge_score import rouge_scorer
    
    results = {}
    
    # BLEUåˆ†æ•° | BLEU score
    bleu = corpus_bleu(hypotheses, [references])
    results['BLEU'] = bleu.score
    
    # ROUGEåˆ†æ•° | ROUGE score
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    rouge_scores = []
    
    for ref, hyp in zip(references, hypotheses):
        scores = scorer.score(ref, hyp)
        rouge_scores.append(scores)
    
    # å¹³å‡ROUGEåˆ†æ•° | Average ROUGE scores
    results['ROUGE-1'] = sum(score['rouge1'].fmeasure for score in rouge_scores) / len(rouge_scores)
    results['ROUGE-2'] = sum(score['rouge2'].fmeasure for score in rouge_scores) / len(rouge_scores)
    results['ROUGE-L'] = sum(score['rougeL'].fmeasure for score in rouge_scores) / len(rouge_scores)
    
    return results
```

é€šè¿‡è¿™ä¸ªæ³¨æ„åŠ›æœºåˆ¶ç¿»è¯‘é¡¹ç›®ï¼Œä½ å°†æ·±å…¥ç†è§£ç°ä»£æœºå™¨ç¿»è¯‘çš„æ ¸å¿ƒæŠ€æœ¯ - æ³¨æ„åŠ›æœºåˆ¶å’ŒTransformeræ¶æ„ï¼ŒæŒæ¡æ„å»ºé«˜è´¨é‡ç¿»è¯‘ç³»ç»Ÿçš„å…³é”®æŠ€èƒ½ï¼

Through this attention mechanism translation project, you will deeply understand the core technologies of modern machine translation - attention mechanism and Transformer architecture, and master the key skills for building high-quality translation systems! 