# ä¸­æ–‡NERç³»ç»Ÿæµ‹è¯•é¢˜
# Chinese NER System Quiz

---

## ğŸ“ ç†è®ºçŸ¥è¯†æµ‹è¯• | Theoretical Knowledge Test

### 1. åŸºç¡€æ¦‚å¿µé¢˜ | Basic Concept Questions

**Question 1:** 
åœ¨ä¸­æ–‡NERä»»åŠ¡ä¸­ï¼Œä¸ºä»€ä¹ˆé€šå¸¸é‡‡ç”¨å­—ç¬¦çº§åˆ«è€Œä¸æ˜¯è¯çº§åˆ«çš„tokenizationï¼Ÿè¯·åˆ—ä¸¾è‡³å°‘3ä¸ªåŸå› ã€‚

In Chinese NER tasks, why is character-level tokenization usually used instead of word-level tokenization? Please list at least 3 reasons.

**Answer:**
1. **é¿å…åˆ†è¯é”™è¯¯ä¼ æ’­** | **Avoid word segmentation error propagation**: ä¸­æ–‡åˆ†è¯æœ¬èº«å°±æœ‰é”™è¯¯ï¼Œè¿™äº›é”™è¯¯ä¼šç›´æ¥å½±å“NERçš„å‡†ç¡®æ€§
2. **å¤„ç†æœªç™»å½•è¯** | **Handle out-of-vocabulary words**: å­—ç¬¦çº§åˆ«å¯ä»¥æ›´å¥½åœ°å¤„ç†è®­ç»ƒæ—¶æœªè§è¿‡çš„æ–°è¯
3. **å®ä½“è¾¹ç•Œæ›´ç²¾ç¡®** | **More precise entity boundaries**: å­—ç¬¦çº§åˆ«å¯ä»¥æ›´å‡†ç¡®åœ°å®šä½å®ä½“çš„èµ·å§‹å’Œç»“æŸä½ç½®
4. **ç»Ÿä¸€å¤„ç†æ–¹å¼** | **Unified processing approach**: ä¸éœ€è¦é’ˆå¯¹ä¸åŒé¢†åŸŸé‡æ–°è®­ç»ƒåˆ†è¯å™¨

---

**Question 2:**
è¯·è§£é‡ŠBIOæ ‡æ³¨ä½“ç³»ï¼Œå¹¶ç»™å‡ºæ ‡æ³¨ç¤ºä¾‹ï¼š"åŒ—äº¬å¤§å­¦çš„å¼ æ•™æˆ"

Please explain the BIO tagging system and provide a tagging example for: "åŒ—äº¬å¤§å­¦çš„å¼ æ•™æˆ"

**Answer:**
**BIOæ ‡æ³¨ä½“ç³»è¯´æ˜ | BIO Tagging System Explanation:**
- **B-TYPE**: å®ä½“çš„å¼€å§‹å­—ç¬¦ | Beginning of entity
- **I-TYPE**: å®ä½“çš„å†…éƒ¨å­—ç¬¦ | Inside of entity  
- **O**: ä¸å±äºä»»ä½•å®ä½“ | Outside any entity

**æ ‡æ³¨ç¤ºä¾‹ | Tagging Example:**
```
åŒ—  B-ORG
äº¬  I-ORG  
å¤§  I-ORG
å­¦  I-ORG
çš„  O
å¼   B-PER
æ•™  I-PER
æˆ  I-PER
```

---

### 2. æŠ€æœ¯åŸç†é¢˜ | Technical Principle Questions

**Question 3:**
ä¸ºä»€ä¹ˆåœ¨BERT+CRFæ¶æ„ä¸­éœ€è¦ä½¿ç”¨CRFå±‚ï¼Ÿç›´æ¥ä½¿ç”¨BERTçš„è¾“å‡ºåšåˆ†ç±»æœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿ

Why is a CRF layer needed in the BERT+CRF architecture? What problems exist with directly using BERT output for classification?

**Answer:**
**CRFå±‚çš„å¿…è¦æ€§ | Necessity of CRF Layer:**

1. **åºåˆ—ä¸€è‡´æ€§çº¦æŸ** | **Sequence consistency constraints**: 
   - é˜²æ­¢æ— æ•ˆçš„æ ‡ç­¾è½¬ç§»ï¼Œå¦‚"I-PER"åç›´æ¥è·Ÿ"B-ORG"
   - Prevent invalid label transitions like "I-PER" directly followed by "B-ORG"

2. **å…¨å±€ä¼˜åŒ–** | **Global optimization**:
   - BERTå•ç‹¬åˆ†ç±»æ˜¯å±€éƒ¨å†³ç­–ï¼ŒCRFè€ƒè™‘æ•´ä¸ªåºåˆ—çš„æœ€ä¼˜æ ‡æ³¨
   - BERT alone makes local decisions, CRF considers optimal labeling for entire sequence

3. **æ ‡æ³¨è§„åˆ™å¼ºåˆ¶** | **Tagging rule enforcement**:
   - ç¡®ä¿å®ä½“å¿…é¡»ä»¥Bå¼€å¤´ï¼Œä¸èƒ½æœ‰å­¤ç«‹çš„Iæ ‡ç­¾
   - Ensure entities must start with B, no isolated I labels

**ç›´æ¥ä½¿ç”¨BERTåˆ†ç±»çš„é—®é¢˜ | Problems with Direct BERT Classification:**
- å¯èƒ½äº§ç”Ÿä¸åˆæ³•çš„æ ‡ç­¾åºåˆ—
- May produce illegal label sequences
- æ— æ³•åˆ©ç”¨æ ‡ç­¾é—´çš„ä¾èµ–å…³ç³»
- Cannot utilize dependencies between labels

---

**Question 4:**
åœ¨å¤„ç†ç±»åˆ«ä¸å¹³è¡¡çš„NERæ•°æ®æ—¶ï¼Œæœ‰å“ªäº›ç­–ç•¥å¯ä»¥æ”¹å–„æ¨¡å‹æ€§èƒ½ï¼Ÿ

What strategies can improve model performance when dealing with class-imbalanced NER data?

**Answer:**
**ç±»åˆ«ä¸å¹³è¡¡å¤„ç†ç­–ç•¥ | Class Imbalance Handling Strategies:**

1. **æ•°æ®å±‚é¢ | Data Level:**
   - **è¿‡é‡‡æ ·** | **Oversampling**: å¢åŠ å°‘æ•°ç±»å®ä½“çš„æ ·æœ¬
   - **æ•°æ®å¢å¼º** | **Data Augmentation**: å®ä½“æ›¿æ¢ã€ä¸Šä¸‹æ–‡æ‰°åŠ¨
   - **åˆæˆæ•°æ®ç”Ÿæˆ** | **Synthetic Data Generation**: ä½¿ç”¨æ¨¡æ¿ç”Ÿæˆæ–°æ ·æœ¬

2. **æŸå¤±å‡½æ•°å±‚é¢ | Loss Function Level:**
   - **Focal Loss**: é™ä½æ˜“åˆ†ç±»æ ·æœ¬çš„æƒé‡
   - **ç±»åˆ«æƒé‡** | **Class Weights**: ç»™å°‘æ•°ç±»æ›´é«˜çš„æƒé‡
   - **æ ‡ç­¾å¹³æ»‘** | **Label Smoothing**: å‡å°‘è¿‡æ‹Ÿåˆ

3. **æ¨¡å‹å±‚é¢ | Model Level:**
   - **é›†æˆå­¦ä¹ ** | **Ensemble Learning**: å¤šæ¨¡å‹æŠ•ç¥¨
   - **ä¸»åŠ¨å­¦ä¹ ** | **Active Learning**: ä¸»åŠ¨é€‰æ‹©æœ‰ä»·å€¼çš„æ ·æœ¬æ ‡æ³¨

---

## ğŸ’» ç¼–ç¨‹å®è·µé¢˜ | Programming Practice Questions

### 3. ä»£ç å®ç°é¢˜ | Code Implementation Questions

**Question 5:**
å®ç°ä¸€ä¸ªå‡½æ•°ï¼Œå°†BIOæ ¼å¼çš„æ ‡æ³¨è½¬æ¢ä¸ºå®ä½“åˆ—è¡¨ã€‚

Implement a function to convert BIO format annotations to entity list.

```python
def bio_to_entities(chars, bio_tags):
    """
    å°†BIOæ ‡æ³¨è½¬æ¢ä¸ºå®ä½“åˆ—è¡¨
    Convert BIO annotations to entity list
    
    Args:
        chars: å­—ç¬¦åˆ—è¡¨ | Character list
        bio_tags: BIOæ ‡ç­¾åˆ—è¡¨ | BIO tag list
    
    Returns:
        entities: å®ä½“åˆ—è¡¨ | Entity list
        æ ¼å¼: [{'text': 'å®ä½“æ–‡æœ¬', 'start': å¼€å§‹ä½ç½®, 'end': ç»“æŸä½ç½®, 'type': 'å®ä½“ç±»å‹'}]
    """
    # è¯·å®ç°æ­¤å‡½æ•° | Please implement this function
    pass
```

**Answer:**
```python
def bio_to_entities(chars, bio_tags):
    """
    å°†BIOæ ‡æ³¨è½¬æ¢ä¸ºå®ä½“åˆ—è¡¨
    Convert BIO annotations to entity list
    """
    entities = []
    current_entity = None
    
    for i, (char, tag) in enumerate(zip(chars, bio_tags)):
        if tag.startswith('B-'):
            # å¦‚æœä¹‹å‰æœ‰å®ä½“ï¼Œå…ˆä¿å­˜
            # If there was a previous entity, save it first
            if current_entity:
                entities.append(current_entity)
            
            # å¼€å§‹æ–°å®ä½“
            # Start new entity
            entity_type = tag[2:]  # å»æ‰'B-'å‰ç¼€
            current_entity = {
                'text': char,
                'start': i,
                'end': i + 1,
                'type': entity_type
            }
        
        elif tag.startswith('I-') and current_entity:
            # ç»§ç»­å½“å‰å®ä½“
            # Continue current entity
            entity_type = tag[2:]
            if entity_type == current_entity['type']:
                current_entity['text'] += char
                current_entity['end'] = i + 1
            else:
                # ç±»å‹ä¸åŒ¹é…ï¼Œä¿å­˜å½“å‰å®ä½“å¹¶å¼€å§‹æ–°å®ä½“
                # Type mismatch, save current entity and start new one
                entities.append(current_entity)
                current_entity = {
                    'text': char,
                    'start': i,
                    'end': i + 1,
                    'type': entity_type
                }
        
        elif tag == 'O':
            # ç»“æŸå½“å‰å®ä½“
            # End current entity
            if current_entity:
                entities.append(current_entity)
                current_entity = None
    
    # å¤„ç†æœ€åä¸€ä¸ªå®ä½“
    # Handle last entity
    if current_entity:
        entities.append(current_entity)
    
    return entities

# æµ‹è¯•ç¤ºä¾‹ | Test Example
chars = ['åŒ—', 'äº¬', 'å¤§', 'å­¦', 'çš„', 'å¼ ', 'æ•™', 'æˆ']
tags = ['B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'B-PER', 'I-PER', 'I-PER']

result = bio_to_entities(chars, tags)
print(result)
# æœŸæœ›è¾“å‡º | Expected Output:
# [
#     {'text': 'åŒ—äº¬å¤§å­¦', 'start': 0, 'end': 4, 'type': 'ORG'},
#     {'text': 'å¼ æ•™æˆ', 'start': 5, 'end': 8, 'type': 'PER'}
# ]
```

---

**Question 6:**
å®ç°ä¸€ä¸ªæ•°æ®å¢å¼ºå‡½æ•°ï¼Œé€šè¿‡å®ä½“æ›¿æ¢æ¥æ‰©å……è®­ç»ƒæ•°æ®ã€‚

Implement a data augmentation function to expand training data through entity substitution.

```python
def entity_substitution_augment(text, entities, entity_dict, num_augments=3):
    """
    é€šè¿‡å®ä½“æ›¿æ¢è¿›è¡Œæ•°æ®å¢å¼º
    Data augmentation through entity substitution
    
    Args:
        text: åŸå§‹æ–‡æœ¬ | Original text
        entities: å®ä½“åˆ—è¡¨ | Entity list  
        entity_dict: å®ä½“æ›¿æ¢è¯å…¸ | Entity substitution dictionary
        num_augments: å¢å¼ºæ ·æœ¬æ•°é‡ | Number of augmented samples
        
    Returns:
        augmented_samples: å¢å¼ºåçš„æ ·æœ¬åˆ—è¡¨ | List of augmented samples
    """
    # è¯·å®ç°æ­¤å‡½æ•° | Please implement this function
    pass
```

**Answer:**
```python
import random

def entity_substitution_augment(text, entities, entity_dict, num_augments=3):
    """
    é€šè¿‡å®ä½“æ›¿æ¢è¿›è¡Œæ•°æ®å¢å¼º
    Data augmentation through entity substitution
    """
    augmented_samples = []
    
    for _ in range(num_augments):
        augmented_text = text
        augmented_entities = []
        
        # ä»åå¾€å‰æ›¿æ¢ï¼Œé¿å…ä½ç½®åç§»
        # Replace from back to front to avoid position offset
        for entity in sorted(entities, key=lambda x: x['start'], reverse=True):
            entity_type = entity['type']
            original_text = entity['text']
            
            # å¦‚æœè¯¥ç±»å‹æœ‰æ›¿æ¢è¯å…¸
            # If substitution dictionary exists for this type
            if entity_type in entity_dict and len(entity_dict[entity_type]) > 0:
                # éšæœºé€‰æ‹©æ›¿æ¢è¯
                # Randomly select replacement word
                replacement = random.choice(entity_dict[entity_type])
                
                # æ‰§è¡Œæ›¿æ¢
                # Perform replacement
                start, end = entity['start'], entity['end']
                augmented_text = augmented_text[:start] + replacement + augmented_text[end:]
                
                # æ›´æ–°å®ä½“ä¿¡æ¯
                # Update entity information
                new_entity = {
                    'text': replacement,
                    'start': start,
                    'end': start + len(replacement),
                    'type': entity_type
                }
                augmented_entities.insert(0, new_entity)  # æ’å…¥åˆ°å‰é¢ä¿æŒé¡ºåº
            else:
                # ä¿æŒåŸå®ä½“
                # Keep original entity
                augmented_entities.insert(0, entity)
        
        augmented_samples.append({
            'text': augmented_text,
            'entities': augmented_entities
        })
    
    return augmented_samples

# æµ‹è¯•ç¤ºä¾‹ | Test Example
text = "å¼ ä¸‰åœ¨åŒ—äº¬å¤§å­¦å·¥ä½œ"
entities = [
    {'text': 'å¼ ä¸‰', 'start': 0, 'end': 2, 'type': 'PER'},
    {'text': 'åŒ—äº¬å¤§å­¦', 'start': 3, 'end': 7, 'type': 'ORG'}
]

entity_dict = {
    'PER': ['æå››', 'ç‹äº”', 'èµµå…­'],
    'ORG': ['æ¸…åå¤§å­¦', 'å¤æ—¦å¤§å­¦', 'æµ™æ±Ÿå¤§å­¦']
}

augmented = entity_substitution_augment(text, entities, entity_dict, num_augments=2)
for i, sample in enumerate(augmented):
    print(f"å¢å¼ºæ ·æœ¬{i+1} | Augmented Sample {i+1}:")
    print(f"æ–‡æœ¬: {sample['text']}")
    print(f"å®ä½“: {sample['entities']}")
    print()
```

---

### 4. æ¡ˆä¾‹åˆ†æé¢˜ | Case Analysis Questions

**Question 7:**
åˆ†æä»¥ä¸‹ä¸­æ–‡NERçš„é”™è¯¯æ¡ˆä¾‹ï¼Œè¯´æ˜å¯èƒ½çš„åŸå› å’Œæ”¹è¿›æ–¹æ³•ï¼š

Analyze the following Chinese NER error cases, explain possible causes and improvement methods:

**æ¡ˆä¾‹1 | Case 1:**
- è¾“å…¥æ–‡æœ¬ | Input Text: "è‹¹æœå…¬å¸çš„æ–°iPhoneæ‰‹æœº"
- æ­£ç¡®æ ‡æ³¨ | Correct Annotation: è‹¹æœå…¬å¸(ORG), iPhone(PRODUCT)  
- æ¨¡å‹é¢„æµ‹ | Model Prediction: è‹¹æœ(O), å…¬å¸(O), iPhone(O)

**æ¡ˆä¾‹2 | Case 2:**
- è¾“å…¥æ–‡æœ¬ | Input Text: "ç‹å»ºå›½å®¶å¾ˆå¯Œæœ‰"
- æ­£ç¡®æ ‡æ³¨ | Correct Annotation: ç‹å»ºå›½(PER), å®¶(O)
- æ¨¡å‹é¢„æµ‹ | Model Prediction: ç‹å»ºå›½å®¶(LOC)

**Answer:**

**æ¡ˆä¾‹1åˆ†æ | Case 1 Analysis:**

**å¯èƒ½åŸå›  | Possible Causes:**
1. **è®­ç»ƒæ•°æ®ä¸è¶³** | **Insufficient Training Data**: ç¼ºå°‘ç›¸å…³çš„å…¬å¸åå’Œäº§å“åæ ·æœ¬
2. **å®ä½“è¾¹ç•Œè¯†åˆ«å›°éš¾** | **Entity Boundary Recognition Difficulty**: "è‹¹æœå…¬å¸"ä½œä¸ºä¸€ä¸ªæ•´ä½“å®ä½“çš„è®­ç»ƒä¸å¤Ÿ
3. **å¤šä¹‰æ€§é—®é¢˜** | **Polysemy Issue**: "è‹¹æœ"æ—¢å¯ä»¥æ˜¯æ°´æœä¹Ÿå¯ä»¥æ˜¯å…¬å¸å

**æ”¹è¿›æ–¹æ³• | Improvement Methods:**
1. **æ•°æ®å¢å¼º** | **Data Augmentation**: å¢åŠ æ›´å¤šå…¬å¸åå’Œäº§å“åçš„è®­ç»ƒæ ·æœ¬
2. **é¢„è®­ç»ƒæ¨¡å‹ä¼˜åŒ–** | **Pretrained Model Optimization**: ä½¿ç”¨åœ¨å•†ä¸šæ–‡æœ¬ä¸Šé¢„è®­ç»ƒçš„æ¨¡å‹
3. **ä¸Šä¸‹æ–‡ç‰¹å¾å¢å¼º** | **Context Feature Enhancement**: åˆ©ç”¨"å…¬å¸"ç­‰å…³é”®è¯ä½œä¸ºå¼ºç‰¹å¾

**æ¡ˆä¾‹2åˆ†æ | Case 2 Analysis:**

**å¯èƒ½åŸå›  | Possible Causes:**
1. **åˆ†è¯æ­§ä¹‰** | **Segmentation Ambiguity**: "ç‹å»ºå›½å®¶"å¯èƒ½è¢«ç†è§£ä¸ºåœ°å
2. **è¯­ä¹‰ç†è§£ä¸è¶³** | **Insufficient Semantic Understanding**: æ¨¡å‹æ²¡æœ‰ç†è§£"å®¶"åœ¨æ­¤å¤„çš„å«ä¹‰
3. **è®­ç»ƒæ•°æ®åå·®** | **Training Data Bias**: è®­ç»ƒæ•°æ®ä¸­"XXå›½å®¶"å½¢å¼çš„åœ°åè¾ƒå¤š

**æ”¹è¿›æ–¹æ³• | Improvement Methods:**
1. **è¯­æ³•ç‰¹å¾èå…¥** | **Grammar Feature Integration**: è€ƒè™‘è¯æ€§å’Œè¯­æ³•ç»“æ„
2. **ä¸Šä¸‹æ–‡çª—å£æ‰©å¤§** | **Context Window Expansion**: å¢å¤§æ¨¡å‹çœ‹åˆ°çš„ä¸Šä¸‹æ–‡èŒƒå›´
3. **å¤šä»»åŠ¡å­¦ä¹ ** | **Multi-task Learning**: ç»“åˆåˆ†è¯å’ŒNERè”åˆè®­ç»ƒ

---

**Question 8:**
è®¾è®¡ä¸€ä¸ªè¯„ä¼°ä¸­æ–‡NERç³»ç»Ÿçš„å®Œæ•´æ–¹æ¡ˆï¼ŒåŒ…æ‹¬æ•°æ®é›†é€‰æ‹©ã€è¯„ä¼°æŒ‡æ ‡å’Œæµ‹è¯•åœºæ™¯ã€‚

Design a complete evaluation scheme for Chinese NER systems, including dataset selection, evaluation metrics, and test scenarios.

**Answer:**

**å®Œæ•´è¯„ä¼°æ–¹æ¡ˆ | Complete Evaluation Scheme:**

### 1. æ•°æ®é›†é€‰æ‹© | Dataset Selection

**æ ‡å‡†æ•°æ®é›† | Standard Datasets:**
- **MSRA NER**: å¾®è½¯äºšæ´²ç ”ç©¶é™¢ä¸­æ–‡NERæ•°æ®é›†
- **People's Daily**: äººæ°‘æ—¥æŠ¥æ ‡æ³¨æ•°æ®é›†  
- **Weibo NER**: ç¤¾äº¤åª’ä½“æ–‡æœ¬æ•°æ®é›†
- **CLUENER**: ä¸­æ–‡ç»†ç²’åº¦NERæ•°æ®é›†

**é¢†åŸŸç‰¹å®šæ•°æ®é›† | Domain-specific Datasets:**
- åŒ»ç–—é¢†åŸŸï¼šç—…å†æ–‡æœ¬ã€åŒ»å­¦è®ºæ–‡
- Medical Domain: Medical records, medical papers
- é‡‘èé¢†åŸŸï¼šè´¢ç»æ–°é—»ã€å…¬å¸å…¬å‘Š
- Financial Domain: Financial news, company announcements
- æ³•å¾‹é¢†åŸŸï¼šæ³•å¾‹æ–‡ä¹¦ã€åˆåŒæ–‡æœ¬
- Legal Domain: Legal documents, contract texts

### 2. è¯„ä¼°æŒ‡æ ‡ | Evaluation Metrics

**æ ¸å¿ƒæŒ‡æ ‡ | Core Metrics:**
```python
# å®ä½“çº§åˆ«ç²¾ç¡®åŒ¹é…
# Entity-level exact match
def entity_level_metrics(true_entities, pred_entities):
    """
    è®¡ç®—å®ä½“çº§åˆ«çš„P/R/F1
    Calculate entity-level P/R/F1
    """
    true_set = set(true_entities)  # (start, end, type)
    pred_set = set(pred_entities)
    
    precision = len(true_set & pred_set) / len(pred_set) if pred_set else 0
    recall = len(true_set & pred_set) / len(true_set) if true_set else 0
    f1 = 2 * precision * recall / (precision + recall) if precision + recall else 0
    
    return precision, recall, f1

# æ¾å¼›åŒ¹é…è¯„ä¼°
# Relaxed matching evaluation  
def relaxed_matching_metrics(true_entities, pred_entities, overlap_threshold=0.5):
    """
    å…è®¸éƒ¨åˆ†é‡å çš„æ¾å¼›åŒ¹é…
    Relaxed matching allowing partial overlap
    """
    matches = 0
    for true_entity in true_entities:
        for pred_entity in pred_entities:
            if entities_overlap(true_entity, pred_entity, overlap_threshold):
                matches += 1
                break
    
    precision = matches / len(pred_entities) if pred_entities else 0
    recall = matches / len(true_entities) if true_entities else 0
    f1 = 2 * precision * recall / (precision + recall) if precision + recall else 0
    
    return precision, recall, f1
```

**ç»†åˆ†æŒ‡æ ‡ | Detailed Metrics:**
- **æŒ‰å®ä½“ç±»å‹è¯„ä¼°** | **Evaluation by Entity Type**: åˆ†åˆ«è®¡ç®—PERã€LOCã€ORGç­‰çš„æ€§èƒ½
- **æŒ‰å®ä½“é•¿åº¦è¯„ä¼°** | **Evaluation by Entity Length**: çŸ­å®ä½“vsé•¿å®ä½“çš„è¯†åˆ«æ•ˆæœ
- **æŒ‰é¢‘ç‡è¯„ä¼°** | **Evaluation by Frequency**: é«˜é¢‘å®ä½“vsä½é¢‘å®ä½“çš„æ€§èƒ½

### 3. æµ‹è¯•åœºæ™¯ | Test Scenarios

**åŸºç¡€åŠŸèƒ½æµ‹è¯• | Basic Functionality Tests:**
```python
def basic_functionality_tests():
    """
    åŸºç¡€åŠŸèƒ½æµ‹è¯•ç”¨ä¾‹
    Basic functionality test cases
    """
    test_cases = [
        # æ ‡å‡†å®ä½“è¯†åˆ«
        # Standard entity recognition
        {
            'text': 'ä¹ è¿‘å¹³ä¸»å¸­åœ¨åŒ—äº¬ä¼šè§äº†ç¾å›½æ€»ç»Ÿæ‹œç™»',
            'expected': [('ä¹ è¿‘å¹³', 'PER'), ('åŒ—äº¬', 'LOC'), ('ç¾å›½', 'LOC'), ('æ‹œç™»', 'PER')]
        },
        
        # åµŒå¥—å®ä½“
        # Nested entities
        {
            'text': 'åŒ—äº¬å¤§å­¦è®¡ç®—æœºå­¦é™¢',
            'expected': [('åŒ—äº¬å¤§å­¦', 'ORG'), ('è®¡ç®—æœºå­¦é™¢', 'ORG')]
        },
        
        # è¾¹ç•Œæƒ…å†µ
        # Boundary cases
        {
            'text': 'ç‹å»ºå›½å®¶å¾ˆå¯Œæœ‰',
            'expected': [('ç‹å»ºå›½', 'PER')]
        }
    ]
    
    return test_cases
```

**é²æ£’æ€§æµ‹è¯• | Robustness Tests:**
```python
def robustness_tests():
    """
    é²æ£’æ€§æµ‹è¯•ç”¨ä¾‹
    Robustness test cases
    """
    return [
        # å™ªå£°æ–‡æœ¬
        # Noisy text
        'é©¬@äº‘#åˆ›#ç«‹$äº†%é˜¿#é‡Œ&å·´*å·´',
        
        # éæ ‡å‡†æ ¼å¼  
        # Non-standard format
        'JACK MAåˆ›ç«‹äº†alibabaå…¬å¸',
        
        # é•¿æ–‡æœ¬
        # Long text
        'åœ¨è¿™ä¸ªå……æ»¡æŒ‘æˆ˜çš„æ—¶ä»£ï¼Œä¼ä¸šå®¶é©¬äº‘å…ˆç”Ÿå‡­å€Ÿå…¶å“è¶Šçš„å•†ä¸šçœ¼å…‰...' * 100
    ]
```

**æ€§èƒ½å‹åŠ›æµ‹è¯• | Performance Stress Tests:**
```python
def performance_tests():
    """
    æ€§èƒ½å‹åŠ›æµ‹è¯•
    Performance stress tests
    """
    return {
        'batch_processing': 'æµ‹è¯•æ‰¹é‡å¤„ç†èƒ½åŠ›',
        'memory_usage': 'ç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ', 
        'inference_speed': 'æ¨ç†é€Ÿåº¦åŸºå‡†æµ‹è¯•',
        'concurrent_requests': 'å¹¶å‘è¯·æ±‚å¤„ç†èƒ½åŠ›'
    }
```

**é¢†åŸŸé€‚åº”æ€§æµ‹è¯• | Domain Adaptation Tests:**
- **è·¨é¢†åŸŸæ³›åŒ–** | **Cross-domain Generalization**: åœ¨é‡‘èæ•°æ®ä¸Šè®­ç»ƒï¼Œåœ¨åŒ»ç–—æ•°æ®ä¸Šæµ‹è¯•
- **æ—¶é—´ç¨³å®šæ€§** | **Temporal Stability**: åœ¨ä¸åŒæ—¶é—´æ®µçš„æ•°æ®ä¸Šæµ‹è¯•
- **æ–‡ä½“é€‚åº”æ€§** | **Genre Adaptability**: æ–°é—»ã€å°è¯´ã€ç¤¾äº¤åª’ä½“ç­‰ä¸åŒæ–‡ä½“

è¿™ä¸ªå®Œæ•´çš„æµ‹è¯•é¢˜é›†æ¶µç›–äº†ä¸­æ–‡NERç³»ç»Ÿçš„ç†è®ºåŸºç¡€ã€æŠ€æœ¯å®ç°å’Œå®é™…åº”ç”¨ï¼Œå¸®åŠ©å­¦ä¹ è€…å…¨é¢æŒæ¡ç›¸å…³çŸ¥è¯†å’ŒæŠ€èƒ½ï¼

This complete quiz set covers the theoretical foundations, technical implementation, and practical applications of Chinese NER systems, helping learners comprehensively master relevant knowledge and skills! 