# Handwritten Digit Recognition Project - Usage Guide

æ‰‹å†™æ•°å­—è¯†åˆ«é¡¹ç›® - ä½¿ç”¨æŒ‡å—

## Quick Start

å¿«é€Ÿå¼€å§‹

### 1. Install Dependencies

å®‰è£…ä¾èµ–

```bash
# Navigate to project directory
# å¯¼èˆªåˆ°é¡¹ç›®ç›®å½•
cd deepStudyByCursor/02_MLP_Backpropagation/æ‰‹å†™æ•°å­—è¯†åˆ«é¡¹ç›®

# Install required packages
# å®‰è£…æ‰€éœ€åŒ…
pip install -r requirements.txt
```

### 2. Download and Prepare Data

ä¸‹è½½å¹¶å‡†å¤‡æ•°æ®

```bash
# Navigate to src directory
# å¯¼èˆªåˆ°srcç›®å½•
cd src

# Download MNIST data and create preprocessed files
# ä¸‹è½½MNISTæ•°æ®å¹¶åˆ›å»ºé¢„å¤„ç†æ–‡ä»¶
python data_loader.py
```

### 3. Train the Model

è®­ç»ƒæ¨¡å‹

```bash
# Train with default settings
# ä½¿ç”¨é»˜è®¤è®¾ç½®è®­ç»ƒ
python train.py

# Train with custom parameters
# ä½¿ç”¨è‡ªå®šä¹‰å‚æ•°è®­ç»ƒ
python train.py --epochs 100 --batch_size 32 --learning_rate 0.005 --hidden_sizes 256 128
```

### 4. Check Results

æŸ¥çœ‹ç»“æœ

è®­ç»ƒå®Œæˆåï¼Œæ£€æŸ¥ä»¥ä¸‹ç›®å½•çš„ç»“æœï¼š

- `results/models/` - Saved model files (ä¿å­˜çš„æ¨¡å‹æ–‡ä»¶)
- `results/plots/` - Training plots and visualizations (è®­ç»ƒå›¾è¡¨å’Œå¯è§†åŒ–)
- `results/logs/` - Training logs and reports (è®­ç»ƒæ—¥å¿—å’ŒæŠ¥å‘Š)

## Detailed Usage

è¯¦ç»†ä½¿ç”¨è¯´æ˜

### Training Parameters

è®­ç»ƒå‚æ•°

You can customize the training process with these parameters:

æ‚¨å¯ä»¥ä½¿ç”¨è¿™äº›å‚æ•°è‡ªå®šä¹‰è®­ç»ƒè¿‡ç¨‹ï¼š

| Parameter | Default | Description |
|-----------|---------|-------------|
| `--epochs` | 50 | Number of training epochs (è®­ç»ƒè½®æ•°) |
| `--batch_size` | 64 | Batch size for training (è®­ç»ƒæ‰¹å¤§å°) |
| `--learning_rate` | 0.01 | Learning rate (å­¦ä¹ ç‡) |
| `--hidden_sizes` | [128, 64] | Hidden layer sizes (éšè—å±‚å¤§å°) |
| `--validation_split` | 0.1 | Validation split ratio (éªŒè¯é›†åˆ†å‰²æ¯”ä¾‹) |

### Examples

ç¤ºä¾‹

```bash
# Example 1: Quick training with small model
# ç¤ºä¾‹1ï¼šä½¿ç”¨å°æ¨¡å‹å¿«é€Ÿè®­ç»ƒ
python train.py --epochs 20 --hidden_sizes 64 32

# Example 2: Deep model with more epochs
# ç¤ºä¾‹2ï¼šä½¿ç”¨æ›´å¤šè½®æ¬¡çš„æ·±åº¦æ¨¡å‹
python train.py --epochs 100 --hidden_sizes 256 128 64 --learning_rate 0.005

# Example 3: Large batch training
# ç¤ºä¾‹3ï¼šå¤§æ‰¹é‡è®­ç»ƒ
python train.py --batch_size 128 --epochs 50
```

## Understanding the Results

ç†è§£ç»“æœ

### Training Plots

è®­ç»ƒå›¾è¡¨

After training, you'll find these visualizations in `results/plots/`:

è®­ç»ƒåï¼Œæ‚¨å°†åœ¨ `results/plots/` ä¸­æ‰¾åˆ°è¿™äº›å¯è§†åŒ–ï¼š

1. **training_history.png** - Loss and accuracy curves (æŸå¤±å’Œå‡†ç¡®ç‡æ›²çº¿)
2. **predictions_visualization.png** - Sample predictions (æ ·æœ¬é¢„æµ‹)
3. **confusion_matrix.png** - Confusion matrix (æ··æ·†çŸ©é˜µ)
4. **sample_images.png** - Sample MNIST images (MNISTæ ·æœ¬å›¾ç‰‡)

### Performance Metrics

æ€§èƒ½æŒ‡æ ‡

Check the training report in `results/logs/training_report.txt` for:

åœ¨ `results/logs/training_report.txt` ä¸­æŸ¥çœ‹è®­ç»ƒæŠ¥å‘Šï¼š

- **Training Accuracy** (è®­ç»ƒå‡†ç¡®ç‡)
- **Validation Accuracy** (éªŒè¯å‡†ç¡®ç‡)
- **Test Accuracy** (æµ‹è¯•å‡†ç¡®ç‡)
- **Per-class accuracy** (æ¯ç±»å‡†ç¡®ç‡)
- **Training time** (è®­ç»ƒæ—¶é—´)

### Expected Performance

é¢„æœŸæ€§èƒ½

With default settings, you should expect:

ä½¿ç”¨é»˜è®¤è®¾ç½®ï¼Œæ‚¨åº”è¯¥æœŸæœ›ï¼š

- **Training Accuracy**: ~98-99% (è®­ç»ƒå‡†ç¡®ç‡ï¼šçº¦98-99%)
- **Validation Accuracy**: ~96-97% (éªŒè¯å‡†ç¡®ç‡ï¼šçº¦96-97%)
- **Test Accuracy**: ~95-96% (æµ‹è¯•å‡†ç¡®ç‡ï¼šçº¦95-96%)
- **Training Time**: 2-5 minutes on CPU (è®­ç»ƒæ—¶é—´ï¼šCPUä¸Š2-5åˆ†é’Ÿ)

## Project Structure

é¡¹ç›®ç»“æ„

```
æ‰‹å†™æ•°å­—è¯†åˆ«é¡¹ç›®/
â”œâ”€â”€ README.md                    # Project overview (é¡¹ç›®æ¦‚è¿°)
â”œâ”€â”€ ä½¿ç”¨è¯´æ˜.md                   # This usage guide (æœ¬ä½¿ç”¨æŒ‡å—)
â”œâ”€â”€ requirements.txt             # Dependencies (ä¾èµ–é¡¹)
â”œâ”€â”€ src/                         # Source code (æºä»£ç )
â”‚   â”œâ”€â”€ data_loader.py          # Data loading (æ•°æ®åŠ è½½)
â”‚   â”œâ”€â”€ mlp_scratch.py          # MLP implementation (MLPå®ç°)
â”‚   â”œâ”€â”€ train.py                # Training script (è®­ç»ƒè„šæœ¬)
â”‚   â””â”€â”€ utils.py                # Utility functions (å·¥å…·å‡½æ•°)
â”œâ”€â”€ data/                        # Data directory (æ•°æ®ç›®å½•)
â”‚   â”œâ”€â”€ raw/                     # Raw MNIST files (åŸå§‹MNISTæ–‡ä»¶)
â”‚   â””â”€â”€ processed/               # Processed data (å¤„ç†åæ•°æ®)
â”œâ”€â”€ results/                     # Results (ç»“æœ)
â”‚   â”œâ”€â”€ models/                  # Saved models (ä¿å­˜çš„æ¨¡å‹)
â”‚   â”œâ”€â”€ plots/                   # Visualizations (å¯è§†åŒ–)
â”‚   â””â”€â”€ logs/                    # Training logs (è®­ç»ƒæ—¥å¿—)
â””â”€â”€ notebooks/                   # Jupyter notebooks (Jupyterç¬”è®°æœ¬)
    â””â”€â”€ 01_data_exploration.ipynb
```

## Mathematical Foundation

æ•°å­¦åŸºç¡€

This project implements the complete mathematical foundation of MLPs:

æœ¬é¡¹ç›®å®ç°äº†MLPçš„å®Œæ•´æ•°å­¦åŸºç¡€ï¼š

### Forward Propagation

å‰å‘ä¼ æ’­

```
z^(l) = W^(l) Â· a^(l-1) + b^(l)
a^(l) = f(z^(l))
```

Where:
- `z^(l)` = Pre-activation at layer l (ç¬¬lå±‚çš„é¢„æ¿€æ´»)
- `W^(l)` = Weight matrix at layer l (ç¬¬lå±‚çš„æƒé‡çŸ©é˜µ)
- `a^(l)` = Activation at layer l (ç¬¬lå±‚çš„æ¿€æ´»å€¼)
- `f()` = Activation function (æ¿€æ´»å‡½æ•°)

### Backpropagation

åå‘ä¼ æ’­

```
Î´^(L) = âˆ‡_a C âŠ™ f'(z^(L))  (Output layer)
Î´^(l) = ((W^(l+1))^T Î´^(l+1)) âŠ™ f'(z^(l))  (Hidden layers)
```

### Weight Updates

æƒé‡æ›´æ–°

```
W^(l) := W^(l) - Î· Â· âˆ‚C/âˆ‚W^(l)
b^(l) := b^(l) - Î· Â· âˆ‚C/âˆ‚b^(l)
```

Where `Î·` is the learning rate (å­¦ä¹ ç‡).

## Troubleshooting

æ•…éšœæ’é™¤

### Common Issues

å¸¸è§é—®é¢˜

1. **Memory Error (å†…å­˜é”™è¯¯)**
   - Reduce batch size: `--batch_size 32`
   - Use smaller model: `--hidden_sizes 64 32`

2. **Slow Training (è®­ç»ƒç¼“æ…¢)**
   - Increase batch size: `--batch_size 128`
   - Reduce epochs: `--epochs 30`

3. **Poor Accuracy (å‡†ç¡®ç‡ä½)**
   - Increase learning rate: `--learning_rate 0.02`
   - Add more hidden units: `--hidden_sizes 256 128 64`
   - Train longer: `--epochs 100`

4. **Data Not Found (æ•°æ®æœªæ‰¾åˆ°)**
   ```bash
   cd src
   python data_loader.py
   ```

### Getting Help

è·å–å¸®åŠ©

If you encounter issues:

å¦‚æœé‡åˆ°é—®é¢˜ï¼š

1. Check the training report in `results/logs/` (æ£€æŸ¥results/logs/ä¸­çš„è®­ç»ƒæŠ¥å‘Š)
2. Review the mathematical foundations in `../å¤šå±‚æ„ŸçŸ¥æœºä¸åå‘ä¼ æ’­.md`
3. Study the notation guide in `../01_Perceptron/æ•°å­¦ç¬¦å·è¯¦è§£ä¸è¯»éŸ³.md`

## Next Steps

ä¸‹ä¸€æ­¥

After successfully training your model:

æˆåŠŸè®­ç»ƒæ¨¡å‹åï¼š

1. **Experiment with architectures** (å®éªŒä¸åŒæ¶æ„)
   - Try different layer sizes (å°è¯•ä¸åŒå±‚å¤§å°)
   - Add more layers (æ·»åŠ æ›´å¤šå±‚)
   - Use different activation functions (ä½¿ç”¨ä¸åŒæ¿€æ´»å‡½æ•°)

2. **Improve performance** (æå‡æ€§èƒ½)
   - Implement learning rate scheduling (å®ç°å­¦ä¹ ç‡è°ƒåº¦)
   - Add regularization techniques (æ·»åŠ æ­£åˆ™åŒ–æŠ€æœ¯)
   - Try advanced optimizers (å°è¯•é«˜çº§ä¼˜åŒ–å™¨)

3. **Compare implementations** (æ¯”è¾ƒå®ç°)
   - Implement PyTorch version (å®ç°PyTorchç‰ˆæœ¬)
   - Compare performance and speed (æ¯”è¾ƒæ€§èƒ½å’Œé€Ÿåº¦)
   - Analyze differences (åˆ†æå·®å¼‚)

---

**Happy Learning! å¿«ä¹å­¦ä¹ ï¼** ğŸš€

For more advanced topics, continue to the next sections in the deep learning curriculum.

è¦äº†è§£æ›´é«˜çº§çš„ä¸»é¢˜ï¼Œè¯·ç»§ç»­æ·±åº¦å­¦ä¹ è¯¾ç¨‹çš„ä¸‹ä¸€èŠ‚ã€‚ 