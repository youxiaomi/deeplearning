# 第六章：自编码器与生成对抗网络 - 测试题

## 选择题

### 1. 自编码器的主要目标是什么？
A. 分类数据  
B. 学习数据的低维表示  
C. 增加数据维度  
D. 预测未来值  

### 2. 在GAN中，生成器和判别器的关系是什么？
A. 合作关系  
B. 对抗关系  
C. 独立关系  
D. 层次关系  

### 3. 自编码器的瓶颈层(Bottleneck Layer)的作用是什么？
A. 增加网络深度  
B. 提供非线性变换  
C. 强制学习压缩表示  
D. 防止过拟合  

### 4. GAN训练中常见的问题是什么？
A. 训练速度太快  
B. 模式崩塌(Mode Collapse)  
C. 参数太少  
D. 内存不足  

### 5. 变分自编码器(VAE)相比传统自编码器的优势是什么？
A. 训练更快  
B. 能够生成新样本  
C. 参数更少  
D. 结构更简单  

## 填空题

### 1. 自编码器由两部分组成：______和______，分别负责压缩和重构数据。

### 2. GAN的训练过程是一个______博弈过程，最终达到______均衡。

### 3. 在自编码器中，重构损失通常使用______损失函数来衡量原始输入和重构输出的差异。

### 4. GAN的生成器试图______判别器，而判别器试图______真实数据和生成数据。

### 5. 去噪自编码器在输入中添加______，强制模型学习更鲁棒的特征表示。

## 简答题

### 1. 解释自编码器的工作原理，并说明编码器和解码器的作用。

### 2. 描述GAN的训练过程，包括生成器和判别器的损失函数。

### 3. 什么是模式崩塌(Mode Collapse)？如何缓解这个问题？

### 4. 比较传统自编码器、去噪自编码器和变分自编码器的区别。

## 编程题

### 1. 实现一个简单的自编码器

### 2. 实现一个基础的GAN

### 3. 实现去噪自编码器

### 4. 使用自编码器进行异常检测

---

## 答案解析

### 选择题答案
1. **B** - 学习数据的低维表示
2. **B** - 对抗关系
3. **C** - 强制学习压缩表示
4. **B** - 模式崩塌(Mode Collapse)
5. **B** - 能够生成新样本

### 填空题答案
1. **编码器**，**解码器**
2. **零和**，**纳什**
3. **均方误差(MSE)**
4. **欺骗**，**区分**
5. **噪声**

### 简答题答案要点

#### 1. 自编码器工作原理：

**编码器 (Encoder):**
- 将高维输入数据压缩到低维潜在空间
- 学习数据的重要特征和模式
- 公式：z = f(x)，其中z是潜在表示

**解码器 (Decoder):**
- 将潜在表示重构回原始数据空间
- 尝试恢复原始数据的细节
- 公式：x' = g(z)，其中x'是重构输出

**训练目标：**
- 最小化重构误差：L = ||x - x'||²
- 学习有意义的数据表示

#### 2. GAN训练过程：

**生成器损失：**
- 目标：欺骗判别器
- 损失函数：L_G = -log(D(G(z)))

**判别器损失：**
- 目标：区分真实和生成数据
- 损失函数：L_D = -log(D(x)) - log(1-D(G(z)))

**训练策略：**
- 交替训练生成器和判别器
- 平衡两者的训练进度
- 监控训练稳定性

#### 3. 模式崩塌问题：

**定义：**
- 生成器只生成有限几种模式的样本
- 忽略了数据分布的多样性

**缓解方法：**
- 使用不同的损失函数（如Wasserstein损失）
- 添加正则化项
- 使用更稳定的训练技巧
- 采用渐进式训练

#### 4. 自编码器类型比较：

**传统自编码器：**
- 目标：数据重构
- 应用：降维、特征学习

**去噪自编码器：**
- 输入：加噪声的数据
- 目标：重构原始干净数据
- 优势：学习更鲁棒的特征

**变分自编码器(VAE)：**
- 潜在空间：概率分布
- 能力：生成新样本
- 优势：理论基础更强，训练更稳定 