# Linear Models and Perceptron Fundamentals

线性模型与感知机基础

## 1. Linear Neural Networks for Regression

线性神经网络用于回归

### 1.1 What is a Regression Problem?

什么是回归问题？

Imagine you are predicting a continuous numerical value. For example, predicting a house's price based on its area, location, and number of rooms; or predicting a person's weight based on their height. These types of problems, where the goal is to predict a specific numerical value rather than a category, are called **regression problems**.

想象一下，你正在预测一个连续的数值。比如，根据房屋的面积、地段和房间数量来预测它的价格；或者根据一个人的身高来预测他的体重。这类问题，目标是预测一个具体的数值，而不是一个类别，我们就称之为**回归问题**。

### 1.2 The Simplest "Model": Linear Regression

最简单的"模型"：线性回归

In regression problems, the simplest and most intuitive model is **linear regression**. It assumes a linear relationship between input features and output. You can think of it as drawing a line that best represents the trend among data points (or a plane in higher dimensions).

在回归问题中，最简单、最直观的模型就是**线性回归**。它假设输入特征和输出之间存在一种线性的关系。你可以把它想象成在数据点之间画一条最能代表它们趋势的直线（或者在更高维度是一个平面）。

**Example:**

**举个例子：**

Suppose we want to predict house prices in a city. We collect some data, such as the house area (input feature $x$) and corresponding prices (output $y$).

假设我们想预测一个城市中房屋的价格。我们收集了一些数据，比如房屋的面积（输入特征 $x$）和对应的价格（输出 $y$）。

If there's only one input feature, the linear regression model can be expressed as:

如果只有一个输入特征，线性回归模型可以表示为：

$$ \text{Price} = \text{Weight} \times \text{Area} + \text{Bias} $$

$$ \text{价格} = \text{权重} \times \text{面积} + \text{偏置} $$

Using mathematical notation:

用数学符号表示就是：

$$ y = wx + b $$

*   $y$: The output we want to predict (e.g., house price).
*   $x$: Input feature (e.g., house area).
*   $w$: Weight, which determines the magnitude and direction of feature $x$'s influence on output $y$, can be understood as "price per square meter".
*   $b$: Bias, a constant that can be understood as "base price" or the baseline value when all input features are 0. It allows the line to move up and down on the Y-axis.

*   $y$：我们想要预测的输出（比如房屋价格）。
*   $x$：输入特征（比如房屋面积）。
*   $w$：权重（weight），它决定了特征 $x$ 对输出 $y$ 影响的大小和方向，可以理解为"每平米的价格"。
*   $b$：偏置（bias），它是一个常数，可以理解为"基础价格"或者当所有输入特征都为0时的基准值。它让这条直线可以在Y轴上上下移动。

If there are multiple input features (e.g., house area $x_1$, location $x_2$, number of rooms $x_3$), the model becomes:

如果有多个输入特征（比如房屋面积 $x_1$、地段 $x_2$、房间数量 $x_3$），模型就会变成：

$$ y = w_1x_1 + w_2x_2 + w_3x_3 + b $$

### 1.3 Loss Function: Mean Squared Error (MSE)

损失函数：均方误差 (Mean Squared Error, MSE)

There's definitely a gap between the prices predicted by the model and the actual prices. How do we measure how large this gap is? This is where we need a **loss function**.

模型预测出来的价格和真实的价格肯定有差距。我们怎么衡量这个差距有多大呢？这就需要**损失函数**。

For regression problems, one of the most commonly used loss functions is **Mean Squared Error (MSE)**. Its calculation method is: square the difference between each predicted value and true value, add them up, then take the average.

对于回归问题，最常用的损失函数之一是**均方误差 (MSE)**。它的计算方法是：把每个预测值和真实值之间的差的平方加起来，然后取平均。

$$ MSE = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2 $$

*   $y_i$: The true price of the $i$-th sample.
*   $\hat{y}_i$: The model's predicted price for the $i$-th sample.
*   $N$: Total number of samples.

*   $y_i$：第 $i$ 个样本的真实价格。
*   $\hat{y}_i$：第 $i$ 个样本的模型预测价格。
*   $N$：样本的总数量。

**Why square?**

**为什么要平方？**

*   **Penalize large errors:** Squaring makes large errors receive greater penalties, making the model focus more on samples with larger prediction deviations.
*   **Ensure positive values:** Whether the predicted value is larger or smaller than the true value, squaring makes it positive, facilitating summation.

**惩罚大误差：** 平方会使得大的误差受到更大的惩罚，让模型更关注那些预测偏离较大的样本。
**保证正数：** 无论预测值比真实值大还是小，平方后都变成正数，方便累加。

Our goal is to find appropriate $w$ and $b$ that minimize this MSE loss function value, which means the model's predictions are as close to the true values as possible.

我们的目标就是找到合适的 $w$ 和 $b$，让这个MSE损失函数的值尽可能小，这意味着模型的预测越接近真实值。

### 1.4 Learning Process: How to Adjust $w$ and $b$

学习过程：如何调整 $w$ 和 $b$

Since our goal is to minimize MSE, the model needs to learn how to adjust $w$ and $b$. In subsequent chapters, we'll introduce **gradient descent** in detail - this powerful algorithm acts like a pathfinder, telling us in which direction to adjust $w$ and $b$ to make the loss function decrease fastest.

既然我们的目标是让MSE最小，那么模型就需要学习如何调整 $w$ 和 $b$。在后续的章节中，我们会详细介绍**梯度下降**这种强大的算法，它就像一个寻路者，告诉我们沿着哪个方向调整 $w$ 和 $b$ 才能让损失函数下降最快。

For now, you just need to know that the model will gradually find the most suitable $w$ and $b$ through continuous "trial and error" and "correction", so that the line (or plane) can best fit the data and accurately predict house prices.

现在你只需要知道，模型会通过不断地"试错"和"修正"，逐渐找到最合适的 $w$ 和 $b$，使得那条（或那个）直线（或平面）能够最好地拟合数据，从而准确地预测房屋价格。 

## 2. Linear Neural Networks for Classification

线性神经网络用于分类

### 2.1 What is a Classification Problem?

什么是分类问题？

Unlike regression which predicts continuous numerical values, **classification problems** aim to predict discrete category labels. For example, determining whether an image is a "cat" or "dog"; determining whether an email is "spam" or "not spam"; or determining whether a patient has a certain disease.

与回归预测连续数值不同，**分类问题**的目标是预测一个离散的类别标签。比如，判断一张图片是"猫"还是"狗"；判断一封邮件是"垃圾邮件"还是"非垃圾邮件"；或者判断一个病人是否患有某种疾病。

### 2.2 Binary Classification and Decision Boundary

二分类问题与决策边界

The simplest classification problem is **binary classification**, which has only two categories. We can use linear models to attempt solving binary classification problems.

最简单的分类问题是**二分类**，即只有两个类别。我们可以用线性模型来尝试解决二分类问题。

**Example:**

**举个例子：**

Suppose we want to determine whether a fruit is an "apple" or "orange" based on its sweetness (feature $x_1$) and size (feature $x_2$).

假设我们要根据水果的甜度（特征 $x_1$）和大小（特征 $x_2$）来判断它是"苹果"还是"橘子"。

In linear classification, the model tries to find a straight line (if there are only two features, it's a line; if there are more features, it's a hyperplane) that separates samples of different categories. This separating line is called the **decision boundary**.

在线性分类中，模型会尝试找到一条直线（如果只有两个特征，就是一条线；如果有更多特征，就是一个超平面），将不同类别的样本分隔开。这条分隔线就被称为**决策边界**。

On one side of the decision boundary are "apples", and on the other side are "oranges". The model determines classification by calculating which side a sample is on.

在决策边界的一侧是"苹果"，另一侧是"橘子"。模型就是通过计算样本在哪一侧来决定它的分类。

### 2.3 Activation Function: Sigmoid

激活函数：Sigmoid

The output of a linear model $y = w_1x_1 + w_2x_2 + ... + b$ can be any real number. However, in classification problems, we usually want to get a probability value between 0 and 1, representing the likelihood of a certain category being selected. This is when we need to introduce a special function - the **activation function**.

线性模型的输出 $y = w_1x_1 + w_2x_2 + ... + b$ 可以是任意的实数。但是，在分类问题中，我们通常希望得到一个介于0到1之间的概率值，表示某个类别被选中的可能性。这时，我们就需要引入一个特殊的函数——**激活函数**。

In binary classification problems, the commonly used activation function is the **Sigmoid function** (also called the S-curve function).

在二分类问题中，常用的激活函数是 **Sigmoid 函数**（也叫 S 型曲线函数）。

$$ \sigma(z) = \frac{1}{1 + e^{-z}} $$

*   $z$: The output of the linear model ($w_1x_1 + w_2x_2 + ... + b$).
*   $\sigma(z)$: The output of the Sigmoid function, with a range in (0, 1).

*   $z$：是线性模型的输出 ($w_1x_1 + w_2x_2 + ... + b$)。
*   $\sigma(z)$：Sigmoid 函数的输出，它的值域在 (0, 1) 之间。

**Characteristics of the Sigmoid Function:**

**Sigmoid 函数的特点：**

*   It can map any real number to the (0, 1) interval, making it very suitable for representing probabilities.
*   When $z$ is very large, $\sigma(z)$ approaches 1; when $z$ is very small, $\sigma(z)$ approaches 0.
*   When $z=0$, $\sigma(0) = 0.5$.

*   它能把任意实数映射到 (0, 1) 区间，非常适合表示概率。
*   当 $z$ 很大时，$\sigma(z)$ 接近 1；当 $z$ 很小时，$\sigma(z)$ 接近 0。
*   当 $z=0$ 时，$\sigma(0) = 0.5$。

**Example:**

**举例：**

If the model outputs $z=2.5$, after passing through the Sigmoid function, the result will be close to 0.92, indicating that the probability of this sample belonging to a certain category is 92%.

如果模型输出 $z=2.5$，经过Sigmoid函数后，结果会接近 0.92，表示这个样本属于某个类别的概率是92%。

### 2.4 Loss Function: Binary Cross-Entropy

损失函数：二元交叉熵 (Binary Cross-Entropy)

For classification problems, especially when outputs are probability values, Mean Squared Error (MSE) is not a good loss function. We more commonly use **cross-entropy loss**.

对于分类问题，尤其是在输出是概率值时，均方误差 (MSE) 并不是一个好的损失函数。我们更常用的是**交叉熵损失**。

In binary classification problems, we use **Binary Cross-Entropy (BCE)**.

在二分类问题中，我们使用**二元交叉熵 (Binary Cross-Entropy, BCE)**。

Assume the true label $y$ is 0 or 1 (for example, 0 represents "not spam", 1 represents "spam"), and the model's predicted probability is $\hat{y}$. Then the binary cross-entropy calculation formula is:

假设真实标签 $y$ 是 0 或 1（例如，0代表"非垃圾邮件"，1代表"垃圾邮件"），模型预测的概率是 $\hat{y}$。那么二元交叉熵的计算公式是：

$$ BCE = - [y \log(\hat{y}) + (1 - y) \log(1 - \hat{y})] $$

**Why use cross-entropy?**

**为什么要用交叉熵？**

*   **Penalize low-accuracy predictions:** If the true label is 1, but the model's predicted probability $\hat{y}$ is very low (close to 0), then the $-y \log(\hat{y})$ term becomes very large, giving the model a big penalty.
*   **Encourage high confidence:** If the model gives a high probability for the correct category, the loss will be small.

**惩罚准确度低的预测：** 如果真实标签是1，但模型预测的概率 $\hat{y}$ 很低（接近0），那么 $-y \log(\hat{y})$ 这一项就会变得非常大，给模型很大的惩罚。
**鼓励高置信度：** 如果模型对正确的类别给出了很高的概率，损失就会很小。

Our goal is still to adjust the model's weights and biases to minimize this binary cross-entropy loss.

我们的目标仍然是调整模型的权重和偏置，使得这个二元交叉熵损失最小。 

### 2.5 Multi-class Classification Problems

多分类问题

In the real world, classification problems often involve more than just binary classification. For example, if we want to recognize handwritten digits, there are 10 categories (0 to 9); recognizing objects in images might have hundreds or even thousands of categories (cats, dogs, cars, planes, etc.). These problems with more than two categories are called **multi-class classification problems**.

在现实世界中，分类问题往往不仅仅是二分类。比如，我们要识别手写数字，那就有10个类别（0到9）；识别图像中的物体，可能有上百甚至上千个类别（猫、狗、汽车、飞机等）。这类包含两个以上类别的问题，就称为**多分类问题**。

Linear models can also be extended to multi-class problems. The most common method is to train an independent linear classifier for each category.

线性模型同样可以扩展到多分类问题。最常见的方法是为每个类别训练一个独立的线性分类器。

**Working Principle:**

**工作原理：**

Imagine if we want to recognize cat, dog, and bird images, the model will output a "score" for each animal:

想象一下，如果我们要识别猫、狗、鸟三种动物的图片，模型会为每种动物输出一个"得分"：

$$ \text{Score}_{cat} = w_{cat,1}x_1 + w_{cat,2}x_2 + ... + b_{cat} $$
$$ \text{Score}_{dog} = w_{dog,1}x_1 + w_{dog,2}x_2 + ... + b_{dog} $$
$$ \text{Score}_{bird} = w_{bird,1}x_1 + w_{bird,2}x_2 + ... + b_{bird} $$

$$ \text{得分}_{猫} = w_{猫,1}x_1 + w_{猫,2}x_2 + ... + b_{猫} $$
$$ \text{得分}_{狗} = w_{狗,1}x_1 + w_{狗,2}x_2 + ... + b_{狗} $$
$$ \text{得分}_{鸟} = w_{鸟,1}x_1 + w_{鸟,2}x_2 + ... + b_{鸟} $$

Each score represents the "tendency" or "raw score" of the input image belonging to that category. These raw scores can be any real numbers.

每个得分表示输入图片属于该类别的"倾向性"或"原始分数"。这些原始分数可以是任意实数。

### 2.6 Activation Function: Softmax

激活函数：Softmax

In multi-class problems, we want to convert these raw scores into a **probability distribution**, where each category's probability is between 0 and 1, and the sum of all category probabilities equals 1. This is when we use a very important activation function - the **Softmax function**.

在多分类问题中，我们希望将这些原始分数转换为**概率分布**，即每个类别的概率都在0到1之间，并且所有类别的概率之和为1。这时，我们就会用到一个非常重要的激活函数——**Softmax 函数**。

The Softmax function formula is as follows:

Softmax 函数的公式如下：

$$ P(y=k | \mathbf{x}) = \frac{e^{z_k}}{\sum_{j=1}^{K} e^{z_j}} $$

*   $z_k$: Represents the raw score of the $k$-th category (output of the linear model).
*   $K$: Represents the total number of categories.
*   $e$: Base of natural logarithm (approximately 2.718).
*   $P(y=k | \mathbf{x})$: Represents the probability of belonging to the $k$-th category given input $\mathbf{x}$.

*   $z_k$：表示第 $k$ 个类别的原始得分（线性模型的输出）。
*   $K$：表示类别的总数量。
*   $e$：自然对数的底数（约等于 2.718）。
*   $P(y=k | \mathbf{x})$：表示在给定输入 $\mathbf{x}$ 的情况下，属于第 $k$ 个类别的概率。

**Characteristics of the Softmax Function:**

**Softmax 函数的特点：**

*   Converts a set of arbitrary real numbers (raw scores) into a probability distribution.
*   Output values are in the (0, 1) range.
*   Sum of all output probabilities equals 1.
*   Through exponential operations, larger scores correspond to larger probabilities, and smaller scores correspond to smaller probabilities, thus highlighting the model's "confidence" in a certain category.

*   将一组任意实数（原始得分）转换为一个概率分布。
*   输出值在 (0, 1) 之间。
*   所有输出概率之和为 1。
*   通过指数运算，使得大的得分对应的概率更大，小的得分对应的概率更小，从而突出模型对某个类别的"信心"。

**Example:**

**举例：**

Suppose the model's raw scores for recognizing cat, dog, and bird in an image are:
*   Cat: $z_{cat} = 2.0$
*   Dog: $z_{dog} = 1.0$
*   Bird: $z_{bird} = -0.5$

假设模型对一张图片识别猫、狗、鸟的原始得分分别是：
*   猫：$z_{猫} = 2.0$
*   狗：$z_{狗} = 1.0$
*   鸟：$z_{鸟} = -0.5$

After applying the Softmax function, we get:
*   $e^{2.0} \approx 7.389$
*   $e^{1.0} \approx 2.718$
*   $e^{-0.5} \approx 0.607$

应用Softmax函数后，我们会得到：
*   $e^{2.0} \approx 7.389$
*   $e^{1.0} \approx 2.718$
*   $e^{-0.5} \approx 0.607$

The sum is $7.389 + 2.718 + 0.607 \approx 10.714$

总和为 $7.389 + 2.718 + 0.607 \approx 10.714$

*   $P(cat) = 7.389 / 10.714 \approx 0.69$
*   $P(dog) = 2.718 / 10.714 \approx 0.25$
*   $P(bird) = 0.607 / 10.714 \approx 0.06$

*   $P(猫) = 7.389 / 10.714 \approx 0.69$
*   $P(狗) = 2.718 / 10.714 \approx 0.25$
*   $P(鸟) = 0.607 / 10.714 \approx 0.06$

这样我们就得到了这张图片是猫、狗、鸟的概率，分别是69%、25%和6%。模型会选择概率最大的类别作为最终的预测结果（在这个例子中是"猫"）。

### 2.7 Loss Function: Categorical Cross-Entropy

损失函数：交叉熵 (Categorical Cross-Entropy)

Similar to binary classification problems using binary cross-entropy, multi-class problems typically use **Categorical Cross-Entropy** to measure the gap between the model's predicted probability distribution and the true labels.

与二分类问题使用二元交叉熵类似，多分类问题通常使用**交叉熵损失 (Categorical Cross-Entropy)** 来衡量模型预测的概率分布与真实标签之间的差距。

The categorical cross-entropy calculation formula is as follows:

交叉熵损失的计算公式如下：

$$ CrossEntropy = - \sum_{k=1}^{K} y_k \log(\hat{y}_k) $$

*   $y_k$: One-hot encoding of the true label (if the image is a cat, then the $y_k$ corresponding to cat is 1, other categories are 0).
*   $\hat{y}_k$: The model's predicted probability for the $k$-th category.
*   $K$: Total number of categories.

*   $y_k$: 真实标签的 one-hot 编码（如果图片是猫，那么猫对应的 $y_k$ 是 1，其他类别是 0）。
*   $\hat{y}_k$: 模型预测的第 $k$ 个类别的概率。
*   $K$: 类别的总数量。

**Why use cross-entropy?**

**为什么要用交叉熵？**

*   It can effectively penalize errors where the model gives too low probability predictions for the correct category, encouraging the model to give high-confidence predictions for the correct category.
*   When the true label is a certain category, only the $\hat{y}_k$ corresponding to that category contributes to the loss function. If $\hat{y}_k$ is closer to 1, the loss is smaller; closer to 0, the loss is larger.

*   它能有效地惩罚那些模型对正确类别预测概率过低的错误，鼓励模型对正确的类别给出高置信度的预测。
*   当真实标签是某个类别时，只有该类别对应的 $\hat{y}_k$ 会对损失函数产生贡献，如果 $\hat{y}_k$ 越接近1，损失越小；越接近0，损失越大。

Our goal is to adjust the model's weights and biases to minimize this cross-entropy loss.

我们的目标就是调整模型的权重和偏置，使得这个交叉熵损失最小。 

## 3. What is a Neuron?

什么是神经元？

### 3.1 Analogy: Human Brain Neurons

类比：人脑神经元

Imagine the neurons in our human brain - they are the basic units that make up the brain, responsible for receiving, processing, and transmitting information. When a neuron receives enough "stimulation", it becomes "activated" and then transmits signals to the next neuron. The "neurons" in deep learning are a mathematical abstraction and simulation of these biological neurons.

想象一下我们人脑中的神经元，它们是构成大脑的基本单位，负责接收、处理和传递信息。当一个神经元接收到足够的"刺激"时，它就会被"激活"，然后将信号传递给下一个神经元。深度学习中的"神经元"就是对这种生物神经元的一种数学抽象和模拟。

**In simple terms:**

**用人话说：**

You can think of a neuron as a **small decision-making unit**. It receives some information (input), does some "thinking" about this information (processing), and then gives a "decision" based on the thinking results (output).

你可以把一个神经元想象成一个**小小的决策单元**。它接收一些信息（输入），对这些信息进行一番"思考"（处理），然后根据思考的结果给出一个"决定"（输出）。

### 3.2 Mathematical Model of Neurons

神经元的数学模型

An artificial neuron (also commonly called a "perceptron") typically contains the following core components:

一个人工神经元（也常称为"感知机"）通常包含以下几个核心部分：

1.  **Inputs:** The data or information that the neuron receives. These inputs are usually from other neurons or raw data (like pixel values of an image, features of a word). We use $x_1, x_2, ..., x_n$ to represent these inputs.

1.  **输入 (Inputs):** 神经元接收的数据或信息。这些输入通常是来自其他神经元或原始数据（比如一张图片的像素值、一个词的特征）。我们用 $x_1, x_2, ..., x_n$ 来表示这些输入。

2.  **Weights:** Each input has a corresponding weight $w_1, w_2, ..., w_n$. Weights determine the importance of each input or the degree of influence on the neuron's output. Imagine when you listen to friends' opinions on something, you trust some friends more, so you assign higher "weights" to their words.

2.  **权重 (Weights):** 每个输入都有一个对应的权重 $w_1, w_2, ..., w_n$。权重决定了每个输入的重要性或者对神经元输出的影响程度。想象一下，你听取朋友们对一件事的意见，有些朋友的话你更信任，所以你给他们的话分配了更高的"权重"。

3.  **Bias:** Bias $b$ is an additional constant term. It allows the neuron to have a basic "activity level" even when receiving no input, or it controls the "threshold" for neuron activation.

3.  **偏置 (Bias):** 偏置 $b$ 是一个额外的常数项。它允许神经元在没有接收任何输入时也能有一个基础的"活跃度"，或者说它控制了神经元被激活的"门槛"。

4.  **Weighted Sum:** The neuron first multiplies all inputs by their respective weights, then adds these products together, and finally adds the bias. This is like calculating a "total score":

4.  **加权和 (Weighted Sum):** 神经元首先会将所有输入乘以它们各自的权重，然后把这些乘积加起来，最后再加上偏置。这就像在计算一个"总分"：

    $$ z = w_1x_1 + w_2x_2 + ... + w_nx_n + b $$

    也可以用更简洁的向量点积形式表示：

    也可以用更简洁的向量点积形式表示：

    $$ z = \mathbf{w} \cdot \mathbf{x} + b $$

    where $\mathbf{w}$ is the weight vector and $\mathbf{x}$ is the input vector.

    其中 $\mathbf{w}$ 是权重向量，$\mathbf{x}$ 是输入向量。

5.  **Activation Function:** This is the "soul" part of the neuron. After calculating the weighted sum $z$, the neuron passes $z$ to a non-linear function, which is the activation function. It determines whether the neuron is ultimately "activated" and to what degree. Its role is to convert this "total score" into the neuron's final output.

5.  **激活函数 (Activation Function):** 这是神经元的"灵魂"部分。在计算出加权和 $z$ 之后，神经元会将 $z$ 传递给一个非线性的函数，这个函数就是激活函数。它决定了神经元最终是否被"激活"以及激活的程度。它的作用是将这个"总分"转换成神经元的最终输出。

    $$ \text{Output} = f(z) = f(\mathbf{w} \cdot \mathbf{x} + b) $$

    where $f$ is the activation function.

    其中 $f$ 就是激活函数。

**Example: Should I bring an umbrella when it rains?**

**举个例子：下雨，我要不要带伞？**

We use a neuron to simulate this decision-making process:

我们用一个神经元来模拟这个决策过程：

*   **Input $x_1$:** "Did the weather forecast say it will rain?" (Yes=1, No=0)
*   **Input $x_2$:** "Will I be out for a long time today?" (Long=1, Short=0)

*   **输入 $x_1$：** "天气预报说下雨了吗？" (是=1，否=0)
*   **输入 $x_2$：** "我今天出门时间长吗？" (长=1，短=0)

*   **Weight $w_1$:** Assume "weather forecast" is very important to you, weight set to 5.
*   **Weight $w_2$:** Assume "time out" is relatively less important, weight set to 2.
*   **Bias $b$:** Assume you are a person who really hates trouble, and unless absolutely necessary, you don't want to bring an umbrella, bias set to -3 (negative bias means higher weighted input sum is needed for activation).

*   **权重 $w_1$：** 假设"天气预报"对你来说非常重要，权重设为 5。
*   **权重 $w_2$：** 假设"出门时间"相对不那么重要，权重设为 2。
*   **偏置 $b$：** 假设你是一个非常怕麻烦的人，除非特别有必要，否则不想带伞，偏置设为 -3（负的偏置意味着需要更高的输入加权和才能激活）。

*   **Activation function $f$:** We use a simple step function: if output is greater than 0, bring umbrella (1); otherwise don't bring umbrella (0).

*   **激活函数 $f$：** 我们用一个简单的阶跃函数：如果输出大于0，就带伞（1）；否则就不带伞（0）。

**Case 1: Weather forecast says rain (x1=1), long time out (x2=1)**

**情况一：天气预报说下雨 (x1=1)，出门时间长 (x2=1)**

1.  **Weighted sum $z$:** $z = (5 \times 1) + (2 \times 1) + (-3) = 5 + 2 - 3 = 4$
2.  **Activation:** $f(4) = 1$ (greater than 0, bring umbrella)

*   **加权和 $z$：** $z = (5 \times 1) + (2 \times 1) + (-3) = 5 + 2 - 3 = 4$
*   **激活：** $f(4) = 1$ (大于0，带伞)

**Case 2: Weather forecast says no rain (x1=0), short time out (x2=0)**

**情况二：天气预报说不下雨 (x1=0)，出门时间短 (x2=0)**

1.  **Weighted sum $z$:** $z = (5 \times 0) + (2 \times 0) + (-3) = 0 + 0 - 3 = -3$
2.  **Activation:** $f(-3) = 0$ (less than 0, don't bring umbrella)

*   **加权和 $z$：** $z = (5 \times 0) + (2 \times 0) + (-3) = 0 + 0 - 3 = -3$
*   **激活：** $f(-3) = 0$ (小于0，不带伞)

By adjusting weights and bias, this neuron can simulate different decision logics. This is the most basic working method of neurons.

通过调整权重和偏置，这个神经元就能模拟不同的决策逻辑。这就是神经元最基础的工作方式。 

## 4. Enter the Perceptron: The Simplest "Intelligent" Model

感知机登场：最简单的"智能"模型

The **Perceptron** is an early model of artificial neural networks, proposed by Frank Rosenblatt in 1957. It is essentially a **single-layer artificial neuron** capable of binary classification (dividing data into two categories).

**感知机 (Perceptron)** 是人工神经网络的早期模型，由Frank Rosenblatt在1957年提出。它本质上就是一个**单层的人工神经元**，能够进行二分类（将数据分成两类）。

### 4.1 How the Perceptron Works

感知机的工作原理

The perceptron works exactly the same as the neuron we introduced earlier:

感知机的工作方式与我们前面介绍的神经元完全一致：

1.  **Receive inputs:** Accept multiple input features $x_1, x_2, ..., x_n$.
2.  **Weighted sum and bias:** Perform weighted sum of inputs and add bias $z = \mathbf{w} \cdot \mathbf{x} + b$.
3.  **Activation function:** Pass the weighted sum $z$ through a **step function** (or sign function) to determine the final output. For the perceptron, this step function is typically:
    *   If $z > 0$, output 1
    *   If $z \le 0$, output 0 (or -1)

1.  **接收输入：** 接收多个输入特征 $x_1, x_2, ..., x_n$。
2.  **加权求和与偏置：** 对输入进行加权求和并加上偏置 $z = \mathbf{w} \cdot \mathbf{x} + b$。
3.  **激活函数：** 将加权和 $z$ 通过一个**阶跃函数**（或者符号函数）来决定最终的输出。对于感知机，这个阶跃函数通常是：
    *   如果 $z > 0$，输出 1
    *   如果 $z \le 0$，输出 0 (或者 -1)

**Analogy: Drawing a line to separate things**

**类比：画条线把东西分开**

Imagine you have a bunch of red and blue balls mixed together. What the perceptron does is find a straight line (in 2D space) or a plane (in 3D space) that can perfectly separate the red balls from the blue balls.

想象你有一堆红色和蓝色的球混在一起。感知机要做的就是找到一条直线（在二维空间）或者一个平面（在三维空间），能把红球和蓝球完美地分开。

*   **If the ball is on this side of the line, it's a red ball.**
*   **If the ball is on that side of the line, it's a blue ball.**

*   **如果球在线的这一边，它就是红球。**
*   **如果球在线的那一边，它就是蓝球。**

This line is the perceptron's **decision boundary**. The perceptron can only find straight lines (or planes) to separate data, making it a **linear classifier**.

这条线就是感知机的**决策边界**。感知机只能找到直线（或平面）来分隔数据，因此它是一个**线性分类器**。

### 4.2 Limitations of the Perceptron: The XOR Problem

感知机的局限性：异或问题

Although the perceptron is simple, it has a fatal weakness: **it can only solve linearly separable problems.**

感知机虽然简单，但它有一个致命的弱点：**它只能解决线性可分的问题。**

What does linearly separable mean? It means different categories of data can be completely separated by a straight line (or a plane). Like the red and blue balls mentioned above, if they are distributed on two sides of a straight line, the perceptron can handle it.

什么叫线性可分？就是可以用一条直线（或一个平面）把不同类别的数据完全分开。比如上面说的红球和蓝球，如果它们是分布在直线的两边，那感知机就能搞定。

However, some problems are **non-linearly separable**. The most famous example is the **"XOR" (Exclusive OR) problem**. The XOR logic is as follows:

但是，有些问题是**非线性可分**的。最有名的例子就是**"异或" (XOR) 问题**。异或逻辑是这样的：

*   Input (0, 0) -> Output 0
*   Input (0, 1) -> Output 1
*   Input (1, 0) -> Output 1
*   Input (1, 1) -> Output 0

*   输入 (0, 0) -> 输出 0
*   输入 (0, 1) -> 输出 1
*   输入 (1, 0) -> 输出 1
*   输入 (1, 1) -> 输出 0

If you plot these four points on a coordinate system, you'll find that no matter how you draw a straight line, you cannot completely separate the points belonging to category 1 ((0,1) and (1,0)) from the points belonging to category 0 ((0,0) and (1,1)).

如果你在坐标轴上画出这四个点，你会发现，无论你怎么画一条直线，都无法把属于类别1的点（(0,1) 和 (1,0)）和属于类别0的点（(0,0) 和 (1,1)）完全分开。

**This means:** A single perceptron cannot solve the XOR problem. This limitation once caused people to lose interest in neural networks. But this also led to the necessity of more complex neural networks in subsequent chapters - **multilayer perceptrons** - because multilayer networks can solve such problems through non-linear combinations.

**这意味着：** 单个感知机无法解决异或问题。这个局限性在当时一度让人们对神经网络失去了兴趣。但这也引出了后续章节中更复杂的神经网络——**多层感知机**的必要性，因为多层网络可以通过非线性的组合来解决这类问题。 

## 5. Activation Functions: The "Excitement" Switch of Neurons

激活函数：神经元的"兴奋度"开关

As mentioned earlier, after a neuron calculates the weighted sum $z$, it produces the final output through an **activation function**. The activation function is like the neuron's "excitement" switch, determining the degree to which the neuron is activated and introducing non-linearity.

前面我们提到，神经元在计算出加权和 $z$ 之后，会通过一个**激活函数 (Activation Function)** 来产生最终的输出。激活函数就像神经元的"兴奋度"开关，它决定了神经元被激活的程度，并引入了非线性。

### 5.1 Why Do We Need Activation Functions?

**核心目的：引入非线性！**

Without activation functions (or using only linear activation functions, like $f(x)=x$), no matter how many layers of neural networks you stack, the entire network will ultimately be just a **linear model**. This means it can still only solve linearly separable problems, just like the perceptron mentioned earlier, unable to handle complex non-linear relationships.

如果没有激活函数（或者只使用线性激活函数，比如 $f(x)=x$），无论你叠加多少层神经网络，整个网络最终都只会是一个**线性模型**。这意味着它仍然只能解决线性可分的问题，就像前面提到的感知机一样，无法处理复杂的非线性关系。

**Example:**

**举个例子：**

If you combine two linear functions $f_1(x) = ax+b$ and $f_2(y) = cy+d$, like $f_2(f_1(x)) = c(ax+b)+d = acx + cb + d$, the result is still a linear function. Deep learning is powerful because it can learn and represent **non-linear** complex patterns, and this is exactly the contribution of activation functions.

如果你把两个线性函数 $f_1(x) = ax+b$ 和 $f_2(y) = cy+d$ 组合起来，比如 $f_2(f_1(x)) = c(ax+b)+d = acx + cb + d$，结果仍然是一个线性函数。深度学习之所以强大，就是因为它能够学习并表示**非线性**的复杂模式，而这正是激活函数带来的贡献。

### 5.2 Common Activation Functions

**常见的激活函数**

#### 5.2.1 Sigmoid Function (S-curve)

**Sigmoid 函数 (S 型曲线)**

We already introduced the Sigmoid function in the "Linear Neural Networks for Classification" section earlier. It was a very popular activation function in early neural networks.

前面在"线性神经网络用于分类"中已经介绍了Sigmoid函数，它是早期神经网络中非常流行的一种激活函数。

$$ \sigma(z) = \frac{1}{1 + e^{-z}} $$

**图像大致形状描述：**

S-shaped curve with output values between (0, 1). When input values are very large or very small, the function's gradient (slope) becomes very small, approaching 0. This is the famous **vanishing gradient problem**.

S 型曲线，输出值在 (0, 1) 之间。在输入值非常大或非常小时，函数的梯度（斜率）会变得非常小，接近于0。这就是著名的**梯度消失问题**。

**Advantages:**

*   Compresses output to (0, 1) range, can be interpreted as probability.
*   Smooth and differentiable, convenient for gradient calculation.

*   将输出压缩到 (0, 1) 之间，可以解释为概率。
*   平滑、可导，方便进行梯度计算。

**Disadvantages:**

*   **Vanishing gradient:** When inputs are very large or very small, gradients approach 0, making it difficult for the network to update weights near the input layer during backpropagation, slowing down or even stopping learning. This makes deep networks difficult to train.
*   **Output not centered around 0:** This may cause "zigzag" phenomena during gradient updates, affecting training efficiency.

*   **梯度消失：** 当输入非常大或非常小时，梯度接近0，导致网络在反向传播时很难更新靠近输入层的权重，学习速度变慢甚至停滞。这使得深层网络难以训练。
*   **输出不以0为中心：** 这可能会导致梯度更新时的"锯齿"现象，影响训练效率。

#### 5.2.2 ReLU Function (Rectified Linear Unit)

**ReLU 函数 (修正线性单元 - Rectified Linear Unit)**

The ReLU function is one of the most commonly used activation functions in deep learning today, especially in convolutional neural networks.

ReLU 函数是目前在深度学习中最常用的激活函数之一，尤其是在卷积神经网络中。

$$ ReLU(z) = \max(0, z) $$

**图像大致形状描述：**

When input $z > 0$, output is $z$; when $z \le 0$, output is 0. It's like a "truncated" linear function.

当输入 $z > 0$ 时，输出就是 $z$；当 $z \le 0$ 时，输出是 0。它就像一个"截断"的线性函数。

```
^ Output
|
|      /
|     /
|    /
|   /
|  /
| /
+-----------> Input
0
```

**Advantages:**

*   **Solves vanishing gradient problem:** When $z > 0$, gradient is constantly 1, effectively avoiding vanishing gradients. This allows networks to be trained deeper.
*   **High computational efficiency:** Only requires simple threshold judgment, much faster computation than Sigmoid and Tanh.
*   **Sparse activation:** When $z \le 0$, output is 0, meaning some neurons are in a non-activated state, helping create sparse networks and improving computational efficiency and feature selection capability.

*   **解决了梯度消失问题：** 当 $z > 0$ 时，梯度恒为 1，有效避免了梯度消失。这使得网络可以训练得更深。
*   **计算效率高：** 只需要进行简单的阈值判断，计算速度比Sigmoid和Tanh快很多。
*   **稀疏激活：** 当 $z \le 0$ 时，输出为0，这意味着一部分神经元处于非激活状态，这有助于创建稀疏网络，提高计算效率和特征选择能力。

**Disadvantages:**

*   **"Dying ReLU" problem:** When input $z$ is always less than or equal to 0, that neuron will never be activated, gradient is always 0, causing the neuron and its connected weights to become unable to update, becoming "dead".
*   **Output not centered around 0:** Like Sigmoid, output is not centered around 0.

*   **"死亡ReLU"问题：** 当输入 $z$ 永远小于等于 0 时，该神经元就不会被激活，梯度永远为 0，导致该神经元及其连接的权重无法更新，变得"死亡"。
*   **输出不以0为中心：** 和Sigmoid一样，输出不以0为中心。

#### 5.2.3 Other Activation Functions (Brief Mention)

**其他激活函数（简单提及）**

*   **Tanh function (Hyperbolic tangent function):** Similar shape to Sigmoid, but output range is (-1, 1), output centered around 0, performs better than Sigmoid, but still has vanishing gradient problem.
*   **Leaky ReLU, PReLU, ELU, etc.:** Improvements to ReLU's "dying ReLU" problem, allowing a small non-zero gradient when input is less than 0.

*   **Tanh 函数 (双曲正切函数):** 形状类似Sigmoid，但输出范围在 (-1, 1) 之间，输出以0为中心，比Sigmoid表现更好，但仍有梯度消失问题。
*   **Leaky ReLU、PReLU、ELU 等：** 针对ReLU的"死亡ReLU"问题进行改进，当输入小于0时，允许一个很小的非零梯度。

### 5.3 Why Not Use Step Functions Directly?

为什么不直接使用阶跃函数？

Although we used step functions in the perceptron example, in actual deep learning, we rarely use them directly because:

虽然我们在感知机的例子中使用了阶跃函数，但在实际深度学习中，我们很少直接使用它，原因在于：

*   **Not differentiable:** Step functions are discontinuous at $z=0$ and have zero derivatives elsewhere. This means during backpropagation, we cannot calculate gradients and thus cannot update weights through gradient descent.

*   **不可导：** 阶跃函数在 $z=0$ 处不连续，在其他地方导数为0。这意味着在反向传播过程中，我们无法计算梯度，也就无法通过梯度下降来更新权重。

Therefore, smooth or piecewise smooth functions like Sigmoid and ReLU, because they are differentiable everywhere (or almost everywhere), are more suitable for training deep learning models.

所以，像Sigmoid和ReLU这样平滑或分段平滑的函数，因为它们处处可导（或几乎处处可导），才更适合用于深度学习模型的训练。 

## 6. Perceptron Training: How to Make It "Learn"

感知机的训练：如何让它"学习"

The core of a perceptron's ability to learn lies in its **training algorithm**. "Learning" here refers to adjusting the neuron's weights $w$ and bias $b$ through training, enabling it to correctly classify new inputs.

感知机能够学习的核心在于它的**训练算法**。这里的"学习"指的是通过调整神经元的权重 $w$ 和偏置 $b$，使其能够正确地对新的输入进行分类。

### 6.1 Loss Function: Measuring the Degree of "Error"

损失函数：衡量"犯错"的程度

During training, we need a metric to measure how "good" or "bad" the model's predictions are, i.e., the degree to which the model "makes errors". This metric is the **loss function**.

在训练过程中，我们需要一个指标来衡量模型预测的"好坏"，也就是模型"犯错"的程度。这个指标就是**损失函数 (Loss Function)**。

For the perceptron (binary classification problem), a simple loss concept can be: **if the prediction is wrong, there is a loss; if the prediction is correct, the loss is 0.** We hope to minimize the total loss through training.

对于感知机（二分类问题），一个简单的损失概念可以是：**如果预测错误，则产生损失；如果预测正确，则损失为0。** 我们希望通过训练让总的损失最小。

### 6.2 Weight Update Rule: Learn from Mistakes

权重更新规则：知错就改

When the perceptron makes an incorrect prediction on a sample, we adjust its weights and bias according to the size of the error. This adjustment rule is the key to the perceptron learning algorithm.

当感知机对一个样本做出错误预测时，我们会根据错误的大小来调整其权重和偏置。这个调整的规则是感知机学习算法的关键。

Assume we have a sample with true label $y_{true}$ and model predicted output $y_{pred}$.

假设我们有一个样本，真实标签是 $y_{true}$，模型预测的输出是 $y_{pred}$。

**If the prediction is wrong:**

**如果预测错误：**

*   **Weight update:** $w_{new} = w_{old} + \alpha \times (y_{true} - y_{pred}) \times x$
*   **Bias update:** $b_{new} = b_{old} + \alpha \times (y_{true} - y_{pred})$

*   **权重更新：** $w_{new} = w_{old} + \alpha \times (y_{true} - y_{pred}) \times x$
*   **偏置更新：** $b_{new} = b_{old} + \alpha \times (y_{true} - y_{pred})$

*   $\alpha$ (alpha): **Learning rate**, a small positive number (like 0.01 or 0.1). It determines the step size for adjusting weights and bias each time. If the learning rate is too small, training will be slow; if too large, it may cause model instability or even miss the optimal point.
*   $(y_{true} - y_{pred})$: This represents the prediction error.
    *   If $y_{true}=1$ but $y_{pred}=0$ (should be 1, but predicted as 0), error is $+1$, weights will increase, making it easier to predict 1 next time.
    *   If $y_{true}=0$ but $y_{pred}=1$ (should be 0, but predicted as 1), error is $-1$, weights will decrease, making it easier to predict 0 next time.
*   $x$: The feature value of the current input. This indicates that the larger the feature value, the larger the magnitude of the corresponding weight adjustment may be.

*   $\alpha$ (alpha)：**学习率 (Learning Rate)**，这是一个很小的正数（比如 0.01 或 0.1）。它决定了每次调整权重和偏置的步长。学习率太小，训练会很慢；学习率太大，可能会导致模型不稳定，甚至错过最佳点。
*   $(y_{true} - y_{pred})$：这表示了预测的误差。
    *   如果 $y_{true}=1$ 但 $y_{pred}=0$ (本应是1，却预测成0)，误差是 $+1$，权重会增加，使得下次更容易预测成1。
    *   如果 $y_{true}=0$ 但 $y_{pred}=1$ (本应是0，却预测成1)，误差是 $-1$，权重会减小，使得下次更容易预测成0。
*   $x$：当前输入的特征值。这表明特征值越大，其对应的权重调整的幅度也可能越大。

**In simple terms:**

**用人话说：**

Imagine you are a shooting athlete, with the target being the bullseye (true label).

想象你是一个射击运动员，目标是靶心（真实标签）。

*   **If you miss the target (prediction error):** Your coach (algorithm) will tell you how much you missed, and based on the direction and degree of your deviation (error), have you slightly adjust the gun barrel (weights and bias).
*   **Learning rate:** Is the magnitude of each gun barrel adjustment. Too large and you might overshoot, too small and it might take a long time to adjust.

*   **如果你射偏了（预测错误）：** 你的教练（算法）会告诉你偏了多少，并根据你偏离的方向和程度（误差），让你稍微调整一下枪口（权重和偏置）。
*   **学习率：** 就是你每次调整枪口的幅度。幅度大了可能一下就过头，幅度小了可能要调很久。

### 6.3 Iterative Learning: Practice Makes Perfect

迭代学习：熟能生巧

The perceptron improves performance through **iterative learning**. It will go through all samples in the training dataset repeatedly (one complete traversal is called an **"epoch"**), and each time it encounters a sample with incorrect prediction, it adjusts weights and bias according to the above rules.

感知机通过**迭代学习**的方式来提高性能。它会一遍又一遍地遍历训练数据集中的所有样本（这一个完整的遍历过程叫做一个**"周期" Epoch**），每次遇到预测错误的样本时，就按照上面的规则调整权重和偏置。

As training epochs increase, the perceptron will continuously learn from errors, gradually finding a set of weights and biases that can correctly classify most or even all training samples. This process is called **convergence**.

随着训练周期的增加，感知机会不断地从错误中学习，逐渐找到一套能够正确分类大多数甚至所有训练样本的权重和偏置。这个过程就叫做**收敛**。

### 6.4 Weight Decay

权重衰减 (Weight Decay)

Weight decay, also known as L2 regularization, is a commonly used regularization technique to prevent model overfitting. It adds a penalty term to the loss function, making the model's weights tend toward smaller values.

权重衰减（Weight Decay），也称为 L2 正则化，是一种常用的正则化技术，用于防止模型过拟合。它通过在损失函数中添加一个惩罚项，使得模型的权重趋向于较小的值。

Its representation in the loss function is:

其在损失函数中的体现为：

$$L_{total} = L_{original} + \frac{1}{2} \lambda \sum_{i} w_i^2$$

Where:
- $L_{total}$ is the total loss function including weight decay.
- $L_{original}$ is the original loss function (e.g., mean squared error or cross-entropy loss).
- $\lambda$ (lambda) is the weight decay coefficient, a hyperparameter used to control regularization strength. The larger $\lambda$, the greater the weight decay penalty, and the smaller the model weights.
- $\sum_{i} w_i^2$ is the sum of squares of all model weights (L2 norm).

其中：
- $L_{total}$ 是包含权重衰减的总损失函数。
- $L_{original}$ 是原始的损失函数（例如，均方误差或交叉熵损失）。
- $\lambda$ (lambda) 是权重衰减系数，一个超参数，用于控制正则化强度。$\lambda$ 越大，权重衰减的惩罚越大，模型权重会更小。
- $\sum_{i} w_i^2$ 是模型所有权重的平方和（L2 范数）。

#### 6.4.1 Gradient Derivation for Weight Decay

权重衰减的梯度推导

For the weight decay term, its gradient with respect to weight $w_i$ is:

对于权重衰减项，其对权重 $w_i$ 的梯度为：

$$\frac{\partial}{\partial w_i} \left(\frac{1}{2} \lambda \sum_{j} w_j^2\right) = \lambda w_i$$

Therefore, the total gradient including weight decay is:

因此，包含权重衰减的总梯度为：

$$\frac{\partial L_{total}}{\partial w_i} = \frac{\partial L_{original}}{\partial w_i} + \lambda w_i$$

The weight update rule becomes:

权重更新规则变为：

$$w_i^{(new)} = w_i^{(old)} - \eta \left(\frac{\partial L_{original}}{\partial w_i} + \lambda w_i^{(old)}\right)$$

这可以重写为：

$$w_i^{(new)} = w_i^{(old)}(1 - \eta \lambda) - \eta \frac{\partial L_{original}}{\partial w_i}$$

Where $(1 - \eta \lambda)$ is the weight decay factor.

其中 $(1 - \eta \lambda)$ 就是权重衰减因子。

#### 6.4.2 Numerical Calculation Example

数值计算例子

**假设场景：** 我们有一个简单的线性回归模型：$y = w_1x_1 + w_2x_2 + b$

**给定参数：**
- 当前权重：$w_1 = 2.5$, $w_2 = -1.8$, $b = 0.3$
- 学习率：$\eta = 0.01$
- 权重衰减系数：$\lambda = 0.1$
- 原始梯度（从数据计算得出）：$\frac{\partial L_{original}}{\partial w_1} = 0.5$, $\frac{\partial L_{original}}{\partial w_2} = -0.3$

**步骤1：计算权重衰减项的梯度**

$$\frac{\partial}{\partial w_1}\left(\frac{1}{2}\lambda(w_1^2 + w_2^2)\right) = \lambda w_1 = 0.1 \times 2.5 = 0.25$$

$$\frac{\partial}{\partial w_2}\left(\frac{1}{2}\lambda(w_1^2 + w_2^2)\right) = \lambda w_2 = 0.1 \times (-1.8) = -0.18$$

**步骤2：计算总梯度**

$$\frac{\partial L_{total}}{\partial w_1} = 0.5 + 0.25 = 0.75$$

$$\frac{\partial L_{total}}{\partial w_2} = -0.3 + (-0.18) = -0.48$$

**步骤3：更新权重**

$$w_1^{(new)} = w_1^{(old)} - \eta \times \frac{\partial L_{total}}{\partial w_1} = 2.5 - 0.01 \times 0.75 = 2.5 - 0.0075 = 2.4925$$

$$w_2^{(new)} = w_2^{(old)} - \eta \times \frac{\partial L_{total}}{\partial w_2} = -1.8 - 0.01 \times (-0.48) = -1.8 + 0.0048 = -1.7952$$

**验证使用衰减因子的计算方法：**

权重衰减因子：$1 - \eta \lambda = 1 - 0.01 \times 0.1 = 1 - 0.001 = 0.999$

$$w_1^{(new)} = w_1^{(old)} \times 0.999 - \eta \times \frac{\partial L_{original}}{\partial w_1}$$
$$= 2.5 \times 0.999 - 0.01 \times 0.5 = 2.4975 - 0.005 = 2.4925$$

验证结果：✓

$$w_2^{(new)} = w_2^{(old)} \times 0.999 - \eta \times \frac{\partial L_{original}}{\partial w_2}$$
$$= -1.8 \times 0.999 - 0.01 \times (-0.3) = -1.7982 + 0.003 = -1.7952$$

验证结果：✓

#### 6.4.3 Weight Decay Effect Analysis

权重衰减的效果分析

从上面的计算可以看到：

1. **权重缩小效应：** 无论原始梯度如何，权重都会乘以小于1的衰减因子 $(1 - \eta \lambda) = 0.999$，使权重整体趋向于减小。
2. **大权重惩罚更重：** 权重衰减的梯度 $\lambda w_i$ 与权重成正比，权重越大，受到的"推向零"的力量越强。在例子中：
   - $w_1 = 2.5$（较大）受到的衰减梯度为 $0.25$
   - $w_2 = -1.8$（绝对值较小）受到的衰减梯度为 $-0.18$（绝对值较小）
3. **防止过拟合：** 通过限制权重的大小，模型变得更加"简单"，减少了对训练数据的过度依赖。

长期效果模拟：

假设我们连续进行10次更新，只考虑权重衰减的影响（假设原始梯度为0）：

$$w_1^{(after\ 10\ steps)} = 2.5 \times (0.999)^{10} = 2.5 \times 0.99004 = 2.4751$$

可以看到，即使没有来自数据的梯度，权重也会逐渐衰减，这正是权重衰减防止过拟合的机制。

在梯度下降更新规则中，权重衰减等效于在每次迭代时，将权重乘以一个小于1的因子（即 $1 - \eta \lambda$），其中 $\eta$ 是学习率。这使得权重在每次更新后都会衰减一部分，从而限制了模型的复杂度。 